{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob, os\n",
    "import numpy as np\n",
    "from comet_ml import Experiment, Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_KERAS'] = '1'\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, Input, concatenate\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import callbacks, optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir_T1 = '/home/ana/eRisk/data/eRisk2020_T1_train/eRISK2020_T1_training_data/eRISK2020_training_data/data/'\n",
    "labels_file_T1 = '/home/ana/eRisk/data/eRisk2020_T1_train/eRISK2020_T1_training_data/eRISK2020_training_data/golden_truth.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_T1 = pd.read_csv(labels_file_T1, delimiter=' ', names=['subject', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ffa77ffa1d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_T1.label.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subject671</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9917</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject6238</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject8581</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject7238</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject2182</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9829</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject3270</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject6464</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject8721</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>340 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             label\n",
       "subject           \n",
       "subject671       1\n",
       "subject9917      1\n",
       "subject6238      1\n",
       "subject8581      1\n",
       "subject7238      1\n",
       "...            ...\n",
       "subject2182      0\n",
       "subject9829      0\n",
       "subject3270      0\n",
       "subject6464      0\n",
       "subject8721      0\n",
       "\n",
       "[340 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_T1 = labels_T1.set_index('subject')\n",
    "labels_T1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject8292.xml\n",
      "subject6644.xml\n",
      "subject7982.xml\n",
      "subject9260.xml\n",
      "subject9918.xml\n",
      "subject4284.xml\n",
      "subject9829.xml\n",
      "subject7661.xml\n",
      "subject8361.xml\n",
      "subject4831.xml\n",
      "subject2181.xml\n",
      "subject9077.xml\n",
      "subject2922.xml\n",
      "subject2238.xml\n",
      "subject4513.xml\n",
      "subject269.xml\n",
      "subject2678.xml\n",
      "subject9197.xml\n",
      "subject4143.xml\n",
      "subject2605.xml\n",
      "subject4226.xml\n",
      "subject7627.xml\n",
      "subject5150.xml\n",
      "subject4510.xml\n",
      "subject2182.xml\n",
      "subject280.xml\n",
      "subject1105.xml\n",
      "subject187.xml\n",
      "subject8001.xml\n",
      "subject9285.xml\n",
      "subject2621.xml\n",
      "subject4414.xml\n",
      "subject2685.xml\n",
      "subject9961.xml\n",
      "subject8065.xml\n",
      "subject8225.xml\n",
      "subject6866.xml\n",
      "subject9949.xml\n",
      "subject1507.xml\n",
      "subject8329.xml\n",
      "subject9411.xml\n",
      "subject7857.xml\n",
      "subject1545.xml\n",
      "subject9811.xml\n",
      "subject5000.xml\n",
      "subject4843.xml\n",
      "subject569.xml\n",
      "subject51.xml\n",
      "subject9156.xml\n",
      "subject6453.xml\n",
      "subject1210.xml\n",
      "subject5528.xml\n",
      "subject1485.xml\n",
      "subject5935.xml\n",
      "subject4527.xml\n",
      "subject3301.xml\n",
      "subject4074.xml\n",
      "subject6093.xml\n",
      "subject2088.xml\n",
      "subject8990.xml\n",
      "subject6459.xml\n",
      "subject7830.xml\n",
      "subject8395.xml\n",
      "subject4247.xml\n",
      "subject3667.xml\n",
      "subject5003.xml\n",
      "subject992.xml\n",
      "subject5644.xml\n",
      "subject242.xml\n",
      "subject7764.xml\n",
      "subject3283.xml\n",
      "subject6322.xml\n",
      "subject7678.xml\n",
      "subject6668.xml\n",
      "subject4333.xml\n",
      "subject1288.xml\n",
      "subject8200.xml\n",
      "subject5383.xml\n",
      "subject9039.xml\n",
      "subject7698.xml\n",
      "subject9652.xml\n",
      "subject5223.xml\n",
      "subject9725.xml\n",
      "subject1512.xml\n",
      "subject3994.xml\n",
      "subject7018.xml\n",
      "subject3644.xml\n",
      "subject1786.xml\n",
      "subject1027.xml\n",
      "subject8094.xml\n",
      "subject974.xml\n",
      "subject2947.xml\n",
      "subject9575.xml\n",
      "subject4570.xml\n",
      "subject5062.xml\n",
      "subject4729.xml\n",
      "subject5100.xml\n",
      "subject5177.xml\n",
      "subject505.xml\n",
      "subject5974.xml\n",
      "subject7499.xml\n",
      "subject1264.xml\n",
      "subject4071.xml\n",
      "subject7740.xml\n",
      "subject8721.xml\n",
      "subject9222.xml\n",
      "subject8432.xml\n",
      "subject2547.xml\n",
      "subject5995.xml\n",
      "subject6930.xml\n",
      "subject8472.xml\n",
      "subject6918.xml\n",
      "subject4198.xml\n",
      "subject501.xml\n",
      "subject7777.xml\n",
      "subject5375.xml\n",
      "subject7229.xml\n",
      "subject4762.xml\n",
      "subject5622.xml\n",
      "subject7637.xml\n",
      "subject47.xml\n",
      "subject1962.xml\n",
      "subject8795.xml\n",
      "subject4785.xml\n",
      "subject5840.xml\n",
      "subject3014.xml\n",
      "subject6464.xml\n",
      "subject522.xml\n",
      "subject5984.xml\n",
      "subject641.xml\n",
      "subject7326.xml\n",
      "subject4227.xml\n",
      "subject7428.xml\n",
      "subject203.xml\n",
      "subject6946.xml\n",
      "subject4563.xml\n",
      "subject682.xml\n",
      "subject9014.xml\n",
      "subject7435.xml\n",
      "subject8626.xml\n",
      "subject4459.xml\n",
      "subject733.xml\n",
      "subject7238.xml\n",
      "subject6428.xml\n",
      "subject7262.xml\n",
      "subject0.xml\n",
      "subject2269.xml\n",
      "subject8233.xml\n",
      "subject2522.xml\n",
      "subject5456.xml\n",
      "subject1064.xml\n",
      "subject8822.xml\n",
      "subject5033.xml\n",
      "subject1089.xml\n",
      "subject3277.xml\n",
      "subject5549.xml\n",
      "subject6352.xml\n",
      "subject6652.xml\n",
      "subject7669.xml\n",
      "subject5833.xml\n",
      "subject4795.xml\n",
      "subject4002.xml\n",
      "subject5878.xml\n",
      "subject1524.xml\n",
      "subject3928.xml\n",
      "subject9318.xml\n",
      "subject2935.xml\n",
      "subject1093.xml\n",
      "subject6786.xml\n",
      "subject3612.xml\n",
      "subject9114.xml\n",
      "subject4719.xml\n",
      "subject7439.xml\n",
      "subject1623.xml\n",
      "subject6290.xml\n",
      "subject8973.xml\n",
      "subject3844.xml\n",
      "subject7898.xml\n",
      "subject3605.xml\n",
      "subject2097.xml\n",
      "subject9381.xml\n",
      "subject3178.xml\n",
      "subject5908.xml\n",
      "subject3191.xml\n",
      "subject4196.xml\n",
      "subject8882.xml\n",
      "subject8845.xml\n",
      "subject5256.xml\n",
      "subject7318.xml\n",
      "subject4777.xml\n",
      "subject6309.xml\n",
      "subject4479.xml\n",
      "subject9393.xml\n",
      "subject4961.xml\n",
      "subject6247.xml\n",
      "subject1055.xml\n",
      "subject4644.xml\n",
      "subject7338.xml\n",
      "subject6284.xml\n",
      "subject5699.xml\n",
      "subject2580.xml\n",
      "subject2446.xml\n",
      "subject5409.xml\n",
      "subject1914.xml\n",
      "subject7263.xml\n",
      "subject5148.xml\n",
      "subject1793.xml\n",
      "subject9729.xml\n",
      "subject7952.xml\n",
      "subject9917.xml\n",
      "subject3868.xml\n",
      "subject5793.xml\n",
      "subject4934.xml\n",
      "subject3674.xml\n",
      "subject6019.xml\n",
      "subject2974.xml\n",
      "subject2857.xml\n",
      "subject855.xml\n",
      "subject5937.xml\n",
      "subject671.xml\n",
      "subject4318.xml\n",
      "subject5112.xml\n",
      "subject9249.xml\n",
      "subject7107.xml\n",
      "subject2996.xml\n",
      "subject5603.xml\n",
      "subject511.xml\n",
      "subject6518.xml\n",
      "subject5140.xml\n",
      "subject3737.xml\n",
      "subject9095.xml\n",
      "subject3227.xml\n",
      "subject7355.xml\n",
      "subject1617.xml\n",
      "subject6670.xml\n",
      "subject5387.xml\n",
      "subject3883.xml\n",
      "subject6146.xml\n",
      "subject2949.xml\n",
      "subject1763.xml\n",
      "subject2980.xml\n",
      "subject8933.xml\n",
      "subject6833.xml\n",
      "subject8802.xml\n",
      "subject8657.xml\n",
      "subject6259.xml\n",
      "subject1947.xml\n",
      "subject3635.xml\n",
      "subject8978.xml\n",
      "subject6423.xml\n",
      "subject1748.xml\n",
      "subject4702.xml\n",
      "subject8062.xml\n",
      "subject3555.xml\n",
      "subject2577.xml\n",
      "subject2475.xml\n",
      "subject8357.xml\n",
      "subject9492.xml\n",
      "subject3914.xml\n",
      "subject2495.xml\n",
      "subject7581.xml\n",
      "subject3725.xml\n",
      "subject6173.xml\n",
      "subject2247.xml\n",
      "subject8481.xml\n",
      "subject7946.xml\n",
      "subject7131.xml\n",
      "subject4848.xml\n",
      "subject747.xml\n",
      "subject5270.xml\n",
      "subject5979.xml\n",
      "subject6041.xml\n",
      "subject1950.xml\n",
      "subject7333.xml\n",
      "subject7247.xml\n",
      "subject814.xml\n",
      "subject5938.xml\n",
      "subject9160.xml\n",
      "subject6238.xml\n",
      "subject6957.xml\n",
      "subject8770.xml\n",
      "subject9497.xml\n",
      "subject807.xml\n",
      "subject6899.xml\n",
      "subject4014.xml\n",
      "subject2696.xml\n",
      "subject1885.xml\n",
      "subject8064.xml\n",
      "subject8081.xml\n",
      "subject2690.xml\n",
      "subject7462.xml\n",
      "subject8193.xml\n",
      "subject4526.xml\n",
      "subject7316.xml\n",
      "subject7290.xml\n",
      "subject463.xml\n",
      "subject4379.xml\n",
      "subject3181.xml\n",
      "subject5920.xml\n",
      "subject1728.xml\n",
      "subject2567.xml\n",
      "subject3904.xml\n",
      "subject4392.xml\n",
      "subject8581.xml\n",
      "subject9242.xml\n",
      "subject379.xml\n",
      "subject3881.xml\n",
      "subject8565.xml\n",
      "subject4505.xml\n",
      "subject3977.xml\n",
      "subject7489.xml\n",
      "subject2948.xml\n",
      "subject5342.xml\n",
      "subject8544.xml\n",
      "subject6903.xml\n",
      "subject7377.xml\n",
      "subject8769.xml\n",
      "subject3270.xml\n",
      "subject3224.xml\n",
      "subject2239.xml\n",
      "subject7801.xml\n",
      "subject3596.xml\n",
      "subject1469.xml\n",
      "subject4278.xml\n",
      "subject5282.xml\n",
      "subject3357.xml\n",
      "subject6013.xml\n",
      "subject5036.xml\n",
      "subject796.xml\n",
      "subject7692.xml\n",
      "subject7560.xml\n",
      "subject6035.xml\n",
      "subject1824.xml\n",
      "subject8726.xml\n",
      "subject6665.xml\n",
      "subject835.xml\n",
      "subject3117.xml\n",
      "subject519.xml\n",
      "subject1655.xml\n",
      "subject217.xml\n"
     ]
    }
   ],
   "source": [
    "writings = []\n",
    "for subject_file in os.listdir(datadir_T1):\n",
    "    print(subject_file)\n",
    "    with open(os.path.join(datadir_T1, subject_file)) as sf:\n",
    "        contents = sf.read()\n",
    "        root = ET.fromstring(contents)\n",
    "        try:\n",
    "            subject = root.findall('ID')[0].text\n",
    "        except Exception:\n",
    "            print('Cannot extract ID', contents[:500], '\\n-------\\n')        \n",
    "        for w in root.iter('WRITING'):\n",
    "            subject_writings = {'subject': subject}\n",
    "            for title in w.findall('TITLE'):\n",
    "                subject_writings['title'] = title.text\n",
    "            for text in w.findall('TEXT'):\n",
    "                subject_writings['text'] = text.text\n",
    "            for date in w.findall('DATE'):\n",
    "                subject_writings['date'] = date.text\n",
    "            writings.append(subject_writings)\n",
    "            # TODO: Date\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df = pd.DataFrame(writings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['label'] = writings_df['subject'].apply(\n",
    "    lambda s: labels_T1.loc[s, 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ffa90cdc2d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "writings_df.label.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def tokenize(t):\n",
    "    return tokenizer.tokenize(t.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['tokenized_title'] = writings_df['title'].apply(lambda t: tokenize(t) if type(t)==str else None)\n",
    "writings_df['title_len'] = writings_df['tokenized_title'].apply(lambda t: len(t) if type(t)==list else None)\n",
    "writings_df['tokenized_text'] = writings_df['text'].apply(lambda t: tokenize(t) if type(t)==str else None)\n",
    "writings_df['text_len'] = writings_df['tokenized_text'].apply(lambda t: len(t) if type(t)==list else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    127941.000000\n",
       "mean         32.268929\n",
       "std          82.590713\n",
       "min           0.000000\n",
       "25%           6.000000\n",
       "50%          13.000000\n",
       "75%          31.000000\n",
       "max        7201.000000\n",
       "Name: text_len, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.text_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    49762.000000\n",
       "mean        10.699771\n",
       "std          9.282454\n",
       "min          0.000000\n",
       "25%          4.000000\n",
       "50%          8.000000\n",
       "75%         14.000000\n",
       "max        149.000000\n",
       "Name: title_len, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.title_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title_len</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subject0</th>\n",
       "      <td>0</td>\n",
       "      <td>20.285714</td>\n",
       "      <td>31.711712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1027</th>\n",
       "      <td>0</td>\n",
       "      <td>7.769231</td>\n",
       "      <td>1.190476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1055</th>\n",
       "      <td>0</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>79.983193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1064</th>\n",
       "      <td>1</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>68.410256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1089</th>\n",
       "      <td>0</td>\n",
       "      <td>9.823529</td>\n",
       "      <td>13.254902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9917</th>\n",
       "      <td>1</td>\n",
       "      <td>8.983607</td>\n",
       "      <td>95.806897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9918</th>\n",
       "      <td>0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.865269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject992</th>\n",
       "      <td>0</td>\n",
       "      <td>5.872928</td>\n",
       "      <td>19.876190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9949</th>\n",
       "      <td>0</td>\n",
       "      <td>10.609756</td>\n",
       "      <td>42.346979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9961</th>\n",
       "      <td>0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>26.389313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>340 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             label  title_len   text_len\n",
       "subject                                 \n",
       "subject0         0  20.285714  31.711712\n",
       "subject1027      0   7.769231   1.190476\n",
       "subject1055      0  16.666667  79.983193\n",
       "subject1064      1  13.000000  68.410256\n",
       "subject1089      0   9.823529  13.254902\n",
       "...            ...        ...        ...\n",
       "subject9917      1   8.983607  95.806897\n",
       "subject9918      0   5.000000  11.865269\n",
       "subject992       0   5.872928  19.876190\n",
       "subject9949      0  10.609756  42.346979\n",
       "subject9961      0   5.000000  26.389313\n",
       "\n",
       "[340 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.groupby('subject').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title_len</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>299</td>\n",
       "      <td>296</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>40</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date  title_len  text_len\n",
       "label                           \n",
       "0       299        296       299\n",
       "1        41         40        41"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.groupby('subject').max().groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of posts per user 146.35882352941175\n",
      "Average number of comments per user 376.2970588235294\n"
     ]
    }
   ],
   "source": [
    "print(\"Average number of posts per user\", writings_df.groupby('subject').count().title.mean())\n",
    "print(\"Average number of comments per user\", writings_df.groupby('subject').count().text.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features and encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_features = {\n",
    "    \"max_features\": 20000,\n",
    "    # cut texts after this number of words\n",
    "    # (among top max_features most common words)\n",
    "    \"maxlen\": 100,\n",
    "    \"embedding_dim\": 100\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_NRC(nrc_path):\n",
    "    word_emotions = {}\n",
    "    emotion_words = {}\n",
    "    with open(nrc_path) as in_f:\n",
    "        for line in in_f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            word, emotion, label = line.split()\n",
    "            if word not in word_emotions:\n",
    "                word_emotions[word] = set()\n",
    "            if emotion not in emotion_words:\n",
    "                emotion_words[emotion] = set()\n",
    "            label = int(label)\n",
    "            if label:\n",
    "                word_emotions[word].add(emotion)\n",
    "                emotion_words[emotion].add(word)\n",
    "    return emotion_words\n",
    "\n",
    "nrc_lexicon_path = '/home/ana/resources/NRC-Sentiment-Emotion-Lexicons/NRC-Sentiment-Emotion-Lexicons/NRC-Emotion-Lexicon-v0.92/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'\n",
    "nrc_lexicon = load_NRC(nrc_lexicon_path)\n",
    "emotions = list(nrc_lexicon.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_emotions(tokens, emotion_lexicon, emotions, relative=True):\n",
    "    text_len = len(tokens)\n",
    "    encoded_emotions = [0 for e in emotions]\n",
    "    for i, emotion in enumerate(emotions):\n",
    "        try:\n",
    "            emotion_words = [t for t in tokens if t in emotion_lexicon[emotion]]\n",
    "            if relative:\n",
    "                encoded_emotions[i] = len(emotion_words) / len(tokens)\n",
    "            else:\n",
    "                encoded_emotions[i] = len(emotion_words)\n",
    "        except ValueError:\n",
    "            print(\"Emotion not found.\")\n",
    "    return encoded_emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Char n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(tokens):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Personal pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_person_pronouns = {\"i\", \"me\", \"my\", \"mine\"}\n",
    "def encode_pronouns(tokens, pronouns={\"i\", \"me\", \"my\", \"mine\"}, relative=True):\n",
    "    if not tokens:\n",
    "        return np.nan\n",
    "    text_len = len(tokens)\n",
    "    nr_pronouns = len([t for t in tokens if t in pronouns])\n",
    "    if relative:\n",
    "        return nr_pronouns/text_len\n",
    "    else:\n",
    "        return nr_pronouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words(\"english\")\n",
    "def encode_stopwords(tokens, stopwords=stopword_list):\n",
    "    encoded_stopwords = [0 for s in stopword_list]\n",
    "    if not tokens:\n",
    "        return encoded_stopwords\n",
    "    for i, stopword in enumerate(stopwords):\n",
    "        if stopword in tokens:\n",
    "            encoded_stopwords[i] += 1\n",
    "    return encoded_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def load_erisk_data(writings_df, voc_size, emotion_lexicon, seq_len, emotions =  \n",
    "                    ['anger', 'anticipation', 'disgust', 'fear', 'joy', \n",
    "                     'negative', 'positive', 'sadness', 'surprise', 'trust'],\n",
    "                    pronouns = [\"i\", \"me\", \"my\", \"mine\"],\n",
    "                    train_prop=0.7, min_post_len=3, min_word_len=1):\n",
    "    print(\"Loading data...\")\n",
    "    vocabulary = {}\n",
    "    word_freqs = Counter()\n",
    "    for words in writings_df.tokenized_text:\n",
    "        word_freqs.update(words)\n",
    "    for words in writings_df.tokenized_title:\n",
    "        word_freqs.update(words)\n",
    "    i = 1\n",
    "    for w, f in word_freqs.most_common(voc_size-2): # keeping voc_size-1 for unk\n",
    "        if len(w) < min_word_len:\n",
    "            continue\n",
    "        vocabulary[w] = i\n",
    "        i += 1\n",
    "    tokens_data_train = []\n",
    "    categ_data_train = []\n",
    "    sparse_data_train = []\n",
    "    tokens_data_test = []\n",
    "    categ_data_test = []\n",
    "    sparse_data_test = []\n",
    "    labels_train = []\n",
    "    labels_test = []\n",
    "    users_train = []\n",
    "    users_test = []\n",
    "    all_subjects = sorted(list(set(writings_df.subject)))\n",
    "    training_subjects_size = int(len(all_subjects) * train_prop)\n",
    "    training_subjects = all_subjects[:training_subjects_size]\n",
    "    print(training_subjects_size, \"training users, \", len(all_subjects)-training_subjects_size, \" test users.\")\n",
    "    training_rows = writings_df[writings_df['subject'].isin(training_subjects)].sample(frac=1) # shuffling\n",
    "    test_rows = writings_df[~writings_df['subject'].isin(training_subjects)].sample(frac=1)\n",
    "    positive_training_users = training_rows.groupby('subject').max().groupby('label').count().date[1]\n",
    "    positive_test_users = test_rows.groupby('subject').max().groupby('label').count().date[1]\n",
    "    print(\"Positive training users: \", positive_training_users, \", positive test users: \", positive_test_users)\n",
    "    def encode_text(tokens):\n",
    "        # Using voc_size-1 value for OOV token\n",
    "        encoded_tokens = [vocabulary.get(w, voc_size-1) for w in tokens]\n",
    "        encoded_emotions = encode_emotions(tokens, emotion_lexicon, emotions)\n",
    "        encoded_pronouns = encode_pronouns(tokens, pronouns)\n",
    "        encoded_stopwords = encode_stopwords(tokens)\n",
    "        return (encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords)\n",
    "    for row in training_rows.itertuples():\n",
    "        words = []\n",
    "        if row.tokenized_title:\n",
    "            words.extend(row.tokenized_title)\n",
    "        if row.tokenized_text:\n",
    "            words.extend(row.tokenized_text)\n",
    "        if not words or len(words)<min_post_len:\n",
    "            continue\n",
    "        label = row.label\n",
    "        encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords = encode_text(words)\n",
    "        tokens_data_train.append(encoded_tokens)\n",
    "        categ_data_train.append(encoded_emotions + [encoded_pronouns])\n",
    "        sparse_data_train.append(encoded_stopwords)\n",
    "        labels_train.append(label)\n",
    "        users_train.append(row.subject)\n",
    "    for row in test_rows[~test_rows['tokenized_text'].isna()].itertuples():\n",
    "        words = []\n",
    "        if row.tokenized_title:\n",
    "            words.extend(row.tokenized_title)\n",
    "        if row.tokenized_text:\n",
    "            words.extend(row.tokenized_text)   \n",
    "        if not words or len(words)<min_post_len:\n",
    "            continue\n",
    "        label = row.label\n",
    "        encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords = encode_text(words)\n",
    "        tokens_data_test.append(encoded_tokens)\n",
    "        categ_data_test.append(encoded_emotions + [encoded_pronouns])\n",
    "        sparse_data_test.append(encoded_stopwords)\n",
    "        labels_test.append(label)\n",
    "        users_test.append(row.subject)\n",
    "        \n",
    "    # using zeros for padding\n",
    "    tokens_data_train_padded = sequence.pad_sequences(tokens_data_train, maxlen=seq_len)\n",
    "    tokens_data_test_padded = sequence.pad_sequences(tokens_data_test, maxlen=seq_len)\n",
    "        \n",
    "    return ([np.array(tokens_data_train_padded), np.array(categ_data_train), np.array(sparse_data_train),\n",
    "            users_train], \n",
    "            np.array(labels_train)), \\\n",
    "            ([np.array(tokens_data_test_padded), np.array(categ_data_test), np.array(sparse_data_test),\n",
    "             users_test], \n",
    "             np.array(labels_test)), vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "237 training users,  103  test users.\n",
      "Positive training users:  24 , positive test users:  17\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test), voc = load_erisk_data(writings_df, \n",
    "                                                            seq_len=hyperparams_features['maxlen'],\n",
    "                                                            voc_size=hyperparams_features['max_features'],\n",
    "                                                           emotion_lexicon=nrc_lexicon,\n",
    "                                                           emotions=emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111375 train sequences\n",
      "31863 test sequences\n"
     ]
    }
   ],
   "source": [
    "x_train_seq, x_train_categ, x_train_sparse, x_training_users = x_train\n",
    "x_test_seq, x_test_categ, x_test_sparse, x_test_users = x_test\n",
    "print(len(x_train_seq), 'train sequences')\n",
    "print(len(x_test_seq), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4430 positive training examples\n",
      "2092 positive test examples\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(y_train).sum(), \"positive training examples\")\n",
    "print(pd.Series(y_test).sum(), \"positive test examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.52071158, 12.57054176])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 20000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "def load_embeddings(path, embedding_dim, voc):\n",
    "    # random matrix with mean value = 0\n",
    "    embedding_matrix = np.random.random((len(voc)+2, embedding_dim)) - 0.5 # voc + unk + pad value(0)\n",
    "#     embedding_matrix = np.zeros((len(voc)+1, embedding_dim))\n",
    "\n",
    "    f = open(path)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_i = voc.get(word)\n",
    "        if word_i is not None:\n",
    "            embedding_matrix[word_i] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print('Total %s word vectors.' % len(embedding_matrix))\n",
    "\n",
    " \n",
    "    return embedding_matrix\n",
    "\n",
    "# pretrained_embeddings_path = '/home/ana/resources/glove.6B/glove.6B.%dd.txt' % hyperparams_features['embedding_dim']\n",
    "pretrained_embeddings_path = '/home/ana/resources/glove.twitter.27B/glove.twitter.27B.%dd.txt' % hyperparams_features['embedding_dim']\n",
    "embedding_matrix = load_embeddings(pretrained_embeddings_path, hyperparams_features['embedding_dim'], voc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_sparse[90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'lstm_units': 32,\n",
    "    'dense_bow_units': 5,\n",
    "    'dropout': 0.14,\n",
    "    'l2_dense': 0.00011,\n",
    "    'optimizer': 'adam', #None,\n",
    "    'decay': 0.0001,\n",
    "    'lr': 0.00001,\n",
    "    \"batch_size\": 128,\n",
    "    \"trainable_embeddings\": True,\n",
    "    \"reduce_lr_factor\": 0.2,\n",
    "    \"reduce_lr_patience\": 2,\n",
    "    \"freeze_patience\": 5,\n",
    "\n",
    "}\n",
    "if not hyperparams['optimizer']:\n",
    "    hyperparams['optimizer'] = optimizers.Adam(lr=hyperparams['lr'], beta_1=0.9, beta_2=0.999, epsilon=1e-08,\n",
    "                                   decay=hyperparams['decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopwords_list,\n",
    "               ignore_layer=[]):\n",
    "\n",
    "    tokens_features = Input(shape=(hyperparams_features['maxlen'],), name='word_seq')\n",
    "    embedding_layer = Embedding(hyperparams_features['max_features'], \n",
    "                                hyperparams_features['embedding_dim'], \n",
    "                                input_length=hyperparams_features['maxlen'],\n",
    "                                mask_zero=True,\n",
    "                                weights=[embedding_matrix], \n",
    "                                trainable=hyperparams['trainable_embeddings'],\n",
    "                               name='embeddings_layer')(\n",
    "        tokens_features)\n",
    "#     lstm_layers = Bidirectional(LSTM(hyperparams['lstm_units']))(embedding_layer)\n",
    "    lstm_layers = LSTM(hyperparams['lstm_units'], dropout=hyperparams['dropout'],\n",
    "                      recurrent_dropout=hyperparams['dropout'],\n",
    "                      name='LSTM_layer')(embedding_layer)\n",
    "    \n",
    "    numerical_features = Input(shape=(len(emotions) + 1,), name='numeric_input') # emotions and pronouns\n",
    "    dense_layer = Dense(units=1,\n",
    "                        kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                        name='numerical_dense_layer',\n",
    "                       )(numerical_features)\n",
    "    sparse_features = Input(shape=(len(stopwords_list),), name='sparse_input') # stopwords\n",
    "    dense_layer_sparse = Dense(units=hyperparams['dense_bow_units'],\n",
    "                              name='sparse_feat_dense_layer',\n",
    "                                kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                              )(sparse_features)\n",
    "#     # TODO: this is getting out of hand. Refactor this ablation part.\n",
    "    if 'lstm_layers' in ignore_layer:\n",
    "        output_layer = Dense(1, activation='sigmoid')(numerical_features)\n",
    "    elif 'numerical_dense_layer' in ignore_layer and 'sparse_feat_dense_layer' in ignore_layer:\n",
    "        output_layer = Dense(1, activation='sigmoid')(lstm_layers)\n",
    "    elif 'numerical_dense_layer' in ignore_layer:\n",
    "        merged_layers = concatenate([lstm_layers, dense_layer_sparse])\n",
    "        output_layer = Dense(1, activation='sigmoid')(merged_layers)\n",
    "\n",
    "    elif 'sparse_feat_dense_layer' in ignore_layer:\n",
    "        merged_layers = concatenate([lstm_layers, dense_layer])\n",
    "        output_layer = Dense(1, activation='sigmoid')(merged_layers)\n",
    "\n",
    "    else:\n",
    "        merged_layers = concatenate([lstm_layers, dense_layer, dense_layer_sparse])\n",
    "        output_layer = Dense(1, activation='sigmoid')(merged_layers)\n",
    "\n",
    "    # Compile model\n",
    "    model = Model(inputs=[tokens_features, numerical_features, sparse_features], outputs=output_layer)\n",
    "    model.compile(hyperparams['optimizer'], 'binary_crossentropy',\n",
    "                  metrics=['binary_accuracy', f1_m, precision_m, recall_m])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "word_seq (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embeddings_layer (Embedding)    (None, 100, 100)     2000000     word_seq[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "numeric_input (InputLayer)      (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sparse_input (InputLayer)       (None, 179)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "LSTM_layer (LSTM)               (None, 32)           17024       embeddings_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "numerical_dense_layer (Dense)   (None, 1)            12          numeric_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sparse_feat_dense_layer (Dense) (None, 5)            900         sparse_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 38)           0           LSTM_layer[0][0]                 \n",
      "                                                                 numerical_dense_layer[0][0]      \n",
      "                                                                 sparse_feat_dense_layer[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            39          concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 2,017,975\n",
      "Trainable params: 2,017,975\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopword_list,\n",
    "                   ignore_layer=[])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, 'models/lstm_plus7.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary:\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     url: https://www.comet.ml/ananana/mental/24160d5b42f4454584ff1ffc85207d5e\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     sys.cpu.percent.01    : (82.9, 82.9)\n",
      "COMET INFO:     sys.cpu.percent.02    : (49.9, 49.9)\n",
      "COMET INFO:     sys.cpu.percent.03    : (36.3, 36.3)\n",
      "COMET INFO:     sys.cpu.percent.04    : (34.9, 34.9)\n",
      "COMET INFO:     sys.cpu.percent.avg   : (51.00000000000001, 51.00000000000001)\n",
      "COMET INFO:     sys.gpu.0.total_memory: (1073414144.0, 1073414144.0)\n",
      "COMET INFO:     sys.load.avg          : (1.27, 1.27)\n",
      "COMET INFO:     sys.ram.total         : (8277307392.0, 8277307392.0)\n",
      "COMET INFO:     sys.ram.used          : (6003113984.0, 6003113984.0)\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     git-patch: 1\n",
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/ananana/mental/76e9847ca5eb4e5982dead933a5c8f85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(api_key=\"eoBdVyznAhfg3bK9pZ58ZSXfv\",\n",
    "                        project_name=\"mental\", workspace=\"ananana\")\n",
    "\n",
    "experiment.log_parameters(hyperparams_features)\n",
    "\n",
    "experiment.log_parameter('emotion_lexicon', nrc_lexicon_path)\n",
    "experiment.log_parameter('emotions', emotions)\n",
    "experiment.log_parameter('embeddings_path', pretrained_embeddings_path)\n",
    "\n",
    "experiment.log_parameters(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'i': 2,\n",
       " 'to': 3,\n",
       " 'a': 4,\n",
       " 'and': 5,\n",
       " 'of': 6,\n",
       " 'it': 7,\n",
       " 'in': 8,\n",
       " 'you': 9,\n",
       " 'that': 10,\n",
       " 'is': 11,\n",
       " 's': 12,\n",
       " 'for': 13,\n",
       " 'this': 14,\n",
       " 't': 15,\n",
       " 'on': 16,\n",
       " 'with': 17,\n",
       " 'but': 18,\n",
       " 'my': 19,\n",
       " '8217': 20,\n",
       " 'be': 21,\n",
       " 'was': 22,\n",
       " 'have': 23,\n",
       " 'are': 24,\n",
       " 'not': 25,\n",
       " 'they': 26,\n",
       " 'as': 27,\n",
       " 'if': 28,\n",
       " 'so': 29,\n",
       " 'just': 30,\n",
       " 'what': 31,\n",
       " 'can': 32,\n",
       " 'like': 33,\n",
       " 'he': 34,\n",
       " 'or': 35,\n",
       " 'at': 36,\n",
       " 'we': 37,\n",
       " 'me': 38,\n",
       " 'from': 39,\n",
       " 'your': 40,\n",
       " 'm': 41,\n",
       " 'do': 42,\n",
       " 'com': 43,\n",
       " 'all': 44,\n",
       " 'about': 45,\n",
       " 'an': 46,\n",
       " 'one': 47,\n",
       " 'there': 48,\n",
       " 'would': 49,\n",
       " 'out': 50,\n",
       " 'up': 51,\n",
       " 'when': 52,\n",
       " 'more': 53,\n",
       " 'get': 54,\n",
       " 'don': 55,\n",
       " 'people': 56,\n",
       " 'by': 57,\n",
       " 'will': 58,\n",
       " 'no': 59,\n",
       " 'how': 60,\n",
       " 'https': 61,\n",
       " 'gt': 62,\n",
       " 'has': 63,\n",
       " 'them': 64,\n",
       " 'his': 65,\n",
       " 'time': 66,\n",
       " 'some': 67,\n",
       " 're': 68,\n",
       " 'know': 69,\n",
       " 'think': 70,\n",
       " 'who': 71,\n",
       " 'their': 72,\n",
       " 'because': 73,\n",
       " 'had': 74,\n",
       " 'she': 75,\n",
       " 'here': 76,\n",
       " 'good': 77,\n",
       " 'really': 78,\n",
       " 'www': 79,\n",
       " 'r': 80,\n",
       " 'now': 81,\n",
       " 've': 82,\n",
       " 'been': 83,\n",
       " 'only': 84,\n",
       " 'her': 85,\n",
       " 'also': 86,\n",
       " 'were': 87,\n",
       " 'than': 88,\n",
       " 'see': 89,\n",
       " 'any': 90,\n",
       " 'http': 91,\n",
       " 'even': 92,\n",
       " 'make': 93,\n",
       " 'other': 94,\n",
       " 'then': 95,\n",
       " '128056': 96,\n",
       " 'much': 97,\n",
       " '1': 98,\n",
       " 'which': 99,\n",
       " 'him': 100,\n",
       " 'could': 101,\n",
       " 'go': 102,\n",
       " '2': 103,\n",
       " 'first': 104,\n",
       " 'd': 105,\n",
       " 'want': 106,\n",
       " 'new': 107,\n",
       " 'why': 108,\n",
       " 'well': 109,\n",
       " 'did': 110,\n",
       " 'too': 111,\n",
       " 'right': 112,\n",
       " 'way': 113,\n",
       " 'into': 114,\n",
       " 'very': 115,\n",
       " 'after': 116,\n",
       " 'being': 117,\n",
       " 'over': 118,\n",
       " 'back': 119,\n",
       " 'still': 120,\n",
       " 'got': 121,\n",
       " 'most': 122,\n",
       " 'reddit': 123,\n",
       " 'should': 124,\n",
       " 'something': 125,\n",
       " 'post': 126,\n",
       " 'going': 127,\n",
       " 'll': 128,\n",
       " '3': 129,\n",
       " 'our': 130,\n",
       " 'these': 131,\n",
       " 'thanks': 132,\n",
       " 'never': 133,\n",
       " '8220': 134,\n",
       " 'need': 135,\n",
       " '8221': 136,\n",
       " 'say': 137,\n",
       " 'where': 138,\n",
       " 'us': 139,\n",
       " 'use': 140,\n",
       " 'am': 141,\n",
       " 'love': 142,\n",
       " 'work': 143,\n",
       " 'day': 144,\n",
       " 'same': 145,\n",
       " 'off': 146,\n",
       " 'sure': 147,\n",
       " 'game': 148,\n",
       " 'before': 149,\n",
       " 'said': 150,\n",
       " 'didn': 151,\n",
       " '10': 152,\n",
       " 'lot': 153,\n",
       " 'those': 154,\n",
       " 'thing': 155,\n",
       " 'years': 156,\n",
       " 'someone': 157,\n",
       " 'does': 158,\n",
       " 'take': 159,\n",
       " 'best': 160,\n",
       " 'made': 161,\n",
       " 'feel': 162,\n",
       " 'great': 163,\n",
       " 'two': 164,\n",
       " 'actually': 165,\n",
       " 'better': 166,\n",
       " 'look': 167,\n",
       " '_': 168,\n",
       " 'its': 169,\n",
       " '5': 170,\n",
       " 'every': 171,\n",
       " 'down': 172,\n",
       " 'year': 173,\n",
       " 'pretty': 174,\n",
       " 'life': 175,\n",
       " 'doesn': 176,\n",
       " 'amp': 177,\n",
       " 'things': 178,\n",
       " 'many': 179,\n",
       " 'though': 180,\n",
       " '4': 181,\n",
       " 'while': 182,\n",
       " 'last': 183,\n",
       " 'always': 184,\n",
       " 'around': 185,\n",
       " 'man': 186,\n",
       " 'imgur': 187,\n",
       " 'thank': 188,\n",
       " 'world': 189,\n",
       " 'give': 190,\n",
       " 'find': 191,\n",
       " 'little': 192,\n",
       " 'help': 193,\n",
       " 'anything': 194,\n",
       " 'u': 195,\n",
       " 'through': 196,\n",
       " 'trump': 197,\n",
       " 'long': 198,\n",
       " 'used': 199,\n",
       " 'o': 200,\n",
       " 'yeah': 201,\n",
       " 'may': 202,\n",
       " 'few': 203,\n",
       " 'ever': 204,\n",
       " 'thought': 205,\n",
       " 'since': 206,\n",
       " 'yes': 207,\n",
       " 'doing': 208,\n",
       " 'getting': 209,\n",
       " 'shit': 210,\n",
       " 'anyone': 211,\n",
       " 'comments': 212,\n",
       " 'probably': 213,\n",
       " 'let': 214,\n",
       " 'point': 215,\n",
       " 'person': 216,\n",
       " 'bad': 217,\n",
       " 'both': 218,\n",
       " 'watch': 219,\n",
       " 'another': 220,\n",
       " 'again': 221,\n",
       " 'own': 222,\n",
       " 'might': 223,\n",
       " 'isn': 224,\n",
       " 'looking': 225,\n",
       " 'maybe': 226,\n",
       " 'everyone': 227,\n",
       " 'old': 228,\n",
       " 'come': 229,\n",
       " 'put': 230,\n",
       " 'mean': 231,\n",
       " 'without': 232,\n",
       " 'try': 233,\n",
       " 'please': 234,\n",
       " 'looks': 235,\n",
       " 'keep': 236,\n",
       " 'trying': 237,\n",
       " 'big': 238,\n",
       " 'video': 239,\n",
       " 'different': 240,\n",
       " 'show': 241,\n",
       " 'guy': 242,\n",
       " 'next': 243,\n",
       " 'e': 244,\n",
       " 'part': 245,\n",
       " 'play': 246,\n",
       " 'using': 247,\n",
       " 'makes': 248,\n",
       " 'already': 249,\n",
       " 'enough': 250,\n",
       " '2018': 251,\n",
       " 'found': 252,\n",
       " 'money': 253,\n",
       " 'oh': 254,\n",
       " 'real': 255,\n",
       " 'such': 256,\n",
       " '9685': 257,\n",
       " 'high': 258,\n",
       " 'end': 259,\n",
       " 'between': 260,\n",
       " '12388': 261,\n",
       " '6': 262,\n",
       " 'done': 263,\n",
       " 'read': 264,\n",
       " 'link': 265,\n",
       " 'lol': 266,\n",
       " 'nice': 267,\n",
       " 'having': 268,\n",
       " 'away': 269,\n",
       " 'tell': 270,\n",
       " 'nothing': 271,\n",
       " 'kind': 272,\n",
       " 'making': 273,\n",
       " 'question': 274,\n",
       " 'live': 275,\n",
       " 'fuck': 276,\n",
       " 'v': 277,\n",
       " 'start': 278,\n",
       " 'else': 279,\n",
       " 'team': 280,\n",
       " 'today': 281,\n",
       " 'hard': 282,\n",
       " '0': 283,\n",
       " 'against': 284,\n",
       " 'once': 285,\n",
       " 'days': 286,\n",
       " 'seen': 287,\n",
       " 'far': 288,\n",
       " 'bit': 289,\n",
       " 'able': 290,\n",
       " 'org': 291,\n",
       " 'place': 292,\n",
       " 'week': 293,\n",
       " 'everything': 294,\n",
       " 'each': 295,\n",
       " 'free': 296,\n",
       " 'wrong': 297,\n",
       " 'school': 298,\n",
       " 'least': 299,\n",
       " 'times': 300,\n",
       " 'less': 301,\n",
       " 'edit': 302,\n",
       " 'source': 303,\n",
       " 'idea': 304,\n",
       " 'top': 305,\n",
       " 'went': 306,\n",
       " 'during': 307,\n",
       " 'name': 308,\n",
       " 'believe': 309,\n",
       " 'until': 310,\n",
       " 'won': 311,\n",
       " 'state': 312,\n",
       " 'definitely': 313,\n",
       " 'seems': 314,\n",
       " 'guys': 315,\n",
       " 'saying': 316,\n",
       " 'home': 317,\n",
       " 'hope': 318,\n",
       " 'title': 319,\n",
       " 'story': 320,\n",
       " '7': 321,\n",
       " 'youtube': 322,\n",
       " 'fucking': 323,\n",
       " 'small': 324,\n",
       " 'says': 325,\n",
       " 'full': 326,\n",
       " 'rep': 327,\n",
       " 'n': 328,\n",
       " 'yet': 329,\n",
       " 'case': 330,\n",
       " 'sorry': 331,\n",
       " 'stop': 332,\n",
       " 'twitter': 333,\n",
       " 'friend': 334,\n",
       " 'under': 335,\n",
       " 'stuff': 336,\n",
       " 'reason': 337,\n",
       " 'ago': 338,\n",
       " 'support': 339,\n",
       " 'change': 340,\n",
       " '8': 341,\n",
       " 'job': 342,\n",
       " 'however': 343,\n",
       " 'house': 344,\n",
       " 'wasn': 345,\n",
       " 'family': 346,\n",
       " 'left': 347,\n",
       " 'second': 348,\n",
       " 'friends': 349,\n",
       " 'etc': 350,\n",
       " 'guess': 351,\n",
       " 'comment': 352,\n",
       " 'true': 353,\n",
       " 'problem': 354,\n",
       " 'power': 355,\n",
       " 'gets': 356,\n",
       " 'myself': 357,\n",
       " 'called': 358,\n",
       " 'news': 359,\n",
       " 'buy': 360,\n",
       " 'either': 361,\n",
       " 'rules': 362,\n",
       " 'sub': 363,\n",
       " 'set': 364,\n",
       " 'wanted': 365,\n",
       " 'happy': 366,\n",
       " 'w': 367,\n",
       " 'water': 368,\n",
       " 'fun': 369,\n",
       " 'white': 370,\n",
       " 'almost': 371,\n",
       " 'fact': 372,\n",
       " 'wiki': 373,\n",
       " 'experience': 374,\n",
       " 'open': 375,\n",
       " 'call': 376,\n",
       " 'whole': 377,\n",
       " 'started': 378,\n",
       " 'understand': 379,\n",
       " 'wouldn': 380,\n",
       " '20': 381,\n",
       " 'night': 382,\n",
       " '9': 383,\n",
       " 'article': 384,\n",
       " 'h': 385,\n",
       " 'side': 386,\n",
       " '100': 387,\n",
       " 'check': 388,\n",
       " 'came': 389,\n",
       " 'dog': 390,\n",
       " 'took': 391,\n",
       " 'net': 392,\n",
       " 'possible': 393,\n",
       " 'months': 394,\n",
       " 'jpg': 395,\n",
       " 'hours': 396,\n",
       " 'face': 397,\n",
       " 'food': 398,\n",
       " 'saw': 399,\n",
       " 'playing': 400,\n",
       " 'doi': 401,\n",
       " 'care': 402,\n",
       " '12': 403,\n",
       " 'internet': 404,\n",
       " 'study': 405,\n",
       " 'talking': 406,\n",
       " 'remember': 407,\n",
       " 'content': 408,\n",
       " 'mind': 409,\n",
       " 'de': 410,\n",
       " 'ask': 411,\n",
       " 'level': 412,\n",
       " 'told': 413,\n",
       " 'instead': 414,\n",
       " 'season': 415,\n",
       " 'god': 416,\n",
       " 'cool': 417,\n",
       " 'thinking': 418,\n",
       " 'working': 419,\n",
       " 'must': 420,\n",
       " 'human': 421,\n",
       " 'based': 422,\n",
       " 'country': 423,\n",
       " 'talk': 424,\n",
       " '225': 425,\n",
       " 'p': 426,\n",
       " 'movie': 427,\n",
       " 'aren': 428,\n",
       " 'run': 429,\n",
       " 'body': 430,\n",
       " '233': 431,\n",
       " 'agree': 432,\n",
       " 'posted': 433,\n",
       " '227': 434,\n",
       " 'black': 435,\n",
       " '65039': 436,\n",
       " 'awesome': 437,\n",
       " 'party': 438,\n",
       " 'wait': 439,\n",
       " 'games': 440,\n",
       " 'journal': 441,\n",
       " 'others': 442,\n",
       " 'group': 443,\n",
       " 'likely': 444,\n",
       " 'exactly': 445,\n",
       " 'worth': 446,\n",
       " 'government': 447,\n",
       " 'gonna': 448,\n",
       " 'que': 449,\n",
       " 'coming': 450,\n",
       " 'women': 451,\n",
       " 'means': 452,\n",
       " 'tried': 453,\n",
       " 'information': 454,\n",
       " '30': 455,\n",
       " 'pay': 456,\n",
       " 'haven': 457,\n",
       " 'head': 458,\n",
       " 'future': 459,\n",
       " '2017': 460,\n",
       " 'hey': 461,\n",
       " 'c': 462,\n",
       " 'taking': 463,\n",
       " 'single': 464,\n",
       " 'literally': 465,\n",
       " 'hear': 466,\n",
       " 'hate': 467,\n",
       " 'super': 468,\n",
       " 'self': 469,\n",
       " 'goes': 470,\n",
       " 'health': 471,\n",
       " 'amazing': 472,\n",
       " 'hand': 473,\n",
       " 'message': 474,\n",
       " 'within': 475,\n",
       " 'issue': 476,\n",
       " 'comes': 477,\n",
       " 'happened': 478,\n",
       " 'sounds': 479,\n",
       " 'system': 480,\n",
       " 'sense': 481,\n",
       " 'car': 482,\n",
       " 'couple': 483,\n",
       " 'type': 484,\n",
       " 'half': 485,\n",
       " 'social': 486,\n",
       " 'usually': 487,\n",
       " 'facebook': 488,\n",
       " 'order': 489,\n",
       " '3901': 490,\n",
       " '8211': 491,\n",
       " 'close': 492,\n",
       " '3900': 493,\n",
       " 'comic': 494,\n",
       " 'book': 495,\n",
       " 'three': 496,\n",
       " 'children': 497,\n",
       " 'needs': 498,\n",
       " 'interesting': 499,\n",
       " 'futurology': 500,\n",
       " 'number': 501,\n",
       " 'past': 502,\n",
       " 'data': 503,\n",
       " 'girl': 504,\n",
       " 'quite': 505,\n",
       " 'low': 506,\n",
       " 'kids': 507,\n",
       " '128514': 508,\n",
       " 'reference': 509,\n",
       " 'course': 510,\n",
       " 'subreddit': 511,\n",
       " 'dont': 512,\n",
       " 'ok': 513,\n",
       " 'heard': 514,\n",
       " 'weeks': 515,\n",
       " 'yourself': 516,\n",
       " '000': 517,\n",
       " 'together': 518,\n",
       " 'front': 519,\n",
       " 'especially': 520,\n",
       " 'war': 521,\n",
       " 'important': 522,\n",
       " 'control': 523,\n",
       " 'picture': 524,\n",
       " 'happen': 525,\n",
       " 'sometimes': 526,\n",
       " 'fine': 527,\n",
       " 'eat': 528,\n",
       " 'parents': 529,\n",
       " 'list': 530,\n",
       " 'rather': 531,\n",
       " 'line': 532,\n",
       " 'law': 533,\n",
       " 'thread': 534,\n",
       " 'opinion': 535,\n",
       " 'release': 536,\n",
       " 'often': 537,\n",
       " 'public': 538,\n",
       " 'works': 539,\n",
       " 'seem': 540,\n",
       " '8216': 541,\n",
       " '50': 542,\n",
       " 'later': 543,\n",
       " 'vote': 544,\n",
       " 'non': 545,\n",
       " 'american': 546,\n",
       " 'matter': 547,\n",
       " 'favorite': 548,\n",
       " 'posts': 549,\n",
       " 'phone': 550,\n",
       " '15': 551,\n",
       " 'wish': 552,\n",
       " 'original': 553,\n",
       " 'deal': 554,\n",
       " 'b': 555,\n",
       " 'hit': 556,\n",
       " 'google': 557,\n",
       " '8212': 558,\n",
       " 'leave': 559,\n",
       " 'win': 560,\n",
       " 'completely': 561,\n",
       " 'im': 562,\n",
       " 'add': 563,\n",
       " 'damn': 564,\n",
       " 'ones': 565,\n",
       " 'example': 566,\n",
       " 'space': 567,\n",
       " 'due': 568,\n",
       " 'cat': 569,\n",
       " 'lost': 570,\n",
       " 'entire': 571,\n",
       " 'fight': 572,\n",
       " 'absolutely': 573,\n",
       " 'kill': 574,\n",
       " 'basically': 575,\n",
       " 'whether': 576,\n",
       " '11': 577,\n",
       " 'image': 578,\n",
       " 'haha': 579,\n",
       " 'history': 580,\n",
       " 'removed': 581,\n",
       " 'early': 582,\n",
       " 'cause': 583,\n",
       " 'easy': 584,\n",
       " 'die': 585,\n",
       " 'room': 586,\n",
       " 'op': 587,\n",
       " 'couldn': 588,\n",
       " 'media': 589,\n",
       " 'become': 590,\n",
       " 'neutrality': 591,\n",
       " 'players': 592,\n",
       " 'death': 593,\n",
       " 'baby': 594,\n",
       " 'men': 595,\n",
       " 'okay': 596,\n",
       " 'minutes': 597,\n",
       " 'similar': 598,\n",
       " 'answer': 599,\n",
       " 'age': 600,\n",
       " 'company': 601,\n",
       " 'child': 602,\n",
       " 'sound': 603,\n",
       " 'gif': 604,\n",
       " 'music': 605,\n",
       " 'huge': 606,\n",
       " 'move': 607,\n",
       " 'whatever': 608,\n",
       " 'results': 609,\n",
       " 'weight': 610,\n",
       " 'dude': 611,\n",
       " 'page': 612,\n",
       " 'city': 613,\n",
       " 'questions': 614,\n",
       " 'pain': 615,\n",
       " 'hell': 616,\n",
       " 'online': 617,\n",
       " 'ban': 618,\n",
       " 'woman': 619,\n",
       " 'community': 620,\n",
       " 'status': 621,\n",
       " 'soon': 622,\n",
       " 'fire': 623,\n",
       " 'month': 624,\n",
       " 'turn': 625,\n",
       " 'class': 626,\n",
       " 'dead': 627,\n",
       " 'red': 628,\n",
       " 'stay': 629,\n",
       " 'quality': 630,\n",
       " 'watching': 631,\n",
       " 'police': 632,\n",
       " 'simply': 633,\n",
       " 'per': 634,\n",
       " 'business': 635,\n",
       " 'president': 636,\n",
       " 'asked': 637,\n",
       " 'general': 638,\n",
       " 'en': 639,\n",
       " 'weird': 640,\n",
       " 'x': 641,\n",
       " 'chance': 642,\n",
       " 'wants': 643,\n",
       " 'research': 644,\n",
       " 'act': 645,\n",
       " 'seeing': 646,\n",
       " 'area': 647,\n",
       " 'higher': 648,\n",
       " 'court': 649,\n",
       " 'song': 650,\n",
       " 'linked': 651,\n",
       " 'reading': 652,\n",
       " 'press': 653,\n",
       " 'dad': 654,\n",
       " 'light': 655,\n",
       " 'rest': 656,\n",
       " 'advice': 657,\n",
       " 'price': 658,\n",
       " 'wife': 659,\n",
       " 'photo': 660,\n",
       " 'shot': 661,\n",
       " 'mine': 662,\n",
       " 'episode': 663,\n",
       " 'project': 664,\n",
       " 'amount': 665,\n",
       " 'sex': 666,\n",
       " 'finally': 667,\n",
       " 'large': 668,\n",
       " 'mods': 669,\n",
       " 'outside': 670,\n",
       " 'shows': 671,\n",
       " 'honestly': 672,\n",
       " 'series': 673,\n",
       " 'funny': 674,\n",
       " 'account': 675,\n",
       " 'taken': 676,\n",
       " 'risk': 677,\n",
       " 'save': 678,\n",
       " 'states': 679,\n",
       " 'given': 680,\n",
       " 'knew': 681,\n",
       " 'mom': 682,\n",
       " 'felt': 683,\n",
       " 'sleep': 684,\n",
       " 'perfect': 685,\n",
       " 'word': 686,\n",
       " 'player': 687,\n",
       " 'issues': 688,\n",
       " 'behind': 689,\n",
       " 'j': 690,\n",
       " 'hot': 691,\n",
       " 'played': 692,\n",
       " 'fan': 693,\n",
       " 'across': 694,\n",
       " 'wow': 695,\n",
       " 'running': 696,\n",
       " 'copy': 697,\n",
       " 'blue': 698,\n",
       " 'cut': 699,\n",
       " 'anyway': 700,\n",
       " 'normal': 701,\n",
       " '2016': 702,\n",
       " 'share': 703,\n",
       " 'uk': 704,\n",
       " 'store': 705,\n",
       " 'feeling': 706,\n",
       " 'current': 707,\n",
       " 'young': 708,\n",
       " 'points': 709,\n",
       " 'abstract': 710,\n",
       " 'kid': 711,\n",
       " 'takes': 712,\n",
       " 'vs': 713,\n",
       " 'short': 714,\n",
       " 'actual': 715,\n",
       " 'g': 716,\n",
       " 'rule': 717,\n",
       " 'although': 718,\n",
       " 'looked': 719,\n",
       " 'recently': 720,\n",
       " 'site': 721,\n",
       " 'xbox': 722,\n",
       " 'bring': 723,\n",
       " 'moment': 724,\n",
       " 'enjoy': 725,\n",
       " 'late': 726,\n",
       " 'f': 727,\n",
       " 'specific': 728,\n",
       " '18': 729,\n",
       " 'happens': 730,\n",
       " 'related': 731,\n",
       " 'stupid': 732,\n",
       " 'version': 733,\n",
       " 'words': 734,\n",
       " 'middle': 735,\n",
       " 'imagine': 736,\n",
       " 'sign': 737,\n",
       " 'bill': 738,\n",
       " 'test': 739,\n",
       " 'along': 740,\n",
       " 'longer': 741,\n",
       " 'poor': 742,\n",
       " 'break': 743,\n",
       " '25': 744,\n",
       " 'tv': 745,\n",
       " 'garfield': 746,\n",
       " 'giving': 747,\n",
       " 'crazy': 748,\n",
       " 'college': 749,\n",
       " 'near': 750,\n",
       " 'unless': 751,\n",
       " 'situation': 752,\n",
       " 'eating': 753,\n",
       " 'million': 754,\n",
       " 'needed': 755,\n",
       " 'learn': 756,\n",
       " 'joke': 757,\n",
       " 'follow': 758,\n",
       " 'living': 759,\n",
       " 'themselves': 760,\n",
       " 'known': 761,\n",
       " 'instagram': 762,\n",
       " 'blood': 763,\n",
       " 'l': 764,\n",
       " 'pick': 765,\n",
       " 'inside': 766,\n",
       " 'totally': 767,\n",
       " 'star': 768,\n",
       " 'evidence': 769,\n",
       " 'common': 770,\n",
       " 'discussion': 771,\n",
       " 'beautiful': 772,\n",
       " 'png': 773,\n",
       " 'album': 774,\n",
       " 'personal': 775,\n",
       " 'serious': 776,\n",
       " 'bought': 777,\n",
       " 'interested': 778,\n",
       " '16': 779,\n",
       " 'currently': 780,\n",
       " 'build': 781,\n",
       " 'plan': 782,\n",
       " 'gave': 783,\n",
       " 'effect': 784,\n",
       " 'killed': 785,\n",
       " 'clear': 786,\n",
       " 'gay': 787,\n",
       " 'pkk': 788,\n",
       " 'character': 789,\n",
       " 'asking': 790,\n",
       " 'worked': 791,\n",
       " 'fast': 792,\n",
       " 'pass': 793,\n",
       " 'relationship': 794,\n",
       " 'local': 795,\n",
       " 'office': 796,\n",
       " 'air': 797,\n",
       " 'league': 798,\n",
       " 'national': 799,\n",
       " 'political': 800,\n",
       " 'several': 801,\n",
       " 'size': 802,\n",
       " 'main': 803,\n",
       " 'gun': 804,\n",
       " 'process': 805,\n",
       " 'brain': 806,\n",
       " 'report': 807,\n",
       " 'difference': 808,\n",
       " 'above': 809,\n",
       " 'sort': 810,\n",
       " 'explain': 811,\n",
       " 'fair': 812,\n",
       " 'major': 813,\n",
       " 'science': 814,\n",
       " 'term': 815,\n",
       " 'certain': 816,\n",
       " 'lose': 817,\n",
       " 'film': 818,\n",
       " 'html': 819,\n",
       " 'boy': 820,\n",
       " '11088': 821,\n",
       " 'eyes': 822,\n",
       " 'strong': 823,\n",
       " 'response': 824,\n",
       " 'anymore': 825,\n",
       " 'himself': 826,\n",
       " 'glad': 827,\n",
       " 'problems': 828,\n",
       " 'obviously': 829,\n",
       " 'associated': 830,\n",
       " 'worst': 831,\n",
       " 'morning': 832,\n",
       " 'knows': 833,\n",
       " 'allowed': 834,\n",
       " 'sell': 835,\n",
       " 'trade': 836,\n",
       " 'congress': 837,\n",
       " 'positive': 838,\n",
       " 'alone': 839,\n",
       " 'fat': 840,\n",
       " 'hold': 841,\n",
       " 'attack': 842,\n",
       " 'card': 843,\n",
       " 'action': 844,\n",
       " 'form': 845,\n",
       " 'credit': 846,\n",
       " 'kinda': 847,\n",
       " 'hour': 848,\n",
       " 'recommend': 849,\n",
       " 'mod': 850,\n",
       " 'starting': 851,\n",
       " 'service': 852,\n",
       " 'decided': 853,\n",
       " '99': 854,\n",
       " 'ass': 855,\n",
       " 'piece': 856,\n",
       " 'special': 857,\n",
       " 'personally': 858,\n",
       " 'effects': 859,\n",
       " 'worse': 860,\n",
       " 'paste': 861,\n",
       " 'average': 862,\n",
       " 'market': 863,\n",
       " 'seriously': 864,\n",
       " 'appreciate': 865,\n",
       " 'heart': 866,\n",
       " 'y': 867,\n",
       " 'dark': 868,\n",
       " 'earth': 869,\n",
       " 'key': 870,\n",
       " 'drop': 871,\n",
       " '160': 872,\n",
       " 'til': 873,\n",
       " 'hi': 874,\n",
       " 'skin': 875,\n",
       " 'event': 876,\n",
       " 'including': 877,\n",
       " 'product': 878,\n",
       " 'wikipedia': 879,\n",
       " 'thoughts': 880,\n",
       " 'simple': 881,\n",
       " 'review': 882,\n",
       " 'nature': 883,\n",
       " 'united': 884,\n",
       " 'luck': 885,\n",
       " 'hands': 886,\n",
       " 'bed': 887,\n",
       " '40': 888,\n",
       " 'ideas': 889,\n",
       " 'send': 890,\n",
       " 'gone': 891,\n",
       " 'value': 892,\n",
       " 'view': 893,\n",
       " 'posting': 894,\n",
       " 'safe': 895,\n",
       " 'extra': 896,\n",
       " 'gives': 897,\n",
       " 'hair': 898,\n",
       " 'attention': 899,\n",
       " 'according': 900,\n",
       " 'drive': 901,\n",
       " 'meme': 902,\n",
       " 'gold': 903,\n",
       " '13': 904,\n",
       " 'info': 905,\n",
       " 'drug': 906,\n",
       " 'mother': 907,\n",
       " 'green': 908,\n",
       " 'except': 909,\n",
       " 'itself': 910,\n",
       " 'k': 911,\n",
       " 'rights': 912,\n",
       " 'race': 913,\n",
       " 'spend': 914,\n",
       " 'ball': 915,\n",
       " 'provide': 916,\n",
       " 'option': 917,\n",
       " 'america': 918,\n",
       " 'iamhoneydill': 919,\n",
       " 'cost': 920,\n",
       " 'app': 921,\n",
       " 'son': 922,\n",
       " 'academic': 923,\n",
       " 'mostly': 924,\n",
       " 'stand': 925,\n",
       " 'force': 926,\n",
       " 'decision': 927,\n",
       " 'avoid': 928,\n",
       " 'available': 929,\n",
       " 'potential': 930,\n",
       " 'create': 931,\n",
       " 'final': 932,\n",
       " 'members': 933,\n",
       " 'energy': 934,\n",
       " 'meant': 935,\n",
       " 'website': 936,\n",
       " '14': 937,\n",
       " 'videos': 938,\n",
       " 'shouldn': 939,\n",
       " 'sad': 940,\n",
       " 'walk': 941,\n",
       " 'fake': 942,\n",
       " 'quick': 943,\n",
       " 'total': 944,\n",
       " 'fit': 945,\n",
       " 'proof': 946,\n",
       " 'lower': 947,\n",
       " 'straight': 948,\n",
       " '24': 949,\n",
       " 'wonder': 950,\n",
       " 'holy': 951,\n",
       " 'ready': 952,\n",
       " 'offers': 953,\n",
       " 'feels': 954,\n",
       " 'king': 955,\n",
       " 'art': 956,\n",
       " 'campaign': 957,\n",
       " 'user': 958,\n",
       " '17': 959,\n",
       " 'countries': 960,\n",
       " 'users': 961,\n",
       " 'compared': 962,\n",
       " 'majority': 963,\n",
       " 'recent': 964,\n",
       " 'park': 965,\n",
       " 'access': 966,\n",
       " 'allow': 967,\n",
       " 'note': 968,\n",
       " 'language': 969,\n",
       " 'clinton': 970,\n",
       " 'correct': 971,\n",
       " 'search': 972,\n",
       " 'south': 973,\n",
       " 'spent': 974,\n",
       " 'changed': 975,\n",
       " 'choice': 976,\n",
       " 'pro': 977,\n",
       " 'damage': 978,\n",
       " 'lead': 979,\n",
       " 'figure': 980,\n",
       " 'paper': 981,\n",
       " 'following': 982,\n",
       " 'plus': 983,\n",
       " 'cannot': 984,\n",
       " 'co': 985,\n",
       " 'turned': 986,\n",
       " '8201': 987,\n",
       " 'multiple': 988,\n",
       " 'four': 989,\n",
       " 'welcome': 990,\n",
       " 'further': 991,\n",
       " 'ability': 992,\n",
       " 'legal': 993,\n",
       " 'official': 994,\n",
       " 'popular': 995,\n",
       " 'expect': 996,\n",
       " 'offer': 997,\n",
       " 'lives': 998,\n",
       " 'upvote': 999,\n",
       " 'added': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightsHistory(callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.log_weights(0)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.log_weights(epoch)\n",
    "        \n",
    "    def log_weights(self, step):\n",
    "        experiment.log_histogram_3d(self.model.get_layer('LSTM_layer').get_weights()[0], name='lstm_weights',\n",
    "                                   step=step)  \n",
    "        experiment.log_histogram_3d(model.get_layer('embeddings_layer').get_weights()[0], \n",
    "                            name='embedding_weights',\n",
    "                           step=step)\n",
    "        experiment.log_histogram_3d(model.get_layer('numerical_dense_layer').get_weights()[0], \n",
    "                                    name='numerical_dense_weights',\n",
    "                                   step=step)\n",
    "        experiment.log_histogram_3d(model.get_layer('sparse_feat_dense_layer').get_weights()[0], \n",
    "                            name='sparse_dense_weights',\n",
    "                           step=step)\n",
    "        \n",
    "        \n",
    "class FreezeLayer(callbacks.Callback):\n",
    "    def __init__(self, logs={}, patience=5, layer='embeddings_layer', verbose=1, set_to=False):\n",
    "        super(FreezeLayer, self).__init__()\n",
    "        self.freeze_epoch = patience\n",
    "        self.freeze_layer = layer\n",
    "        self.verbose = verbose\n",
    "        self.set_to = set_to\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        if epoch == self.freeze_epoch:\n",
    "            layer = model.get_layer(self.freeze_layer)\n",
    "            old_value = layer.trainable\n",
    "            layer.trainable = self.set_to\n",
    "            if self.verbose:\n",
    "                print(\"Setting %s layer from %s to trainable=%s...\" % (layer.name, old_value,\n",
    "                                                               model.get_layer(self.freeze_layer).trainable))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, \n",
    "                x_train, y_train, x_test, y_test, \n",
    "                batch_size, epochs, class_weight, start_epoch=0, workers=4,\n",
    "                callback_list = [],\n",
    "                model_path='/tmp/model'):\n",
    "    print('Train...')\n",
    "    experiment.log_parameter('class_weight', class_weight.values())\n",
    "    experiment.log_parameter('callbacks', callbacks)\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs, initial_epoch=start_epoch, \n",
    "              class_weight=class_weight,\n",
    "              validation_data=[x_test, y_test],\n",
    "#               validation_split=0.3,\n",
    "                       workers=workers,\n",
    "            callbacks = [\n",
    "                callbacks.ModelCheckpoint(filepath='%s_best' % model_path, verbose=1, save_best_only=True),\n",
    "                callbacks.EarlyStopping(patience=5), *callback_list\n",
    "            ])\n",
    "    model.save(model_path)\n",
    "    experiment.log_parameter('model_path', model_path)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 111375 samples, validate on 31863 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: Set model graph ignored; already set and not overwrite\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      " 30592/111375 [=======>......................] - ETA: 3:40 - loss: 0.6312 - binary_accuracy: 0.7075 - f1_m: 0.1291 - precision_m: 0.0753 - recall_m: 0.5474"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "freeze_layer = FreezeLayer(hyperparams['freeze_patience'], set_to=not hyperparams['trainable_embeddings'])\n",
    "weights_history = WeightsHistory()\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=hyperparams['reduce_lr_factor'],\n",
    "                          patience=hyperparams['reduce_lr_patience'], min_lr=0.000001, verbose=1)\n",
    "\n",
    "history = train_model(model, x_train, y_train, x_test, y_test,\n",
    "           epochs=15, batch_size=hyperparams['batch_size'],\n",
    "                      class_weight={0:0.5, 1:12}, \n",
    "                      callback_list = [freeze_layer, weights_history, reduce_lr],\n",
    "                      model_path='models/lstm_plus7', workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.5279915e-02,  4.7590178e-01, -4.8616432e-02, ...,\n",
       "          4.0647617e-01,  4.0538330e-02,  2.4765652e-01],\n",
       "        [ 9.5151998e-02,  3.7024000e-01,  5.4290998e-01, ...,\n",
       "         -5.1082999e-01,  4.6877000e-01,  3.4882000e-01],\n",
       "        [-3.9621000e-04,  4.5670000e-01,  3.3890000e-01, ...,\n",
       "         -4.2910001e-01,  1.0746000e+00, -3.6550000e-01],\n",
       "        ...,\n",
       "        [-7.0870000e-01, -6.0604000e-01, -4.4224000e-01, ...,\n",
       "         -8.3099999e-02,  5.5158000e-02,  2.5769001e-01],\n",
       "        [ 3.9036000e-01, -5.7444000e-01, -6.5509999e-01, ...,\n",
       "         -5.7653999e-01,  9.5931001e-02,  1.0454000e+00],\n",
       "        [-4.4363201e-01,  1.0039248e-01,  6.3836761e-02, ...,\n",
       "          2.6907095e-01,  7.8583762e-02, -3.8874221e-01]], dtype=float32),\n",
       " array([[ 0.05118892,  0.01830989, -0.0848855 , ...,  0.14976056,\n",
       "          0.04705308, -0.06339781],\n",
       "        [ 0.00329915, -0.05218123,  0.13718773, ..., -0.03093715,\n",
       "         -0.07668457, -0.17208682],\n",
       "        [ 0.01844802,  0.02189548, -0.00731012, ..., -0.05627529,\n",
       "         -0.18513116,  0.19061656],\n",
       "        ...,\n",
       "        [ 0.04387943,  0.08289824,  0.05411699, ...,  0.07897766,\n",
       "         -0.12853241,  0.1670007 ],\n",
       "        [ 0.0991696 , -0.04590872,  0.15341274, ...,  0.15449671,\n",
       "          0.1037897 , -0.17932229],\n",
       "        [ 0.0085191 , -0.10500465, -0.0686437 , ...,  0.06574072,\n",
       "         -0.0785232 ,  0.03170392]], dtype=float32),\n",
       " array([[ 0.07453418,  0.12480614,  0.17416482, ...,  0.02458899,\n",
       "          0.00751785, -0.10959871],\n",
       "        [-0.07993217, -0.0818868 ,  0.04584314, ...,  0.23799859,\n",
       "          0.04569918,  0.0681384 ],\n",
       "        [-0.03585472,  0.04860685,  0.23403557, ...,  0.03499698,\n",
       "          0.08423676, -0.07567842],\n",
       "        ...,\n",
       "        [-0.05759744, -0.09783065,  0.0588273 , ..., -0.27038258,\n",
       "         -0.08521509, -0.13021852],\n",
       "        [-0.3172284 , -0.13776621,  0.08145676, ...,  0.15626734,\n",
       "          0.15197396,  0.03287846],\n",
       "        [-0.13182122, -0.08663318, -0.09979406, ...,  0.02887152,\n",
       "         -0.06464721,  0.08106919]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[ 0.37094146],\n",
       "        [ 0.49199992],\n",
       "        [-0.56553584],\n",
       "        [ 0.59059805],\n",
       "        [ 0.5729721 ],\n",
       "        [-0.23792958],\n",
       "        [-0.44875318],\n",
       "        [-0.1319989 ],\n",
       "        [ 0.4291708 ],\n",
       "        [ 0.25332344],\n",
       "        [ 0.06117398]], dtype=float32),\n",
       " array([0.], dtype=float32),\n",
       " array([[-0.01288342],\n",
       "        [-0.06914698],\n",
       "        [ 0.03442031],\n",
       "        [ 0.17096347],\n",
       "        [-0.1499336 ],\n",
       "        [ 0.12004247],\n",
       "        [-0.13439816],\n",
       "        [-0.11461184],\n",
       "        [ 0.06042401],\n",
       "        [ 0.16935137],\n",
       "        [ 0.15654448],\n",
       "        [-0.172692  ],\n",
       "        [ 0.02534354],\n",
       "        [ 0.0973188 ],\n",
       "        [ 0.17279512],\n",
       "        [-0.11394954],\n",
       "        [ 0.07916525],\n",
       "        [ 0.03863297],\n",
       "        [-0.17468241],\n",
       "        [ 0.15209725],\n",
       "        [-0.10351463],\n",
       "        [ 0.02917166],\n",
       "        [ 0.12345469],\n",
       "        [ 0.13695502],\n",
       "        [-0.11244771],\n",
       "        [-0.09101442],\n",
       "        [-0.04528591],\n",
       "        [ 0.18233821],\n",
       "        [ 0.15982676],\n",
       "        [-0.08461174],\n",
       "        [ 0.15710184],\n",
       "        [ 0.13434225],\n",
       "        [ 0.15880448],\n",
       "        [-0.02636094],\n",
       "        [ 0.11199388],\n",
       "        [-0.15960607],\n",
       "        [ 0.1486251 ],\n",
       "        [-0.12936829],\n",
       "        [-0.07162622],\n",
       "        [ 0.06310493],\n",
       "        [ 0.10237396],\n",
       "        [-0.04671401],\n",
       "        [ 0.1394844 ],\n",
       "        [-0.05927807],\n",
       "        [-0.09370953],\n",
       "        [ 0.09022471],\n",
       "        [-0.09304192],\n",
       "        [ 0.1567123 ],\n",
       "        [-0.01688628],\n",
       "        [-0.0386668 ],\n",
       "        [-0.1058961 ],\n",
       "        [-0.03420123],\n",
       "        [-0.1481019 ],\n",
       "        [-0.01600882],\n",
       "        [ 0.07232621],\n",
       "        [ 0.04418449],\n",
       "        [ 0.1344336 ],\n",
       "        [ 0.12782347],\n",
       "        [-0.14241338],\n",
       "        [ 0.09130433],\n",
       "        [-0.09618729],\n",
       "        [ 0.11456919],\n",
       "        [-0.06625548],\n",
       "        [-0.0815348 ],\n",
       "        [-0.02674462],\n",
       "        [ 0.01506963],\n",
       "        [ 0.16003966],\n",
       "        [-0.02838871],\n",
       "        [-0.01456478],\n",
       "        [-0.04822038],\n",
       "        [-0.08034745],\n",
       "        [-0.02718483],\n",
       "        [-0.10809406],\n",
       "        [ 0.04401526],\n",
       "        [ 0.03234585],\n",
       "        [-0.14131945],\n",
       "        [ 0.10579297],\n",
       "        [ 0.09716156],\n",
       "        [ 0.05646235],\n",
       "        [ 0.01029581],\n",
       "        [-0.0884954 ],\n",
       "        [-0.13239996],\n",
       "        [-0.0465893 ],\n",
       "        [ 0.05972934],\n",
       "        [ 0.1663155 ],\n",
       "        [ 0.0072321 ],\n",
       "        [-0.11402772],\n",
       "        [-0.02397403],\n",
       "        [ 0.1690414 ],\n",
       "        [-0.03905198],\n",
       "        [-0.03020157],\n",
       "        [-0.1413488 ],\n",
       "        [-0.1757709 ],\n",
       "        [ 0.04925075],\n",
       "        [-0.13186808],\n",
       "        [-0.01748645],\n",
       "        [ 0.17858827],\n",
       "        [ 0.11609527],\n",
       "        [-0.1527115 ],\n",
       "        [ 0.10822079],\n",
       "        [ 0.04713111],\n",
       "        [ 0.00323552],\n",
       "        [ 0.06817284],\n",
       "        [-0.07289954],\n",
       "        [-0.10009059],\n",
       "        [-0.09847327],\n",
       "        [ 0.0756712 ],\n",
       "        [ 0.08463567],\n",
       "        [ 0.00973497],\n",
       "        [ 0.06649871],\n",
       "        [ 0.06809357],\n",
       "        [ 0.05177671],\n",
       "        [ 0.14709023],\n",
       "        [ 0.09434262],\n",
       "        [ 0.15210706],\n",
       "        [-0.06853779],\n",
       "        [ 0.11649156],\n",
       "        [-0.15567291],\n",
       "        [ 0.08546665],\n",
       "        [-0.08461361],\n",
       "        [ 0.02931052],\n",
       "        [-0.13530113],\n",
       "        [ 0.1149959 ],\n",
       "        [-0.09834481],\n",
       "        [-0.00312182],\n",
       "        [-0.09585077],\n",
       "        [-0.11263832],\n",
       "        [-0.06925663],\n",
       "        [-0.02755456],\n",
       "        [ 0.10941413],\n",
       "        [-0.09607302],\n",
       "        [ 0.1438491 ],\n",
       "        [ 0.02229238],\n",
       "        [-0.13799545],\n",
       "        [-0.10816231],\n",
       "        [-0.01954147],\n",
       "        [ 0.12259337],\n",
       "        [ 0.148386  ],\n",
       "        [ 0.15831125],\n",
       "        [-0.03972603],\n",
       "        [ 0.08060971],\n",
       "        [-0.11860041],\n",
       "        [ 0.15712708],\n",
       "        [ 0.12356949],\n",
       "        [-0.07121854],\n",
       "        [ 0.00311521],\n",
       "        [ 0.07559159],\n",
       "        [ 0.04115857],\n",
       "        [ 0.13217857],\n",
       "        [ 0.11782354],\n",
       "        [ 0.12994984],\n",
       "        [ 0.14411297],\n",
       "        [ 0.12399307],\n",
       "        [ 0.07464892],\n",
       "        [-0.06668293],\n",
       "        [-0.14123784],\n",
       "        [ 0.03783596],\n",
       "        [ 0.12026447],\n",
       "        [ 0.02459763],\n",
       "        [ 0.07270265],\n",
       "        [ 0.16530573],\n",
       "        [ 0.10318941],\n",
       "        [ 0.12784198],\n",
       "        [-0.13307418],\n",
       "        [-0.12036051],\n",
       "        [-0.06411798],\n",
       "        [-0.04414919],\n",
       "        [-0.01385495],\n",
       "        [ 0.106673  ],\n",
       "        [ 0.16241121],\n",
       "        [-0.06831423],\n",
       "        [ 0.13635796],\n",
       "        [ 0.13676915],\n",
       "        [ 0.10989729],\n",
       "        [-0.17696986],\n",
       "        [-0.0627553 ],\n",
       "        [-0.10097318],\n",
       "        [ 0.13735461],\n",
       "        [-0.12529737]], dtype=float32),\n",
       " array([0.], dtype=float32),\n",
       " array([[-0.16837412],\n",
       "        [ 0.03350872],\n",
       "        [ 0.28554004],\n",
       "        [ 0.30946112],\n",
       "        [ 0.434861  ],\n",
       "        [ 0.15532547],\n",
       "        [-0.3073992 ],\n",
       "        [ 0.36874515],\n",
       "        [ 0.12435603],\n",
       "        [-0.45473167],\n",
       "        [-0.15130258],\n",
       "        [ 0.1621173 ],\n",
       "        [-0.23174852],\n",
       "        [ 0.10658967],\n",
       "        [ 0.4470542 ],\n",
       "        [ 0.47189516],\n",
       "        [-0.26151612],\n",
       "        [-0.55310786]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='models/lstm_plus4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies = {\n",
    "    'f1_m': f1_m,\n",
    "    'precision_m': precision_m,\n",
    "    'recall_m': recall_m\n",
    "}\n",
    "# model = load_model('models/lstm_plus1', custom_objects=dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(predictions.flatten()).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(predictions>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(predictions<0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tokens(row):\n",
    "    tokens = []\n",
    "    if row.tokenized_text:\n",
    "        tokens += row.tokenized_text\n",
    "    if row.tokenized_title:\n",
    "        tokens += row.tokenized_title\n",
    "    return tokens\n",
    "writings_df['all_tokens'] = writings_df.apply (lambda row: merge_tokens(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: include the title\n",
    "def extract_emotions(tokens, emotion, relative=True):\n",
    "    if not tokens:\n",
    "        return None\n",
    "    emotion_words = [t for t in tokens \n",
    "                     if t in nrc_lexicon[emotion]]\n",
    "    if relative:\n",
    "        return len(emotion_words) / len(tokens)\n",
    "    else:\n",
    "        return len(emotion_words)\n",
    "    \n",
    "    return encoded_emotions\n",
    "\n",
    "from functools import partial\n",
    "for emotion in emotions:\n",
    "    writings_df[emotion] = writings_df['all_tokens'].apply(partial(extract_emotions, emotion=emotion, \n",
    "                                                                   relative=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['pronouns'] = writings_df['all_tokens'].apply(partial(encode_pronouns, relative=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>text_len</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.104269</td>\n",
       "      <td>0.011986</td>\n",
       "      <td>0.020197</td>\n",
       "      <td>0.031982</td>\n",
       "      <td>0.031271</td>\n",
       "      <td>0.019335</td>\n",
       "      <td>0.040782</td>\n",
       "      <td>0.023853</td>\n",
       "      <td>0.023621</td>\n",
       "      <td>0.032969</td>\n",
       "      <td>0.020421</td>\n",
       "      <td>0.023590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pronouns</th>\n",
       "      <td>0.104269</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.636745</td>\n",
       "      <td>0.449384</td>\n",
       "      <td>0.567496</td>\n",
       "      <td>0.452098</td>\n",
       "      <td>0.464899</td>\n",
       "      <td>0.548570</td>\n",
       "      <td>0.513029</td>\n",
       "      <td>0.571303</td>\n",
       "      <td>0.524614</td>\n",
       "      <td>0.461328</td>\n",
       "      <td>0.538335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_len</th>\n",
       "      <td>0.011986</td>\n",
       "      <td>0.636745</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.708853</td>\n",
       "      <td>0.791715</td>\n",
       "      <td>0.642980</td>\n",
       "      <td>0.738146</td>\n",
       "      <td>0.728836</td>\n",
       "      <td>0.823974</td>\n",
       "      <td>0.867609</td>\n",
       "      <td>0.723653</td>\n",
       "      <td>0.650420</td>\n",
       "      <td>0.834939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.020197</td>\n",
       "      <td>0.449384</td>\n",
       "      <td>0.708853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.643459</td>\n",
       "      <td>0.762591</td>\n",
       "      <td>0.858442</td>\n",
       "      <td>0.564162</td>\n",
       "      <td>0.835345</td>\n",
       "      <td>0.681573</td>\n",
       "      <td>0.774846</td>\n",
       "      <td>0.583704</td>\n",
       "      <td>0.671042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anticipation</th>\n",
       "      <td>0.031982</td>\n",
       "      <td>0.567496</td>\n",
       "      <td>0.791715</td>\n",
       "      <td>0.643459</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.573916</td>\n",
       "      <td>0.668326</td>\n",
       "      <td>0.834784</td>\n",
       "      <td>0.684882</td>\n",
       "      <td>0.849864</td>\n",
       "      <td>0.668269</td>\n",
       "      <td>0.727331</td>\n",
       "      <td>0.818885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.031271</td>\n",
       "      <td>0.452098</td>\n",
       "      <td>0.642980</td>\n",
       "      <td>0.762591</td>\n",
       "      <td>0.573916</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.729799</td>\n",
       "      <td>0.526733</td>\n",
       "      <td>0.765865</td>\n",
       "      <td>0.603013</td>\n",
       "      <td>0.737717</td>\n",
       "      <td>0.540439</td>\n",
       "      <td>0.589641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.019335</td>\n",
       "      <td>0.464899</td>\n",
       "      <td>0.738146</td>\n",
       "      <td>0.858442</td>\n",
       "      <td>0.668326</td>\n",
       "      <td>0.729799</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.570632</td>\n",
       "      <td>0.862778</td>\n",
       "      <td>0.706676</td>\n",
       "      <td>0.824782</td>\n",
       "      <td>0.569688</td>\n",
       "      <td>0.687232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.040782</td>\n",
       "      <td>0.548570</td>\n",
       "      <td>0.728836</td>\n",
       "      <td>0.564162</td>\n",
       "      <td>0.834784</td>\n",
       "      <td>0.526733</td>\n",
       "      <td>0.570632</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.604964</td>\n",
       "      <td>0.850961</td>\n",
       "      <td>0.603296</td>\n",
       "      <td>0.722710</td>\n",
       "      <td>0.811529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0.023853</td>\n",
       "      <td>0.513029</td>\n",
       "      <td>0.823974</td>\n",
       "      <td>0.835345</td>\n",
       "      <td>0.684882</td>\n",
       "      <td>0.765865</td>\n",
       "      <td>0.862778</td>\n",
       "      <td>0.604964</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.735431</td>\n",
       "      <td>0.840379</td>\n",
       "      <td>0.597634</td>\n",
       "      <td>0.706808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>0.023621</td>\n",
       "      <td>0.571303</td>\n",
       "      <td>0.867609</td>\n",
       "      <td>0.681573</td>\n",
       "      <td>0.849864</td>\n",
       "      <td>0.603013</td>\n",
       "      <td>0.706676</td>\n",
       "      <td>0.850961</td>\n",
       "      <td>0.735431</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.702751</td>\n",
       "      <td>0.678778</td>\n",
       "      <td>0.916526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.032969</td>\n",
       "      <td>0.524614</td>\n",
       "      <td>0.723653</td>\n",
       "      <td>0.774846</td>\n",
       "      <td>0.668269</td>\n",
       "      <td>0.737717</td>\n",
       "      <td>0.824782</td>\n",
       "      <td>0.603296</td>\n",
       "      <td>0.840379</td>\n",
       "      <td>0.702751</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.584816</td>\n",
       "      <td>0.665750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.020421</td>\n",
       "      <td>0.461328</td>\n",
       "      <td>0.650420</td>\n",
       "      <td>0.583704</td>\n",
       "      <td>0.727331</td>\n",
       "      <td>0.540439</td>\n",
       "      <td>0.569688</td>\n",
       "      <td>0.722710</td>\n",
       "      <td>0.597634</td>\n",
       "      <td>0.678778</td>\n",
       "      <td>0.584816</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.660681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust</th>\n",
       "      <td>0.023590</td>\n",
       "      <td>0.538335</td>\n",
       "      <td>0.834939</td>\n",
       "      <td>0.671042</td>\n",
       "      <td>0.818885</td>\n",
       "      <td>0.589641</td>\n",
       "      <td>0.687232</td>\n",
       "      <td>0.811529</td>\n",
       "      <td>0.706808</td>\n",
       "      <td>0.916526</td>\n",
       "      <td>0.665750</td>\n",
       "      <td>0.660681</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 label  pronouns  text_len     anger  anticipation   disgust  \\\n",
       "label         1.000000  0.104269  0.011986  0.020197      0.031982  0.031271   \n",
       "pronouns      0.104269  1.000000  0.636745  0.449384      0.567496  0.452098   \n",
       "text_len      0.011986  0.636745  1.000000  0.708853      0.791715  0.642980   \n",
       "anger         0.020197  0.449384  0.708853  1.000000      0.643459  0.762591   \n",
       "anticipation  0.031982  0.567496  0.791715  0.643459      1.000000  0.573916   \n",
       "disgust       0.031271  0.452098  0.642980  0.762591      0.573916  1.000000   \n",
       "fear          0.019335  0.464899  0.738146  0.858442      0.668326  0.729799   \n",
       "joy           0.040782  0.548570  0.728836  0.564162      0.834784  0.526733   \n",
       "negative      0.023853  0.513029  0.823974  0.835345      0.684882  0.765865   \n",
       "positive      0.023621  0.571303  0.867609  0.681573      0.849864  0.603013   \n",
       "sadness       0.032969  0.524614  0.723653  0.774846      0.668269  0.737717   \n",
       "surprise      0.020421  0.461328  0.650420  0.583704      0.727331  0.540439   \n",
       "trust         0.023590  0.538335  0.834939  0.671042      0.818885  0.589641   \n",
       "\n",
       "                  fear       joy  negative  positive   sadness  surprise  \\\n",
       "label         0.019335  0.040782  0.023853  0.023621  0.032969  0.020421   \n",
       "pronouns      0.464899  0.548570  0.513029  0.571303  0.524614  0.461328   \n",
       "text_len      0.738146  0.728836  0.823974  0.867609  0.723653  0.650420   \n",
       "anger         0.858442  0.564162  0.835345  0.681573  0.774846  0.583704   \n",
       "anticipation  0.668326  0.834784  0.684882  0.849864  0.668269  0.727331   \n",
       "disgust       0.729799  0.526733  0.765865  0.603013  0.737717  0.540439   \n",
       "fear          1.000000  0.570632  0.862778  0.706676  0.824782  0.569688   \n",
       "joy           0.570632  1.000000  0.604964  0.850961  0.603296  0.722710   \n",
       "negative      0.862778  0.604964  1.000000  0.735431  0.840379  0.597634   \n",
       "positive      0.706676  0.850961  0.735431  1.000000  0.702751  0.678778   \n",
       "sadness       0.824782  0.603296  0.840379  0.702751  1.000000  0.584816   \n",
       "surprise      0.569688  0.722710  0.597634  0.678778  0.584816  1.000000   \n",
       "trust         0.687232  0.811529  0.706808  0.916526  0.665750  0.660681   \n",
       "\n",
       "                 trust  \n",
       "label         0.023590  \n",
       "pronouns      0.538335  \n",
       "text_len      0.834939  \n",
       "anger         0.671042  \n",
       "anticipation  0.818885  \n",
       "disgust       0.589641  \n",
       "fear          0.687232  \n",
       "joy           0.811529  \n",
       "negative      0.706808  \n",
       "positive      0.916526  \n",
       "sadness       0.665750  \n",
       "surprise      0.660681  \n",
       "trust         1.000000  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len'] + emotions].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pronouns</th>\n",
       "      <th>text_len</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.868213</td>\n",
       "      <td>32.031615</td>\n",
       "      <td>0.386069</td>\n",
       "      <td>0.58984</td>\n",
       "      <td>0.263683</td>\n",
       "      <td>0.478014</td>\n",
       "      <td>0.479908</td>\n",
       "      <td>0.818800</td>\n",
       "      <td>1.280788</td>\n",
       "      <td>0.385315</td>\n",
       "      <td>0.284790</td>\n",
       "      <td>0.830560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.484271</td>\n",
       "      <td>36.398389</td>\n",
       "      <td>0.529232</td>\n",
       "      <td>0.86985</td>\n",
       "      <td>0.416203</td>\n",
       "      <td>0.654371</td>\n",
       "      <td>0.769766</td>\n",
       "      <td>1.152422</td>\n",
       "      <td>1.717428</td>\n",
       "      <td>0.627088</td>\n",
       "      <td>0.375418</td>\n",
       "      <td>1.128341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pronouns   text_len     anger  anticipation   disgust      fear  \\\n",
       "label                                                                    \n",
       "0      0.868213  32.031615  0.386069       0.58984  0.263683  0.478014   \n",
       "1      2.484271  36.398389  0.529232       0.86985  0.416203  0.654371   \n",
       "\n",
       "            joy  negative  positive   sadness  surprise     trust  \n",
       "label                                                              \n",
       "0      0.479908  0.818800  1.280788  0.385315  0.284790  0.830560  \n",
       "1      0.769766  1.152422  1.717428  0.627088  0.375418  1.128341  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len'] + emotions].groupby('label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentAnalyzer, SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.66, 'pos': 0.34, 'compound': 0.5574}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid.polarity_scores(\"We are here today happiness is all around\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['neg_vader'] = writings_df.text.apply(lambda t: sid.polarity_scores(t)['neg']\n",
    "                                                 if type(t)==str else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>title_len</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>text_len</th>\n",
       "      <th>emotions</th>\n",
       "      <th>...</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>all_tokens</th>\n",
       "      <th>neg_vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>If anyone could help with which sub to put thi...</td>\n",
       "      <td>2016-08-02 09:22:12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[if, anyone, could, help, with, which, sub, to...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[if, anyone, could, help, with, which, sub, to...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>I'm literally never gonna stop waiting...</td>\n",
       "      <td>2016-08-05 09:35:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, m, literally, never, gonna, stop, waiting]</td>\n",
       "      <td>7.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[i, m, literally, never, gonna, stop, waiting]</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>This is a really interesting study! Makes sens...</td>\n",
       "      <td>2016-08-05 21:36:24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[this, is, a, really, interesting, study, make...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[this, is, a, really, interesting, study, make...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>The only thing Frank is building ...</td>\n",
       "      <td>2016-08-07 23:35:23</td>\n",
       "      <td>... Is hype. Think about it, every time he wor...</td>\n",
       "      <td>0</td>\n",
       "      <td>[the, only, thing, frank, is, building]</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[is, hype, think, about, it, every, time, he, ...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[is, hype, think, about, it, every, time, he, ...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>Mostly always me during this whole charade</td>\n",
       "      <td>2016-08-09 08:39:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[mostly, always, me, during, this, whole, char...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[mostly, always, me, during, this, whole, char...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170693</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 11:29:21</td>\n",
       "      <td>this is my personal experience ,it may not ref...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[this, is, my, personal, experience, it, may, ...</td>\n",
       "      <td>153.0</td>\n",
       "      <td>0.026144</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[this, is, my, personal, experience, it, may, ...</td>\n",
       "      <td>0.089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170694</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 16:17:34</td>\n",
       "      <td>stop looking at 20 million saudis as one entit...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[stop, looking, at, 20, million, saudis, as, o...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[stop, looking, at, 20, million, saudis, as, o...</td>\n",
       "      <td>0.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170695</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 20:00:31</td>\n",
       "      <td>i am aware of stats now and then. i was just s...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[i, am, aware, of, stats, now, and, then, i, w...</td>\n",
       "      <td>198.0</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>[i, am, aware, of, stats, now, and, then, i, w...</td>\n",
       "      <td>0.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170696</th>\n",
       "      <td>subject217</td>\n",
       "      <td>WHAT DID YOU SAY TO ME?</td>\n",
       "      <td>2018-08-20 10:54:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[what, did, you, say, to, me]</td>\n",
       "      <td>6.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[what, did, you, say, to, me]</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170697</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-20 12:07:44</td>\n",
       "      <td>me smellz fish,me find no fish!...what the fuc...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[me, smellz, fish, me, find, no, fish, what, t...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[me, smellz, fish, me, find, no, fish, what, t...</td>\n",
       "      <td>0.484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170698 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            subject                                              title  \\\n",
       "0       subject8292  If anyone could help with which sub to put thi...   \n",
       "1       subject8292          I'm literally never gonna stop waiting...   \n",
       "2       subject8292  This is a really interesting study! Makes sens...   \n",
       "3       subject8292               The only thing Frank is building ...   \n",
       "4       subject8292         Mostly always me during this whole charade   \n",
       "...             ...                                                ...   \n",
       "170693   subject217                                                NaN   \n",
       "170694   subject217                                                NaN   \n",
       "170695   subject217                                                NaN   \n",
       "170696   subject217                            WHAT DID YOU SAY TO ME?   \n",
       "170697   subject217                                                NaN   \n",
       "\n",
       "                       date  \\\n",
       "0       2016-08-02 09:22:12   \n",
       "1       2016-08-05 09:35:55   \n",
       "2       2016-08-05 21:36:24   \n",
       "3       2016-08-07 23:35:23   \n",
       "4       2016-08-09 08:39:41   \n",
       "...                     ...   \n",
       "170693  2018-08-19 11:29:21   \n",
       "170694  2018-08-19 16:17:34   \n",
       "170695  2018-08-19 20:00:31   \n",
       "170696  2018-08-20 10:54:11   \n",
       "170697  2018-08-20 12:07:44   \n",
       "\n",
       "                                                     text  label  \\\n",
       "0                                                     NaN      0   \n",
       "1                                                     NaN      0   \n",
       "2                                                     NaN      0   \n",
       "3       ... Is hype. Think about it, every time he wor...      0   \n",
       "4                                                     NaN      0   \n",
       "...                                                   ...    ...   \n",
       "170693  this is my personal experience ,it may not ref...      0   \n",
       "170694  stop looking at 20 million saudis as one entit...      0   \n",
       "170695  i am aware of stats now and then. i was just s...      0   \n",
       "170696                                                NaN      0   \n",
       "170697  me smellz fish,me find no fish!...what the fuc...      0   \n",
       "\n",
       "                                          tokenized_title  title_len  \\\n",
       "0       [if, anyone, could, help, with, which, sub, to...       11.0   \n",
       "1          [i, m, literally, never, gonna, stop, waiting]        7.0   \n",
       "2       [this, is, a, really, interesting, study, make...        9.0   \n",
       "3                 [the, only, thing, frank, is, building]        6.0   \n",
       "4       [mostly, always, me, during, this, whole, char...        7.0   \n",
       "...                                                   ...        ...   \n",
       "170693                                               None        NaN   \n",
       "170694                                               None        NaN   \n",
       "170695                                               None        NaN   \n",
       "170696                      [what, did, you, say, to, me]        6.0   \n",
       "170697                                               None        NaN   \n",
       "\n",
       "                                           tokenized_text  text_len  emotions  \\\n",
       "0                                                    None       NaN       NaN   \n",
       "1                                                    None       NaN       NaN   \n",
       "2                                                    None       NaN       NaN   \n",
       "3       [is, hype, think, about, it, every, time, he, ...      26.0  0.000000   \n",
       "4                                                    None       NaN       NaN   \n",
       "...                                                   ...       ...       ...   \n",
       "170693  [this, is, my, personal, experience, it, may, ...     153.0  0.026144   \n",
       "170694  [stop, looking, at, 20, million, saudis, as, o...      15.0  0.000000   \n",
       "170695  [i, am, aware, of, stats, now, and, then, i, w...     198.0  0.030303   \n",
       "170696                                               None       NaN       NaN   \n",
       "170697  [me, smellz, fish, me, find, no, fish, what, t...      11.0  0.000000   \n",
       "\n",
       "        ...  fear  joy  negative  positive  sadness  surprise  trust  \\\n",
       "0       ...   0.0  0.0       0.0       0.0      0.0       0.0    0.0   \n",
       "1       ...   0.0  0.0       0.0       0.0      0.0       0.0    0.0   \n",
       "2       ...   0.0  0.0       0.0       3.0      0.0       0.0    0.0   \n",
       "3       ...   0.0  0.0       3.0       3.0      0.0       0.0    1.0   \n",
       "4       ...   0.0  0.0       1.0       0.0      0.0       0.0    0.0   \n",
       "...     ...   ...  ...       ...       ...      ...       ...    ...   \n",
       "170693  ...   1.0  1.0       1.0       7.0      0.0       1.0    4.0   \n",
       "170694  ...   1.0  0.0       1.0       0.0      0.0       0.0    0.0   \n",
       "170695  ...   2.0  3.0       4.0      11.0      3.0       0.0    6.0   \n",
       "170696  ...   0.0  0.0       0.0       0.0      0.0       0.0    0.0   \n",
       "170697  ...   0.0  0.0       0.0       0.0      0.0       0.0    0.0   \n",
       "\n",
       "        pronouns                                         all_tokens  neg_vader  \n",
       "0            0.0  [if, anyone, could, help, with, which, sub, to...      0.000  \n",
       "1            1.0     [i, m, literally, never, gonna, stop, waiting]      0.000  \n",
       "2            0.0  [this, is, a, really, interesting, study, make...      0.000  \n",
       "3            0.0  [is, hype, think, about, it, every, time, he, ...      0.000  \n",
       "4            1.0  [mostly, always, me, during, this, whole, char...      0.000  \n",
       "...          ...                                                ...        ...  \n",
       "170693       4.0  [this, is, my, personal, experience, it, may, ...      0.089  \n",
       "170694       0.0  [stop, looking, at, 20, million, saudis, as, o...      0.145  \n",
       "170695      16.0  [i, am, aware, of, stats, now, and, then, i, w...      0.070  \n",
       "170696       1.0                      [what, did, you, say, to, me]      0.000  \n",
       "170697       2.0  [me, smellz, fish, me, find, no, fish, what, t...      0.484  \n",
       "\n",
       "[170698 rows x 23 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['pos_vader'] = writings_df.text.apply(lambda t: sid.polarity_scores(t)['pos']\n",
    "                                                 if type(t)==str else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pronouns</th>\n",
       "      <th>text_len</th>\n",
       "      <th>neg_vader</th>\n",
       "      <th>pos_vader</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.868213</td>\n",
       "      <td>32.031615</td>\n",
       "      <td>0.054259</td>\n",
       "      <td>0.109981</td>\n",
       "      <td>0.386069</td>\n",
       "      <td>0.58984</td>\n",
       "      <td>0.263683</td>\n",
       "      <td>0.478014</td>\n",
       "      <td>0.479908</td>\n",
       "      <td>0.818800</td>\n",
       "      <td>1.280788</td>\n",
       "      <td>0.385315</td>\n",
       "      <td>0.284790</td>\n",
       "      <td>0.830560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.484271</td>\n",
       "      <td>36.398389</td>\n",
       "      <td>0.079191</td>\n",
       "      <td>0.148154</td>\n",
       "      <td>0.529232</td>\n",
       "      <td>0.86985</td>\n",
       "      <td>0.416203</td>\n",
       "      <td>0.654371</td>\n",
       "      <td>0.769766</td>\n",
       "      <td>1.152422</td>\n",
       "      <td>1.717428</td>\n",
       "      <td>0.627088</td>\n",
       "      <td>0.375418</td>\n",
       "      <td>1.128341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pronouns   text_len  neg_vader  pos_vader     anger  anticipation  \\\n",
       "label                                                                      \n",
       "0      0.868213  32.031615   0.054259   0.109981  0.386069       0.58984   \n",
       "1      2.484271  36.398389   0.079191   0.148154  0.529232       0.86985   \n",
       "\n",
       "        disgust      fear       joy  negative  positive   sadness  surprise  \\\n",
       "label                                                                         \n",
       "0      0.263683  0.478014  0.479908  0.818800  1.280788  0.385315  0.284790   \n",
       "1      0.416203  0.654371  0.769766  1.152422  1.717428  0.627088  0.375418   \n",
       "\n",
       "          trust  \n",
       "label            \n",
       "0      0.830560  \n",
       "1      1.128341  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len', 'neg_vader', 'pos_vader'] + emotions].groupby('label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>text_len</th>\n",
       "      <th>neg_vader</th>\n",
       "      <th>pos_vader</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.097800</td>\n",
       "      <td>0.033477</td>\n",
       "      <td>0.067170</td>\n",
       "      <td>0.065211</td>\n",
       "      <td>0.022057</td>\n",
       "      <td>0.025666</td>\n",
       "      <td>0.030664</td>\n",
       "      <td>0.019114</td>\n",
       "      <td>0.033977</td>\n",
       "      <td>0.022934</td>\n",
       "      <td>0.019590</td>\n",
       "      <td>0.032641</td>\n",
       "      <td>0.018109</td>\n",
       "      <td>0.024014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pronouns</th>\n",
       "      <td>0.097800</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.332071</td>\n",
       "      <td>0.193938</td>\n",
       "      <td>0.221419</td>\n",
       "      <td>0.076345</td>\n",
       "      <td>0.128030</td>\n",
       "      <td>0.094069</td>\n",
       "      <td>0.063176</td>\n",
       "      <td>0.144011</td>\n",
       "      <td>0.076670</td>\n",
       "      <td>0.106055</td>\n",
       "      <td>0.100827</td>\n",
       "      <td>0.106790</td>\n",
       "      <td>0.122914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_len</th>\n",
       "      <td>0.033477</td>\n",
       "      <td>0.332071</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.343154</td>\n",
       "      <td>0.159673</td>\n",
       "      <td>0.360460</td>\n",
       "      <td>0.386351</td>\n",
       "      <td>0.312393</td>\n",
       "      <td>0.381410</td>\n",
       "      <td>0.339398</td>\n",
       "      <td>0.370250</td>\n",
       "      <td>0.330075</td>\n",
       "      <td>0.384031</td>\n",
       "      <td>0.349498</td>\n",
       "      <td>0.389620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_vader</th>\n",
       "      <td>0.067170</td>\n",
       "      <td>0.193938</td>\n",
       "      <td>0.343154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.169624</td>\n",
       "      <td>0.384510</td>\n",
       "      <td>0.141868</td>\n",
       "      <td>0.362582</td>\n",
       "      <td>0.339245</td>\n",
       "      <td>0.126042</td>\n",
       "      <td>0.431111</td>\n",
       "      <td>0.099767</td>\n",
       "      <td>0.374256</td>\n",
       "      <td>0.159302</td>\n",
       "      <td>0.143060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_vader</th>\n",
       "      <td>0.065211</td>\n",
       "      <td>0.221419</td>\n",
       "      <td>0.159673</td>\n",
       "      <td>0.169624</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.079693</td>\n",
       "      <td>0.225925</td>\n",
       "      <td>0.087309</td>\n",
       "      <td>0.071450</td>\n",
       "      <td>0.323148</td>\n",
       "      <td>0.058266</td>\n",
       "      <td>0.270687</td>\n",
       "      <td>0.095040</td>\n",
       "      <td>0.186243</td>\n",
       "      <td>0.231954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.022057</td>\n",
       "      <td>0.076345</td>\n",
       "      <td>0.360460</td>\n",
       "      <td>0.384510</td>\n",
       "      <td>0.079693</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.196795</td>\n",
       "      <td>0.583864</td>\n",
       "      <td>0.587460</td>\n",
       "      <td>0.157202</td>\n",
       "      <td>0.631708</td>\n",
       "      <td>0.128169</td>\n",
       "      <td>0.528980</td>\n",
       "      <td>0.273195</td>\n",
       "      <td>0.169261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anticipation</th>\n",
       "      <td>0.025666</td>\n",
       "      <td>0.128030</td>\n",
       "      <td>0.386351</td>\n",
       "      <td>0.141868</td>\n",
       "      <td>0.225925</td>\n",
       "      <td>0.196795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.164649</td>\n",
       "      <td>0.241958</td>\n",
       "      <td>0.583107</td>\n",
       "      <td>0.178827</td>\n",
       "      <td>0.452457</td>\n",
       "      <td>0.198972</td>\n",
       "      <td>0.460851</td>\n",
       "      <td>0.469028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.030664</td>\n",
       "      <td>0.094069</td>\n",
       "      <td>0.312393</td>\n",
       "      <td>0.362582</td>\n",
       "      <td>0.087309</td>\n",
       "      <td>0.583864</td>\n",
       "      <td>0.164649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.440376</td>\n",
       "      <td>0.152731</td>\n",
       "      <td>0.552021</td>\n",
       "      <td>0.116588</td>\n",
       "      <td>0.490181</td>\n",
       "      <td>0.232166</td>\n",
       "      <td>0.153723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.019114</td>\n",
       "      <td>0.063176</td>\n",
       "      <td>0.381410</td>\n",
       "      <td>0.339245</td>\n",
       "      <td>0.071450</td>\n",
       "      <td>0.587460</td>\n",
       "      <td>0.241958</td>\n",
       "      <td>0.440376</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.159907</td>\n",
       "      <td>0.576962</td>\n",
       "      <td>0.141985</td>\n",
       "      <td>0.583703</td>\n",
       "      <td>0.248160</td>\n",
       "      <td>0.184240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.033977</td>\n",
       "      <td>0.144011</td>\n",
       "      <td>0.339398</td>\n",
       "      <td>0.126042</td>\n",
       "      <td>0.323148</td>\n",
       "      <td>0.157202</td>\n",
       "      <td>0.583107</td>\n",
       "      <td>0.152731</td>\n",
       "      <td>0.159907</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.113400</td>\n",
       "      <td>0.645827</td>\n",
       "      <td>0.176440</td>\n",
       "      <td>0.477317</td>\n",
       "      <td>0.582920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0.022934</td>\n",
       "      <td>0.076670</td>\n",
       "      <td>0.370250</td>\n",
       "      <td>0.431111</td>\n",
       "      <td>0.058266</td>\n",
       "      <td>0.631708</td>\n",
       "      <td>0.178827</td>\n",
       "      <td>0.552021</td>\n",
       "      <td>0.576962</td>\n",
       "      <td>0.113400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.105821</td>\n",
       "      <td>0.612781</td>\n",
       "      <td>0.226230</td>\n",
       "      <td>0.145220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>0.019590</td>\n",
       "      <td>0.106055</td>\n",
       "      <td>0.330075</td>\n",
       "      <td>0.099767</td>\n",
       "      <td>0.270687</td>\n",
       "      <td>0.128169</td>\n",
       "      <td>0.452457</td>\n",
       "      <td>0.116588</td>\n",
       "      <td>0.141985</td>\n",
       "      <td>0.645827</td>\n",
       "      <td>0.105821</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.139827</td>\n",
       "      <td>0.333998</td>\n",
       "      <td>0.648163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.032641</td>\n",
       "      <td>0.100827</td>\n",
       "      <td>0.384031</td>\n",
       "      <td>0.374256</td>\n",
       "      <td>0.095040</td>\n",
       "      <td>0.528980</td>\n",
       "      <td>0.198972</td>\n",
       "      <td>0.490181</td>\n",
       "      <td>0.583703</td>\n",
       "      <td>0.176440</td>\n",
       "      <td>0.612781</td>\n",
       "      <td>0.139827</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.265026</td>\n",
       "      <td>0.171245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.018109</td>\n",
       "      <td>0.106790</td>\n",
       "      <td>0.349498</td>\n",
       "      <td>0.159302</td>\n",
       "      <td>0.186243</td>\n",
       "      <td>0.273195</td>\n",
       "      <td>0.460851</td>\n",
       "      <td>0.232166</td>\n",
       "      <td>0.248160</td>\n",
       "      <td>0.477317</td>\n",
       "      <td>0.226230</td>\n",
       "      <td>0.333998</td>\n",
       "      <td>0.265026</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.354746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust</th>\n",
       "      <td>0.024014</td>\n",
       "      <td>0.122914</td>\n",
       "      <td>0.389620</td>\n",
       "      <td>0.143060</td>\n",
       "      <td>0.231954</td>\n",
       "      <td>0.169261</td>\n",
       "      <td>0.469028</td>\n",
       "      <td>0.153723</td>\n",
       "      <td>0.184240</td>\n",
       "      <td>0.582920</td>\n",
       "      <td>0.145220</td>\n",
       "      <td>0.648163</td>\n",
       "      <td>0.171245</td>\n",
       "      <td>0.354746</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 label  pronouns  text_len  neg_vader  pos_vader     anger  \\\n",
       "label         1.000000  0.097800  0.033477   0.067170   0.065211  0.022057   \n",
       "pronouns      0.097800  1.000000  0.332071   0.193938   0.221419  0.076345   \n",
       "text_len      0.033477  0.332071  1.000000   0.343154   0.159673  0.360460   \n",
       "neg_vader     0.067170  0.193938  0.343154   1.000000   0.169624  0.384510   \n",
       "pos_vader     0.065211  0.221419  0.159673   0.169624   1.000000  0.079693   \n",
       "anger         0.022057  0.076345  0.360460   0.384510   0.079693  1.000000   \n",
       "anticipation  0.025666  0.128030  0.386351   0.141868   0.225925  0.196795   \n",
       "disgust       0.030664  0.094069  0.312393   0.362582   0.087309  0.583864   \n",
       "fear          0.019114  0.063176  0.381410   0.339245   0.071450  0.587460   \n",
       "joy           0.033977  0.144011  0.339398   0.126042   0.323148  0.157202   \n",
       "negative      0.022934  0.076670  0.370250   0.431111   0.058266  0.631708   \n",
       "positive      0.019590  0.106055  0.330075   0.099767   0.270687  0.128169   \n",
       "sadness       0.032641  0.100827  0.384031   0.374256   0.095040  0.528980   \n",
       "surprise      0.018109  0.106790  0.349498   0.159302   0.186243  0.273195   \n",
       "trust         0.024014  0.122914  0.389620   0.143060   0.231954  0.169261   \n",
       "\n",
       "              anticipation   disgust      fear       joy  negative  positive  \\\n",
       "label             0.025666  0.030664  0.019114  0.033977  0.022934  0.019590   \n",
       "pronouns          0.128030  0.094069  0.063176  0.144011  0.076670  0.106055   \n",
       "text_len          0.386351  0.312393  0.381410  0.339398  0.370250  0.330075   \n",
       "neg_vader         0.141868  0.362582  0.339245  0.126042  0.431111  0.099767   \n",
       "pos_vader         0.225925  0.087309  0.071450  0.323148  0.058266  0.270687   \n",
       "anger             0.196795  0.583864  0.587460  0.157202  0.631708  0.128169   \n",
       "anticipation      1.000000  0.164649  0.241958  0.583107  0.178827  0.452457   \n",
       "disgust           0.164649  1.000000  0.440376  0.152731  0.552021  0.116588   \n",
       "fear              0.241958  0.440376  1.000000  0.159907  0.576962  0.141985   \n",
       "joy               0.583107  0.152731  0.159907  1.000000  0.113400  0.645827   \n",
       "negative          0.178827  0.552021  0.576962  0.113400  1.000000  0.105821   \n",
       "positive          0.452457  0.116588  0.141985  0.645827  0.105821  1.000000   \n",
       "sadness           0.198972  0.490181  0.583703  0.176440  0.612781  0.139827   \n",
       "surprise          0.460851  0.232166  0.248160  0.477317  0.226230  0.333998   \n",
       "trust             0.469028  0.153723  0.184240  0.582920  0.145220  0.648163   \n",
       "\n",
       "               sadness  surprise     trust  \n",
       "label         0.032641  0.018109  0.024014  \n",
       "pronouns      0.100827  0.106790  0.122914  \n",
       "text_len      0.384031  0.349498  0.389620  \n",
       "neg_vader     0.374256  0.159302  0.143060  \n",
       "pos_vader     0.095040  0.186243  0.231954  \n",
       "anger         0.528980  0.273195  0.169261  \n",
       "anticipation  0.198972  0.460851  0.469028  \n",
       "disgust       0.490181  0.232166  0.153723  \n",
       "fear          0.583703  0.248160  0.184240  \n",
       "joy           0.176440  0.477317  0.582920  \n",
       "negative      0.612781  0.226230  0.145220  \n",
       "positive      0.139827  0.333998  0.648163  \n",
       "sadness       1.000000  0.265026  0.171245  \n",
       "surprise      0.265026  1.000000  0.354746  \n",
       "trust         0.171245  0.354746  1.000000  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len', 'neg_vader', 'pos_vader'] + emotions].corr('spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from liwc_readDict import readDict\n",
    "\n",
    "liwc = readDict('/home/ana/resources/FakeOrFact/features/LIWC/LIWC/liwc.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'achieve',\n",
       " 'adverb',\n",
       " 'affect',\n",
       " 'anger',\n",
       " 'anx',\n",
       " 'article',\n",
       " 'assent',\n",
       " 'auxverb',\n",
       " 'bio',\n",
       " 'body',\n",
       " 'cause',\n",
       " 'certain',\n",
       " 'cogmech',\n",
       " 'conj',\n",
       " 'death',\n",
       " 'discrep',\n",
       " 'excl',\n",
       " 'family',\n",
       " 'feel',\n",
       " 'filler',\n",
       " 'friend',\n",
       " 'funct',\n",
       " 'future',\n",
       " 'health',\n",
       " 'hear',\n",
       " 'home',\n",
       " 'humans',\n",
       " 'i',\n",
       " 'incl',\n",
       " 'ingest',\n",
       " 'inhib',\n",
       " 'insight',\n",
       " 'ipron',\n",
       " 'leisure',\n",
       " 'money',\n",
       " 'motion',\n",
       " 'negate',\n",
       " 'negemo',\n",
       " 'nonfl',\n",
       " 'number',\n",
       " 'past',\n",
       " 'percept',\n",
       " 'posemo',\n",
       " 'ppron',\n",
       " 'preps',\n",
       " 'present',\n",
       " 'pronoun',\n",
       " 'quant',\n",
       " 'relativ',\n",
       " 'relig',\n",
       " 'sad',\n",
       " 'see',\n",
       " 'sexual',\n",
       " 'shehe',\n",
       " 'social',\n",
       " 'space',\n",
       " 'swear',\n",
       " 'tentat',\n",
       " 'they',\n",
       " 'time',\n",
       " 'verb',\n",
       " 'we',\n",
       " 'work',\n",
       " 'you'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = [c for (w,c) in liwc]\n",
    "set(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 'funct'],\n",
       " ['a', 'article'],\n",
       " ['abandon*', 'affect'],\n",
       " ['abandon*', 'negemo'],\n",
       " ['abandon*', 'sad'],\n",
       " ['abandon*', 'cogmech'],\n",
       " ['abandon*', 'inhib'],\n",
       " ['abdomen*', 'bio'],\n",
       " ['abdomen*', 'body'],\n",
       " ['abilit*', 'achieve'],\n",
       " ['able*', 'achieve'],\n",
       " ['abortion*', 'bio'],\n",
       " ['abortion*', 'health'],\n",
       " ['abortion*', 'sexual'],\n",
       " ['about', 'funct'],\n",
       " ['about', 'adverb'],\n",
       " ['about', 'preps'],\n",
       " ['above', 'funct'],\n",
       " ['above', 'preps'],\n",
       " ['above', 'space'],\n",
       " ['above', 'relativ'],\n",
       " ['abrupt*', 'time'],\n",
       " ['abrupt*', 'relativ'],\n",
       " ['abs', 'bio'],\n",
       " ['abs', 'body'],\n",
       " ['absent*', 'work'],\n",
       " ['absolute', 'cogmech'],\n",
       " ['absolute', 'certain'],\n",
       " ['absolutely', 'funct'],\n",
       " ['absolutely', 'adverb'],\n",
       " ['absolutely', 'cogmech'],\n",
       " ['absolutely', 'certain'],\n",
       " ['absolutely', 'assent'],\n",
       " ['abstain*', 'cogmech'],\n",
       " ['abstain*', 'inhib'],\n",
       " ['abuse*', 'affect'],\n",
       " ['abuse*', 'negemo'],\n",
       " ['abuse*', 'anger'],\n",
       " ['abusi*', 'affect'],\n",
       " ['abusi*', 'negemo'],\n",
       " ['abusi*', 'anger'],\n",
       " ['academ*', 'work'],\n",
       " ['accept', 'affect'],\n",
       " ['accept', 'posemo'],\n",
       " ['accept', 'cogmech'],\n",
       " ['accept', 'insight'],\n",
       " ['accepta*', 'affect'],\n",
       " ['accepta*', 'posemo'],\n",
       " ['accepta*', 'cogmech'],\n",
       " ['accepta*', 'insight'],\n",
       " ['accepted', 'verb'],\n",
       " ['accepted', 'past'],\n",
       " ['accepted', 'affect'],\n",
       " ['accepted', 'posemo'],\n",
       " ['accepted', 'cogmech'],\n",
       " ['accepted', 'insight'],\n",
       " ['accepting', 'affect'],\n",
       " ['accepting', 'posemo'],\n",
       " ['accepting', 'cogmech'],\n",
       " ['accepting', 'insight'],\n",
       " ['accepts', 'affect'],\n",
       " ['accepts', 'posemo'],\n",
       " ['accepts', 'cogmech'],\n",
       " ['accepts', 'insight'],\n",
       " ['accomplish*', 'work'],\n",
       " ['accomplish*', 'achieve'],\n",
       " ['account*', 'money'],\n",
       " ['accura*', 'cogmech'],\n",
       " ['accura*', 'certain'],\n",
       " ['ace', 'achieve'],\n",
       " ['ache*', 'affect'],\n",
       " ['ache*', 'negemo'],\n",
       " ['ache*', 'sad'],\n",
       " ['ache*', 'bio'],\n",
       " ['ache*', 'health'],\n",
       " ['achiev*', 'work'],\n",
       " ['achiev*', 'achieve'],\n",
       " ['aching', 'affect'],\n",
       " ['aching', 'negemo'],\n",
       " ['aching', 'sad'],\n",
       " ['aching', 'bio'],\n",
       " ['aching', 'health'],\n",
       " ['acid*', 'percept'],\n",
       " ['acknowledg*', 'cogmech'],\n",
       " ['acknowledg*', 'insight'],\n",
       " ['acne', 'bio'],\n",
       " ['acne', 'health'],\n",
       " ['acquainta*', 'social'],\n",
       " ['acquainta*', 'friend'],\n",
       " ['acquir*', 'achieve'],\n",
       " ['acquisition*', 'achieve'],\n",
       " ['acrid*', 'percept'],\n",
       " ['across', 'funct'],\n",
       " ['across', 'preps'],\n",
       " ['across', 'space'],\n",
       " ['across', 'relativ'],\n",
       " ['act', 'relativ'],\n",
       " ['act', 'motion'],\n",
       " ['action*', 'motion'],\n",
       " ['action*', 'relativ'],\n",
       " ['activat*', 'cogmech'],\n",
       " ['activat*', 'cause'],\n",
       " ['active*', 'affect'],\n",
       " ['active*', 'posemo'],\n",
       " ['actor*', 'leisure'],\n",
       " ['actress*', 'leisure'],\n",
       " ['actually', 'funct'],\n",
       " ['actually', 'adverb'],\n",
       " ['add', 'cogmech'],\n",
       " ['add', 'incl'],\n",
       " ['addict*', 'bio'],\n",
       " ['addict*', 'health'],\n",
       " ['addit*', 'cogmech'],\n",
       " ['addit*', 'incl'],\n",
       " ['address', 'home'],\n",
       " ['adequa*', 'achieve'],\n",
       " ['adjust*', 'cogmech'],\n",
       " ['adjust*', 'insight'],\n",
       " ['administrat*', 'work'],\n",
       " ['admir*', 'affect'],\n",
       " ['admir*', 'posemo'],\n",
       " ['admit', 'verb'],\n",
       " ['admit', 'present'],\n",
       " ['admit', 'social'],\n",
       " ['admit', 'cogmech'],\n",
       " ['admit', 'insight'],\n",
       " ['admits', 'verb'],\n",
       " ['admits', 'present'],\n",
       " ['admits', 'social'],\n",
       " ['admits', 'cogmech'],\n",
       " ['admits', 'insight'],\n",
       " ['admitted', 'verb'],\n",
       " ['admitted', 'past'],\n",
       " ['admitted', 'social'],\n",
       " ['admitted', 'cogmech'],\n",
       " ['admitted', 'insight'],\n",
       " ['admitting', 'social'],\n",
       " ['admitting', 'cogmech'],\n",
       " ['admitting', 'insight'],\n",
       " ['ador*', 'affect'],\n",
       " ['ador*', 'posemo'],\n",
       " ['adult', 'social'],\n",
       " ['adult', 'humans'],\n",
       " ['adults', 'social'],\n",
       " ['adults', 'humans'],\n",
       " ['advanc*', 'motion'],\n",
       " ['advanc*', 'relativ'],\n",
       " ['advanc*', 'achieve'],\n",
       " ['advantag*', 'affect'],\n",
       " ['advantag*', 'posemo'],\n",
       " ['advantag*', 'achieve'],\n",
       " ['adventur*', 'affect'],\n",
       " ['adventur*', 'posemo'],\n",
       " ['advers*', 'affect'],\n",
       " ['advers*', 'negemo'],\n",
       " ['advertising', 'work'],\n",
       " ['advice', 'social'],\n",
       " ['advil', 'bio'],\n",
       " ['advil', 'health'],\n",
       " ['advis*', 'social'],\n",
       " ['advis*', 'work'],\n",
       " ['aerobic*', 'leisure'],\n",
       " ['affair*', 'social'],\n",
       " ['affect', 'cogmech'],\n",
       " ['affect', 'cause'],\n",
       " ['affected', 'verb'],\n",
       " ['affected', 'past'],\n",
       " ['affected', 'cogmech'],\n",
       " ['affected', 'cause'],\n",
       " ['affecting', 'cogmech'],\n",
       " ['affecting', 'cause'],\n",
       " ['affection*', 'affect'],\n",
       " ['affection*', 'posemo'],\n",
       " ['affects', 'cogmech'],\n",
       " ['affects', 'cause'],\n",
       " ['afraid', 'affect'],\n",
       " ['afraid', 'negemo'],\n",
       " ['afraid', 'anx'],\n",
       " ['after', 'funct'],\n",
       " ['after', 'preps'],\n",
       " ['after', 'time'],\n",
       " ['after', 'relativ'],\n",
       " ['afterlife*', 'time'],\n",
       " ['afterlife*', 'relativ'],\n",
       " ['afterlife*', 'relig'],\n",
       " ['aftermath*', 'time'],\n",
       " ['aftermath*', 'relativ'],\n",
       " ['afternoon*', 'time'],\n",
       " ['afternoon*', 'relativ'],\n",
       " ['afterthought*', 'cogmech'],\n",
       " ['afterthought*', 'insight'],\n",
       " ['afterthought*', 'time'],\n",
       " ['afterthought*', 'relativ'],\n",
       " ['afterward*', 'time'],\n",
       " ['afterward*', 'relativ'],\n",
       " ['again', 'funct'],\n",
       " ['again', 'adverb'],\n",
       " ['again', 'time'],\n",
       " ['again', 'relativ'],\n",
       " ['against', 'funct'],\n",
       " ['against', 'preps'],\n",
       " ['age', 'time'],\n",
       " ['age', 'relativ'],\n",
       " ['aged', 'time'],\n",
       " ['aged', 'relativ'],\n",
       " ['agent', 'work'],\n",
       " ['agents', 'work'],\n",
       " ['ages', 'time'],\n",
       " ['ages', 'relativ'],\n",
       " ['aggravat*', 'affect'],\n",
       " ['aggravat*', 'negemo'],\n",
       " ['aggravat*', 'anger'],\n",
       " ['aggravat*', 'cogmech'],\n",
       " ['aggravat*', 'cause'],\n",
       " ['aggress*', 'affect'],\n",
       " ['aggress*', 'negemo'],\n",
       " ['aggress*', 'anger'],\n",
       " ['aging', 'time'],\n",
       " ['aging', 'relativ'],\n",
       " ['agitat*', 'affect'],\n",
       " ['agitat*', 'negemo'],\n",
       " ['agitat*', 'anger'],\n",
       " ['agnost*', 'relig'],\n",
       " ['ago', 'time'],\n",
       " ['ago', 'relativ'],\n",
       " ['agoniz*', 'affect'],\n",
       " ['agoniz*', 'negemo'],\n",
       " ['agoniz*', 'sad'],\n",
       " ['agony', 'affect'],\n",
       " ['agony', 'negemo'],\n",
       " ['agony', 'sad'],\n",
       " ['agree', 'affect'],\n",
       " ['agree', 'posemo'],\n",
       " ['agree', 'assent'],\n",
       " ['agreeab*', 'affect'],\n",
       " ['agreeab*', 'posemo'],\n",
       " ['agreed', 'affect'],\n",
       " ['agreed', 'posemo'],\n",
       " ['agreeing', 'affect'],\n",
       " ['agreeing', 'posemo'],\n",
       " ['agreement*', 'affect'],\n",
       " ['agreement*', 'posemo'],\n",
       " ['agrees', 'affect'],\n",
       " ['agrees', 'posemo'],\n",
       " ['ah', 'assent'],\n",
       " ['ahead', 'funct'],\n",
       " ['ahead', 'preps'],\n",
       " ['ahead', 'time'],\n",
       " ['ahead', 'relativ'],\n",
       " ['ahead', 'achieve'],\n",
       " ['aids', 'bio'],\n",
       " ['aids', 'health'],\n",
       " ['aids', 'sexual'],\n",
       " [\"ain't\", 'verb'],\n",
       " [\"ain't\", 'funct'],\n",
       " [\"ain't\", 'auxverb'],\n",
       " [\"ain't\", 'present'],\n",
       " [\"ain't\", 'negate'],\n",
       " ['aint', 'verb'],\n",
       " ['aint', 'funct'],\n",
       " ['aint', 'auxverb'],\n",
       " ['aint', 'present'],\n",
       " ['aint', 'negate'],\n",
       " ['air', 'relativ'],\n",
       " ['air', 'space'],\n",
       " ['alarm*', 'affect'],\n",
       " ['alarm*', 'negemo'],\n",
       " ['alarm*', 'anx'],\n",
       " ['alcohol*', 'bio'],\n",
       " ['alcohol*', 'health'],\n",
       " ['alcohol*', 'ingest'],\n",
       " ['alive', 'bio'],\n",
       " ['alive', 'health'],\n",
       " ['alive', 'death'],\n",
       " ['all', 'funct'],\n",
       " ['all', 'quant'],\n",
       " ['all', 'cogmech'],\n",
       " ['all', 'certain'],\n",
       " ['alla', 'relig'],\n",
       " ['allah*', 'relig'],\n",
       " ['allerg*', 'bio'],\n",
       " ['allerg*', 'health'],\n",
       " ['allot', 'funct'],\n",
       " ['allot', 'quant'],\n",
       " ['allot', 'cogmech'],\n",
       " ['allot', 'tentat'],\n",
       " ['allow*', 'cogmech'],\n",
       " ['allow*', 'cause'],\n",
       " ['almost', 'cogmech'],\n",
       " ['almost', 'tentat'],\n",
       " ['alone', 'affect'],\n",
       " ['alone', 'negemo'],\n",
       " ['alone', 'sad'],\n",
       " ['along', 'funct'],\n",
       " ['along', 'preps'],\n",
       " ['along', 'cogmech'],\n",
       " ['along', 'incl'],\n",
       " ['alot', 'funct'],\n",
       " ['alot', 'article'],\n",
       " ['alot', 'quant'],\n",
       " ['alot', 'cogmech'],\n",
       " ['alot', 'tentat'],\n",
       " ['already', 'time'],\n",
       " ['already', 'relativ'],\n",
       " ['alright*', 'affect'],\n",
       " ['alright*', 'posemo'],\n",
       " ['alright*', 'assent'],\n",
       " ['also', 'funct'],\n",
       " ['also', 'adverb'],\n",
       " ['also', 'conj'],\n",
       " ['altar*', 'relig'],\n",
       " ['although', 'funct'],\n",
       " ['although', 'conj'],\n",
       " ['altogether', 'cogmech'],\n",
       " ['altogether', 'certain'],\n",
       " ['always', 'cogmech'],\n",
       " ['always', 'certain'],\n",
       " ['always', 'time'],\n",
       " ['always', 'relativ'],\n",
       " ['am', 'verb'],\n",
       " ['am', 'funct'],\n",
       " ['am', 'auxverb'],\n",
       " ['am', 'present'],\n",
       " ['amaz*', 'affect'],\n",
       " ['amaz*', 'posemo'],\n",
       " ['ambigu*', 'cogmech'],\n",
       " ['ambigu*', 'tentat'],\n",
       " ['ambiti*', 'work'],\n",
       " ['ambiti*', 'achieve'],\n",
       " ['amen', 'relig'],\n",
       " ['amigo*', 'social'],\n",
       " ['amigo*', 'friend'],\n",
       " ['amish', 'relig'],\n",
       " ['among*', 'funct'],\n",
       " ['among*', 'preps'],\n",
       " ['among*', 'space'],\n",
       " ['among*', 'relativ'],\n",
       " ['amor*', 'affect'],\n",
       " ['amor*', 'posemo'],\n",
       " ['amount*', 'quant'],\n",
       " ['amput*', 'bio'],\n",
       " ['amput*', 'health'],\n",
       " ['amus*', 'affect'],\n",
       " ['amus*', 'posemo'],\n",
       " ['amus*', 'leisure'],\n",
       " ['an', 'funct'],\n",
       " ['an', 'article'],\n",
       " ['anal', 'cogmech'],\n",
       " ['anal', 'inhib'],\n",
       " ['anal', 'bio'],\n",
       " ['anal', 'body'],\n",
       " ['analy*', 'cogmech'],\n",
       " ['analy*', 'insight'],\n",
       " ['ancient*', 'time'],\n",
       " ['ancient*', 'relativ'],\n",
       " ['and', 'funct'],\n",
       " ['and', 'conj'],\n",
       " ['and', 'cogmech'],\n",
       " ['and', 'incl'],\n",
       " ['angel', 'relig'],\n",
       " ['angelic*', 'relig'],\n",
       " ['angels', 'relig'],\n",
       " ['anger*', 'affect'],\n",
       " ['anger*', 'negemo'],\n",
       " ['anger*', 'anger'],\n",
       " ['angr*', 'affect'],\n",
       " ['angr*', 'negemo'],\n",
       " ['angr*', 'anger'],\n",
       " ['anguish*', 'affect'],\n",
       " ['anguish*', 'negemo'],\n",
       " ['anguish*', 'anx'],\n",
       " ['ankle*', 'bio'],\n",
       " ['ankle*', 'body'],\n",
       " ['annoy*', 'affect'],\n",
       " ['annoy*', 'negemo'],\n",
       " ['annoy*', 'anger'],\n",
       " ['annual*', 'time'],\n",
       " ['annual*', 'relativ'],\n",
       " ['anorexi*', 'bio'],\n",
       " ['anorexi*', 'health'],\n",
       " ['anorexi*', 'ingest'],\n",
       " ['another', 'funct'],\n",
       " ['another', 'quant'],\n",
       " ['answer*', 'cogmech'],\n",
       " ['answer*', 'insight'],\n",
       " ['antacid*', 'bio'],\n",
       " ['antacid*', 'health'],\n",
       " ['antagoni*', 'affect'],\n",
       " ['antagoni*', 'negemo'],\n",
       " ['antagoni*', 'anger'],\n",
       " ['antidepressant*', 'bio'],\n",
       " ['antidepressant*', 'health'],\n",
       " ['anus*', 'bio'],\n",
       " ['anus*', 'body'],\n",
       " ['anxi*', 'affect'],\n",
       " ['anxi*', 'negemo'],\n",
       " ['anxi*', 'anx'],\n",
       " ['any', 'funct'],\n",
       " ['any', 'quant'],\n",
       " ['any', 'cogmech'],\n",
       " ['any', 'tentat'],\n",
       " ['anybod*', 'funct'],\n",
       " ['anybod*', 'pronoun'],\n",
       " ['anybod*', 'ipron'],\n",
       " ['anybod*', 'social'],\n",
       " ['anybod*', 'cogmech'],\n",
       " ['anybod*', 'tentat'],\n",
       " ['anyhow', 'cogmech'],\n",
       " ['anyhow', 'tentat'],\n",
       " ['anymore', 'funct'],\n",
       " ['anymore', 'quant'],\n",
       " ['anymore', 'relativ'],\n",
       " ['anymore', 'time'],\n",
       " ['anyone*', 'funct'],\n",
       " ['anyone*', 'pronoun'],\n",
       " ['anyone*', 'ipron'],\n",
       " ['anyone*', 'social'],\n",
       " ['anyone*', 'cogmech'],\n",
       " ['anyone*', 'tentat'],\n",
       " ['anything', 'funct'],\n",
       " ['anything', 'pronoun'],\n",
       " ['anything', 'ipron'],\n",
       " ['anything', 'cogmech'],\n",
       " ['anything', 'tentat'],\n",
       " ['anytime', 'cogmech'],\n",
       " ['anytime', 'tentat'],\n",
       " ['anytime', 'time'],\n",
       " ['anytime', 'relativ'],\n",
       " ['anyway*', 'funct'],\n",
       " ['anyway*', 'adverb'],\n",
       " ['anywhere', 'funct'],\n",
       " ['anywhere', 'adverb'],\n",
       " ['anywhere', 'cogmech'],\n",
       " ['anywhere', 'tentat'],\n",
       " ['anywhere', 'space'],\n",
       " ['anywhere', 'relativ'],\n",
       " ['aok', 'affect'],\n",
       " ['aok', 'posemo'],\n",
       " ['aok', 'assent'],\n",
       " ['apart', 'space'],\n",
       " ['apart', 'relativ'],\n",
       " ['apartment*', 'leisure'],\n",
       " ['apartment*', 'home'],\n",
       " ['apath*', 'affect'],\n",
       " ['apath*', 'negemo'],\n",
       " ['apolog*', 'social'],\n",
       " ['appall*', 'affect'],\n",
       " ['appall*', 'negemo'],\n",
       " ['apparent', 'cogmech'],\n",
       " ['apparent', 'certain'],\n",
       " ['apparently', 'funct'],\n",
       " ['apparently', 'adverb'],\n",
       " ['apparently', 'cogmech'],\n",
       " ['apparently', 'tentat'],\n",
       " ['appear', 'verb'],\n",
       " ['appear', 'present'],\n",
       " ['appear', 'cogmech'],\n",
       " ['appear', 'tentat'],\n",
       " ['appear', 'motion'],\n",
       " ['appear', 'relativ'],\n",
       " ['appeared', 'verb'],\n",
       " ['appeared', 'past'],\n",
       " ['appeared', 'cogmech'],\n",
       " ['appeared', 'tentat'],\n",
       " ['appeared', 'motion'],\n",
       " ['appeared', 'relativ'],\n",
       " ['appearing', 'cogmech'],\n",
       " ['appearing', 'tentat'],\n",
       " ['appearing', 'motion'],\n",
       " ['appearing', 'relativ'],\n",
       " ['appears', 'verb'],\n",
       " ['appears', 'present'],\n",
       " ['appears', 'cogmech'],\n",
       " ['appears', 'tentat'],\n",
       " ['appears', 'motion'],\n",
       " ['appears', 'relativ'],\n",
       " ['appendic*', 'bio'],\n",
       " ['appendic*', 'health'],\n",
       " ['appendix', 'bio'],\n",
       " ['appendix', 'body'],\n",
       " ['appeti*', 'bio'],\n",
       " ['appeti*', 'ingest'],\n",
       " ['applicant*', 'work'],\n",
       " ['applicat*', 'work'],\n",
       " ['appreciat*', 'affect'],\n",
       " ['appreciat*', 'posemo'],\n",
       " ['appreciat*', 'cogmech'],\n",
       " ['appreciat*', 'insight'],\n",
       " ['apprehens*', 'affect'],\n",
       " ['apprehens*', 'negemo'],\n",
       " ['apprehens*', 'anx'],\n",
       " ['apprentic*', 'work'],\n",
       " ['approach*', 'motion'],\n",
       " ['approach*', 'relativ'],\n",
       " ['approv*', 'achieve'],\n",
       " ['approximat*', 'cogmech'],\n",
       " ['approximat*', 'tentat'],\n",
       " ['april', 'time'],\n",
       " ['april', 'relativ'],\n",
       " ['arbitrar*', 'cogmech'],\n",
       " ['arbitrar*', 'tentat'],\n",
       " ['arch', 'bio'],\n",
       " ['arch', 'body'],\n",
       " ['are', 'verb'],\n",
       " ['are', 'funct'],\n",
       " ['are', 'auxverb'],\n",
       " ['are', 'present'],\n",
       " ['area*', 'space'],\n",
       " ['area*', 'relativ'],\n",
       " [\"aren't\", 'verb'],\n",
       " [\"aren't\", 'funct'],\n",
       " [\"aren't\", 'auxverb'],\n",
       " [\"aren't\", 'present'],\n",
       " [\"aren't\", 'negate'],\n",
       " ['arent', 'verb'],\n",
       " ['arent', 'funct'],\n",
       " ['arent', 'auxverb'],\n",
       " ['arent', 'present'],\n",
       " ['arent', 'negate'],\n",
       " ['argh*', 'affect'],\n",
       " ['argh*', 'negemo'],\n",
       " ['argh*', 'anger'],\n",
       " ['argu*', 'social'],\n",
       " ['argu*', 'affect'],\n",
       " ['argu*', 'negemo'],\n",
       " ['argu*', 'anger'],\n",
       " ['arm', 'bio'],\n",
       " ['arm', 'body'],\n",
       " ['armies', 'social'],\n",
       " ['armpit*', 'bio'],\n",
       " ['armpit*', 'body'],\n",
       " ['arms*', 'bio'],\n",
       " ['arms*', 'body'],\n",
       " ['army', 'social'],\n",
       " ['aroma*', 'percept'],\n",
       " ['around', 'funct'],\n",
       " ['around', 'adverb'],\n",
       " ['around', 'preps'],\n",
       " ['around', 'cogmech'],\n",
       " ['around', 'incl'],\n",
       " ['around', 'space'],\n",
       " ['around', 'relativ'],\n",
       " ['arous*', 'bio'],\n",
       " ['arous*', 'body'],\n",
       " ['arous*', 'sexual'],\n",
       " ['arrival*', 'motion'],\n",
       " ['arrival*', 'relativ'],\n",
       " ['arrive', 'verb'],\n",
       " ['arrive', 'present'],\n",
       " ['arrive', 'motion'],\n",
       " ['arrive', 'relativ'],\n",
       " ['arrived', 'verb'],\n",
       " ['arrived', 'past'],\n",
       " ['arrived', 'motion'],\n",
       " ['arrived', 'relativ'],\n",
       " ['arrives', 'verb'],\n",
       " ['arrives', 'present'],\n",
       " ['arrives', 'motion'],\n",
       " ['arrives', 'relativ'],\n",
       " ['arriving', 'motion'],\n",
       " ['arriving', 'relativ'],\n",
       " ['arrogan*', 'affect'],\n",
       " ['arrogan*', 'negemo'],\n",
       " ['arrogan*', 'anger'],\n",
       " ['arse', 'bio'],\n",
       " ['arse', 'body'],\n",
       " ['arse', 'swear'],\n",
       " ['arsehole*', 'swear'],\n",
       " ['arses', 'bio'],\n",
       " ['arses', 'body'],\n",
       " ['arses', 'swear'],\n",
       " ['art', 'leisure'],\n",
       " ['arter*', 'bio'],\n",
       " ['arter*', 'body'],\n",
       " ['arthr*', 'bio'],\n",
       " ['arthr*', 'health'],\n",
       " ['artist*', 'leisure'],\n",
       " ['arts', 'leisure'],\n",
       " ['as', 'funct'],\n",
       " ['as', 'preps'],\n",
       " ['as', 'conj'],\n",
       " ['asham*', 'affect'],\n",
       " ['asham*', 'negemo'],\n",
       " ['asham*', 'anx'],\n",
       " ['ask', 'verb'],\n",
       " ['ask', 'present'],\n",
       " ['ask', 'social'],\n",
       " ['asked', 'verb'],\n",
       " ['asked', 'past'],\n",
       " ['asked', 'social'],\n",
       " ['asking', 'social'],\n",
       " ['asks', 'verb'],\n",
       " ['asks', 'present'],\n",
       " ['asks', 'social'],\n",
       " ['asleep', 'bio'],\n",
       " ['asleep', 'body'],\n",
       " ['aspirin*', 'bio'],\n",
       " ['aspirin*', 'health'],\n",
       " ['ass', 'bio'],\n",
       " ['ass', 'body'],\n",
       " ['ass', 'sexual'],\n",
       " ['ass', 'swear'],\n",
       " ['assault*', 'affect'],\n",
       " ['assault*', 'negemo'],\n",
       " ['assault*', 'anger'],\n",
       " ['assembl*', 'social'],\n",
       " ['asses', 'bio'],\n",
       " ['asses', 'body'],\n",
       " ['asses', 'sexual'],\n",
       " ['asses', 'swear'],\n",
       " ['asshole*', 'affect'],\n",
       " ['asshole*', 'negemo'],\n",
       " ['asshole*', 'anger'],\n",
       " ['asshole*', 'swear'],\n",
       " ['assign*', 'work'],\n",
       " ['assistan*', 'work'],\n",
       " ['associat*', 'work'],\n",
       " ['assum*', 'cogmech'],\n",
       " ['assum*', 'insight'],\n",
       " ['assum*', 'tentat'],\n",
       " ['assur*', 'affect'],\n",
       " ['assur*', 'posemo'],\n",
       " ['assur*', 'cogmech'],\n",
       " ['assur*', 'certain'],\n",
       " ['asthma*', 'bio'],\n",
       " ['asthma*', 'health'],\n",
       " ['at', 'funct'],\n",
       " ['at', 'preps'],\n",
       " ['at', 'space'],\n",
       " ['at', 'relativ'],\n",
       " ['ate', 'verb'],\n",
       " ['ate', 'past'],\n",
       " ['ate', 'bio'],\n",
       " ['ate', 'ingest'],\n",
       " ['athletic*', 'leisure'],\n",
       " ['atho', 'funct'],\n",
       " ['atho', 'conj'],\n",
       " ['atm', 'money'],\n",
       " ['atms', 'money'],\n",
       " ['atop', 'funct'],\n",
       " ['atop', 'preps'],\n",
       " ['atop', 'space'],\n",
       " ['atop', 'relativ'],\n",
       " ['attachment*', 'affect'],\n",
       " ['attachment*', 'posemo'],\n",
       " ['attack*', 'affect'],\n",
       " ['attack*', 'negemo'],\n",
       " ['attack*', 'anger'],\n",
       " ['attain*', 'achieve'],\n",
       " ['attempt*', 'achieve'],\n",
       " ['attend', 'motion'],\n",
       " ['attend', 'relativ'],\n",
       " ['attended', 'motion'],\n",
       " ['attended', 'relativ'],\n",
       " ['attending', 'motion'],\n",
       " ['attending', 'relativ'],\n",
       " ['attends', 'motion'],\n",
       " ['attends', 'relativ'],\n",
       " ['attent*', 'cogmech'],\n",
       " ['attent*', 'insight'],\n",
       " ['attract*', 'affect'],\n",
       " ['attract*', 'posemo'],\n",
       " ['attribut*', 'cogmech'],\n",
       " ['attribut*', 'cause'],\n",
       " ['auction*', 'money'],\n",
       " ['audibl*', 'percept'],\n",
       " ['audibl*', 'hear'],\n",
       " ['audio*', 'percept'],\n",
       " ['audio*', 'hear'],\n",
       " ['audit', 'money'],\n",
       " ['audited', 'money'],\n",
       " ['auditing', 'money'],\n",
       " ['auditor', 'money'],\n",
       " ['auditorium*', 'work'],\n",
       " ['auditors', 'money'],\n",
       " ['audits', 'money'],\n",
       " ['august', 'time'],\n",
       " ['august', 'relativ'],\n",
       " ['aunt*', 'social'],\n",
       " ['aunt*', 'family'],\n",
       " ['authorit*', 'achieve'],\n",
       " ['autops*', 'death'],\n",
       " ['autumn', 'time'],\n",
       " ['autumn', 'relativ'],\n",
       " ['aversi*', 'affect'],\n",
       " ['aversi*', 'negemo'],\n",
       " ['aversi*', 'anx'],\n",
       " ['avert*', 'cogmech'],\n",
       " ['avert*', 'inhib'],\n",
       " ['avoid*', 'affect'],\n",
       " ['avoid*', 'negemo'],\n",
       " ['avoid*', 'anx'],\n",
       " ['avoid*', 'cogmech'],\n",
       " ['avoid*', 'inhib'],\n",
       " ['aw', 'assent'],\n",
       " ['award*', 'affect'],\n",
       " ['award*', 'posemo'],\n",
       " ['award*', 'work'],\n",
       " ['award*', 'achieve'],\n",
       " ['aware*', 'cogmech'],\n",
       " ['aware*', 'insight'],\n",
       " ['away', 'funct'],\n",
       " ['away', 'preps'],\n",
       " ['away', 'space'],\n",
       " ['away', 'relativ'],\n",
       " ['awesome', 'affect'],\n",
       " ['awesome', 'posemo'],\n",
       " ['awesome', 'assent'],\n",
       " ['awful', 'affect'],\n",
       " ['awful', 'negemo'],\n",
       " ['awhile', 'time'],\n",
       " ['awhile', 'relativ'],\n",
       " ['awkward*', 'affect'],\n",
       " ['awkward*', 'negemo'],\n",
       " ['awkward*', 'anx'],\n",
       " ['babe*', 'social'],\n",
       " ['babe*', 'humans'],\n",
       " ['babies', 'social'],\n",
       " ['babies', 'humans'],\n",
       " ['baby*', 'social'],\n",
       " ['baby*', 'humans'],\n",
       " ['back', 'funct'],\n",
       " ['back', 'adverb'],\n",
       " ['back', 'time'],\n",
       " ['back', 'relativ'],\n",
       " ['backward*', 'space'],\n",
       " ['backward*', 'relativ'],\n",
       " ['backyard', 'home'],\n",
       " ['bad', 'affect'],\n",
       " ['bad', 'negemo'],\n",
       " ['bake*', 'bio'],\n",
       " ['bake*', 'ingest'],\n",
       " ['bake*', 'home'],\n",
       " ['baking', 'bio'],\n",
       " ['baking', 'ingest'],\n",
       " ['baking', 'home'],\n",
       " ['balcon*', 'home'],\n",
       " ['bald', 'bio'],\n",
       " ['bald', 'body'],\n",
       " ['ball', 'leisure'],\n",
       " ['ballet*', 'leisure'],\n",
       " ['bambino*', 'social'],\n",
       " ['bambino*', 'humans'],\n",
       " ['ban', 'cogmech'],\n",
       " ['ban', 'inhib'],\n",
       " ['band', 'social'],\n",
       " ['band', 'leisure'],\n",
       " ['bandage*', 'bio'],\n",
       " ['bandage*', 'health'],\n",
       " ['bandaid', 'bio'],\n",
       " ['bandaid', 'health'],\n",
       " ['bands', 'social'],\n",
       " ['bands', 'leisure'],\n",
       " ['bank*', 'money'],\n",
       " ['banned', 'cogmech'],\n",
       " ['banned', 'inhib'],\n",
       " ['banning', 'cogmech'],\n",
       " ['banning', 'inhib'],\n",
       " ['bans', 'cogmech'],\n",
       " ['bans', 'inhib'],\n",
       " ['baptis*', 'relig'],\n",
       " ['baptiz*', 'relig'],\n",
       " ['bar', 'bio'],\n",
       " ['bar', 'ingest'],\n",
       " ['bar', 'leisure'],\n",
       " ['barely', 'cogmech'],\n",
       " ['barely', 'tentat'],\n",
       " ['bargain*', 'money'],\n",
       " ['barrier*', 'cogmech'],\n",
       " ['barrier*', 'inhib'],\n",
       " ['bars', 'bio'],\n",
       " ['bars', 'ingest'],\n",
       " ['bars', 'leisure'],\n",
       " ['baseball*', 'leisure'],\n",
       " ['based', 'cogmech'],\n",
       " ['based', 'cause'],\n",
       " ['bases', 'cogmech'],\n",
       " ['bases', 'cause'],\n",
       " ['bashful*', 'affect'],\n",
       " ['bashful*', 'negemo'],\n",
       " ['basically', 'funct'],\n",
       " ['basically', 'adverb'],\n",
       " ['basis', 'cogmech'],\n",
       " ['basis', 'cause'],\n",
       " ['basketball*', 'leisure'],\n",
       " ['bastard*', 'affect'],\n",
       " ['bastard*', 'negemo'],\n",
       " ['bastard*', 'anger'],\n",
       " ['bastard*', 'swear'],\n",
       " ['bath*', 'leisure'],\n",
       " ['bath*', 'home'],\n",
       " ['battl*', 'affect'],\n",
       " ['battl*', 'negemo'],\n",
       " ['battl*', 'anger'],\n",
       " ['be', 'verb'],\n",
       " ['be', 'funct'],\n",
       " ['be', 'auxverb'],\n",
       " ['beach*', 'leisure'],\n",
       " ['beat', 'achieve'],\n",
       " ['beaten', 'affect'],\n",
       " ['beaten', 'negemo'],\n",
       " ['beaten', 'anger'],\n",
       " ['beaten', 'work'],\n",
       " ['beaten', 'achieve'],\n",
       " ['beaut*', 'affect'],\n",
       " ['beaut*', 'posemo'],\n",
       " ['beaut*', 'percept'],\n",
       " ['beaut*', 'see'],\n",
       " ['became', 'verb'],\n",
       " ['became', 'funct'],\n",
       " ['became', 'auxverb'],\n",
       " ['became', 'past'],\n",
       " ['became', 'cogmech'],\n",
       " ['became', 'insight'],\n",
       " ['because', 'funct'],\n",
       " ['because', 'conj'],\n",
       " ['because', 'cogmech'],\n",
       " ['because', 'cause'],\n",
       " ['become', 'verb'],\n",
       " ['become', 'funct'],\n",
       " ['become', 'auxverb'],\n",
       " ['become', 'present'],\n",
       " ['become', 'cogmech'],\n",
       " ['become', 'insight'],\n",
       " ['becomes', 'verb'],\n",
       " ['becomes', 'funct'],\n",
       " ['becomes', 'auxverb'],\n",
       " ['becomes', 'present'],\n",
       " ['becomes', 'cogmech'],\n",
       " ['becomes', 'insight'],\n",
       " ['becoming', 'verb'],\n",
       " ['becoming', 'funct'],\n",
       " ['becoming', 'auxverb'],\n",
       " ['becoming', 'cogmech'],\n",
       " ['becoming', 'insight'],\n",
       " ['bed', 'home'],\n",
       " ['bedding', 'home'],\n",
       " ['bedroom*', 'home'],\n",
       " ['beds', 'home'],\n",
       " ['been', 'verb'],\n",
       " ['been', 'funct'],\n",
       " ['been', 'auxverb'],\n",
       " ['been', 'past'],\n",
       " ['beer*', 'bio'],\n",
       " ['beer*', 'ingest'],\n",
       " ['beer*', 'leisure'],\n",
       " ['before', 'funct'],\n",
       " ['before', 'preps'],\n",
       " ['before', 'time'],\n",
       " ['before', 'relativ'],\n",
       " ['began', 'verb'],\n",
       " ['began', 'past'],\n",
       " ['began', 'time'],\n",
       " ['began', 'relativ'],\n",
       " ['beggar*', 'money'],\n",
       " ['begging', 'money'],\n",
       " ['begin', 'verb'],\n",
       " ['begin', 'present'],\n",
       " ['begin', 'time'],\n",
       " ['begin', 'relativ'],\n",
       " ['beginn*', 'time'],\n",
       " ['beginn*', 'relativ'],\n",
       " ['begins', 'verb'],\n",
       " ['begins', 'present'],\n",
       " ['begins', 'time'],\n",
       " ['begins', 'relativ'],\n",
       " ['begun', 'time'],\n",
       " ['begun', 'relativ'],\n",
       " ['behavio*', 'relativ'],\n",
       " ['behavio*', 'motion'],\n",
       " ['behind', 'funct'],\n",
       " ['behind', 'preps'],\n",
       " ['being', 'verb'],\n",
       " ['being', 'funct'],\n",
       " ['being', 'auxverb'],\n",
       " ['belief*', 'cogmech'],\n",
       " ['belief*', 'insight'],\n",
       " ['belief*', 'relig'],\n",
       " ['believe', 'verb'],\n",
       " ['believe', 'present'],\n",
       " ['believe', 'cogmech'],\n",
       " ['believe', 'insight'],\n",
       " ['believed', 'verb'],\n",
       " ['believed', 'past'],\n",
       " ['believed', 'cogmech'],\n",
       " ['believed', 'insight'],\n",
       " ['believes', 'verb'],\n",
       " ['believes', 'present'],\n",
       " ['believes', 'cogmech'],\n",
       " ['believes', 'insight'],\n",
       " ['believing', 'cogmech'],\n",
       " ['believing', 'insight'],\n",
       " ['bellies', 'bio'],\n",
       " ['bellies', 'body'],\n",
       " ['belly', 'bio'],\n",
       " ['belly', 'body'],\n",
       " ['beloved', 'affect'],\n",
       " ['beloved', 'posemo'],\n",
       " ['below', 'funct'],\n",
       " ['below', 'preps'],\n",
       " ['below', 'space'],\n",
       " ['below', 'relativ'],\n",
       " ['bend', 'space'],\n",
       " ['bend', 'relativ'],\n",
       " ['bending', 'space'],\n",
       " ['bending', 'relativ'],\n",
       " ['bends', 'space'],\n",
       " ['bends', 'relativ'],\n",
       " ['beneath', 'funct'],\n",
       " ['beneath', 'preps'],\n",
       " ['beneath', 'space'],\n",
       " ['beneath', 'relativ'],\n",
       " ['benefic*', 'affect'],\n",
       " ['benefic*', 'posemo'],\n",
       " ['benefit', 'affect'],\n",
       " ['benefit', 'posemo'],\n",
       " ['benefits', 'affect'],\n",
       " ['benefits', 'posemo'],\n",
       " ['benefits', 'work'],\n",
       " ['benefitt*', 'affect'],\n",
       " ['benefitt*', 'posemo'],\n",
       " ['benevolen*', 'affect'],\n",
       " ['benevolen*', 'posemo'],\n",
       " ['benign*', 'affect'],\n",
       " ['benign*', 'posemo'],\n",
       " ['bent', 'space'],\n",
       " ['bent', 'relativ'],\n",
       " ['bereave*', 'death'],\n",
       " ['beside', 'funct'],\n",
       " ['beside', 'preps'],\n",
       " ['beside', 'space'],\n",
       " ['beside', 'relativ'],\n",
       " ['besides', 'funct'],\n",
       " ['besides', 'preps'],\n",
       " ['besides', 'quant'],\n",
       " ['besides', 'cogmech'],\n",
       " ['besides', 'discrep'],\n",
       " ['best', 'affect'],\n",
       " ['best', 'posemo'],\n",
       " ['best', 'achieve'],\n",
       " ['best', 'funct'],\n",
       " ['best', 'quant'],\n",
       " ['bet', 'cogmech'],\n",
       " ['bet', 'tentat'],\n",
       " ['bet', 'money'],\n",
       " ['bets', 'cogmech'],\n",
       " ['bets', 'tentat'],\n",
       " ['bets', 'money'],\n",
       " ['better', 'affect'],\n",
       " ['better', 'posemo'],\n",
       " ['better', 'achieve'],\n",
       " ['betting', 'cogmech'],\n",
       " ['betting', 'tentat'],\n",
       " ['betting', 'money'],\n",
       " ['between', 'funct'],\n",
       " ['between', 'preps'],\n",
       " ['beyond', 'funct'],\n",
       " ['beyond', 'adverb'],\n",
       " ['beyond', 'preps'],\n",
       " ['beyond', 'space'],\n",
       " ['beyond', 'relativ'],\n",
       " ['bf*', 'social'],\n",
       " ['bf*', 'friend'],\n",
       " ['bi', 'bio'],\n",
       " ['bi', 'sexual'],\n",
       " ['biannu*', 'time'],\n",
       " ['biannu*', 'relativ'],\n",
       " ['bible*', 'relig'],\n",
       " ['biblic*', 'relig'],\n",
       " ['bicep*', 'bio'],\n",
       " ['bicep*', 'body'],\n",
       " ['bicyc*', 'leisure'],\n",
       " ['big', 'space'],\n",
       " ['big', 'relativ'],\n",
       " ['bigger', 'space'],\n",
       " ['bigger', 'relativ'],\n",
       " ['biggest', 'space'],\n",
       " ['biggest', 'relativ'],\n",
       " ['bike*', 'leisure'],\n",
       " ['bill', 'money'],\n",
       " ['billed', 'money'],\n",
       " ['billing*', 'money'],\n",
       " ['billion*', 'funct'],\n",
       " ['billion*', 'number'],\n",
       " ['bills', 'money'],\n",
       " ['bimonth*', 'time'],\n",
       " ['bimonth*', 'relativ'],\n",
       " ['binding', 'cogmech'],\n",
       " ['binding', 'inhib'],\n",
       " ['binge*', 'bio'],\n",
       " ['binge*', 'health'],\n",
       " ['binge*', 'ingest'],\n",
       " ['binging', 'bio'],\n",
       " ['binging', 'health'],\n",
       " ['binging', 'ingest'],\n",
       " ['biolog*', 'work'],\n",
       " ['bipolar', 'bio'],\n",
       " ['bipolar', 'health'],\n",
       " ['birdie*', 'leisure'],\n",
       " ['birth*', 'time'],\n",
       " ...]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liwc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_dict = {}\n",
    "for (w, c) in liwc:\n",
    "    if c not in liwc_dict:\n",
    "        liwc_dict[c] = []\n",
    "    liwc_dict[c].append(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anybod*',\n",
       " 'anyone*',\n",
       " 'anything',\n",
       " 'everybod*',\n",
       " 'everyone*',\n",
       " 'everything*',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he's\",\n",
       " 'hed',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'hes',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'id',\n",
       " 'im',\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'itd',\n",
       " 'itll',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'ive',\n",
       " \"let's\",\n",
       " 'lets',\n",
       " 'me',\n",
       " 'mine',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'nobod*',\n",
       " 'oneself',\n",
       " 'other',\n",
       " 'others',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'shes',\n",
       " 'somebod*',\n",
       " 'someone*',\n",
       " 'something*',\n",
       " 'somewhere',\n",
       " 'stuff',\n",
       " 'that',\n",
       " \"that'd\",\n",
       " \"that'll\",\n",
       " \"that's\",\n",
       " 'thatd',\n",
       " 'thatll',\n",
       " 'thats',\n",
       " 'thee',\n",
       " 'their*',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they've\",\n",
       " 'theyd',\n",
       " 'theyll',\n",
       " 'theyve',\n",
       " 'thine',\n",
       " 'thing*',\n",
       " 'this',\n",
       " 'those',\n",
       " 'thou',\n",
       " 'thoust',\n",
       " 'thy',\n",
       " 'us',\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'weve',\n",
       " 'what',\n",
       " \"what's\",\n",
       " 'whatever',\n",
       " 'whats',\n",
       " 'which',\n",
       " 'whichever',\n",
       " 'who',\n",
       " \"who'd\",\n",
       " \"who'll\",\n",
       " 'whod',\n",
       " 'wholl',\n",
       " 'whom',\n",
       " 'whose',\n",
       " \"y'all\",\n",
       " 'ya',\n",
       " 'yall',\n",
       " 'ye',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'youd',\n",
       " 'youll',\n",
       " 'your',\n",
       " 'youre',\n",
       " 'yours',\n",
       " 'youve']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liwc_dict['pronoun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_liwc_categories(tokens, category_words, relative=True):\n",
    "    category_cnt = 0\n",
    "    if not tokens:\n",
    "        return None\n",
    "    text_len = len(tokens)\n",
    "    for t in tokens:\n",
    "        for word in category_words:\n",
    "            if t==word or (word[-1]=='*' and t.startswith(word[:-1])) \\\n",
    "            or (t==word.split(\"'\")[0]):\n",
    "                category_cnt += 1\n",
    "                break # one token cannot belong to more than one word in the category\n",
    "    if relative:\n",
    "        return category_cnt/text_len\n",
    "    else:\n",
    "        return category_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding for category funct...\n",
      "Encoding for category article...\n",
      "Encoding for category affect...\n",
      "Encoding for category negemo...\n",
      "Encoding for category sad...\n",
      "Encoding for category cogmech...\n",
      "Encoding for category inhib...\n",
      "Encoding for category bio...\n",
      "Encoding for category body...\n",
      "Encoding for category achieve...\n",
      "Encoding for category health...\n",
      "Encoding for category sexual...\n",
      "Encoding for category adverb...\n",
      "Encoding for category preps...\n",
      "Encoding for category space...\n",
      "Encoding for category relativ...\n",
      "Encoding for category time...\n",
      "Encoding for category work...\n",
      "Encoding for category certain...\n",
      "Encoding for category assent...\n",
      "Encoding for category anger...\n",
      "Encoding for category posemo...\n",
      "Encoding for category insight...\n",
      "Encoding for category verb...\n",
      "Encoding for category past...\n",
      "Encoding for category money...\n",
      "Encoding for category percept...\n",
      "Encoding for category social...\n",
      "Encoding for category friend...\n",
      "Encoding for category motion...\n",
      "Encoding for category cause...\n",
      "Encoding for category leisure...\n",
      "Encoding for category incl...\n",
      "Encoding for category home...\n",
      "Encoding for category present...\n",
      "Encoding for category humans...\n",
      "Encoding for category anx...\n",
      "Encoding for category relig...\n",
      "Encoding for category auxverb...\n",
      "Encoding for category negate...\n",
      "Encoding for category ingest...\n",
      "Encoding for category death...\n",
      "Encoding for category quant...\n",
      "Encoding for category tentat...\n",
      "Encoding for category conj...\n",
      "Encoding for category pronoun...\n",
      "Encoding for category ipron...\n",
      "Encoding for category swear...\n",
      "Encoding for category hear...\n",
      "Encoding for category family...\n",
      "Encoding for category see...\n",
      "Encoding for category discrep...\n",
      "Encoding for category number...\n",
      "Encoding for category filler...\n",
      "Encoding for category feel...\n",
      "Encoding for category excl...\n",
      "Encoding for category future...\n",
      "Encoding for category nonfl...\n",
      "Encoding for category ppron...\n",
      "Encoding for category shehe...\n",
      "Encoding for category i...\n",
      "Encoding for category we...\n",
      "Encoding for category you...\n",
      "Encoding for category they...\n",
      "CPU times: user 5h 57min 55s, sys: 618 ms, total: 5h 57min 56s\n",
      "Wall time: 5h 58min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from functools import partial\n",
    "# for categ in ['negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']:#liwc_dict.keys():\n",
    "for categ in liwc_dict.keys():\n",
    "    if categ in writings_df.columns:\n",
    "        continue\n",
    "    print(\"Encoding for category %s...\" % categ)\n",
    "    writings_df[categ] = writings_df['all_tokens'].apply(partial(encode_liwc_categories, \n",
    "                                                                   category_words=liwc_dict[categ], \n",
    "                                                                   relative=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[writings_df['i']>1]\n",
    "encode_liwc_categories(['i','m','going'], liwc_dict['i'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(writings_df, open(\"writings_df_liwc3\", \"wb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject', 'title', 'date', 'text', 'label', 'tokenized_title',\n",
       "       'title_len', 'tokenized_text', 'text_len', 'all_tokens', 'funct',\n",
       "       'article', 'affect', 'negemo', 'sad', 'cogmech', 'inhib', 'bio', 'body',\n",
       "       'achieve', 'health', 'sexual', 'adverb', 'preps', 'space', 'relativ',\n",
       "       'time', 'work', 'certain', 'assent', 'anger', 'posemo', 'insight',\n",
       "       'verb', 'past', 'money', 'percept'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "writings_df = pickle.load(open(\"writings_df_liwc_part\", \"rb\"))\n",
    "writings_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>funct</th>\n",
       "      <th>article</th>\n",
       "      <th>affect</th>\n",
       "      <th>negemo</th>\n",
       "      <th>sad</th>\n",
       "      <th>cogmech</th>\n",
       "      <th>inhib</th>\n",
       "      <th>bio</th>\n",
       "      <th>body</th>\n",
       "      <th>...</th>\n",
       "      <th>feel</th>\n",
       "      <th>excl</th>\n",
       "      <th>future</th>\n",
       "      <th>nonfl</th>\n",
       "      <th>ppron</th>\n",
       "      <th>shehe</th>\n",
       "      <th>i</th>\n",
       "      <th>we</th>\n",
       "      <th>you</th>\n",
       "      <th>they</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.066626</td>\n",
       "      <td>-0.011048</td>\n",
       "      <td>0.012005</td>\n",
       "      <td>0.007687</td>\n",
       "      <td>0.003770</td>\n",
       "      <td>0.041658</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.013280</td>\n",
       "      <td>0.005446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012789</td>\n",
       "      <td>0.030731</td>\n",
       "      <td>0.057426</td>\n",
       "      <td>-0.007159</td>\n",
       "      <td>0.072440</td>\n",
       "      <td>0.017103</td>\n",
       "      <td>0.072866</td>\n",
       "      <td>-0.005369</td>\n",
       "      <td>0.018728</td>\n",
       "      <td>0.016379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>0.072866</td>\n",
       "      <td>0.272193</td>\n",
       "      <td>-0.085553</td>\n",
       "      <td>-0.029747</td>\n",
       "      <td>-0.014977</td>\n",
       "      <td>0.007909</td>\n",
       "      <td>0.053720</td>\n",
       "      <td>-0.015168</td>\n",
       "      <td>0.001689</td>\n",
       "      <td>-0.004427</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003370</td>\n",
       "      <td>0.039280</td>\n",
       "      <td>0.395652</td>\n",
       "      <td>-0.012364</td>\n",
       "      <td>0.672227</td>\n",
       "      <td>-0.035447</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.042941</td>\n",
       "      <td>-0.034232</td>\n",
       "      <td>-0.028312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ppron</th>\n",
       "      <td>0.072440</td>\n",
       "      <td>0.434561</td>\n",
       "      <td>-0.116059</td>\n",
       "      <td>-0.001906</td>\n",
       "      <td>-0.016102</td>\n",
       "      <td>0.009627</td>\n",
       "      <td>0.099705</td>\n",
       "      <td>-0.010223</td>\n",
       "      <td>-0.001588</td>\n",
       "      <td>-0.001485</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010589</td>\n",
       "      <td>0.056509</td>\n",
       "      <td>0.620799</td>\n",
       "      <td>-0.021183</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.290994</td>\n",
       "      <td>0.672227</td>\n",
       "      <td>0.202615</td>\n",
       "      <td>0.529791</td>\n",
       "      <td>0.202899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pronoun</th>\n",
       "      <td>0.070690</td>\n",
       "      <td>0.593894</td>\n",
       "      <td>-0.113121</td>\n",
       "      <td>-0.021588</td>\n",
       "      <td>-0.012200</td>\n",
       "      <td>0.006597</td>\n",
       "      <td>0.140101</td>\n",
       "      <td>-0.009750</td>\n",
       "      <td>-0.016443</td>\n",
       "      <td>-0.018998</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016266</td>\n",
       "      <td>0.095948</td>\n",
       "      <td>0.722701</td>\n",
       "      <td>-0.027504</td>\n",
       "      <td>0.769984</td>\n",
       "      <td>0.201579</td>\n",
       "      <td>0.529107</td>\n",
       "      <td>0.149585</td>\n",
       "      <td>0.412793</td>\n",
       "      <td>0.149853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funct</th>\n",
       "      <td>0.066626</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.296737</td>\n",
       "      <td>-0.137328</td>\n",
       "      <td>-0.049658</td>\n",
       "      <td>-0.005907</td>\n",
       "      <td>0.351275</td>\n",
       "      <td>-0.007653</td>\n",
       "      <td>-0.066292</td>\n",
       "      <td>-0.044489</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028299</td>\n",
       "      <td>0.275794</td>\n",
       "      <td>0.490716</td>\n",
       "      <td>-0.034641</td>\n",
       "      <td>0.434561</td>\n",
       "      <td>0.134153</td>\n",
       "      <td>0.272193</td>\n",
       "      <td>0.108061</td>\n",
       "      <td>0.215911</td>\n",
       "      <td>0.148942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verb</th>\n",
       "      <td>0.061880</td>\n",
       "      <td>0.555489</td>\n",
       "      <td>-0.054100</td>\n",
       "      <td>0.089376</td>\n",
       "      <td>-0.003435</td>\n",
       "      <td>0.011896</td>\n",
       "      <td>0.207386</td>\n",
       "      <td>0.013583</td>\n",
       "      <td>-0.011748</td>\n",
       "      <td>-0.053059</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016769</td>\n",
       "      <td>0.080685</td>\n",
       "      <td>0.760086</td>\n",
       "      <td>-0.024497</td>\n",
       "      <td>0.513685</td>\n",
       "      <td>0.110016</td>\n",
       "      <td>0.293276</td>\n",
       "      <td>0.105723</td>\n",
       "      <td>0.360806</td>\n",
       "      <td>0.101307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>present</th>\n",
       "      <td>0.060737</td>\n",
       "      <td>0.489522</td>\n",
       "      <td>-0.065232</td>\n",
       "      <td>0.118123</td>\n",
       "      <td>-0.000837</td>\n",
       "      <td>-0.002619</td>\n",
       "      <td>0.165773</td>\n",
       "      <td>0.014491</td>\n",
       "      <td>0.002479</td>\n",
       "      <td>-0.046410</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010617</td>\n",
       "      <td>0.065952</td>\n",
       "      <td>0.710962</td>\n",
       "      <td>-0.026025</td>\n",
       "      <td>0.513764</td>\n",
       "      <td>0.084266</td>\n",
       "      <td>0.284551</td>\n",
       "      <td>0.108646</td>\n",
       "      <td>0.389053</td>\n",
       "      <td>0.094309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>future</th>\n",
       "      <td>0.057426</td>\n",
       "      <td>0.490716</td>\n",
       "      <td>-0.072864</td>\n",
       "      <td>-0.001918</td>\n",
       "      <td>-0.018312</td>\n",
       "      <td>0.002283</td>\n",
       "      <td>0.184515</td>\n",
       "      <td>-0.010025</td>\n",
       "      <td>-0.035488</td>\n",
       "      <td>-0.041999</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014748</td>\n",
       "      <td>0.105019</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.017047</td>\n",
       "      <td>0.620799</td>\n",
       "      <td>0.142641</td>\n",
       "      <td>0.395652</td>\n",
       "      <td>0.102732</td>\n",
       "      <td>0.392511</td>\n",
       "      <td>0.116274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auxverb</th>\n",
       "      <td>0.056858</td>\n",
       "      <td>0.635402</td>\n",
       "      <td>-0.038364</td>\n",
       "      <td>-0.046281</td>\n",
       "      <td>-0.021598</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.167013</td>\n",
       "      <td>-0.008930</td>\n",
       "      <td>-0.052483</td>\n",
       "      <td>-0.049837</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025299</td>\n",
       "      <td>0.103584</td>\n",
       "      <td>0.809318</td>\n",
       "      <td>-0.023971</td>\n",
       "      <td>0.508941</td>\n",
       "      <td>0.117414</td>\n",
       "      <td>0.298278</td>\n",
       "      <td>0.112858</td>\n",
       "      <td>0.333056</td>\n",
       "      <td>0.115425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conj</th>\n",
       "      <td>0.046422</td>\n",
       "      <td>0.342014</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>-0.057705</td>\n",
       "      <td>-0.015910</td>\n",
       "      <td>0.008813</td>\n",
       "      <td>0.316203</td>\n",
       "      <td>-0.004944</td>\n",
       "      <td>-0.015471</td>\n",
       "      <td>-0.015445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006521</td>\n",
       "      <td>0.290814</td>\n",
       "      <td>0.099640</td>\n",
       "      <td>-0.008195</td>\n",
       "      <td>0.068227</td>\n",
       "      <td>0.039059</td>\n",
       "      <td>0.022491</td>\n",
       "      <td>0.009470</td>\n",
       "      <td>0.030777</td>\n",
       "      <td>0.071332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cogmech</th>\n",
       "      <td>0.041658</td>\n",
       "      <td>0.351275</td>\n",
       "      <td>-0.004270</td>\n",
       "      <td>-0.019908</td>\n",
       "      <td>-0.027507</td>\n",
       "      <td>-0.003507</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.206335</td>\n",
       "      <td>-0.062591</td>\n",
       "      <td>-0.039793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005936</td>\n",
       "      <td>0.412027</td>\n",
       "      <td>0.184515</td>\n",
       "      <td>-0.032613</td>\n",
       "      <td>0.099705</td>\n",
       "      <td>0.015583</td>\n",
       "      <td>0.053720</td>\n",
       "      <td>0.123286</td>\n",
       "      <td>0.014188</td>\n",
       "      <td>0.066594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excl</th>\n",
       "      <td>0.030731</td>\n",
       "      <td>0.275794</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.037601</td>\n",
       "      <td>-0.008722</td>\n",
       "      <td>0.004203</td>\n",
       "      <td>0.412027</td>\n",
       "      <td>-0.004705</td>\n",
       "      <td>-0.029667</td>\n",
       "      <td>-0.019392</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002804</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.105019</td>\n",
       "      <td>-0.012296</td>\n",
       "      <td>0.056509</td>\n",
       "      <td>0.017047</td>\n",
       "      <td>0.039280</td>\n",
       "      <td>0.004426</td>\n",
       "      <td>0.018027</td>\n",
       "      <td>0.041932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adverb</th>\n",
       "      <td>0.027288</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>-0.034601</td>\n",
       "      <td>0.006429</td>\n",
       "      <td>-0.012142</td>\n",
       "      <td>0.001793</td>\n",
       "      <td>0.187511</td>\n",
       "      <td>-0.011579</td>\n",
       "      <td>-0.035317</td>\n",
       "      <td>-0.021559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003014</td>\n",
       "      <td>0.221568</td>\n",
       "      <td>0.089067</td>\n",
       "      <td>0.060074</td>\n",
       "      <td>0.060779</td>\n",
       "      <td>0.018070</td>\n",
       "      <td>0.041777</td>\n",
       "      <td>0.007283</td>\n",
       "      <td>0.023775</td>\n",
       "      <td>0.033985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>social</th>\n",
       "      <td>0.027191</td>\n",
       "      <td>0.210192</td>\n",
       "      <td>-0.054084</td>\n",
       "      <td>0.041221</td>\n",
       "      <td>-0.008303</td>\n",
       "      <td>-0.007526</td>\n",
       "      <td>0.067898</td>\n",
       "      <td>-0.008004</td>\n",
       "      <td>0.020175</td>\n",
       "      <td>-0.013658</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024387</td>\n",
       "      <td>0.009465</td>\n",
       "      <td>0.303901</td>\n",
       "      <td>-0.023019</td>\n",
       "      <td>0.471502</td>\n",
       "      <td>0.298573</td>\n",
       "      <td>-0.039286</td>\n",
       "      <td>0.206444</td>\n",
       "      <td>0.498013</td>\n",
       "      <td>0.205197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tentat</th>\n",
       "      <td>0.026786</td>\n",
       "      <td>0.198213</td>\n",
       "      <td>-0.003291</td>\n",
       "      <td>-0.003701</td>\n",
       "      <td>-0.008462</td>\n",
       "      <td>-0.004294</td>\n",
       "      <td>0.420639</td>\n",
       "      <td>-0.014288</td>\n",
       "      <td>-0.032606</td>\n",
       "      <td>-0.020990</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003285</td>\n",
       "      <td>0.278641</td>\n",
       "      <td>0.093556</td>\n",
       "      <td>-0.009766</td>\n",
       "      <td>0.033940</td>\n",
       "      <td>-0.001348</td>\n",
       "      <td>0.022583</td>\n",
       "      <td>-0.003091</td>\n",
       "      <td>0.016722</td>\n",
       "      <td>0.036520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discrep</th>\n",
       "      <td>0.023834</td>\n",
       "      <td>0.198473</td>\n",
       "      <td>-0.005429</td>\n",
       "      <td>-0.016319</td>\n",
       "      <td>0.016224</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.320204</td>\n",
       "      <td>-0.004038</td>\n",
       "      <td>-0.026199</td>\n",
       "      <td>-0.018487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014136</td>\n",
       "      <td>0.176059</td>\n",
       "      <td>0.244915</td>\n",
       "      <td>-0.010595</td>\n",
       "      <td>0.111659</td>\n",
       "      <td>0.018886</td>\n",
       "      <td>0.068521</td>\n",
       "      <td>0.031781</td>\n",
       "      <td>0.065578</td>\n",
       "      <td>0.036880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ipron</th>\n",
       "      <td>0.022934</td>\n",
       "      <td>0.403734</td>\n",
       "      <td>-0.036531</td>\n",
       "      <td>-0.031521</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>-0.001337</td>\n",
       "      <td>0.098648</td>\n",
       "      <td>-0.002881</td>\n",
       "      <td>-0.023844</td>\n",
       "      <td>-0.027974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012650</td>\n",
       "      <td>0.081838</td>\n",
       "      <td>0.379735</td>\n",
       "      <td>-0.017414</td>\n",
       "      <td>-0.006049</td>\n",
       "      <td>-0.036994</td>\n",
       "      <td>0.013963</td>\n",
       "      <td>-0.011296</td>\n",
       "      <td>0.004416</td>\n",
       "      <td>-0.011221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>incl</th>\n",
       "      <td>0.021681</td>\n",
       "      <td>0.211569</td>\n",
       "      <td>0.020696</td>\n",
       "      <td>-0.069290</td>\n",
       "      <td>-0.025971</td>\n",
       "      <td>-0.000619</td>\n",
       "      <td>0.368911</td>\n",
       "      <td>-0.001318</td>\n",
       "      <td>-0.004113</td>\n",
       "      <td>-0.000511</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003887</td>\n",
       "      <td>0.003132</td>\n",
       "      <td>0.046892</td>\n",
       "      <td>-0.021247</td>\n",
       "      <td>0.057438</td>\n",
       "      <td>0.019194</td>\n",
       "      <td>-0.017903</td>\n",
       "      <td>0.286058</td>\n",
       "      <td>-0.029890</td>\n",
       "      <td>0.041463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insight</th>\n",
       "      <td>0.021134</td>\n",
       "      <td>0.088996</td>\n",
       "      <td>-0.003535</td>\n",
       "      <td>-0.032930</td>\n",
       "      <td>-0.030499</td>\n",
       "      <td>-0.013816</td>\n",
       "      <td>0.388386</td>\n",
       "      <td>-0.017192</td>\n",
       "      <td>-0.044351</td>\n",
       "      <td>-0.029040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038984</td>\n",
       "      <td>0.032009</td>\n",
       "      <td>0.134727</td>\n",
       "      <td>-0.010981</td>\n",
       "      <td>0.105032</td>\n",
       "      <td>0.010011</td>\n",
       "      <td>0.096398</td>\n",
       "      <td>0.008664</td>\n",
       "      <td>0.042757</td>\n",
       "      <td>0.018630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negate</th>\n",
       "      <td>0.020580</td>\n",
       "      <td>0.259552</td>\n",
       "      <td>-0.049322</td>\n",
       "      <td>-0.040522</td>\n",
       "      <td>0.003851</td>\n",
       "      <td>-0.001668</td>\n",
       "      <td>0.159371</td>\n",
       "      <td>0.014878</td>\n",
       "      <td>-0.028610</td>\n",
       "      <td>-0.015331</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014056</td>\n",
       "      <td>0.241241</td>\n",
       "      <td>0.107771</td>\n",
       "      <td>-0.011366</td>\n",
       "      <td>0.055377</td>\n",
       "      <td>0.006618</td>\n",
       "      <td>0.034814</td>\n",
       "      <td>0.030120</td>\n",
       "      <td>0.019681</td>\n",
       "      <td>0.035796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>0.018728</td>\n",
       "      <td>0.215911</td>\n",
       "      <td>-0.069260</td>\n",
       "      <td>0.071879</td>\n",
       "      <td>-0.005009</td>\n",
       "      <td>0.002473</td>\n",
       "      <td>0.014188</td>\n",
       "      <td>-0.009083</td>\n",
       "      <td>-0.004495</td>\n",
       "      <td>-0.004775</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008260</td>\n",
       "      <td>0.018027</td>\n",
       "      <td>0.392511</td>\n",
       "      <td>-0.010004</td>\n",
       "      <td>0.529791</td>\n",
       "      <td>-0.051853</td>\n",
       "      <td>-0.034232</td>\n",
       "      <td>-0.021652</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.030284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shehe</th>\n",
       "      <td>0.017103</td>\n",
       "      <td>0.134153</td>\n",
       "      <td>-0.017107</td>\n",
       "      <td>-0.024140</td>\n",
       "      <td>0.003586</td>\n",
       "      <td>0.005793</td>\n",
       "      <td>0.015583</td>\n",
       "      <td>0.004160</td>\n",
       "      <td>0.015728</td>\n",
       "      <td>0.022142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003835</td>\n",
       "      <td>0.017047</td>\n",
       "      <td>0.142641</td>\n",
       "      <td>-0.009616</td>\n",
       "      <td>0.290994</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.035447</td>\n",
       "      <td>-0.008059</td>\n",
       "      <td>-0.051853</td>\n",
       "      <td>-0.025635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>they</th>\n",
       "      <td>0.016379</td>\n",
       "      <td>0.148942</td>\n",
       "      <td>-0.016753</td>\n",
       "      <td>-0.032593</td>\n",
       "      <td>-0.007684</td>\n",
       "      <td>-0.003857</td>\n",
       "      <td>0.066594</td>\n",
       "      <td>0.006629</td>\n",
       "      <td>-0.008299</td>\n",
       "      <td>-0.003644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003726</td>\n",
       "      <td>0.041932</td>\n",
       "      <td>0.116274</td>\n",
       "      <td>-0.006211</td>\n",
       "      <td>0.202899</td>\n",
       "      <td>-0.025635</td>\n",
       "      <td>-0.028312</td>\n",
       "      <td>0.007304</td>\n",
       "      <td>-0.030284</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>past</th>\n",
       "      <td>0.015903</td>\n",
       "      <td>0.273535</td>\n",
       "      <td>0.012001</td>\n",
       "      <td>-0.041500</td>\n",
       "      <td>-0.002169</td>\n",
       "      <td>0.045307</td>\n",
       "      <td>0.176945</td>\n",
       "      <td>-0.002016</td>\n",
       "      <td>-0.032615</td>\n",
       "      <td>-0.026046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018492</td>\n",
       "      <td>0.045403</td>\n",
       "      <td>0.290613</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.174977</td>\n",
       "      <td>0.095345</td>\n",
       "      <td>0.091180</td>\n",
       "      <td>0.235544</td>\n",
       "      <td>0.013552</td>\n",
       "      <td>0.036275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>percept</th>\n",
       "      <td>0.014197</td>\n",
       "      <td>-0.049155</td>\n",
       "      <td>-0.016153</td>\n",
       "      <td>0.021227</td>\n",
       "      <td>-0.027524</td>\n",
       "      <td>-0.013142</td>\n",
       "      <td>-0.040280</td>\n",
       "      <td>-0.001280</td>\n",
       "      <td>0.080472</td>\n",
       "      <td>0.117016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.508844</td>\n",
       "      <td>-0.012598</td>\n",
       "      <td>-0.005324</td>\n",
       "      <td>-0.010131</td>\n",
       "      <td>-0.008034</td>\n",
       "      <td>0.009734</td>\n",
       "      <td>-0.005434</td>\n",
       "      <td>-0.016820</td>\n",
       "      <td>-0.006523</td>\n",
       "      <td>0.001670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quant</th>\n",
       "      <td>0.014140</td>\n",
       "      <td>0.258698</td>\n",
       "      <td>0.025899</td>\n",
       "      <td>-0.022688</td>\n",
       "      <td>-0.024025</td>\n",
       "      <td>-0.004461</td>\n",
       "      <td>0.225507</td>\n",
       "      <td>-0.019190</td>\n",
       "      <td>-0.025527</td>\n",
       "      <td>-0.018297</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011681</td>\n",
       "      <td>0.027675</td>\n",
       "      <td>-0.008119</td>\n",
       "      <td>-0.016496</td>\n",
       "      <td>-0.011811</td>\n",
       "      <td>-0.014612</td>\n",
       "      <td>-0.014970</td>\n",
       "      <td>0.017490</td>\n",
       "      <td>-0.013293</td>\n",
       "      <td>0.030822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>family</th>\n",
       "      <td>0.013364</td>\n",
       "      <td>-0.031237</td>\n",
       "      <td>-0.021085</td>\n",
       "      <td>-0.004840</td>\n",
       "      <td>-0.007354</td>\n",
       "      <td>-0.002489</td>\n",
       "      <td>-0.027806</td>\n",
       "      <td>-0.003369</td>\n",
       "      <td>-0.003517</td>\n",
       "      <td>-0.003657</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004636</td>\n",
       "      <td>-0.017871</td>\n",
       "      <td>-0.028144</td>\n",
       "      <td>-0.005697</td>\n",
       "      <td>0.026115</td>\n",
       "      <td>0.038107</td>\n",
       "      <td>0.012229</td>\n",
       "      <td>-0.001211</td>\n",
       "      <td>0.008657</td>\n",
       "      <td>-0.004157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bio</th>\n",
       "      <td>0.013280</td>\n",
       "      <td>-0.066292</td>\n",
       "      <td>-0.018656</td>\n",
       "      <td>0.087418</td>\n",
       "      <td>0.181580</td>\n",
       "      <td>-0.008744</td>\n",
       "      <td>-0.062591</td>\n",
       "      <td>-0.002863</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.604169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082299</td>\n",
       "      <td>-0.029667</td>\n",
       "      <td>-0.035488</td>\n",
       "      <td>-0.010379</td>\n",
       "      <td>-0.001588</td>\n",
       "      <td>0.015728</td>\n",
       "      <td>0.001689</td>\n",
       "      <td>-0.014957</td>\n",
       "      <td>-0.004495</td>\n",
       "      <td>-0.008299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feel</th>\n",
       "      <td>0.012789</td>\n",
       "      <td>-0.028299</td>\n",
       "      <td>-0.007182</td>\n",
       "      <td>0.028439</td>\n",
       "      <td>-0.008366</td>\n",
       "      <td>-0.005264</td>\n",
       "      <td>0.005936</td>\n",
       "      <td>0.023954</td>\n",
       "      <td>0.082299</td>\n",
       "      <td>0.130285</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002804</td>\n",
       "      <td>-0.014748</td>\n",
       "      <td>-0.007147</td>\n",
       "      <td>-0.010589</td>\n",
       "      <td>-0.003835</td>\n",
       "      <td>-0.003370</td>\n",
       "      <td>-0.011640</td>\n",
       "      <td>-0.008260</td>\n",
       "      <td>0.003726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>affect</th>\n",
       "      <td>0.012005</td>\n",
       "      <td>-0.137328</td>\n",
       "      <td>-0.089039</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.456381</td>\n",
       "      <td>0.162322</td>\n",
       "      <td>-0.019908</td>\n",
       "      <td>-0.000280</td>\n",
       "      <td>0.087418</td>\n",
       "      <td>0.024085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028439</td>\n",
       "      <td>-0.037601</td>\n",
       "      <td>-0.001918</td>\n",
       "      <td>0.031093</td>\n",
       "      <td>-0.001906</td>\n",
       "      <td>-0.024140</td>\n",
       "      <td>-0.029747</td>\n",
       "      <td>-0.023626</td>\n",
       "      <td>0.071879</td>\n",
       "      <td>-0.032593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anx</th>\n",
       "      <td>0.009488</td>\n",
       "      <td>-0.005577</td>\n",
       "      <td>-0.011406</td>\n",
       "      <td>0.131990</td>\n",
       "      <td>0.303006</td>\n",
       "      <td>0.004730</td>\n",
       "      <td>0.016068</td>\n",
       "      <td>0.023891</td>\n",
       "      <td>-0.009222</td>\n",
       "      <td>-0.004672</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004540</td>\n",
       "      <td>-0.003384</td>\n",
       "      <td>-0.005140</td>\n",
       "      <td>-0.004697</td>\n",
       "      <td>-0.008276</td>\n",
       "      <td>0.003428</td>\n",
       "      <td>-0.002927</td>\n",
       "      <td>-0.006789</td>\n",
       "      <td>-0.009475</td>\n",
       "      <td>-0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>posemo</th>\n",
       "      <td>0.008943</td>\n",
       "      <td>-0.125642</td>\n",
       "      <td>-0.087551</td>\n",
       "      <td>0.860121</td>\n",
       "      <td>-0.058048</td>\n",
       "      <td>-0.020983</td>\n",
       "      <td>-0.006464</td>\n",
       "      <td>-0.012924</td>\n",
       "      <td>-0.005219</td>\n",
       "      <td>-0.027763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036856</td>\n",
       "      <td>-0.037399</td>\n",
       "      <td>0.008671</td>\n",
       "      <td>0.031413</td>\n",
       "      <td>0.007151</td>\n",
       "      <td>-0.029133</td>\n",
       "      <td>-0.024884</td>\n",
       "      <td>-0.022470</td>\n",
       "      <td>0.083729</td>\n",
       "      <td>-0.032236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sexual</th>\n",
       "      <td>0.008160</td>\n",
       "      <td>-0.041299</td>\n",
       "      <td>-0.017843</td>\n",
       "      <td>0.154321</td>\n",
       "      <td>0.202793</td>\n",
       "      <td>-0.008194</td>\n",
       "      <td>-0.049932</td>\n",
       "      <td>-0.009066</td>\n",
       "      <td>0.577803</td>\n",
       "      <td>0.181425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004550</td>\n",
       "      <td>-0.021470</td>\n",
       "      <td>0.015554</td>\n",
       "      <td>-0.006867</td>\n",
       "      <td>0.021625</td>\n",
       "      <td>0.008935</td>\n",
       "      <td>0.023872</td>\n",
       "      <td>-0.004288</td>\n",
       "      <td>0.007622</td>\n",
       "      <td>-0.008863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relig</th>\n",
       "      <td>0.008118</td>\n",
       "      <td>-0.053764</td>\n",
       "      <td>-0.009896</td>\n",
       "      <td>0.035286</td>\n",
       "      <td>0.064105</td>\n",
       "      <td>0.005678</td>\n",
       "      <td>-0.037706</td>\n",
       "      <td>-0.004732</td>\n",
       "      <td>0.027185</td>\n",
       "      <td>0.037362</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008124</td>\n",
       "      <td>-0.020337</td>\n",
       "      <td>-0.032402</td>\n",
       "      <td>-0.007436</td>\n",
       "      <td>-0.024013</td>\n",
       "      <td>-0.008650</td>\n",
       "      <td>-0.015078</td>\n",
       "      <td>-0.001819</td>\n",
       "      <td>-0.011401</td>\n",
       "      <td>-0.011735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negemo</th>\n",
       "      <td>0.007687</td>\n",
       "      <td>-0.049658</td>\n",
       "      <td>-0.021405</td>\n",
       "      <td>0.456381</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.356025</td>\n",
       "      <td>-0.027507</td>\n",
       "      <td>0.022394</td>\n",
       "      <td>0.181580</td>\n",
       "      <td>0.096205</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008366</td>\n",
       "      <td>-0.008722</td>\n",
       "      <td>-0.018312</td>\n",
       "      <td>-0.009018</td>\n",
       "      <td>-0.016102</td>\n",
       "      <td>0.003586</td>\n",
       "      <td>-0.014977</td>\n",
       "      <td>-0.006911</td>\n",
       "      <td>-0.005009</td>\n",
       "      <td>-0.007684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <td>0.007580</td>\n",
       "      <td>-0.021233</td>\n",
       "      <td>-0.002114</td>\n",
       "      <td>-0.009287</td>\n",
       "      <td>0.033167</td>\n",
       "      <td>-0.001094</td>\n",
       "      <td>-0.014490</td>\n",
       "      <td>-0.003574</td>\n",
       "      <td>0.440736</td>\n",
       "      <td>0.034428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001157</td>\n",
       "      <td>-0.010017</td>\n",
       "      <td>-0.028148</td>\n",
       "      <td>-0.007100</td>\n",
       "      <td>-0.017934</td>\n",
       "      <td>0.004077</td>\n",
       "      <td>-0.016032</td>\n",
       "      <td>-0.008869</td>\n",
       "      <td>-0.006926</td>\n",
       "      <td>-0.006230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>certain</th>\n",
       "      <td>0.007550</td>\n",
       "      <td>0.056778</td>\n",
       "      <td>-0.017213</td>\n",
       "      <td>0.115478</td>\n",
       "      <td>-0.012082</td>\n",
       "      <td>0.003246</td>\n",
       "      <td>0.408055</td>\n",
       "      <td>-0.002800</td>\n",
       "      <td>-0.019197</td>\n",
       "      <td>-0.010651</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015844</td>\n",
       "      <td>0.015940</td>\n",
       "      <td>0.011454</td>\n",
       "      <td>-0.008087</td>\n",
       "      <td>-0.011502</td>\n",
       "      <td>-0.005406</td>\n",
       "      <td>-0.011781</td>\n",
       "      <td>0.009580</td>\n",
       "      <td>-0.010735</td>\n",
       "      <td>0.011480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hear</th>\n",
       "      <td>0.006693</td>\n",
       "      <td>0.004627</td>\n",
       "      <td>-0.010233</td>\n",
       "      <td>-0.008482</td>\n",
       "      <td>-0.001044</td>\n",
       "      <td>-0.003356</td>\n",
       "      <td>-0.012961</td>\n",
       "      <td>-0.004250</td>\n",
       "      <td>-0.001875</td>\n",
       "      <td>0.011051</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010571</td>\n",
       "      <td>0.002473</td>\n",
       "      <td>0.032772</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.015253</td>\n",
       "      <td>0.027445</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>-0.004061</td>\n",
       "      <td>0.007564</td>\n",
       "      <td>0.005810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>body</th>\n",
       "      <td>0.005446</td>\n",
       "      <td>-0.044489</td>\n",
       "      <td>-0.013600</td>\n",
       "      <td>0.024085</td>\n",
       "      <td>0.096205</td>\n",
       "      <td>-0.003152</td>\n",
       "      <td>-0.039793</td>\n",
       "      <td>0.006820</td>\n",
       "      <td>0.604169</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130285</td>\n",
       "      <td>-0.019392</td>\n",
       "      <td>-0.041999</td>\n",
       "      <td>-0.003558</td>\n",
       "      <td>-0.001485</td>\n",
       "      <td>0.022142</td>\n",
       "      <td>-0.004427</td>\n",
       "      <td>-0.009904</td>\n",
       "      <td>-0.004775</td>\n",
       "      <td>-0.003644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>humans</th>\n",
       "      <td>0.005428</td>\n",
       "      <td>-0.021195</td>\n",
       "      <td>0.010043</td>\n",
       "      <td>0.004775</td>\n",
       "      <td>0.009399</td>\n",
       "      <td>0.002031</td>\n",
       "      <td>-0.019345</td>\n",
       "      <td>-0.001534</td>\n",
       "      <td>0.010058</td>\n",
       "      <td>0.005519</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002157</td>\n",
       "      <td>-0.013441</td>\n",
       "      <td>-0.029081</td>\n",
       "      <td>-0.007971</td>\n",
       "      <td>-0.018812</td>\n",
       "      <td>0.006027</td>\n",
       "      <td>-0.023458</td>\n",
       "      <td>-0.005314</td>\n",
       "      <td>-0.008772</td>\n",
       "      <td>0.009856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ingest</th>\n",
       "      <td>0.005012</td>\n",
       "      <td>-0.033327</td>\n",
       "      <td>-0.005303</td>\n",
       "      <td>-0.029693</td>\n",
       "      <td>-0.013464</td>\n",
       "      <td>-0.006666</td>\n",
       "      <td>-0.020905</td>\n",
       "      <td>-0.003472</td>\n",
       "      <td>0.461305</td>\n",
       "      <td>0.041873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014227</td>\n",
       "      <td>-0.008672</td>\n",
       "      <td>-0.025695</td>\n",
       "      <td>-0.005690</td>\n",
       "      <td>-0.013871</td>\n",
       "      <td>-0.003198</td>\n",
       "      <td>-0.008133</td>\n",
       "      <td>-0.007849</td>\n",
       "      <td>-0.008156</td>\n",
       "      <td>-0.000553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>see</th>\n",
       "      <td>0.004064</td>\n",
       "      <td>-0.035446</td>\n",
       "      <td>-0.005125</td>\n",
       "      <td>-0.005052</td>\n",
       "      <td>-0.031832</td>\n",
       "      <td>-0.010736</td>\n",
       "      <td>-0.046643</td>\n",
       "      <td>-0.016289</td>\n",
       "      <td>0.022211</td>\n",
       "      <td>0.053808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011118</td>\n",
       "      <td>-0.016052</td>\n",
       "      <td>-0.007273</td>\n",
       "      <td>-0.011358</td>\n",
       "      <td>-0.004228</td>\n",
       "      <td>0.004968</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>-0.009792</td>\n",
       "      <td>-0.006255</td>\n",
       "      <td>-0.004169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sad</th>\n",
       "      <td>0.003770</td>\n",
       "      <td>-0.005907</td>\n",
       "      <td>-0.010455</td>\n",
       "      <td>0.162322</td>\n",
       "      <td>0.356025</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.003507</td>\n",
       "      <td>0.011269</td>\n",
       "      <td>-0.008744</td>\n",
       "      <td>-0.003152</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005264</td>\n",
       "      <td>0.004203</td>\n",
       "      <td>0.002283</td>\n",
       "      <td>-0.005923</td>\n",
       "      <td>0.009627</td>\n",
       "      <td>0.005793</td>\n",
       "      <td>0.007909</td>\n",
       "      <td>0.005514</td>\n",
       "      <td>0.002473</td>\n",
       "      <td>-0.003857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home</th>\n",
       "      <td>0.003063</td>\n",
       "      <td>-0.018186</td>\n",
       "      <td>0.025780</td>\n",
       "      <td>-0.024561</td>\n",
       "      <td>-0.010184</td>\n",
       "      <td>-0.002654</td>\n",
       "      <td>-0.025220</td>\n",
       "      <td>-0.002571</td>\n",
       "      <td>0.007947</td>\n",
       "      <td>-0.001860</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004178</td>\n",
       "      <td>-0.014044</td>\n",
       "      <td>-0.033798</td>\n",
       "      <td>-0.006542</td>\n",
       "      <td>-0.010036</td>\n",
       "      <td>0.006242</td>\n",
       "      <td>-0.006818</td>\n",
       "      <td>-0.004046</td>\n",
       "      <td>-0.009460</td>\n",
       "      <td>-0.003155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swear</th>\n",
       "      <td>0.002709</td>\n",
       "      <td>-0.052514</td>\n",
       "      <td>-0.015183</td>\n",
       "      <td>0.225986</td>\n",
       "      <td>0.472177</td>\n",
       "      <td>-0.001558</td>\n",
       "      <td>-0.054581</td>\n",
       "      <td>-0.009729</td>\n",
       "      <td>0.388321</td>\n",
       "      <td>0.313320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009847</td>\n",
       "      <td>-0.021923</td>\n",
       "      <td>-0.007406</td>\n",
       "      <td>0.002901</td>\n",
       "      <td>-0.012909</td>\n",
       "      <td>-0.002706</td>\n",
       "      <td>-0.014247</td>\n",
       "      <td>-0.006533</td>\n",
       "      <td>0.004479</td>\n",
       "      <td>-0.010712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.002480</td>\n",
       "      <td>-0.058934</td>\n",
       "      <td>-0.010586</td>\n",
       "      <td>0.334055</td>\n",
       "      <td>0.722670</td>\n",
       "      <td>-0.002968</td>\n",
       "      <td>-0.056701</td>\n",
       "      <td>0.002059</td>\n",
       "      <td>0.244889</td>\n",
       "      <td>0.143005</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000621</td>\n",
       "      <td>-0.021594</td>\n",
       "      <td>-0.020651</td>\n",
       "      <td>-0.002606</td>\n",
       "      <td>-0.021716</td>\n",
       "      <td>0.003896</td>\n",
       "      <td>-0.024981</td>\n",
       "      <td>-0.004693</td>\n",
       "      <td>-0.005086</td>\n",
       "      <td>-0.003657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assent</th>\n",
       "      <td>0.001831</td>\n",
       "      <td>-0.140359</td>\n",
       "      <td>-0.079256</td>\n",
       "      <td>0.235887</td>\n",
       "      <td>-0.010531</td>\n",
       "      <td>-0.012687</td>\n",
       "      <td>-0.068752</td>\n",
       "      <td>-0.013197</td>\n",
       "      <td>-0.021024</td>\n",
       "      <td>-0.014262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063660</td>\n",
       "      <td>-0.033139</td>\n",
       "      <td>-0.036612</td>\n",
       "      <td>0.333346</td>\n",
       "      <td>-0.041445</td>\n",
       "      <td>-0.023643</td>\n",
       "      <td>-0.015536</td>\n",
       "      <td>-0.016438</td>\n",
       "      <td>-0.021492</td>\n",
       "      <td>-0.020773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cause</th>\n",
       "      <td>0.001037</td>\n",
       "      <td>0.057825</td>\n",
       "      <td>-0.004966</td>\n",
       "      <td>-0.040745</td>\n",
       "      <td>-0.022357</td>\n",
       "      <td>-0.003472</td>\n",
       "      <td>0.366576</td>\n",
       "      <td>0.007141</td>\n",
       "      <td>-0.023796</td>\n",
       "      <td>-0.018634</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005043</td>\n",
       "      <td>0.010123</td>\n",
       "      <td>0.022566</td>\n",
       "      <td>-0.013874</td>\n",
       "      <td>0.005575</td>\n",
       "      <td>-0.004560</td>\n",
       "      <td>-0.009069</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.010994</td>\n",
       "      <td>0.029184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inhib</th>\n",
       "      <td>0.000530</td>\n",
       "      <td>-0.007653</td>\n",
       "      <td>-0.004455</td>\n",
       "      <td>-0.000280</td>\n",
       "      <td>0.022394</td>\n",
       "      <td>0.011269</td>\n",
       "      <td>0.206335</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002863</td>\n",
       "      <td>0.006820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023954</td>\n",
       "      <td>-0.004705</td>\n",
       "      <td>-0.010025</td>\n",
       "      <td>-0.008368</td>\n",
       "      <td>-0.010223</td>\n",
       "      <td>0.004160</td>\n",
       "      <td>-0.015168</td>\n",
       "      <td>0.011856</td>\n",
       "      <td>-0.009083</td>\n",
       "      <td>0.006629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preps</th>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.385074</td>\n",
       "      <td>0.137381</td>\n",
       "      <td>-0.143983</td>\n",
       "      <td>-0.048028</td>\n",
       "      <td>-0.019362</td>\n",
       "      <td>0.080231</td>\n",
       "      <td>0.020991</td>\n",
       "      <td>-0.030081</td>\n",
       "      <td>-0.008894</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015460</td>\n",
       "      <td>-0.006035</td>\n",
       "      <td>-0.053166</td>\n",
       "      <td>-0.038181</td>\n",
       "      <td>-0.029605</td>\n",
       "      <td>0.026515</td>\n",
       "      <td>-0.039154</td>\n",
       "      <td>0.012331</td>\n",
       "      <td>-0.043308</td>\n",
       "      <td>0.046606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            label     funct   article    affect    negemo       sad   cogmech  \\\n",
       "label    1.000000  0.066626 -0.011048  0.012005  0.007687  0.003770  0.041658   \n",
       "i        0.072866  0.272193 -0.085553 -0.029747 -0.014977  0.007909  0.053720   \n",
       "ppron    0.072440  0.434561 -0.116059 -0.001906 -0.016102  0.009627  0.099705   \n",
       "pronoun  0.070690  0.593894 -0.113121 -0.021588 -0.012200  0.006597  0.140101   \n",
       "funct    0.066626  1.000000  0.296737 -0.137328 -0.049658 -0.005907  0.351275   \n",
       "verb     0.061880  0.555489 -0.054100  0.089376 -0.003435  0.011896  0.207386   \n",
       "present  0.060737  0.489522 -0.065232  0.118123 -0.000837 -0.002619  0.165773   \n",
       "future   0.057426  0.490716 -0.072864 -0.001918 -0.018312  0.002283  0.184515   \n",
       "auxverb  0.056858  0.635402 -0.038364 -0.046281 -0.021598  0.000881  0.167013   \n",
       "conj     0.046422  0.342014  0.001092 -0.057705 -0.015910  0.008813  0.316203   \n",
       "cogmech  0.041658  0.351275 -0.004270 -0.019908 -0.027507 -0.003507  1.000000   \n",
       "excl     0.030731  0.275794 -0.000053 -0.037601 -0.008722  0.004203  0.412027   \n",
       "adverb   0.027288  0.367347 -0.034601  0.006429 -0.012142  0.001793  0.187511   \n",
       "social   0.027191  0.210192 -0.054084  0.041221 -0.008303 -0.007526  0.067898   \n",
       "tentat   0.026786  0.198213 -0.003291 -0.003701 -0.008462 -0.004294  0.420639   \n",
       "discrep  0.023834  0.198473 -0.005429 -0.016319  0.016224  0.000723  0.320204   \n",
       "ipron    0.022934  0.403734 -0.036531 -0.031521  0.000407 -0.001337  0.098648   \n",
       "incl     0.021681  0.211569  0.020696 -0.069290 -0.025971 -0.000619  0.368911   \n",
       "insight  0.021134  0.088996 -0.003535 -0.032930 -0.030499 -0.013816  0.388386   \n",
       "negate   0.020580  0.259552 -0.049322 -0.040522  0.003851 -0.001668  0.159371   \n",
       "you      0.018728  0.215911 -0.069260  0.071879 -0.005009  0.002473  0.014188   \n",
       "shehe    0.017103  0.134153 -0.017107 -0.024140  0.003586  0.005793  0.015583   \n",
       "they     0.016379  0.148942 -0.016753 -0.032593 -0.007684 -0.003857  0.066594   \n",
       "past     0.015903  0.273535  0.012001 -0.041500 -0.002169  0.045307  0.176945   \n",
       "percept  0.014197 -0.049155 -0.016153  0.021227 -0.027524 -0.013142 -0.040280   \n",
       "quant    0.014140  0.258698  0.025899 -0.022688 -0.024025 -0.004461  0.225507   \n",
       "family   0.013364 -0.031237 -0.021085 -0.004840 -0.007354 -0.002489 -0.027806   \n",
       "bio      0.013280 -0.066292 -0.018656  0.087418  0.181580 -0.008744 -0.062591   \n",
       "feel     0.012789 -0.028299 -0.007182  0.028439 -0.008366 -0.005264  0.005936   \n",
       "affect   0.012005 -0.137328 -0.089039  1.000000  0.456381  0.162322 -0.019908   \n",
       "anx      0.009488 -0.005577 -0.011406  0.131990  0.303006  0.004730  0.016068   \n",
       "posemo   0.008943 -0.125642 -0.087551  0.860121 -0.058048 -0.020983 -0.006464   \n",
       "sexual   0.008160 -0.041299 -0.017843  0.154321  0.202793 -0.008194 -0.049932   \n",
       "relig    0.008118 -0.053764 -0.009896  0.035286  0.064105  0.005678 -0.037706   \n",
       "negemo   0.007687 -0.049658 -0.021405  0.456381  1.000000  0.356025 -0.027507   \n",
       "health   0.007580 -0.021233 -0.002114 -0.009287  0.033167 -0.001094 -0.014490   \n",
       "certain  0.007550  0.056778 -0.017213  0.115478 -0.012082  0.003246  0.408055   \n",
       "hear     0.006693  0.004627 -0.010233 -0.008482 -0.001044 -0.003356 -0.012961   \n",
       "body     0.005446 -0.044489 -0.013600  0.024085  0.096205 -0.003152 -0.039793   \n",
       "humans   0.005428 -0.021195  0.010043  0.004775  0.009399  0.002031 -0.019345   \n",
       "ingest   0.005012 -0.033327 -0.005303 -0.029693 -0.013464 -0.006666 -0.020905   \n",
       "see      0.004064 -0.035446 -0.005125 -0.005052 -0.031832 -0.010736 -0.046643   \n",
       "sad      0.003770 -0.005907 -0.010455  0.162322  0.356025  1.000000 -0.003507   \n",
       "home     0.003063 -0.018186  0.025780 -0.024561 -0.010184 -0.002654 -0.025220   \n",
       "swear    0.002709 -0.052514 -0.015183  0.225986  0.472177 -0.001558 -0.054581   \n",
       "anger    0.002480 -0.058934 -0.010586  0.334055  0.722670 -0.002968 -0.056701   \n",
       "assent   0.001831 -0.140359 -0.079256  0.235887 -0.010531 -0.012687 -0.068752   \n",
       "cause    0.001037  0.057825 -0.004966 -0.040745 -0.022357 -0.003472  0.366576   \n",
       "inhib    0.000530 -0.007653 -0.004455 -0.000280  0.022394  0.011269  0.206335   \n",
       "preps    0.000441  0.385074  0.137381 -0.143983 -0.048028 -0.019362  0.080231   \n",
       "\n",
       "            inhib       bio      body  ...      feel      excl    future  \\\n",
       "label    0.000530  0.013280  0.005446  ...  0.012789  0.030731  0.057426   \n",
       "i       -0.015168  0.001689 -0.004427  ... -0.003370  0.039280  0.395652   \n",
       "ppron   -0.010223 -0.001588 -0.001485  ... -0.010589  0.056509  0.620799   \n",
       "pronoun -0.009750 -0.016443 -0.018998  ... -0.016266  0.095948  0.722701   \n",
       "funct   -0.007653 -0.066292 -0.044489  ... -0.028299  0.275794  0.490716   \n",
       "verb     0.013583 -0.011748 -0.053059  ... -0.016769  0.080685  0.760086   \n",
       "present  0.014491  0.002479 -0.046410  ... -0.010617  0.065952  0.710962   \n",
       "future  -0.010025 -0.035488 -0.041999  ... -0.014748  0.105019  1.000000   \n",
       "auxverb -0.008930 -0.052483 -0.049837  ... -0.025299  0.103584  0.809318   \n",
       "conj    -0.004944 -0.015471 -0.015445  ...  0.006521  0.290814  0.099640   \n",
       "cogmech  0.206335 -0.062591 -0.039793  ...  0.005936  0.412027  0.184515   \n",
       "excl    -0.004705 -0.029667 -0.019392  ... -0.002804  1.000000  0.105019   \n",
       "adverb  -0.011579 -0.035317 -0.021559  ...  0.003014  0.221568  0.089067   \n",
       "social  -0.008004  0.020175 -0.013658  ... -0.024387  0.009465  0.303901   \n",
       "tentat  -0.014288 -0.032606 -0.020990  ... -0.003285  0.278641  0.093556   \n",
       "discrep -0.004038 -0.026199 -0.018487  ... -0.014136  0.176059  0.244915   \n",
       "ipron   -0.002881 -0.023844 -0.027974  ... -0.012650  0.081838  0.379735   \n",
       "incl    -0.001318 -0.004113 -0.000511  ... -0.003887  0.003132  0.046892   \n",
       "insight -0.017192 -0.044351 -0.029040  ...  0.038984  0.032009  0.134727   \n",
       "negate   0.014878 -0.028610 -0.015331  ... -0.014056  0.241241  0.107771   \n",
       "you     -0.009083 -0.004495 -0.004775  ... -0.008260  0.018027  0.392511   \n",
       "shehe    0.004160  0.015728  0.022142  ... -0.003835  0.017047  0.142641   \n",
       "they     0.006629 -0.008299 -0.003644  ...  0.003726  0.041932  0.116274   \n",
       "past    -0.002016 -0.032615 -0.026046  ... -0.018492  0.045403  0.290613   \n",
       "percept -0.001280  0.080472  0.117016  ...  0.508844 -0.012598 -0.005324   \n",
       "quant   -0.019190 -0.025527 -0.018297  ... -0.011681  0.027675 -0.008119   \n",
       "family  -0.003369 -0.003517 -0.003657  ... -0.004636 -0.017871 -0.028144   \n",
       "bio     -0.002863  1.000000  0.604169  ...  0.082299 -0.029667 -0.035488   \n",
       "feel     0.023954  0.082299  0.130285  ...  1.000000 -0.002804 -0.014748   \n",
       "affect  -0.000280  0.087418  0.024085  ...  0.028439 -0.037601 -0.001918   \n",
       "anx      0.023891 -0.009222 -0.004672  ... -0.004540 -0.003384 -0.005140   \n",
       "posemo  -0.012924 -0.005219 -0.027763  ...  0.036856 -0.037399  0.008671   \n",
       "sexual  -0.009066  0.577803  0.181425  ...  0.004550 -0.021470  0.015554   \n",
       "relig   -0.004732  0.027185  0.037362  ... -0.008124 -0.020337 -0.032402   \n",
       "negemo   0.022394  0.181580  0.096205  ... -0.008366 -0.008722 -0.018312   \n",
       "health  -0.003574  0.440736  0.034428  ...  0.001157 -0.010017 -0.028148   \n",
       "certain -0.002800 -0.019197 -0.010651  ... -0.015844  0.015940  0.011454   \n",
       "hear    -0.004250 -0.001875  0.011051  ... -0.010571  0.002473  0.032772   \n",
       "body     0.006820  0.604169  1.000000  ...  0.130285 -0.019392 -0.041999   \n",
       "humans  -0.001534  0.010058  0.005519  ... -0.002157 -0.013441 -0.029081   \n",
       "ingest  -0.003472  0.461305  0.041873  ...  0.014227 -0.008672 -0.025695   \n",
       "see     -0.016289  0.022211  0.053808  ...  0.011118 -0.016052 -0.007273   \n",
       "sad      0.011269 -0.008744 -0.003152  ... -0.005264  0.004203  0.002283   \n",
       "home    -0.002571  0.007947 -0.001860  ... -0.004178 -0.014044 -0.033798   \n",
       "swear   -0.009729  0.388321  0.313320  ...  0.009847 -0.021923 -0.007406   \n",
       "anger    0.002059  0.244889  0.143005  ... -0.000621 -0.021594 -0.020651   \n",
       "assent  -0.013197 -0.021024 -0.014262  ...  0.063660 -0.033139 -0.036612   \n",
       "cause    0.007141 -0.023796 -0.018634  ... -0.005043  0.010123  0.022566   \n",
       "inhib    1.000000 -0.002863  0.006820  ...  0.023954 -0.004705 -0.010025   \n",
       "preps    0.020991 -0.030081 -0.008894  ... -0.015460 -0.006035 -0.053166   \n",
       "\n",
       "            nonfl     ppron     shehe         i        we       you      they  \n",
       "label   -0.007159  0.072440  0.017103  0.072866 -0.005369  0.018728  0.016379  \n",
       "i       -0.012364  0.672227 -0.035447  1.000000 -0.042941 -0.034232 -0.028312  \n",
       "ppron   -0.021183  1.000000  0.290994  0.672227  0.202615  0.529791  0.202899  \n",
       "pronoun -0.027504  0.769984  0.201579  0.529107  0.149585  0.412793  0.149853  \n",
       "funct   -0.034641  0.434561  0.134153  0.272193  0.108061  0.215911  0.148942  \n",
       "verb    -0.024497  0.513685  0.110016  0.293276  0.105723  0.360806  0.101307  \n",
       "present -0.026025  0.513764  0.084266  0.284551  0.108646  0.389053  0.094309  \n",
       "future  -0.017047  0.620799  0.142641  0.395652  0.102732  0.392511  0.116274  \n",
       "auxverb -0.023971  0.508941  0.117414  0.298278  0.112858  0.333056  0.115425  \n",
       "conj    -0.008195  0.068227  0.039059  0.022491  0.009470  0.030777  0.071332  \n",
       "cogmech -0.032613  0.099705  0.015583  0.053720  0.123286  0.014188  0.066594  \n",
       "excl    -0.012296  0.056509  0.017047  0.039280  0.004426  0.018027  0.041932  \n",
       "adverb   0.060074  0.060779  0.018070  0.041777  0.007283  0.023775  0.033985  \n",
       "social  -0.023019  0.471502  0.298573 -0.039286  0.206444  0.498013  0.205197  \n",
       "tentat  -0.009766  0.033940 -0.001348  0.022583 -0.003091  0.016722  0.036520  \n",
       "discrep -0.010595  0.111659  0.018886  0.068521  0.031781  0.065578  0.036880  \n",
       "ipron   -0.017414 -0.006049 -0.036994  0.013963 -0.011296  0.004416 -0.011221  \n",
       "incl    -0.021247  0.057438  0.019194 -0.017903  0.286058 -0.029890  0.041463  \n",
       "insight -0.010981  0.105032  0.010011  0.096398  0.008664  0.042757  0.018630  \n",
       "negate  -0.011366  0.055377  0.006618  0.034814  0.030120  0.019681  0.035796  \n",
       "you     -0.010004  0.529791 -0.051853 -0.034232 -0.021652  1.000000 -0.030284  \n",
       "shehe   -0.009616  0.290994  1.000000 -0.035447 -0.008059 -0.051853 -0.025635  \n",
       "they    -0.006211  0.202899 -0.025635 -0.028312  0.007304 -0.030284  1.000000  \n",
       "past     0.000997  0.174977  0.095345  0.091180  0.235544  0.013552  0.036275  \n",
       "percept -0.010131 -0.008034  0.009734 -0.005434 -0.016820 -0.006523  0.001670  \n",
       "quant   -0.016496 -0.011811 -0.014612 -0.014970  0.017490 -0.013293  0.030822  \n",
       "family  -0.005697  0.026115  0.038107  0.012229 -0.001211  0.008657 -0.004157  \n",
       "bio     -0.010379 -0.001588  0.015728  0.001689 -0.014957 -0.004495 -0.008299  \n",
       "feel    -0.007147 -0.010589 -0.003835 -0.003370 -0.011640 -0.008260  0.003726  \n",
       "affect   0.031093 -0.001906 -0.024140 -0.029747 -0.023626  0.071879 -0.032593  \n",
       "anx     -0.004697 -0.008276  0.003428 -0.002927 -0.006789 -0.009475 -0.000600  \n",
       "posemo   0.031413  0.007151 -0.029133 -0.024884 -0.022470  0.083729 -0.032236  \n",
       "sexual  -0.006867  0.021625  0.008935  0.023872 -0.004288  0.007622 -0.008863  \n",
       "relig   -0.007436 -0.024013 -0.008650 -0.015078 -0.001819 -0.011401 -0.011735  \n",
       "negemo  -0.009018 -0.016102  0.003586 -0.014977 -0.006911 -0.005009 -0.007684  \n",
       "health  -0.007100 -0.017934  0.004077 -0.016032 -0.008869 -0.006926 -0.006230  \n",
       "certain -0.008087 -0.011502 -0.005406 -0.011781  0.009580 -0.010735  0.011480  \n",
       "hear     0.004700  0.015253  0.027445  0.000869 -0.004061  0.007564  0.005810  \n",
       "body    -0.003558 -0.001485  0.022142 -0.004427 -0.009904 -0.004775 -0.003644  \n",
       "humans  -0.007971 -0.018812  0.006027 -0.023458 -0.005314 -0.008772  0.009856  \n",
       "ingest  -0.005690 -0.013871 -0.003198 -0.008133 -0.007849 -0.008156 -0.000553  \n",
       "see     -0.011358 -0.004228  0.004968  0.001565 -0.009792 -0.006255 -0.004169  \n",
       "sad     -0.005923  0.009627  0.005793  0.007909  0.005514  0.002473 -0.003857  \n",
       "home    -0.006542 -0.010036  0.006242 -0.006818 -0.004046 -0.009460 -0.003155  \n",
       "swear    0.002901 -0.012909 -0.002706 -0.014247 -0.006533  0.004479 -0.010712  \n",
       "anger   -0.002606 -0.021716  0.003896 -0.024981 -0.004693 -0.005086 -0.003657  \n",
       "assent   0.333346 -0.041445 -0.023643 -0.015536 -0.016438 -0.021492 -0.020773  \n",
       "cause   -0.013874  0.005575 -0.004560 -0.009069  0.000219  0.010994  0.029184  \n",
       "inhib   -0.008368 -0.010223  0.004160 -0.015168  0.011856 -0.009083  0.006629  \n",
       "preps   -0.038181 -0.029605  0.026515 -0.039154  0.012331 -0.043308  0.046606  \n",
       "\n",
       "[50 rows x 65 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['label'] + list(liwc_dict.keys())].corr().sort_values(by='label', ascending=False)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>negemo</th>\n",
       "      <th>posemo</th>\n",
       "      <th>affect</th>\n",
       "      <th>sad</th>\n",
       "      <th>anx</th>\n",
       "      <th>pronoun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007687</td>\n",
       "      <td>0.008943</td>\n",
       "      <td>0.012005</td>\n",
       "      <td>0.003770</td>\n",
       "      <td>0.009488</td>\n",
       "      <td>0.070690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negemo</th>\n",
       "      <td>0.007687</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.058048</td>\n",
       "      <td>0.456381</td>\n",
       "      <td>0.356025</td>\n",
       "      <td>0.303006</td>\n",
       "      <td>-0.012200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>posemo</th>\n",
       "      <td>0.008943</td>\n",
       "      <td>-0.058048</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.860121</td>\n",
       "      <td>-0.020983</td>\n",
       "      <td>-0.024652</td>\n",
       "      <td>-0.016989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>affect</th>\n",
       "      <td>0.012005</td>\n",
       "      <td>0.456381</td>\n",
       "      <td>0.860121</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.162322</td>\n",
       "      <td>0.131990</td>\n",
       "      <td>-0.021588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sad</th>\n",
       "      <td>0.003770</td>\n",
       "      <td>0.356025</td>\n",
       "      <td>-0.020983</td>\n",
       "      <td>0.162322</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004730</td>\n",
       "      <td>0.006597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anx</th>\n",
       "      <td>0.009488</td>\n",
       "      <td>0.303006</td>\n",
       "      <td>-0.024652</td>\n",
       "      <td>0.131990</td>\n",
       "      <td>0.004730</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.005745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pronoun</th>\n",
       "      <td>0.070690</td>\n",
       "      <td>-0.012200</td>\n",
       "      <td>-0.016989</td>\n",
       "      <td>-0.021588</td>\n",
       "      <td>0.006597</td>\n",
       "      <td>-0.005745</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            label    negemo    posemo    affect       sad       anx   pronoun\n",
       "label    1.000000  0.007687  0.008943  0.012005  0.003770  0.009488  0.070690\n",
       "negemo   0.007687  1.000000 -0.058048  0.456381  0.356025  0.303006 -0.012200\n",
       "posemo   0.008943 -0.058048  1.000000  0.860121 -0.020983 -0.024652 -0.016989\n",
       "affect   0.012005  0.456381  0.860121  1.000000  0.162322  0.131990 -0.021588\n",
       "sad      0.003770  0.356025 -0.020983  0.162322  1.000000  0.004730  0.006597\n",
       "anx      0.009488  0.303006 -0.024652  0.131990  0.004730  1.000000 -0.005745\n",
       "pronoun  0.070690 -0.012200 -0.016989 -0.021588  0.006597 -0.005745  1.000000"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['label', 'negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>negemo</th>\n",
       "      <th>posemo</th>\n",
       "      <th>affect</th>\n",
       "      <th>sad</th>\n",
       "      <th>anx</th>\n",
       "      <th>pronoun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.089955</td>\n",
       "      <td>0.052222</td>\n",
       "      <td>0.083142</td>\n",
       "      <td>0.160260</td>\n",
       "      <td>0.256804</td>\n",
       "      <td>0.426963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negemo</th>\n",
       "      <td>0.089955</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.116086</td>\n",
       "      <td>0.219462</td>\n",
       "      <td>0.471302</td>\n",
       "      <td>0.368127</td>\n",
       "      <td>0.140835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>posemo</th>\n",
       "      <td>0.052222</td>\n",
       "      <td>-0.116086</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.943433</td>\n",
       "      <td>-0.006218</td>\n",
       "      <td>-0.123012</td>\n",
       "      <td>0.155976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>affect</th>\n",
       "      <td>0.083142</td>\n",
       "      <td>0.219462</td>\n",
       "      <td>0.943433</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.152016</td>\n",
       "      <td>0.003445</td>\n",
       "      <td>0.202396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sad</th>\n",
       "      <td>0.160260</td>\n",
       "      <td>0.471302</td>\n",
       "      <td>-0.006218</td>\n",
       "      <td>0.152016</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.124741</td>\n",
       "      <td>0.151703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anx</th>\n",
       "      <td>0.256804</td>\n",
       "      <td>0.368127</td>\n",
       "      <td>-0.123012</td>\n",
       "      <td>0.003445</td>\n",
       "      <td>0.124741</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.189642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pronoun</th>\n",
       "      <td>0.426963</td>\n",
       "      <td>0.140835</td>\n",
       "      <td>0.155976</td>\n",
       "      <td>0.202396</td>\n",
       "      <td>0.151703</td>\n",
       "      <td>0.189642</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            label    negemo    posemo    affect       sad       anx   pronoun\n",
       "label    1.000000  0.089955  0.052222  0.083142  0.160260  0.256804  0.426963\n",
       "negemo   0.089955  1.000000 -0.116086  0.219462  0.471302  0.368127  0.140835\n",
       "posemo   0.052222 -0.116086  1.000000  0.943433 -0.006218 -0.123012  0.155976\n",
       "affect   0.083142  0.219462  0.943433  1.000000  0.152016  0.003445  0.202396\n",
       "sad      0.160260  0.471302 -0.006218  0.152016  1.000000  0.124741  0.151703\n",
       "anx      0.256804  0.368127 -0.123012  0.003445  0.124741  1.000000  0.189642\n",
       "pronoun  0.426963  0.140835  0.155976  0.202396  0.151703  0.189642  1.000000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.groupby('subject').mean()[['label', 'negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>funct</th>\n",
       "      <th>article</th>\n",
       "      <th>affect</th>\n",
       "      <th>negemo</th>\n",
       "      <th>sad</th>\n",
       "      <th>cogmech</th>\n",
       "      <th>inhib</th>\n",
       "      <th>bio</th>\n",
       "      <th>body</th>\n",
       "      <th>...</th>\n",
       "      <th>feel</th>\n",
       "      <th>excl</th>\n",
       "      <th>future</th>\n",
       "      <th>nonfl</th>\n",
       "      <th>ppron</th>\n",
       "      <th>shehe</th>\n",
       "      <th>i</th>\n",
       "      <th>we</th>\n",
       "      <th>you</th>\n",
       "      <th>they</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>-0.197958</td>\n",
       "      <td>-0.078190</td>\n",
       "      <td>0.164134</td>\n",
       "      <td>-0.161045</td>\n",
       "      <td>-0.110000</td>\n",
       "      <td>-0.119536</td>\n",
       "      <td>-0.001790</td>\n",
       "      <td>0.173579</td>\n",
       "      <td>-0.185892</td>\n",
       "      <td>-0.108384</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.159383</td>\n",
       "      <td>-0.140420</td>\n",
       "      <td>-0.213641</td>\n",
       "      <td>-0.152713</td>\n",
       "      <td>-0.255025</td>\n",
       "      <td>-0.089858</td>\n",
       "      <td>-0.225474</td>\n",
       "      <td>0.030485</td>\n",
       "      <td>-0.160479</td>\n",
       "      <td>0.007994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article</th>\n",
       "      <td>-0.124233</td>\n",
       "      <td>0.396806</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.240369</td>\n",
       "      <td>0.008891</td>\n",
       "      <td>-0.012702</td>\n",
       "      <td>0.263779</td>\n",
       "      <td>0.074441</td>\n",
       "      <td>0.052556</td>\n",
       "      <td>0.050284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020717</td>\n",
       "      <td>0.138179</td>\n",
       "      <td>-0.001219</td>\n",
       "      <td>-0.063174</td>\n",
       "      <td>-0.118329</td>\n",
       "      <td>0.128260</td>\n",
       "      <td>-0.185701</td>\n",
       "      <td>0.204698</td>\n",
       "      <td>-0.171638</td>\n",
       "      <td>0.200865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>-0.116822</td>\n",
       "      <td>0.151113</td>\n",
       "      <td>0.204698</td>\n",
       "      <td>-0.155738</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.092360</td>\n",
       "      <td>0.157903</td>\n",
       "      <td>0.036809</td>\n",
       "      <td>-0.062165</td>\n",
       "      <td>-0.041142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.093426</td>\n",
       "      <td>0.016650</td>\n",
       "      <td>0.092645</td>\n",
       "      <td>-0.102804</td>\n",
       "      <td>0.071740</td>\n",
       "      <td>0.061737</td>\n",
       "      <td>-0.146144</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.067507</td>\n",
       "      <td>0.235515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leisure</th>\n",
       "      <td>-0.112231</td>\n",
       "      <td>-0.143708</td>\n",
       "      <td>-0.004798</td>\n",
       "      <td>-0.086731</td>\n",
       "      <td>-0.062152</td>\n",
       "      <td>-0.124786</td>\n",
       "      <td>-0.083327</td>\n",
       "      <td>-0.067570</td>\n",
       "      <td>-0.094987</td>\n",
       "      <td>-0.002267</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036881</td>\n",
       "      <td>-0.117458</td>\n",
       "      <td>-0.208380</td>\n",
       "      <td>-0.010841</td>\n",
       "      <td>-0.219850</td>\n",
       "      <td>-0.015029</td>\n",
       "      <td>-0.224056</td>\n",
       "      <td>0.060854</td>\n",
       "      <td>-0.111190</td>\n",
       "      <td>-0.074291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relig</th>\n",
       "      <td>-0.111693</td>\n",
       "      <td>-0.168276</td>\n",
       "      <td>-0.113201</td>\n",
       "      <td>-0.002934</td>\n",
       "      <td>0.131581</td>\n",
       "      <td>0.019087</td>\n",
       "      <td>-0.168061</td>\n",
       "      <td>-0.009963</td>\n",
       "      <td>0.131917</td>\n",
       "      <td>0.120496</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117415</td>\n",
       "      <td>-0.182714</td>\n",
       "      <td>-0.124955</td>\n",
       "      <td>0.141870</td>\n",
       "      <td>-0.088337</td>\n",
       "      <td>0.022296</td>\n",
       "      <td>-0.016868</td>\n",
       "      <td>0.006271</td>\n",
       "      <td>-0.155717</td>\n",
       "      <td>-0.087777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nonfl</th>\n",
       "      <td>-0.106273</td>\n",
       "      <td>-0.142126</td>\n",
       "      <td>-0.063174</td>\n",
       "      <td>-0.014140</td>\n",
       "      <td>-0.049783</td>\n",
       "      <td>-0.008109</td>\n",
       "      <td>-0.178262</td>\n",
       "      <td>0.035345</td>\n",
       "      <td>-0.021263</td>\n",
       "      <td>-0.073612</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.116472</td>\n",
       "      <td>-0.077630</td>\n",
       "      <td>-0.085311</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.081097</td>\n",
       "      <td>-0.074875</td>\n",
       "      <td>-0.038144</td>\n",
       "      <td>-0.102804</td>\n",
       "      <td>-0.002113</td>\n",
       "      <td>-0.111409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>body</th>\n",
       "      <td>-0.104611</td>\n",
       "      <td>0.020413</td>\n",
       "      <td>0.050284</td>\n",
       "      <td>0.027907</td>\n",
       "      <td>0.286784</td>\n",
       "      <td>0.029595</td>\n",
       "      <td>-0.016621</td>\n",
       "      <td>0.095378</td>\n",
       "      <td>0.726711</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197435</td>\n",
       "      <td>-0.053477</td>\n",
       "      <td>0.034320</td>\n",
       "      <td>-0.073612</td>\n",
       "      <td>0.085061</td>\n",
       "      <td>0.265476</td>\n",
       "      <td>0.023886</td>\n",
       "      <td>-0.041142</td>\n",
       "      <td>0.029384</td>\n",
       "      <td>-0.052626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>achieve</th>\n",
       "      <td>-0.084893</td>\n",
       "      <td>0.194331</td>\n",
       "      <td>0.211335</td>\n",
       "      <td>0.049867</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.105555</td>\n",
       "      <td>0.209111</td>\n",
       "      <td>0.121165</td>\n",
       "      <td>-0.018950</td>\n",
       "      <td>-0.013264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013355</td>\n",
       "      <td>0.088796</td>\n",
       "      <td>0.022842</td>\n",
       "      <td>-0.116076</td>\n",
       "      <td>-0.067931</td>\n",
       "      <td>-0.028410</td>\n",
       "      <td>-0.081458</td>\n",
       "      <td>0.108604</td>\n",
       "      <td>-0.066106</td>\n",
       "      <td>0.079667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swear</th>\n",
       "      <td>-0.081571</td>\n",
       "      <td>-0.058793</td>\n",
       "      <td>-0.140552</td>\n",
       "      <td>0.268508</td>\n",
       "      <td>0.612112</td>\n",
       "      <td>0.116701</td>\n",
       "      <td>-0.062664</td>\n",
       "      <td>0.104361</td>\n",
       "      <td>0.504148</td>\n",
       "      <td>0.447440</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037921</td>\n",
       "      <td>-0.074382</td>\n",
       "      <td>0.034313</td>\n",
       "      <td>0.062913</td>\n",
       "      <td>0.037438</td>\n",
       "      <td>0.095309</td>\n",
       "      <td>0.011738</td>\n",
       "      <td>-0.047332</td>\n",
       "      <td>0.047864</td>\n",
       "      <td>-0.066267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inhib</th>\n",
       "      <td>-0.076475</td>\n",
       "      <td>0.062455</td>\n",
       "      <td>0.074441</td>\n",
       "      <td>-0.035806</td>\n",
       "      <td>0.265704</td>\n",
       "      <td>0.210793</td>\n",
       "      <td>0.147004</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142278</td>\n",
       "      <td>0.095378</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028874</td>\n",
       "      <td>-0.031337</td>\n",
       "      <td>-0.045026</td>\n",
       "      <td>0.035345</td>\n",
       "      <td>-0.084656</td>\n",
       "      <td>0.019355</td>\n",
       "      <td>-0.092080</td>\n",
       "      <td>0.036809</td>\n",
       "      <td>-0.061055</td>\n",
       "      <td>-0.002713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>money</th>\n",
       "      <td>-0.066214</td>\n",
       "      <td>-0.141405</td>\n",
       "      <td>-0.026991</td>\n",
       "      <td>-0.130331</td>\n",
       "      <td>-0.127368</td>\n",
       "      <td>-0.086381</td>\n",
       "      <td>-0.083509</td>\n",
       "      <td>0.029778</td>\n",
       "      <td>-0.134046</td>\n",
       "      <td>-0.104462</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081396</td>\n",
       "      <td>-0.072982</td>\n",
       "      <td>-0.132298</td>\n",
       "      <td>-0.043420</td>\n",
       "      <td>-0.146678</td>\n",
       "      <td>-0.108371</td>\n",
       "      <td>-0.102759</td>\n",
       "      <td>0.020604</td>\n",
       "      <td>-0.093555</td>\n",
       "      <td>-0.029986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number</th>\n",
       "      <td>-0.063380</td>\n",
       "      <td>0.175534</td>\n",
       "      <td>0.221680</td>\n",
       "      <td>0.027090</td>\n",
       "      <td>0.149369</td>\n",
       "      <td>0.121581</td>\n",
       "      <td>0.098661</td>\n",
       "      <td>0.186419</td>\n",
       "      <td>0.076490</td>\n",
       "      <td>0.176470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014203</td>\n",
       "      <td>-0.025672</td>\n",
       "      <td>0.011961</td>\n",
       "      <td>-0.135283</td>\n",
       "      <td>-0.030934</td>\n",
       "      <td>0.079841</td>\n",
       "      <td>-0.062611</td>\n",
       "      <td>0.163814</td>\n",
       "      <td>-0.102463</td>\n",
       "      <td>0.097785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>family</th>\n",
       "      <td>-0.057358</td>\n",
       "      <td>-0.030461</td>\n",
       "      <td>0.070532</td>\n",
       "      <td>0.172200</td>\n",
       "      <td>0.174419</td>\n",
       "      <td>0.163581</td>\n",
       "      <td>-0.086515</td>\n",
       "      <td>0.026708</td>\n",
       "      <td>0.195471</td>\n",
       "      <td>0.207133</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017711</td>\n",
       "      <td>-0.109350</td>\n",
       "      <td>-0.047496</td>\n",
       "      <td>0.021657</td>\n",
       "      <td>0.036514</td>\n",
       "      <td>0.148021</td>\n",
       "      <td>0.014022</td>\n",
       "      <td>-0.046513</td>\n",
       "      <td>0.007770</td>\n",
       "      <td>-0.055353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home</th>\n",
       "      <td>-0.054663</td>\n",
       "      <td>0.057260</td>\n",
       "      <td>0.310268</td>\n",
       "      <td>-0.125735</td>\n",
       "      <td>-0.125190</td>\n",
       "      <td>-0.156657</td>\n",
       "      <td>0.010718</td>\n",
       "      <td>-0.020488</td>\n",
       "      <td>0.246681</td>\n",
       "      <td>0.201146</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122355</td>\n",
       "      <td>0.102779</td>\n",
       "      <td>-0.066263</td>\n",
       "      <td>-0.062195</td>\n",
       "      <td>-0.060477</td>\n",
       "      <td>0.086516</td>\n",
       "      <td>-0.082920</td>\n",
       "      <td>0.046507</td>\n",
       "      <td>-0.063759</td>\n",
       "      <td>-0.004337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>friend</th>\n",
       "      <td>-0.040543</td>\n",
       "      <td>0.017715</td>\n",
       "      <td>-0.006040</td>\n",
       "      <td>0.113635</td>\n",
       "      <td>0.113798</td>\n",
       "      <td>-0.034322</td>\n",
       "      <td>-0.064167</td>\n",
       "      <td>-0.050444</td>\n",
       "      <td>0.154167</td>\n",
       "      <td>0.107149</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049302</td>\n",
       "      <td>-0.086967</td>\n",
       "      <td>0.082772</td>\n",
       "      <td>-0.042236</td>\n",
       "      <td>0.066502</td>\n",
       "      <td>-0.002548</td>\n",
       "      <td>0.032758</td>\n",
       "      <td>0.055686</td>\n",
       "      <td>0.060553</td>\n",
       "      <td>0.048102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>-0.040316</td>\n",
       "      <td>-0.039554</td>\n",
       "      <td>0.001515</td>\n",
       "      <td>0.221396</td>\n",
       "      <td>0.863893</td>\n",
       "      <td>0.235585</td>\n",
       "      <td>0.029638</td>\n",
       "      <td>0.200038</td>\n",
       "      <td>0.379100</td>\n",
       "      <td>0.316070</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060532</td>\n",
       "      <td>-0.038914</td>\n",
       "      <td>-0.071847</td>\n",
       "      <td>-0.023896</td>\n",
       "      <td>-0.114155</td>\n",
       "      <td>0.030633</td>\n",
       "      <td>-0.128871</td>\n",
       "      <td>-0.021323</td>\n",
       "      <td>-0.066870</td>\n",
       "      <td>0.041615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>death</th>\n",
       "      <td>-0.038380</td>\n",
       "      <td>-0.104172</td>\n",
       "      <td>0.068713</td>\n",
       "      <td>0.005066</td>\n",
       "      <td>0.261716</td>\n",
       "      <td>0.196990</td>\n",
       "      <td>-0.024707</td>\n",
       "      <td>0.014161</td>\n",
       "      <td>-0.022115</td>\n",
       "      <td>-0.028437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109282</td>\n",
       "      <td>-0.066896</td>\n",
       "      <td>-0.226198</td>\n",
       "      <td>-0.053829</td>\n",
       "      <td>-0.250789</td>\n",
       "      <td>-0.071418</td>\n",
       "      <td>-0.178617</td>\n",
       "      <td>-0.008100</td>\n",
       "      <td>-0.235992</td>\n",
       "      <td>0.041696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filler</th>\n",
       "      <td>-0.032250</td>\n",
       "      <td>0.011146</td>\n",
       "      <td>0.023580</td>\n",
       "      <td>-0.021385</td>\n",
       "      <td>0.035422</td>\n",
       "      <td>0.039473</td>\n",
       "      <td>0.035512</td>\n",
       "      <td>0.052824</td>\n",
       "      <td>0.014915</td>\n",
       "      <td>0.020539</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014071</td>\n",
       "      <td>-0.007199</td>\n",
       "      <td>0.029450</td>\n",
       "      <td>-0.029302</td>\n",
       "      <td>0.018826</td>\n",
       "      <td>0.055148</td>\n",
       "      <td>0.017465</td>\n",
       "      <td>0.018397</td>\n",
       "      <td>-0.006829</td>\n",
       "      <td>-0.055360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shehe</th>\n",
       "      <td>-0.030856</td>\n",
       "      <td>0.192110</td>\n",
       "      <td>0.128260</td>\n",
       "      <td>-0.111148</td>\n",
       "      <td>0.059616</td>\n",
       "      <td>-0.010392</td>\n",
       "      <td>0.080960</td>\n",
       "      <td>0.019355</td>\n",
       "      <td>0.216391</td>\n",
       "      <td>0.265476</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033586</td>\n",
       "      <td>0.008646</td>\n",
       "      <td>0.146655</td>\n",
       "      <td>-0.074875</td>\n",
       "      <td>0.260467</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.028076</td>\n",
       "      <td>0.061737</td>\n",
       "      <td>-0.078889</td>\n",
       "      <td>0.047042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>motion</th>\n",
       "      <td>-0.025241</td>\n",
       "      <td>0.134544</td>\n",
       "      <td>0.299300</td>\n",
       "      <td>-0.206854</td>\n",
       "      <td>-0.104126</td>\n",
       "      <td>-0.117297</td>\n",
       "      <td>0.044136</td>\n",
       "      <td>-0.017227</td>\n",
       "      <td>-0.035220</td>\n",
       "      <td>-0.006516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004256</td>\n",
       "      <td>-0.005705</td>\n",
       "      <td>-0.096154</td>\n",
       "      <td>-0.041407</td>\n",
       "      <td>-0.026585</td>\n",
       "      <td>0.089990</td>\n",
       "      <td>-0.083622</td>\n",
       "      <td>0.210421</td>\n",
       "      <td>-0.095900</td>\n",
       "      <td>0.144276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relativ</th>\n",
       "      <td>-0.016769</td>\n",
       "      <td>0.382077</td>\n",
       "      <td>0.461819</td>\n",
       "      <td>-0.170882</td>\n",
       "      <td>0.036667</td>\n",
       "      <td>-0.016066</td>\n",
       "      <td>0.266994</td>\n",
       "      <td>0.079101</td>\n",
       "      <td>-0.003115</td>\n",
       "      <td>0.048891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014976</td>\n",
       "      <td>0.216237</td>\n",
       "      <td>0.007868</td>\n",
       "      <td>-0.107408</td>\n",
       "      <td>-0.016180</td>\n",
       "      <td>0.122478</td>\n",
       "      <td>-0.078470</td>\n",
       "      <td>0.276064</td>\n",
       "      <td>-0.111174</td>\n",
       "      <td>0.107704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>-0.016062</td>\n",
       "      <td>0.172807</td>\n",
       "      <td>0.143510</td>\n",
       "      <td>-0.030927</td>\n",
       "      <td>0.017802</td>\n",
       "      <td>0.035009</td>\n",
       "      <td>0.119924</td>\n",
       "      <td>0.055335</td>\n",
       "      <td>-0.065916</td>\n",
       "      <td>-0.032126</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050558</td>\n",
       "      <td>0.136601</td>\n",
       "      <td>0.007353</td>\n",
       "      <td>-0.007379</td>\n",
       "      <td>-0.000823</td>\n",
       "      <td>0.062632</td>\n",
       "      <td>0.032847</td>\n",
       "      <td>0.153258</td>\n",
       "      <td>-0.117242</td>\n",
       "      <td>-0.088523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>space</th>\n",
       "      <td>-0.015298</td>\n",
       "      <td>0.417842</td>\n",
       "      <td>0.536466</td>\n",
       "      <td>-0.178366</td>\n",
       "      <td>0.098822</td>\n",
       "      <td>-0.014570</td>\n",
       "      <td>0.317983</td>\n",
       "      <td>0.089860</td>\n",
       "      <td>0.080113</td>\n",
       "      <td>0.119014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080679</td>\n",
       "      <td>0.247723</td>\n",
       "      <td>0.035598</td>\n",
       "      <td>-0.159341</td>\n",
       "      <td>-0.035181</td>\n",
       "      <td>0.100638</td>\n",
       "      <td>-0.143731</td>\n",
       "      <td>0.209334</td>\n",
       "      <td>-0.045342</td>\n",
       "      <td>0.215022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sexual</th>\n",
       "      <td>0.002814</td>\n",
       "      <td>-0.032020</td>\n",
       "      <td>-0.008338</td>\n",
       "      <td>0.167084</td>\n",
       "      <td>0.399189</td>\n",
       "      <td>0.163547</td>\n",
       "      <td>-0.054817</td>\n",
       "      <td>0.019614</td>\n",
       "      <td>0.741959</td>\n",
       "      <td>0.483542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210769</td>\n",
       "      <td>-0.113247</td>\n",
       "      <td>0.022892</td>\n",
       "      <td>-0.032710</td>\n",
       "      <td>0.066258</td>\n",
       "      <td>0.128797</td>\n",
       "      <td>0.053806</td>\n",
       "      <td>-0.081230</td>\n",
       "      <td>0.046864</td>\n",
       "      <td>-0.099952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assent</th>\n",
       "      <td>0.015094</td>\n",
       "      <td>-0.182103</td>\n",
       "      <td>-0.213583</td>\n",
       "      <td>0.596066</td>\n",
       "      <td>0.010383</td>\n",
       "      <td>-0.048535</td>\n",
       "      <td>-0.191352</td>\n",
       "      <td>-0.093977</td>\n",
       "      <td>0.008261</td>\n",
       "      <td>-0.049401</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039458</td>\n",
       "      <td>-0.097976</td>\n",
       "      <td>-0.027942</td>\n",
       "      <td>0.137758</td>\n",
       "      <td>-0.026176</td>\n",
       "      <td>-0.161641</td>\n",
       "      <td>0.045564</td>\n",
       "      <td>-0.147258</td>\n",
       "      <td>0.063566</td>\n",
       "      <td>-0.156367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>see</th>\n",
       "      <td>0.035324</td>\n",
       "      <td>0.035397</td>\n",
       "      <td>0.047597</td>\n",
       "      <td>-0.017130</td>\n",
       "      <td>0.001309</td>\n",
       "      <td>-0.003400</td>\n",
       "      <td>0.037285</td>\n",
       "      <td>-0.037972</td>\n",
       "      <td>0.156676</td>\n",
       "      <td>0.173769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098295</td>\n",
       "      <td>0.014312</td>\n",
       "      <td>0.074343</td>\n",
       "      <td>-0.070651</td>\n",
       "      <td>0.094744</td>\n",
       "      <td>0.104876</td>\n",
       "      <td>0.043671</td>\n",
       "      <td>0.064856</td>\n",
       "      <td>0.048159</td>\n",
       "      <td>0.024698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>posemo</th>\n",
       "      <td>0.052222</td>\n",
       "      <td>-0.026410</td>\n",
       "      <td>-0.248179</td>\n",
       "      <td>0.943433</td>\n",
       "      <td>-0.116086</td>\n",
       "      <td>-0.006218</td>\n",
       "      <td>-0.121412</td>\n",
       "      <td>-0.127332</td>\n",
       "      <td>-0.029748</td>\n",
       "      <td>-0.069820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090034</td>\n",
       "      <td>-0.016271</td>\n",
       "      <td>0.185438</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>0.212693</td>\n",
       "      <td>-0.134086</td>\n",
       "      <td>0.135828</td>\n",
       "      <td>-0.158847</td>\n",
       "      <td>0.437934</td>\n",
       "      <td>-0.160205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cause</th>\n",
       "      <td>0.055588</td>\n",
       "      <td>0.331479</td>\n",
       "      <td>0.212976</td>\n",
       "      <td>-0.109368</td>\n",
       "      <td>0.154597</td>\n",
       "      <td>0.068408</td>\n",
       "      <td>0.438526</td>\n",
       "      <td>0.104797</td>\n",
       "      <td>0.072546</td>\n",
       "      <td>0.098374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111236</td>\n",
       "      <td>0.240675</td>\n",
       "      <td>0.141663</td>\n",
       "      <td>-0.124910</td>\n",
       "      <td>0.027941</td>\n",
       "      <td>-0.019075</td>\n",
       "      <td>0.032143</td>\n",
       "      <td>0.034119</td>\n",
       "      <td>-0.024885</td>\n",
       "      <td>0.101496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>past</th>\n",
       "      <td>0.078816</td>\n",
       "      <td>0.626629</td>\n",
       "      <td>0.335198</td>\n",
       "      <td>-0.063070</td>\n",
       "      <td>0.071721</td>\n",
       "      <td>0.061218</td>\n",
       "      <td>0.557608</td>\n",
       "      <td>0.060071</td>\n",
       "      <td>0.062819</td>\n",
       "      <td>0.042844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134028</td>\n",
       "      <td>0.482203</td>\n",
       "      <td>0.516111</td>\n",
       "      <td>-0.084087</td>\n",
       "      <td>0.372295</td>\n",
       "      <td>0.229019</td>\n",
       "      <td>0.264924</td>\n",
       "      <td>0.283598</td>\n",
       "      <td>0.063654</td>\n",
       "      <td>0.302900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>humans</th>\n",
       "      <td>0.081364</td>\n",
       "      <td>0.057715</td>\n",
       "      <td>0.049887</td>\n",
       "      <td>0.156367</td>\n",
       "      <td>0.212564</td>\n",
       "      <td>0.156809</td>\n",
       "      <td>0.103706</td>\n",
       "      <td>0.024925</td>\n",
       "      <td>0.184545</td>\n",
       "      <td>0.116373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062507</td>\n",
       "      <td>-0.014449</td>\n",
       "      <td>0.060340</td>\n",
       "      <td>-0.057187</td>\n",
       "      <td>0.030117</td>\n",
       "      <td>0.068063</td>\n",
       "      <td>0.004225</td>\n",
       "      <td>0.113278</td>\n",
       "      <td>-0.055373</td>\n",
       "      <td>0.110287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>affect</th>\n",
       "      <td>0.083142</td>\n",
       "      <td>0.021015</td>\n",
       "      <td>-0.240369</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.219462</td>\n",
       "      <td>0.152016</td>\n",
       "      <td>-0.045391</td>\n",
       "      <td>-0.035806</td>\n",
       "      <td>0.101298</td>\n",
       "      <td>0.027907</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097027</td>\n",
       "      <td>0.019966</td>\n",
       "      <td>0.204579</td>\n",
       "      <td>-0.014140</td>\n",
       "      <td>0.220055</td>\n",
       "      <td>-0.111148</td>\n",
       "      <td>0.144567</td>\n",
       "      <td>-0.155738</td>\n",
       "      <td>0.417700</td>\n",
       "      <td>-0.135927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negemo</th>\n",
       "      <td>0.089955</td>\n",
       "      <td>0.132959</td>\n",
       "      <td>0.008891</td>\n",
       "      <td>0.219462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.471302</td>\n",
       "      <td>0.215557</td>\n",
       "      <td>0.265704</td>\n",
       "      <td>0.389332</td>\n",
       "      <td>0.286784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022040</td>\n",
       "      <td>0.104414</td>\n",
       "      <td>0.061049</td>\n",
       "      <td>-0.049783</td>\n",
       "      <td>0.027077</td>\n",
       "      <td>0.059616</td>\n",
       "      <td>0.027541</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>-0.040434</td>\n",
       "      <td>0.063469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bio</th>\n",
       "      <td>0.100062</td>\n",
       "      <td>0.109116</td>\n",
       "      <td>0.052556</td>\n",
       "      <td>0.101298</td>\n",
       "      <td>0.389332</td>\n",
       "      <td>0.192593</td>\n",
       "      <td>0.070002</td>\n",
       "      <td>0.142278</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.726711</td>\n",
       "      <td>...</td>\n",
       "      <td>0.301380</td>\n",
       "      <td>-0.029979</td>\n",
       "      <td>0.095976</td>\n",
       "      <td>-0.021263</td>\n",
       "      <td>0.160854</td>\n",
       "      <td>0.216391</td>\n",
       "      <td>0.133956</td>\n",
       "      <td>-0.062165</td>\n",
       "      <td>0.040954</td>\n",
       "      <td>-0.014294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hear</th>\n",
       "      <td>0.103158</td>\n",
       "      <td>0.175221</td>\n",
       "      <td>0.048662</td>\n",
       "      <td>-0.023983</td>\n",
       "      <td>0.163438</td>\n",
       "      <td>0.008302</td>\n",
       "      <td>0.144274</td>\n",
       "      <td>0.121683</td>\n",
       "      <td>0.029983</td>\n",
       "      <td>0.038734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>0.060432</td>\n",
       "      <td>0.089652</td>\n",
       "      <td>-0.074511</td>\n",
       "      <td>0.071994</td>\n",
       "      <td>0.151645</td>\n",
       "      <td>0.027674</td>\n",
       "      <td>0.023809</td>\n",
       "      <td>-0.006720</td>\n",
       "      <td>0.079261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>certain</th>\n",
       "      <td>0.135147</td>\n",
       "      <td>0.249127</td>\n",
       "      <td>0.071646</td>\n",
       "      <td>0.182095</td>\n",
       "      <td>0.274970</td>\n",
       "      <td>0.121895</td>\n",
       "      <td>0.387503</td>\n",
       "      <td>0.062220</td>\n",
       "      <td>0.167026</td>\n",
       "      <td>0.022543</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036726</td>\n",
       "      <td>0.133873</td>\n",
       "      <td>0.243309</td>\n",
       "      <td>-0.083710</td>\n",
       "      <td>0.213235</td>\n",
       "      <td>0.013312</td>\n",
       "      <td>0.208894</td>\n",
       "      <td>0.026969</td>\n",
       "      <td>0.055009</td>\n",
       "      <td>0.184485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preps</th>\n",
       "      <td>0.138777</td>\n",
       "      <td>0.626970</td>\n",
       "      <td>0.528382</td>\n",
       "      <td>-0.244138</td>\n",
       "      <td>0.011855</td>\n",
       "      <td>-0.001374</td>\n",
       "      <td>0.522994</td>\n",
       "      <td>0.160622</td>\n",
       "      <td>-0.073246</td>\n",
       "      <td>-0.077232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054613</td>\n",
       "      <td>0.337073</td>\n",
       "      <td>0.206549</td>\n",
       "      <td>-0.219824</td>\n",
       "      <td>0.098945</td>\n",
       "      <td>0.181223</td>\n",
       "      <td>-0.000489</td>\n",
       "      <td>0.206249</td>\n",
       "      <td>-0.050468</td>\n",
       "      <td>0.287484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>they</th>\n",
       "      <td>0.148906</td>\n",
       "      <td>0.409581</td>\n",
       "      <td>0.200865</td>\n",
       "      <td>-0.135927</td>\n",
       "      <td>0.063469</td>\n",
       "      <td>0.018368</td>\n",
       "      <td>0.383802</td>\n",
       "      <td>-0.002713</td>\n",
       "      <td>-0.014294</td>\n",
       "      <td>-0.052626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022161</td>\n",
       "      <td>0.303590</td>\n",
       "      <td>0.298638</td>\n",
       "      <td>-0.111409</td>\n",
       "      <td>0.268851</td>\n",
       "      <td>0.047042</td>\n",
       "      <td>0.060356</td>\n",
       "      <td>0.235515</td>\n",
       "      <td>0.061960</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>social</th>\n",
       "      <td>0.149181</td>\n",
       "      <td>0.488201</td>\n",
       "      <td>0.122192</td>\n",
       "      <td>0.290314</td>\n",
       "      <td>0.208048</td>\n",
       "      <td>0.145339</td>\n",
       "      <td>0.356061</td>\n",
       "      <td>0.044304</td>\n",
       "      <td>0.204433</td>\n",
       "      <td>0.158070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055756</td>\n",
       "      <td>0.212875</td>\n",
       "      <td>0.548437</td>\n",
       "      <td>-0.129193</td>\n",
       "      <td>0.575687</td>\n",
       "      <td>0.430465</td>\n",
       "      <td>0.167820</td>\n",
       "      <td>0.338752</td>\n",
       "      <td>0.528548</td>\n",
       "      <td>0.341970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>0.152005</td>\n",
       "      <td>0.300101</td>\n",
       "      <td>-0.171638</td>\n",
       "      <td>0.417700</td>\n",
       "      <td>-0.040434</td>\n",
       "      <td>-0.032245</td>\n",
       "      <td>0.129937</td>\n",
       "      <td>-0.061055</td>\n",
       "      <td>0.040954</td>\n",
       "      <td>0.029384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182947</td>\n",
       "      <td>0.186221</td>\n",
       "      <td>0.582411</td>\n",
       "      <td>-0.002113</td>\n",
       "      <td>0.598472</td>\n",
       "      <td>-0.078889</td>\n",
       "      <td>0.234335</td>\n",
       "      <td>-0.067507</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.061960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sad</th>\n",
       "      <td>0.160260</td>\n",
       "      <td>0.142369</td>\n",
       "      <td>-0.012702</td>\n",
       "      <td>0.152016</td>\n",
       "      <td>0.471302</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.189993</td>\n",
       "      <td>0.210793</td>\n",
       "      <td>0.192593</td>\n",
       "      <td>0.029595</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068168</td>\n",
       "      <td>0.063963</td>\n",
       "      <td>0.111547</td>\n",
       "      <td>-0.008109</td>\n",
       "      <td>0.097677</td>\n",
       "      <td>-0.010392</td>\n",
       "      <td>0.132377</td>\n",
       "      <td>0.092360</td>\n",
       "      <td>-0.032245</td>\n",
       "      <td>0.018368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ingest</th>\n",
       "      <td>0.160314</td>\n",
       "      <td>0.145475</td>\n",
       "      <td>0.045701</td>\n",
       "      <td>-0.002106</td>\n",
       "      <td>0.068202</td>\n",
       "      <td>0.210136</td>\n",
       "      <td>0.119412</td>\n",
       "      <td>0.182387</td>\n",
       "      <td>0.591708</td>\n",
       "      <td>0.214880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172116</td>\n",
       "      <td>0.023971</td>\n",
       "      <td>0.101572</td>\n",
       "      <td>0.123298</td>\n",
       "      <td>0.110629</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>0.131787</td>\n",
       "      <td>0.012163</td>\n",
       "      <td>-0.012691</td>\n",
       "      <td>0.116952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negate</th>\n",
       "      <td>0.182730</td>\n",
       "      <td>0.498104</td>\n",
       "      <td>0.023105</td>\n",
       "      <td>-0.011414</td>\n",
       "      <td>0.267775</td>\n",
       "      <td>0.116965</td>\n",
       "      <td>0.540578</td>\n",
       "      <td>0.053330</td>\n",
       "      <td>0.078198</td>\n",
       "      <td>0.013783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049988</td>\n",
       "      <td>0.505672</td>\n",
       "      <td>0.466675</td>\n",
       "      <td>-0.025023</td>\n",
       "      <td>0.345419</td>\n",
       "      <td>-0.009901</td>\n",
       "      <td>0.314204</td>\n",
       "      <td>0.063428</td>\n",
       "      <td>0.144320</td>\n",
       "      <td>0.285479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>percept</th>\n",
       "      <td>0.197352</td>\n",
       "      <td>0.220380</td>\n",
       "      <td>0.074863</td>\n",
       "      <td>0.009061</td>\n",
       "      <td>0.066330</td>\n",
       "      <td>0.045496</td>\n",
       "      <td>0.161058</td>\n",
       "      <td>0.010090</td>\n",
       "      <td>0.336793</td>\n",
       "      <td>0.310308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.589919</td>\n",
       "      <td>0.120110</td>\n",
       "      <td>0.173163</td>\n",
       "      <td>-0.140485</td>\n",
       "      <td>0.203965</td>\n",
       "      <td>0.136310</td>\n",
       "      <td>0.159610</td>\n",
       "      <td>-0.015202</td>\n",
       "      <td>0.106495</td>\n",
       "      <td>0.040759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>incl</th>\n",
       "      <td>0.228745</td>\n",
       "      <td>0.640153</td>\n",
       "      <td>0.395110</td>\n",
       "      <td>-0.193123</td>\n",
       "      <td>-0.091508</td>\n",
       "      <td>0.010981</td>\n",
       "      <td>0.617582</td>\n",
       "      <td>0.006358</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>-0.026356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161650</td>\n",
       "      <td>0.423537</td>\n",
       "      <td>0.387452</td>\n",
       "      <td>-0.206714</td>\n",
       "      <td>0.301946</td>\n",
       "      <td>0.211110</td>\n",
       "      <td>0.145484</td>\n",
       "      <td>0.446742</td>\n",
       "      <td>0.037687</td>\n",
       "      <td>0.356405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <td>0.240740</td>\n",
       "      <td>0.208738</td>\n",
       "      <td>0.032398</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.172073</td>\n",
       "      <td>0.044008</td>\n",
       "      <td>0.208694</td>\n",
       "      <td>0.021567</td>\n",
       "      <td>0.429970</td>\n",
       "      <td>0.147925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196986</td>\n",
       "      <td>0.129795</td>\n",
       "      <td>0.132203</td>\n",
       "      <td>-0.125171</td>\n",
       "      <td>0.191166</td>\n",
       "      <td>0.167239</td>\n",
       "      <td>0.167652</td>\n",
       "      <td>-0.066935</td>\n",
       "      <td>0.066003</td>\n",
       "      <td>0.038302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feel</th>\n",
       "      <td>0.256453</td>\n",
       "      <td>0.270226</td>\n",
       "      <td>0.020717</td>\n",
       "      <td>0.097027</td>\n",
       "      <td>0.022040</td>\n",
       "      <td>0.068168</td>\n",
       "      <td>0.215292</td>\n",
       "      <td>-0.028874</td>\n",
       "      <td>0.301380</td>\n",
       "      <td>0.197435</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.252941</td>\n",
       "      <td>0.242500</td>\n",
       "      <td>-0.116472</td>\n",
       "      <td>0.255024</td>\n",
       "      <td>-0.033586</td>\n",
       "      <td>0.262713</td>\n",
       "      <td>-0.093426</td>\n",
       "      <td>0.182947</td>\n",
       "      <td>0.022161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anx</th>\n",
       "      <td>0.256804</td>\n",
       "      <td>0.248162</td>\n",
       "      <td>-0.053994</td>\n",
       "      <td>0.003445</td>\n",
       "      <td>0.368127</td>\n",
       "      <td>0.124741</td>\n",
       "      <td>0.305764</td>\n",
       "      <td>0.117004</td>\n",
       "      <td>0.048307</td>\n",
       "      <td>0.008154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196602</td>\n",
       "      <td>0.287684</td>\n",
       "      <td>0.164882</td>\n",
       "      <td>-0.078449</td>\n",
       "      <td>0.190047</td>\n",
       "      <td>0.072816</td>\n",
       "      <td>0.218079</td>\n",
       "      <td>-0.011243</td>\n",
       "      <td>0.009134</td>\n",
       "      <td>0.057175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quant</th>\n",
       "      <td>0.265267</td>\n",
       "      <td>0.565886</td>\n",
       "      <td>0.306324</td>\n",
       "      <td>0.052717</td>\n",
       "      <td>0.047611</td>\n",
       "      <td>0.047515</td>\n",
       "      <td>0.573652</td>\n",
       "      <td>-0.032326</td>\n",
       "      <td>0.098994</td>\n",
       "      <td>-0.002502</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123680</td>\n",
       "      <td>0.394865</td>\n",
       "      <td>0.369423</td>\n",
       "      <td>-0.081862</td>\n",
       "      <td>0.254329</td>\n",
       "      <td>0.002855</td>\n",
       "      <td>0.245567</td>\n",
       "      <td>0.015915</td>\n",
       "      <td>0.052438</td>\n",
       "      <td>0.318454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ipron</th>\n",
       "      <td>0.267656</td>\n",
       "      <td>0.694076</td>\n",
       "      <td>0.120278</td>\n",
       "      <td>0.101047</td>\n",
       "      <td>0.277833</td>\n",
       "      <td>0.186476</td>\n",
       "      <td>0.584538</td>\n",
       "      <td>0.060796</td>\n",
       "      <td>0.191533</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273341</td>\n",
       "      <td>0.465585</td>\n",
       "      <td>0.638971</td>\n",
       "      <td>-0.018683</td>\n",
       "      <td>0.464724</td>\n",
       "      <td>0.079604</td>\n",
       "      <td>0.410686</td>\n",
       "      <td>0.066010</td>\n",
       "      <td>0.213369</td>\n",
       "      <td>0.243572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discrep</th>\n",
       "      <td>0.298733</td>\n",
       "      <td>0.601229</td>\n",
       "      <td>0.077622</td>\n",
       "      <td>-0.026253</td>\n",
       "      <td>0.106542</td>\n",
       "      <td>0.086917</td>\n",
       "      <td>0.643363</td>\n",
       "      <td>0.099218</td>\n",
       "      <td>-0.015998</td>\n",
       "      <td>-0.077617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160200</td>\n",
       "      <td>0.646415</td>\n",
       "      <td>0.568031</td>\n",
       "      <td>0.009820</td>\n",
       "      <td>0.443175</td>\n",
       "      <td>0.027699</td>\n",
       "      <td>0.376335</td>\n",
       "      <td>0.015435</td>\n",
       "      <td>0.241868</td>\n",
       "      <td>0.327973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            label     funct   article    affect    negemo       sad   cogmech  \\\n",
       "work    -0.197958 -0.078190  0.164134 -0.161045 -0.110000 -0.119536 -0.001790   \n",
       "article -0.124233  0.396806  1.000000 -0.240369  0.008891 -0.012702  0.263779   \n",
       "we      -0.116822  0.151113  0.204698 -0.155738  0.001092  0.092360  0.157903   \n",
       "leisure -0.112231 -0.143708 -0.004798 -0.086731 -0.062152 -0.124786 -0.083327   \n",
       "relig   -0.111693 -0.168276 -0.113201 -0.002934  0.131581  0.019087 -0.168061   \n",
       "nonfl   -0.106273 -0.142126 -0.063174 -0.014140 -0.049783 -0.008109 -0.178262   \n",
       "body    -0.104611  0.020413  0.050284  0.027907  0.286784  0.029595 -0.016621   \n",
       "achieve -0.084893  0.194331  0.211335  0.049867  0.018182  0.105555  0.209111   \n",
       "swear   -0.081571 -0.058793 -0.140552  0.268508  0.612112  0.116701 -0.062664   \n",
       "inhib   -0.076475  0.062455  0.074441 -0.035806  0.265704  0.210793  0.147004   \n",
       "money   -0.066214 -0.141405 -0.026991 -0.130331 -0.127368 -0.086381 -0.083509   \n",
       "number  -0.063380  0.175534  0.221680  0.027090  0.149369  0.121581  0.098661   \n",
       "family  -0.057358 -0.030461  0.070532  0.172200  0.174419  0.163581 -0.086515   \n",
       "home    -0.054663  0.057260  0.310268 -0.125735 -0.125190 -0.156657  0.010718   \n",
       "friend  -0.040543  0.017715 -0.006040  0.113635  0.113798 -0.034322 -0.064167   \n",
       "anger   -0.040316 -0.039554  0.001515  0.221396  0.863893  0.235585  0.029638   \n",
       "death   -0.038380 -0.104172  0.068713  0.005066  0.261716  0.196990 -0.024707   \n",
       "filler  -0.032250  0.011146  0.023580 -0.021385  0.035422  0.039473  0.035512   \n",
       "shehe   -0.030856  0.192110  0.128260 -0.111148  0.059616 -0.010392  0.080960   \n",
       "motion  -0.025241  0.134544  0.299300 -0.206854 -0.104126 -0.117297  0.044136   \n",
       "relativ -0.016769  0.382077  0.461819 -0.170882  0.036667 -0.016066  0.266994   \n",
       "time    -0.016062  0.172807  0.143510 -0.030927  0.017802  0.035009  0.119924   \n",
       "space   -0.015298  0.417842  0.536466 -0.178366  0.098822 -0.014570  0.317983   \n",
       "sexual   0.002814 -0.032020 -0.008338  0.167084  0.399189  0.163547 -0.054817   \n",
       "assent   0.015094 -0.182103 -0.213583  0.596066  0.010383 -0.048535 -0.191352   \n",
       "see      0.035324  0.035397  0.047597 -0.017130  0.001309 -0.003400  0.037285   \n",
       "posemo   0.052222 -0.026410 -0.248179  0.943433 -0.116086 -0.006218 -0.121412   \n",
       "cause    0.055588  0.331479  0.212976 -0.109368  0.154597  0.068408  0.438526   \n",
       "past     0.078816  0.626629  0.335198 -0.063070  0.071721  0.061218  0.557608   \n",
       "humans   0.081364  0.057715  0.049887  0.156367  0.212564  0.156809  0.103706   \n",
       "affect   0.083142  0.021015 -0.240369  1.000000  0.219462  0.152016 -0.045391   \n",
       "negemo   0.089955  0.132959  0.008891  0.219462  1.000000  0.471302  0.215557   \n",
       "bio      0.100062  0.109116  0.052556  0.101298  0.389332  0.192593  0.070002   \n",
       "hear     0.103158  0.175221  0.048662 -0.023983  0.163438  0.008302  0.144274   \n",
       "certain  0.135147  0.249127  0.071646  0.182095  0.274970  0.121895  0.387503   \n",
       "preps    0.138777  0.626970  0.528382 -0.244138  0.011855 -0.001374  0.522994   \n",
       "they     0.148906  0.409581  0.200865 -0.135927  0.063469  0.018368  0.383802   \n",
       "social   0.149181  0.488201  0.122192  0.290314  0.208048  0.145339  0.356061   \n",
       "you      0.152005  0.300101 -0.171638  0.417700 -0.040434 -0.032245  0.129937   \n",
       "sad      0.160260  0.142369 -0.012702  0.152016  0.471302  1.000000  0.189993   \n",
       "ingest   0.160314  0.145475  0.045701 -0.002106  0.068202  0.210136  0.119412   \n",
       "negate   0.182730  0.498104  0.023105 -0.011414  0.267775  0.116965  0.540578   \n",
       "percept  0.197352  0.220380  0.074863  0.009061  0.066330  0.045496  0.161058   \n",
       "incl     0.228745  0.640153  0.395110 -0.193123 -0.091508  0.010981  0.617582   \n",
       "health   0.240740  0.208738  0.032398  0.002850  0.172073  0.044008  0.208694   \n",
       "feel     0.256453  0.270226  0.020717  0.097027  0.022040  0.068168  0.215292   \n",
       "anx      0.256804  0.248162 -0.053994  0.003445  0.368127  0.124741  0.305764   \n",
       "quant    0.265267  0.565886  0.306324  0.052717  0.047611  0.047515  0.573652   \n",
       "ipron    0.267656  0.694076  0.120278  0.101047  0.277833  0.186476  0.584538   \n",
       "discrep  0.298733  0.601229  0.077622 -0.026253  0.106542  0.086917  0.643363   \n",
       "\n",
       "            inhib       bio      body  ...      feel      excl    future  \\\n",
       "work     0.173579 -0.185892 -0.108384  ... -0.159383 -0.140420 -0.213641   \n",
       "article  0.074441  0.052556  0.050284  ...  0.020717  0.138179 -0.001219   \n",
       "we       0.036809 -0.062165 -0.041142  ... -0.093426  0.016650  0.092645   \n",
       "leisure -0.067570 -0.094987 -0.002267  ... -0.036881 -0.117458 -0.208380   \n",
       "relig   -0.009963  0.131917  0.120496  ... -0.117415 -0.182714 -0.124955   \n",
       "nonfl    0.035345 -0.021263 -0.073612  ... -0.116472 -0.077630 -0.085311   \n",
       "body     0.095378  0.726711  1.000000  ...  0.197435 -0.053477  0.034320   \n",
       "achieve  0.121165 -0.018950 -0.013264  ...  0.013355  0.088796  0.022842   \n",
       "swear    0.104361  0.504148  0.447440  ... -0.037921 -0.074382  0.034313   \n",
       "inhib    1.000000  0.142278  0.095378  ... -0.028874 -0.031337 -0.045026   \n",
       "money    0.029778 -0.134046 -0.104462  ... -0.081396 -0.072982 -0.132298   \n",
       "number   0.186419  0.076490  0.176470  ...  0.014203 -0.025672  0.011961   \n",
       "family   0.026708  0.195471  0.207133  ... -0.017711 -0.109350 -0.047496   \n",
       "home    -0.020488  0.246681  0.201146  ...  0.122355  0.102779 -0.066263   \n",
       "friend  -0.050444  0.154167  0.107149  ... -0.049302 -0.086967  0.082772   \n",
       "anger    0.200038  0.379100  0.316070  ... -0.060532 -0.038914 -0.071847   \n",
       "death    0.014161 -0.022115 -0.028437  ...  0.109282 -0.066896 -0.226198   \n",
       "filler   0.052824  0.014915  0.020539  ... -0.014071 -0.007199  0.029450   \n",
       "shehe    0.019355  0.216391  0.265476  ... -0.033586  0.008646  0.146655   \n",
       "motion  -0.017227 -0.035220 -0.006516  ...  0.004256 -0.005705 -0.096154   \n",
       "relativ  0.079101 -0.003115  0.048891  ...  0.014976  0.216237  0.007868   \n",
       "time     0.055335 -0.065916 -0.032126  ... -0.050558  0.136601  0.007353   \n",
       "space    0.089860  0.080113  0.119014  ...  0.080679  0.247723  0.035598   \n",
       "sexual   0.019614  0.741959  0.483542  ...  0.210769 -0.113247  0.022892   \n",
       "assent  -0.093977  0.008261 -0.049401  ... -0.039458 -0.097976 -0.027942   \n",
       "see     -0.037972  0.156676  0.173769  ...  0.098295  0.014312  0.074343   \n",
       "posemo  -0.127332 -0.029748 -0.069820  ...  0.090034 -0.016271  0.185438   \n",
       "cause    0.104797  0.072546  0.098374  ...  0.111236  0.240675  0.141663   \n",
       "past     0.060071  0.062819  0.042844  ...  0.134028  0.482203  0.516111   \n",
       "humans   0.024925  0.184545  0.116373  ...  0.062507 -0.014449  0.060340   \n",
       "affect  -0.035806  0.101298  0.027907  ...  0.097027  0.019966  0.204579   \n",
       "negemo   0.265704  0.389332  0.286784  ...  0.022040  0.104414  0.061049   \n",
       "bio      0.142278  1.000000  0.726711  ...  0.301380 -0.029979  0.095976   \n",
       "hear     0.121683  0.029983  0.038734  ...  0.069000  0.060432  0.089652   \n",
       "certain  0.062220  0.167026  0.022543  ... -0.036726  0.133873  0.243309   \n",
       "preps    0.160622 -0.073246 -0.077232  ...  0.054613  0.337073  0.206549   \n",
       "they    -0.002713 -0.014294 -0.052626  ...  0.022161  0.303590  0.298638   \n",
       "social   0.044304  0.204433  0.158070  ...  0.055756  0.212875  0.548437   \n",
       "you     -0.061055  0.040954  0.029384  ...  0.182947  0.186221  0.582411   \n",
       "sad      0.210793  0.192593  0.029595  ...  0.068168  0.063963  0.111547   \n",
       "ingest   0.182387  0.591708  0.214880  ...  0.172116  0.023971  0.101572   \n",
       "negate   0.053330  0.078198  0.013783  ...  0.049988  0.505672  0.466675   \n",
       "percept  0.010090  0.336793  0.310308  ...  0.589919  0.120110  0.173163   \n",
       "incl     0.006358  0.022095 -0.026356  ...  0.161650  0.423537  0.387452   \n",
       "health   0.021567  0.429970  0.147925  ...  0.196986  0.129795  0.132203   \n",
       "feel    -0.028874  0.301380  0.197435  ...  1.000000  0.252941  0.242500   \n",
       "anx      0.117004  0.048307  0.008154  ...  0.196602  0.287684  0.164882   \n",
       "quant   -0.032326  0.098994 -0.002502  ...  0.123680  0.394865  0.369423   \n",
       "ipron    0.060796  0.191533  0.082500  ...  0.273341  0.465585  0.638971   \n",
       "discrep  0.099218 -0.015998 -0.077617  ...  0.160200  0.646415  0.568031   \n",
       "\n",
       "            nonfl     ppron     shehe         i        we       you      they  \n",
       "work    -0.152713 -0.255025 -0.089858 -0.225474  0.030485 -0.160479  0.007994  \n",
       "article -0.063174 -0.118329  0.128260 -0.185701  0.204698 -0.171638  0.200865  \n",
       "we      -0.102804  0.071740  0.061737 -0.146144  1.000000 -0.067507  0.235515  \n",
       "leisure -0.010841 -0.219850 -0.015029 -0.224056  0.060854 -0.111190 -0.074291  \n",
       "relig    0.141870 -0.088337  0.022296 -0.016868  0.006271 -0.155717 -0.087777  \n",
       "nonfl    1.000000 -0.081097 -0.074875 -0.038144 -0.102804 -0.002113 -0.111409  \n",
       "body    -0.073612  0.085061  0.265476  0.023886 -0.041142  0.029384 -0.052626  \n",
       "achieve -0.116076 -0.067931 -0.028410 -0.081458  0.108604 -0.066106  0.079667  \n",
       "swear    0.062913  0.037438  0.095309  0.011738 -0.047332  0.047864 -0.066267  \n",
       "inhib    0.035345 -0.084656  0.019355 -0.092080  0.036809 -0.061055 -0.002713  \n",
       "money   -0.043420 -0.146678 -0.108371 -0.102759  0.020604 -0.093555 -0.029986  \n",
       "number  -0.135283 -0.030934  0.079841 -0.062611  0.163814 -0.102463  0.097785  \n",
       "family   0.021657  0.036514  0.148021  0.014022 -0.046513  0.007770 -0.055353  \n",
       "home    -0.062195 -0.060477  0.086516 -0.082920  0.046507 -0.063759 -0.004337  \n",
       "friend  -0.042236  0.066502 -0.002548  0.032758  0.055686  0.060553  0.048102  \n",
       "anger   -0.023896 -0.114155  0.030633 -0.128871 -0.021323 -0.066870  0.041615  \n",
       "death   -0.053829 -0.250789 -0.071418 -0.178617 -0.008100 -0.235992  0.041696  \n",
       "filler  -0.029302  0.018826  0.055148  0.017465  0.018397 -0.006829 -0.055360  \n",
       "shehe   -0.074875  0.260467  1.000000  0.028076  0.061737 -0.078889  0.047042  \n",
       "motion  -0.041407 -0.026585  0.089990 -0.083622  0.210421 -0.095900  0.144276  \n",
       "relativ -0.107408 -0.016180  0.122478 -0.078470  0.276064 -0.111174  0.107704  \n",
       "time    -0.007379 -0.000823  0.062632  0.032847  0.153258 -0.117242 -0.088523  \n",
       "space   -0.159341 -0.035181  0.100638 -0.143731  0.209334 -0.045342  0.215022  \n",
       "sexual  -0.032710  0.066258  0.128797  0.053806 -0.081230  0.046864 -0.099952  \n",
       "assent   0.137758 -0.026176 -0.161641  0.045564 -0.147258  0.063566 -0.156367  \n",
       "see     -0.070651  0.094744  0.104876  0.043671  0.064856  0.048159  0.024698  \n",
       "posemo   0.002531  0.212693 -0.134086  0.135828 -0.158847  0.437934 -0.160205  \n",
       "cause   -0.124910  0.027941 -0.019075  0.032143  0.034119 -0.024885  0.101496  \n",
       "past    -0.084087  0.372295  0.229019  0.264924  0.283598  0.063654  0.302900  \n",
       "humans  -0.057187  0.030117  0.068063  0.004225  0.113278 -0.055373  0.110287  \n",
       "affect  -0.014140  0.220055 -0.111148  0.144567 -0.155738  0.417700 -0.135927  \n",
       "negemo  -0.049783  0.027077  0.059616  0.027541  0.001092 -0.040434  0.063469  \n",
       "bio     -0.021263  0.160854  0.216391  0.133956 -0.062165  0.040954 -0.014294  \n",
       "hear    -0.074511  0.071994  0.151645  0.027674  0.023809 -0.006720  0.079261  \n",
       "certain -0.083710  0.213235  0.013312  0.208894  0.026969  0.055009  0.184485  \n",
       "preps   -0.219824  0.098945  0.181223 -0.000489  0.206249 -0.050468  0.287484  \n",
       "they    -0.111409  0.268851  0.047042  0.060356  0.235515  0.061960  1.000000  \n",
       "social  -0.129193  0.575687  0.430465  0.167820  0.338752  0.528548  0.341970  \n",
       "you     -0.002113  0.598472 -0.078889  0.234335 -0.067507  1.000000  0.061960  \n",
       "sad     -0.008109  0.097677 -0.010392  0.132377  0.092360 -0.032245  0.018368  \n",
       "ingest   0.123298  0.110629  0.000811  0.131787  0.012163 -0.012691  0.116952  \n",
       "negate  -0.025023  0.345419 -0.009901  0.314204  0.063428  0.144320  0.285479  \n",
       "percept -0.140485  0.203965  0.136310  0.159610 -0.015202  0.106495  0.040759  \n",
       "incl    -0.206714  0.301946  0.211110  0.145484  0.446742  0.037687  0.356405  \n",
       "health  -0.125171  0.191166  0.167239  0.167652 -0.066935  0.066003  0.038302  \n",
       "feel    -0.116472  0.255024 -0.033586  0.262713 -0.093426  0.182947  0.022161  \n",
       "anx     -0.078449  0.190047  0.072816  0.218079 -0.011243  0.009134  0.057175  \n",
       "quant   -0.081862  0.254329  0.002855  0.245567  0.015915  0.052438  0.318454  \n",
       "ipron   -0.018683  0.464724  0.079604  0.410686  0.066010  0.213369  0.243572  \n",
       "discrep  0.009820  0.443175  0.027699  0.376335  0.015435  0.241868  0.327973  \n",
       "\n",
       "[50 rows x 65 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.groupby('subject').mean()[['label'] + list(liwc_dict.keys())].corr().sort_values(by='label', \n",
    "                                                                                             ascending=True\n",
    "                                                                                             \n",
    "                                                                                            )[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_emotion_distr_per_user(subject, category):\n",
    "    writings_df[writings_df['subject']==subject][category].hist(bins=50)\n",
    "    print(writings_df[writings_df['subject']==subject][category].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject9811\n",
      "count    131.000000\n",
      "mean       0.003908\n",
      "std        0.013842\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        0.000000\n",
      "75%        0.000000\n",
      "max        0.090909\n",
      "Name: sad, dtype: float64\n",
      "subject1093\n",
      "count    165.000000\n",
      "mean       0.006969\n",
      "std        0.078109\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        0.000000\n",
      "75%        0.000000\n",
      "max        1.000000\n",
      "Name: sad, dtype: float64\n",
      "subject9729\n",
      "count    160.000000\n",
      "mean       0.004915\n",
      "std        0.041732\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        0.000000\n",
      "75%        0.000000\n",
      "max        0.500000\n",
      "Name: sad, dtype: float64\n",
      "subject7435\n",
      "count    1368.000000\n",
      "mean        0.002042\n",
      "std         0.017148\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         0.285714\n",
      "Name: sad, dtype: float64\n",
      "subject733\n",
      "count    461.000000\n",
      "mean       0.004637\n",
      "std        0.026922\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        0.000000\n",
      "75%        0.000000\n",
      "max        0.500000\n",
      "Name: sad, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for s in list(set(writings_df.subject.values))[:5]:\n",
    "    print(s)\n",
    "    plot_emotion_distr_per_user(s,'sad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>negemo</th>\n",
       "      <th>posemo</th>\n",
       "      <th>affect</th>\n",
       "      <th>sad</th>\n",
       "      <th>anx</th>\n",
       "      <th>pronoun</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subject0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.067568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1027</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1055</th>\n",
       "      <td>0</td>\n",
       "      <td>0.013077</td>\n",
       "      <td>0.019434</td>\n",
       "      <td>0.047143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1064</th>\n",
       "      <td>1</td>\n",
       "      <td>0.020798</td>\n",
       "      <td>0.052242</td>\n",
       "      <td>0.092119</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1089</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9917</th>\n",
       "      <td>1</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.020309</td>\n",
       "      <td>0.059115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9918</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject992</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9949</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.113302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9961</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>340 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             label    negemo    posemo    affect  sad  anx   pronoun\n",
       "subject                                                             \n",
       "subject0         0  0.000000  0.045455  0.067568  0.0  0.0  0.200000\n",
       "subject1027      0  0.000000  0.000000  0.000000  0.0  0.0  0.000000\n",
       "subject1055      0  0.013077  0.019434  0.047143  0.0  0.0  0.166667\n",
       "subject1064      1  0.020798  0.052242  0.092119  0.0  0.0  0.185790\n",
       "subject1089      0  0.000000  0.000000  0.041667  0.0  0.0  0.166667\n",
       "...            ...       ...       ...       ...  ...  ...       ...\n",
       "subject9917      1  0.029412  0.020309  0.059115  0.0  0.0  0.200000\n",
       "subject9918      0  0.000000  0.000000  0.038462  0.0  0.0  0.133333\n",
       "subject992       0  0.000000  0.000000  0.045455  0.0  0.0  0.111111\n",
       "subject9949      0  0.000000  0.000000  0.052632  0.0  0.0  0.113302\n",
       "subject9961      0  0.000000  0.000000  0.000000  0.0  0.0  0.111111\n",
       "\n",
       "[340 rows x 7 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# writings_df.groupby('subject').mean()[['label', 'negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']]\n",
    "writings_df.groupby('subject').median()[['label', 'negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>i</th>\n",
       "      <th>sad</th>\n",
       "      <th>anx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>170137</th>\n",
       "      <td>7 hour halflife : after 7 hours there is 50% o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170307</th>\n",
       "      <td>from the article: \"Two persons once committed ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170308</th>\n",
       "      <td>I can't wait to see /r/behindthegifs version o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170309</th>\n",
       "      <td>HEY! how can you be a lurker and you have 4000...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170310</th>\n",
       "      <td>Red labial you say?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170519</th>\n",
       "      <td>NaN</td>\n",
       "      <td>you startled the bitch!</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170536</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Ignorance leads to Fear....</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170163</th>\n",
       "      <td>Yourself, looking very scared.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170085</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Horrible mom?</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170664</th>\n",
       "      <td>NaN</td>\n",
       "      <td>scary</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1120 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "170137  7 hour halflife : after 7 hours there is 50% o...   \n",
       "170307  from the article: \"Two persons once committed ...   \n",
       "170308  I can't wait to see /r/behindthegifs version o...   \n",
       "170309  HEY! how can you be a lurker and you have 4000...   \n",
       "170310                                Red labial you say?   \n",
       "...                                                   ...   \n",
       "170519                                                NaN   \n",
       "170536                                                NaN   \n",
       "170163                     Yourself, looking very scared.   \n",
       "170085                                                NaN   \n",
       "170664                                                NaN   \n",
       "\n",
       "                              title         i  sad   anx  \n",
       "170137                          NaN  0.000000  0.0  0.00  \n",
       "170307                          NaN  0.000000  0.0  0.00  \n",
       "170308                          NaN  0.090909  0.0  0.00  \n",
       "170309                          NaN  0.000000  0.0  0.00  \n",
       "170310                          NaN  0.000000  0.0  0.00  \n",
       "...                             ...       ...  ...   ...  \n",
       "170519      you startled the bitch!  0.000000  0.0  0.25  \n",
       "170536  Ignorance leads to Fear....  0.000000  0.0  0.25  \n",
       "170163                          NaN  0.000000  0.0  0.25  \n",
       "170085                Horrible mom?  0.000000  0.0  0.50  \n",
       "170664                        scary  0.000000  0.0  1.00  \n",
       "\n",
       "[1120 rows x 5 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[writings_df['subject']=='subject217']['i'].sort_values()\n",
    "writings_df[writings_df['subject']=='subject217'][['text', 'title', 'i', 'sad', 'anx']].sort_values(by='anx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negemo</th>\n",
       "      <th>posemo</th>\n",
       "      <th>affect</th>\n",
       "      <th>sad</th>\n",
       "      <th>anx</th>\n",
       "      <th>pronoun</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.023493</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>0.074548</td>\n",
       "      <td>0.003242</td>\n",
       "      <td>0.002606</td>\n",
       "      <td>0.121043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.026116</td>\n",
       "      <td>0.056145</td>\n",
       "      <td>0.082611</td>\n",
       "      <td>0.003706</td>\n",
       "      <td>0.003591</td>\n",
       "      <td>0.162980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         negemo    posemo    affect       sad       anx   pronoun\n",
       "label                                                            \n",
       "0      0.023493  0.050800  0.074548  0.003242  0.002606  0.121043\n",
       "1      0.026116  0.056145  0.082611  0.003706  0.003591  0.162980"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['label', 'negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']].groupby('label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject', 'title', 'date', 'text', 'label', 'tokenized_title',\n",
       "       'title_len', 'tokenized_text', 'text_len', 'all_tokens', 'funct',\n",
       "       'article', 'affect', 'negemo', 'sad', 'cogmech', 'inhib', 'bio', 'body',\n",
       "       'achieve', 'health', 'sexual', 'adverb', 'preps', 'space', 'relativ',\n",
       "       'time', 'work', 'certain', 'assent', 'anger', 'posemo', 'insight',\n",
       "       'verb', 'past', 'money', 'percept', 'social', 'friend', 'motion',\n",
       "       'cause', 'leisure', 'incl', 'home', 'present', 'humans', 'anx', 'relig',\n",
       "       'auxverb', 'negate', 'ingest', 'death', 'quant', 'tentat', 'conj',\n",
       "       'pronoun', 'ipron', 'swear', 'hear', 'family', 'see', 'discrep',\n",
       "       'number', 'filler', 'feel', 'excl', 'future', 'nonfl', 'ppron', 'shehe',\n",
       "       'i', 'we', 'you', 'they'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_history_df = pd.DataFrame()\n",
    "users_history_df['subject'] = writings_df.subject.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subject8292</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject6644</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject7982</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9260</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9918</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject835</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject3117</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject519</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1655</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject217</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>340 rows × 0 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [subject8292, subject6644, subject7982, subject9260, subject9918, subject4284, subject9829, subject7661, subject8361, subject4831, subject2181, subject9077, subject2922, subject2238, subject4513, subject269, subject2678, subject9197, subject4143, subject2605, subject4226, subject7627, subject5150, subject4510, subject2182, subject280, subject1105, subject187, subject8001, subject9285, subject2621, subject4414, subject2685, subject9961, subject8065, subject8225, subject6866, subject9949, subject1507, subject8329, subject9411, subject7857, subject1545, subject9811, subject5000, subject4843, subject569, subject51, subject9156, subject6453, subject1210, subject5528, subject1485, subject5935, subject4527, subject3301, subject4074, subject6093, subject2088, subject8990, subject6459, subject7830, subject8395, subject4247, subject3667, subject5003, subject992, subject5644, subject242, subject7764, subject3283, subject6322, subject7678, subject6668, subject4333, subject1288, subject8200, subject5383, subject9039, subject7698, subject9652, subject5223, subject9725, subject1512, subject3994, subject7018, subject3644, subject1786, subject1027, subject8094, subject974, subject2947, subject9575, subject4570, subject5062, subject4729, subject5100, subject5177, subject505, subject5974, ...]\n",
       "\n",
       "[340 rows x 0 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_history_df.set_index('subject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-24d8daac8211>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mwritings_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwritings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcateg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0musers_history_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcateg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_hist'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwritings_sorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcateg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3486\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3487\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3563\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3564\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3565\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3748\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3749\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3750\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3751\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Length of values does not match length of index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "categs = ['negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']\n",
    "for subject in users_history_df.subject:\n",
    "    writings = writings_df[writings_df['subject']==subject]\n",
    "    writings_sorted = writings.sort_values(by='date')\n",
    "    for categ in categs:\n",
    "        users_history_df[categ + '_hist'] = writings_sorted[categ].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_history = writings_df.groupby('subject').aggregate(lambda x: tuple(x))\n",
    "\n",
    "# df['grouped'] = df['B'] + df['C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_history['label'] = writings_history['label'].apply(lambda v: v[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for hist in writings_history[writings_history['label']==1].affect.values:\n",
    "    if len(hist)<100:\n",
    "        continue\n",
    "    pd.Series(hist).rolling(window=100).mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: COMET_OPTIMIZER_ID=786fc2b3654047e69f492db122f55b95\n",
      "COMET INFO: Using optimizer config: {'algorithm': 'random', 'configSpaceSize': 600000000000, 'endTime': None, 'id': '786fc2b3654047e69f492db122f55b95', 'lastUpdateTime': None, 'maxCombo': 0, 'name': '786fc2b3654047e69f492db122f55b95', 'parameters': {'batch_size': {'max': 512, 'min': 10, 'scalingType': 'loguniform', 'type': 'integer'}, 'decay': {'max': 0.5, 'min': 1e-08, 'scalingType': 'loguniform', 'type': 'float'}, 'dense_bow_units': {'max': 20, 'min': 1, 'scalingType': 'uniform', 'type': 'integer'}, 'dropout': {'max': 0.7, 'min': 0, 'scalingType': 'uniform', 'type': 'float'}, 'freeze_patience': {'max': 16, 'min': 2, 'scalingType': 'uniform', 'type': 'integer'}, 'l2_dense': {'max': 0.5, 'min': 1e-05, 'scalingType': 'loguniform', 'type': 'float'}, 'lr': {'max': 0.05, 'min': 1e-05, 'scalingType': 'loguniform', 'type': 'float'}, 'lr_reduce_factor': {'max': 0.8, 'min': 0.0001, 'scalingType': 'uniform', 'type': 'float'}, 'lr_reduce_patience': {'max': 16, 'min': 2, 'scalingType': 'uniform', 'type': 'integer'}, 'lstm_units': {'max': 100, 'min': 10, 'scalingType': 'uniform', 'type': 'integer'}, 'optimizer': {'type': 'categorical', 'values': ['adam', 'adagrad', '']}, 'positive_class_weight': {'max': 25, 'min': 1, 'scalingType': 'uniform', 'type': 'integer'}, 'trainable_embeddings': {'type': 'discrete', 'values': [True, False]}}, 'predictor': None, 'spec': {'gridSize': 10, 'maxCombo': 0, 'metric': 'loss', 'minSampleSize': 100, 'objective': 'minimize', 'retryAssignLimit': 0, 'retryLimit': 20, 'seed': 3968493229}, 'startTime': 15879681523, 'state': {'sequence_i': 0, 'sequence_pid': None, 'sequence_retry': 0}, 'status': 'running', 'suggestion_count': 0, 'trials': 1, 'version': '1.0.24'}\n",
      "COMET INFO: Optimizer metrics is 'loss' but no logged values found. Experiment ignored in sweep.\n",
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary:\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     url: https://www.comet.ml/ananana/mental/1ebbfc14aa9440e1948aabac5664d837\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     batch_binary_accuracy [29]: (0.5306122303009033, 0.9572663307189941)\n",
      "COMET INFO:     batch_f1_m [29]           : (0.0030426757875829935, 0.0336134247481823)\n",
      "COMET INFO:     batch_loss [29]           : (0.30054065585136414, 1.4031785726547241)\n",
      "COMET INFO:     batch_precision_m [29]    : (0.011335793882608414, 0.09260831028223038)\n",
      "COMET INFO:     batch_recall_m [29]       : (0.002346971072256565, 0.1666666716337204)\n",
      "COMET INFO:     sys.cpu.percent.01 [20]   : (4.8, 82.9)\n",
      "COMET INFO:     sys.cpu.percent.02 [20]   : (5.2, 73.9)\n",
      "COMET INFO:     sys.cpu.percent.03 [20]   : (4.6, 60.6)\n",
      "COMET INFO:     sys.cpu.percent.04 [20]   : (3.3, 57.8)\n",
      "COMET INFO:     sys.cpu.percent.avg [20]  : (4.925, 66.2)\n",
      "COMET INFO:     sys.gpu.0.total_memory    : (1073414144.0, 1073414144.0)\n",
      "COMET INFO:     sys.load.avg [20]         : (0.16, 4.02)\n",
      "COMET INFO:     sys.ram.total [20]        : (8277307392.0, 8277307392.0)\n",
      "COMET INFO:     sys.ram.used [20]         : (7586635776.0, 7714193408.0)\n",
      "COMET INFO:   Other [count]:\n",
      "COMET INFO:     optimizer_count       : 3\n",
      "COMET INFO:     optimizer_id          : 57eb178250e2401aa14cfc6860a4217a\n",
      "COMET INFO:     optimizer_metric      : loss\n",
      "COMET INFO:     optimizer_metric_value: None\n",
      "COMET INFO:     optimizer_parameters  : {'batch_size': 245, 'decay': 0.1402033634855056, 'dense_bow_units': 4, 'dropout': 0.46002110523805456, 'freeze_patience': 15, 'l2_dense': 0.0814165473266369, 'lr': 1.6244588122197995e-05, 'lr_reduce_factor': 0.6638163734300414, 'lr_reduce_patience': 10, 'lstm_units': 92, 'optimizer': 'adagrad', 'positive_class_weight': 5, 'set_trainable': True, 'trainable_embeddings': False}\n",
      "COMET INFO:     optimizer_pid         : 681f4971a3f98a865eb3100a742e6621e62a3420\n",
      "COMET INFO:     optimizer_process     : 2985\n",
      "COMET INFO:     optimizer_trial       : 1\n",
      "COMET INFO:     optimizer_version     : 1.0.24\n",
      "COMET INFO:     trainable_params      : 2071854\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     git-patch  : 1\n",
      "COMET INFO:     histogram3d: 1\n",
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/ananana/mental/6925a9644c034cbf9cd7f12c0a96e0f3\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 111375 samples, validate on 31863 samples\n",
      "Epoch 1/15\n",
      "111320/111375 [============================>.] - ETA: 0s - loss: 0.6696 - binary_accuracy: 0.8707 - f1_m: 0.1200 - precision_m: 0.0902 - recall_m: 0.2582\n",
      "Epoch 00001: val_loss improved from inf to 0.48204, saving model to models/experiment_best\n",
      "111375/111375 [==============================] - 317s 3ms/sample - loss: 0.6695 - binary_accuracy: 0.8707 - f1_m: 0.1201 - precision_m: 0.0902 - recall_m: 0.2584 - val_loss: 0.4820 - val_binary_accuracy: 0.7968 - val_f1_m: 0.2121 - val_precision_m: 0.1454 - val_recall_m: 0.4314\n",
      "Epoch 2/15\n",
      " 47190/111375 [===========>..................] - ETA: 2:52 - loss: 0.5882 - binary_accuracy: 0.8703 - f1_m: 0.1490 - precision_m: 0.1088 - recall_m: 0.2949"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-f94476829a7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m                           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                           \u001b[0mcallback_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfreeze_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_lr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                       model_path='models/experiment')\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-124-48d6d530785b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, x_train, y_train, x_test, y_test, batch_size, epochs, class_weight, start_epoch, workers, callback_list, model_path)\u001b[0m\n\u001b[1;32m     17\u001b[0m             callbacks = [\n\u001b[1;32m     18\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%s_best'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcallback_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             ])\n\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/comet_ml/monkey_patching.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m                     )\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Call after callbacks once we have the return value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Declare your hyperparameters search:\n",
    "tune_epochs=15\n",
    "config = {\n",
    "      \"algorithm\": \"random\",\n",
    "      \"parameters\": {\n",
    "          \"lstm_units\": {\"type\": \"integer\", \"min\": 10, \"max\": 100},\n",
    "          \"dense_bow_units\": {\"type\": \"integer\", \"min\": 1, \"max\": 20},\n",
    "          \"lr\": {\"type\": \"float\", \"min\": 0.00001, \"max\": 0.05, \"scalingType\": \"loguniform\"},\n",
    "          \"l2_dense\": {\"type\": \"float\", \"min\": 0.00001, \"max\": 0.5, \"scalingType\": \"loguniform\"},\n",
    "          \"dropout\": {\"type\": \"float\", \"min\": 0, \"max\": 0.7, \"scalingType\": \"uniform\"},\n",
    "          \"optimizer\": {\"type\": \"categorical\", \"values\": [\"adam\", \"adagrad\", \"\"]},\n",
    "          \"batch_size\": {\"type\": \"integer\", \"min\": 10, \"max\": 512, \"scalingType\": \"loguniform\"},\n",
    "          \"positive_class_weight\": {\"type\": \"integer\", \"min\": 1, \"max\": 25},\n",
    "          \"trainable_embeddings\": {\"type\": \"discrete\", \"values\": [True, False]},\n",
    "          \"freeze_patience\": {\"type\": \"integer\", \"min\": 2, \"max\": tune_epochs+1},\n",
    "          \"lr_reduce_factor\": {\"type\": \"float\", \"min\": 0.0001, \"max\": 0.8},\n",
    "          \"lr_reduce_patience\": {\"type\": \"integer\", \"min\": 2, \"max\": tune_epochs+1},\n",
    "          \"decay\": {\"type\": \"float\", \"min\": 0.00000001, \"max\": 0.5, \"scalingType\": \"loguniform\"},          \n",
    "      },\n",
    "      \"spec\": {\n",
    "          \"metric\": \"loss\",\n",
    "          \"objective\": \"minimize\",\n",
    "      },\n",
    "  }\n",
    "optimizer = Optimizer(config, api_key=\"eoBdVyznAhfg3bK9pZ58ZSXfv\")\n",
    "\n",
    "for experiment in optimizer.get_experiments(project_name=\"mental\"):\n",
    "    experiment.add_tag(\"tune\")\n",
    "    \n",
    "    # Test the model\n",
    "    hyperparams_config = {\n",
    "        param: experiment.get_parameter(param) for param in config['parameters'].keys()}\n",
    "    if not hyperparams_config['optimizer']:\n",
    "        hyperparams_config['optimizer'] = optimizers.Adam(lr=hyperparams_config['lr'], \n",
    "                                   decay=hyperparams_config['decay'])\n",
    "    model = build_model(hyperparams=hyperparams_config,\n",
    "                        hyperparams_features=hyperparams_features, \n",
    "                        embedding_matrix=embedding_matrix, emotions=emotions,\n",
    "                       stopwords_list=stopword_list)\n",
    "    freeze_layer = FreezeLayer(patience=experiment.get_parameter('freeze_patience'),\n",
    "                              set_to=not experiment.get_parameter('trainable_embeddings'))\n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            factor=experiment.get_parameter('lr_reduce_factor'),\n",
    "                                            patience=experiment.get_parameter('lr_reduce_patience'), \n",
    "                                            min_lr=0.000001, verbose=1)\n",
    "    history = train_model(model, \n",
    "            x_train, y_train, x_test, y_test,\n",
    "            epochs=tune_epochs, batch_size=experiment.get_parameter('batch_size'),\n",
    "                      class_weight={0:1, 1:experiment.get_parameter('positive_class_weight')}, \n",
    "                          workers=2,\n",
    "                          callback_list = [freeze_layer, reduce_lr],\n",
    "                      model_path='models/experiment')\n",
    "    loss = history.history['loss'][-1]\n",
    "    \n",
    "    # Report the loss, if not auto-logged:\n",
    "    experiment.log_metric(\"loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
