{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob, os\n",
    "import numpy as np\n",
    "from comet_ml import Experiment, Optimizer\n",
    "import pickle\n",
    "import logging\n",
    "import sys\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # When cudnn implementation not found, run this\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Note: when starting kernel, for gpu_available to be true, this needs to be run\n",
    "# only reserve 1 GPU\n",
    "os.environ['TFHUB_CACHE_DIR'] = '/home/anasab/tf_cache'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "assert 'GPU' in str(device_lib.list_local_devices())\n",
    "\n",
    "# # confirm Keras sees the GPU (for TensorFlow 1.X + Keras)\n",
    "# from keras import backend\n",
    "# assert len(backend.tensorflow_backend._get_available_gpus()) > 0\n",
    "\n",
    "# # confirm PyTorch sees the GPU\n",
    "# from torch import cuda\n",
    "# assert cuda.is_available()\n",
    "# assert cuda.device_count() > 0\n",
    "# print(cuda.get_device_name(cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 3431631627159801543,\n",
       " name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 8910092503156153153\n",
       " physical_device_desc: \"device: XLA_CPU device\",\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 7839989760\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 11211423501550573564\n",
       " physical_device_desc: \"device: 0, name: GeForce RTX 2080, pci bus id: 0000:17:00.0, compute capability: 7.5\",\n",
       " name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 12405901940991862560\n",
       " physical_device_desc: \"device: XLA_GPU device\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "os.environ['TF_KERAS'] = '1'\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Lambda, BatchNormalization, TimeDistributed, \\\n",
    "    Bidirectional, Input, concatenate, Flatten, RepeatVector, Activation, Multiply, Permute, \\\n",
    "    Conv1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import callbacks, optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model, Sequence\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer, TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = \"combined\"\n",
    "transfer_type = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('training')\n",
    "ch = logging.StreamHandler(sys.stdout)\n",
    "# create formatter\n",
    "formatter = logging.Formatter(\"%(asctime)s;%(levelname)s;%(message)s\")\n",
    "# add formatter to ch\n",
    "ch.setFormatter(formatter)\n",
    "# add ch to logger\n",
    "logger.addHandler(ch)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.enable_eager_execution()\n",
    "my_seed = 1234\n",
    "# tf.set_random_seed(my_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend import manual_variable_initialization \n",
    "manual_variable_initialization(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_subject_writings(subject_file):\n",
    "    writings = []\n",
    "    with open(subject_file) as sf:\n",
    "        contents = sf.read()\n",
    "        root = ET.fromstring(contents)\n",
    "        try:\n",
    "            subject = root.findall('ID')[0].text.strip()\n",
    "        except Exception:\n",
    "            print('Cannot extract ID', contents[:500], '\\n-------\\n')        \n",
    "        for w in root.iter('WRITING'):\n",
    "            subject_writings = {'subject': subject}\n",
    "            for title in w.findall('TITLE'):\n",
    "                subject_writings['title'] = title.text\n",
    "            for text in w.findall('TEXT'):\n",
    "                subject_writings['text'] = text.text\n",
    "            for date in w.findall('DATE'):\n",
    "                subject_writings['date'] = date.text\n",
    "            writings.append(subject_writings)\n",
    "    return writings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/anasab/' \n",
    "# root_dir = '/home/ana/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eRisk 2020 T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datadir_T1 = root_dir + '/eRisk/data/eRisk2020_T1_train/eRISK2020_T1_training_data/eRISK2020_training_data/data/'\n",
    "# labels_file_T1 = root_dir + '/eRisk/data//eRisk2020_T1_train/eRISK2020_T1_training_data/eRISK2020_training_data/golden_truth.txt'\n",
    "\n",
    "datadirs_T1_2020 = {\n",
    "    'train': ['./data/'],\n",
    "    'test': ['./DATA/']\n",
    "}\n",
    "datadir_root_T1_2020 = {\n",
    "    'train': root_dir + '/eRisk/data/eRisk2020_T1_train/eRISK2020_T1_training_data/eRISK2020_training_data/',\n",
    "    'test': root_dir + '/eRisk/data/2020/T1/'\n",
    "}\n",
    "    \n",
    "labels_files_T1_2020 = {\n",
    "    'train': ['golden_truth.txt'],\n",
    "    'test': ['T1_erisk_golden_truth.txt']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_2020(datadir_root_T1_2020,\n",
    "                   datadirs_T1_2020,\n",
    "                   labels_files_T1_2020,\n",
    "                   test_suffix='0000',\n",
    "                    chunked_subsets=None):\n",
    "\n",
    "\n",
    "    writings = {'train': [], 'test': []}\n",
    "    writings_df = pd.DataFrame()\n",
    "    labels_df = pd.DataFrame()\n",
    "#     for subset in ('train', 'test'):\n",
    "    for subset in ('test',):\n",
    "        for subdir in [os.path.join(datadir_root_T1_2020[subset], subp) for subp in datadirs_T1_2020[subset]]:\n",
    "\n",
    "            for subject_file in os.listdir(subdir):\n",
    "                writings[subset].extend(read_subject_writings(os.path.join(subdir, subject_file)))\n",
    "        writings_df_part = pd.DataFrame(writings[subset])\n",
    "        # add a suffix for users in the test -- the numbers are duplicated with the ones in train\n",
    "        if subset=='test':\n",
    "            writings_df_part['subject'] = writings_df_part['subject'].apply(lambda s: s+test_suffix)\n",
    "            print(subset, writings_df_part.subject)\n",
    "        writings_df_part['subset'] = subset\n",
    "        writings_df = pd.concat([writings_df, writings_df_part])\n",
    "        writings_df.reindex()\n",
    "\n",
    "        for label_file in labels_files_T1_2020[subset]:\n",
    "            labels = pd.read_csv(os.path.join(datadir_root_T1_2020[subset], label_file), \n",
    "                                 delimiter='\\s+', names=['subject', 'label'])\n",
    "            # add a suffix for users in the test -- the numbers are duplicated with the ones in train\n",
    "            if subset=='test':\n",
    "                labels['subject'] = labels['subject'].apply(lambda s: s+test_suffix)\n",
    "            labels_df = pd.concat([labels_df, labels])\n",
    "    labels_df = labels_df.drop_duplicates()\n",
    "    labels_df = labels_df.set_index('subject')\n",
    "\n",
    "    writings_df = writings_df.drop_duplicates()\n",
    "    \n",
    "    writings_df = writings_df.join(labels_df, on='subject')\n",
    "    \n",
    "    return writings_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eRisk 2019 T1 (Anorexia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadirs_T1_2019 = {\n",
    "    'train': ['2018 test/', '2018 train/positive_examples/', '2018 train/negative_examples/'],\n",
    "    'test': ['data/']\n",
    "}\n",
    "datadir_root_T1_2019 = {\n",
    "    'train': root_dir + '/eRisk/data/past/eRisk2019_T1/training data - t1/',\n",
    "    'test': root_dir + '/eRisk/data/past/eRisk2019_T1/test data - T1/'\n",
    "}\n",
    "    \n",
    "labels_files_T1_2019 = {\n",
    "    'train': ['2018 train/risk_golden_truth.txt', '2018 test/risk-golden-truth-test.txt'],\n",
    "    'test': ['T1_erisk_golden_truth.txt']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_2019(datadir_root_T1_2019,\n",
    "                   datadirs_T1_2019,\n",
    "                   labels_files_T1_2019,\n",
    "                   test_suffix='0000',\n",
    "                    chunked_subsets='train'):\n",
    "    writings = {'train': [], 'test': []}\n",
    "    writings_df = pd.DataFrame()\n",
    "    labels_df = pd.DataFrame()\n",
    "\n",
    "    for subset in ('train', 'test'):\n",
    "        for subdir in [os.path.join(datadir_root_T1_2019[subset], subp) for subp in datadirs_T1_2019[subset]]:\n",
    "            if subset in chunked_subsets:\n",
    "                chunkdirs = [os.path.join(datadir_root_T1_2019[subset], subdir, chunkdir) \n",
    "                             for chunkdir in os.listdir(subdir)]\n",
    "            else:\n",
    "                chunkdirs = [os.path.join(datadir_root_T1_2019[subset], subdir)]\n",
    "                \n",
    "            for chunkdir in chunkdirs:\n",
    "                print(chunkdir)\n",
    "                if not os.path.isdir(chunkdir):\n",
    "                    continue\n",
    "                for subject_file in os.listdir(chunkdir):\n",
    "                    writings[subset].extend(read_subject_writings(os.path.join(chunkdir, subject_file)))\n",
    "        writings_df_part = pd.DataFrame(writings[subset])\n",
    "        # add a suffix for users in the test -- the numbers are duplicated with the ones in train\n",
    "        if subset=='test':\n",
    "            writings_df_part['subject'] = writings_df_part['subject'].apply(lambda s: s+test_suffix)\n",
    "            print(subset, writings_df_part.subject)\n",
    "        writings_df_part['subset'] = subset\n",
    "        writings_df = pd.concat([writings_df, writings_df_part])\n",
    "        writings_df.reindex()\n",
    "\n",
    "        for label_file in labels_files_T1_2019[subset]:\n",
    "            labels = pd.read_csv(os.path.join(datadir_root_T1_2019[subset], label_file), \n",
    "                                 delimiter='\\s+', names=['subject', 'label'])\n",
    "            # add a suffix for users in the test -- the numbers are duplicated with the ones in train\n",
    "            if subset=='test':\n",
    "                labels['subject'] = labels['subject'].apply(lambda s: s+test_suffix)\n",
    "            labels_df = pd.concat([labels_df, labels])\n",
    "    labels_df = labels_df.drop_duplicates()\n",
    "    labels_df = labels_df.set_index('subject')\n",
    "\n",
    "    writings_df = writings_df.drop_duplicates()\n",
    "    \n",
    "    writings_df = writings_df.join(labels_df, on='subject')\n",
    "    \n",
    "    return writings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eRisk 2018 (Depression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadirs_T1_2018 = {\n",
    "    'train': ['train/positive_examples_anonymous_chunks/', 'train/positive_examples_anonymous_chunks/', 'test/'],\n",
    "    'test': ['task 1 - depression (test split, train split is 2017 data)/']\n",
    "}\n",
    "datadir_root_T1_2018 = {\n",
    "    'train': root_dir + '/eRisk/data/2017/',\n",
    "    'test': root_dir + '/eRisk/data/2018/'\n",
    "}\n",
    "    \n",
    "labels_files_T1_2018 = {\n",
    "    'train': ['train/risk_golden_truth.txt', 'test/test_golden_truth.txt'],\n",
    "    'test': ['task 1 - depression (test split, train split is 2017 data)/risk-golden-truth-test.txt']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLPsych\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadirs_clpsych = {\n",
    "    'train': [''],\n",
    "    'test': ['']\n",
    "}\n",
    "datadir_root_clpsych = {\n",
    "    'train': root_dir + '/eRisk/data/clpsych/final_training_data/',\n",
    "    'test': root_dir + '/eRisk/data/clpsych/final_testing_data/'\n",
    "}\n",
    "    \n",
    "labels_files_clpsych = [root_dir + '/eRisk/data/clpsych/anonymized_user_info_by_chunk.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_subject_data_clpsych(subject_file):\n",
    "    writings = []\n",
    "    with open(subject_file, \"rt\", encoding=\"utf-8\") as sf:\n",
    "        user = subject_file.split(\"/\")[-1].split(\".\")[0]\n",
    "        print(subject_file)\n",
    "\n",
    "        for line in sf:\n",
    "            data = json.loads(line)#.encode('utf-16','surrogatepass').decode('utf-16'))\n",
    "            data['subject'] = user\n",
    "            writings.append(data)\n",
    "    return writings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_clpsych(datadir_root_clpsych,\n",
    "                   datadirs_clpsych,\n",
    "                   labels_files_T1_2019,\n",
    "                      label_by=['depression']):\n",
    "    writings = {'train': [], 'test': []}\n",
    "    writings_df = pd.DataFrame()\n",
    "    labels_df = pd.DataFrame()\n",
    "\n",
    "    for subset in ('test',):#, 'test'):\n",
    "        for subdir in [os.path.join(datadir_root_clpsych[subset], subp) for subp in datadirs_clpsych[subset]]:\n",
    "            for subject_file in glob.glob(subdir + \"/*.tweets\"):\n",
    "#                 if subject_file.split(\"/\")[-1] != 'sZVVktDN8qqjA.tweets':\n",
    "#                     continue\n",
    "                writings[subset].extend(read_subject_data_clpsych(os.path.join(subdir, subject_file)))\n",
    "        writings_df_part = pd.DataFrame(writings[subset])\n",
    "        writings_df_part['subset'] = subset\n",
    "        writings_df = pd.concat([writings_df, writings_df_part])\n",
    "        writings_df.reindex()\n",
    "\n",
    "    for label_file in labels_files_clpsych:\n",
    "        labels = pd.read_csv(label_file, \n",
    "                    names=['subject','age','num_tweets','gender','condition','chunk_index'])\n",
    "        labels['label'] = labels['condition'].apply(lambda c: 1 if c in label_by else 0)\n",
    "        \n",
    "        labels_df = pd.concat([labels_df, labels])\n",
    "        labels_df = labels_df.drop_duplicates()\n",
    "        labels_df = labels_df.set_index('subject')\n",
    "\n",
    "        # TODO: this deduplication throws some unicode, surrogates not allowed, exception\n",
    "#     writings_df = writings_df.drop_duplicates(subset=['id', 'subject', 'subset', 'created_at', 'text'])\n",
    "    \n",
    "    writings_df = writings_df.join(labels_df, on='subject')\n",
    "    writings_df['date'] = writings_df['created_at']\n",
    "    \n",
    "    return writings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symanto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_symanto(tsv_path=\"/eRisk/data/symanto/clean_dataset_with_timeline_balancedUsers.tsv\"):\n",
    "    label_key = {'REAL_LABEL_IS_DEPRESSED': 1,\n",
    "             'REAL_LABEL_IS_NON_DEPRESSED': 0}\n",
    "    prediction_key = {'predicted_as_depressed': 1,\n",
    "                     'predicted_as_non_depressed': 0,\n",
    "                     'DEPRESS_STRING_WAS_MENTIONED': -1}\n",
    "    \n",
    "    writings_df = pd.read_csv(root_dir + tsv_path, \n",
    "                              sep='\\t', names=['subject', 'date', 'text', 'prediction_text', 'real_label_text'])\n",
    "\n",
    "    writings_df = writings_df.dropna()\n",
    "    writings_df['label'] = writings_df['real_label_text'].apply(lambda l: label_key[l])\n",
    "    writings_df['prediction'] = writings_df['prediction_text'].apply(lambda l: prediction_key[l])\n",
    "#     By ommitting -1s we are excluding tweets where \"DEPRESSED\" was mentioned, and all after\n",
    "    writings_df = writings_df.drop(writings_df[writings_df['prediction']==-1].index)\n",
    "\n",
    "    \n",
    "    return writings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df = read_texts_2020(datadir_T1, labels_file_T1)\n",
    "# writings_df = read_texts_2019(datadir_root_T1_2019,\n",
    "#                    datadirs_T1_2019,\n",
    "#                    labels_files_T1_2019)\n",
    "# writings_df_depression = read_texts_2019(datadir_root_T1_2018,\n",
    "#                    datadirs_T1_2018,\n",
    "#                    labels_files_T1_2018,\n",
    "#                              chunked_subsets=['train', 'test'])\n",
    "\n",
    "if dataset_type == \"combined\":\n",
    "    writings_df_selfharm = pickle.load(open('data/writings_df_selfharm_all', 'rb'))\n",
    "    writings_df_anorexia = pickle.load(open('data/writings_df_anorexia_liwc', 'rb'))\n",
    "    writings_df_depression = pickle.load(open('data/writings_df_depression_liwc', 'rb'))\n",
    "    writings_df_selfharm['source'] = 'selfharm'\n",
    "    writings_df_anorexia['source'] = 'anorexia'\n",
    "    writings_df_depression['source'] = 'depression'\n",
    "    writings_df = pd.DataFrame()\n",
    "    writings_df = pd.concat([writings_df, writings_df_depression])\n",
    "    writings_df = pd.concat([writings_df, writings_df_selfharm])\n",
    "    writings_df = pd.concat([writings_df, writings_df_anorexia])\n",
    "elif dataset_type == \"combined_depr\":\n",
    "    writings_df = pd.DataFrame.from_dict(json.load(open('data/writings_df_depression_all.json')))\n",
    "elif dataset_type == \"clpsych\":\n",
    "    writings_df = pd.DataFrame.from_dict(json.load(open('data/writings_df_%s_liwc_affect.json' % dataset_type)))#read_texts_clpsych(datadir_root_clpsych, datadirs_clpsych, labels_files_clpsych)\n",
    "#     writings_df_test = pd.DataFrame.from_dict(json.load(open('writings_df_%s_test.json' % dataset_type)))#read_texts_clpsych(datadir_root_clpsych, datadirs_clpsych, labels_files_clpsych)\n",
    "#     writings_df_test = read_texts_clpsych(datadir_root_clpsych, datadirs_clpsych, labels_files_clpsych)\n",
    "    label_by = ['depression', 'ptsd']\n",
    "    writings_df.drop(writings_df[writings_df['condition']=='depression'].index, inplace=True)\n",
    "#     writings_df['label'] = writings_df['condition'].apply(lambda c: 1 if c in label_by else 0)\n",
    "#     writings_df['date'] = writings_df['created_at']\n",
    "elif dataset_type == \"symanto\":\n",
    "    writings_df = read_texts_symanto()\n",
    "elif dataset_type == 'selfharm':\n",
    "    writings_df = pickle.load(open('data/writings_df_%s_all' % dataset_type, 'rb'))\n",
    "elif dataset_type in [\"depression\", \"anorexia\", \"selfharm\"]:\n",
    "    writings_df = pickle.load(open('data/writings_df_%s_liwc' % dataset_type, 'rb'))\n",
    "else:\n",
    "    logger.error(\"Unknown dataset %s\" % dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df[writings_df['condition']!='control'].head(150)[['label', 'condition', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df.label.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "regtokenizer = RegexpTokenizer(r'\\w+')\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "def tokenize(t, tokenizer=regtokenizer):\n",
    "    return regtokenizer.tokenize(t.lower())\n",
    "\n",
    "def tokenize_tweets(t, stop=True):\n",
    "    tokens = tweet_tokenizer.tokenize(t.lower())\n",
    "    tokens_clean = [re.sub(\"^#\", \"\", token) for token in tokens]\n",
    "    tokens_clean = [token for token in tokens_clean \n",
    "                            if re.match(\"^[a-z]*$\", token)]\n",
    "    if not stop:\n",
    "        tokens_clean = [token for token in tokens\n",
    "                       if (token not in sw)]\n",
    "    return tokens_clean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.sub(\"^#\", \"\", \"#h#ash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fields(writings_df, tokenize_fct=tokenize, columns=['title', 'text']):\n",
    "    for c in columns:\n",
    "        writings_df['tokenized_%s' % c] = writings_df['%s' % c].apply(lambda t: tokenize_fct(t) \n",
    "                                                                if type(t)==str and t else None)\n",
    "        writings_df['%s_len' % c] = writings_df['tokenized_%s' % c].apply(lambda t: len(t) \n",
    "                                                                    if type(t)==list and t else None)\n",
    "    return writings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df = tokenize_fields(writings_df, tokenize_fct=tokenize_tweets, columns=['title'])\n",
    "# writings_df = tokenize_fields(writings_df, tokenize_fct=tokenize_tweets, columns=['text'])\n",
    "# writings_df = tokenize_fields(writings_df, tokenize_fct=tokenize, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df.text_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df.title_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df.groupby('subject').mean().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df.groupby('subject').max().groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Average number of posts per user\", writings_df.groupby('subject').count().title.mean())\n",
    "# print(\"Average number of comments per user\", writings_df.groupby('subject').count().text.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df.groupby('subject').count().title.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df.groupby('subject').count().text.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df.tokenized_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_dict = writings_df.to_dict()\n",
    "# json.dump(writings_dict, open(\"writings_df_clpsych_all.json\", \"w+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Total examples\", writings_df[['subject', 'subset', 'label'\n",
    "#                                     ]].groupby('subject').min().groupby('subset').count())\n",
    "# print(\"Positive examples\", writings_df[['subject', 'subset', 'label'\n",
    "#                                     ]].groupby('subject').min().groupby('subset').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anorexia</th>\n",
       "      <td>42487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depression</th>\n",
       "      <td>90215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>selfharm</th>\n",
       "      <td>18883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            label\n",
       "source           \n",
       "anorexia    42487\n",
       "depression  90215\n",
       "selfharm    18883"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['label', 'source']].groupby('source').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['text'] = writings_df['text'].fillna(\"\")\n",
    "writings_df['title'] = writings_df['title'].fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features and encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_params(model, model_path, hyperparams, hyperparams_features):\n",
    "    model.save_weights(model_path + \"_weights.h5\", save_format='h5')\n",
    "    model.save_weights(model_path + \"_weights\")\n",
    "#     model.save(model_path + \"_model.model\")\n",
    "#     model.save(model_path + \"_model.h5\")\n",
    "    with open(model_path + '.hp.json', 'w+') as hpf:\n",
    "        hpf.write(json.dumps({k:v for (k,v) in hyperparams.items() if k!='optimizer'}))\n",
    "    with open(model_path + '.hpf.json', 'w+') as hpff:\n",
    "        hpff.write(json.dumps(hyperparams_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_params(model_path):\n",
    "    with open(model_path + '.hp.json', 'r') as hpf:\n",
    "        hyperparams = json.loads(hpf.read())\n",
    "    with open(model_path + '.hpf.json', 'r') as hpff:\n",
    "        hyperparams_features = json.loads(hpff.read())\n",
    "    return hyperparams, hyperparams_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_features = {\n",
    "    \"max_features\": 20002,\n",
    "    # cut texts after this number of words\n",
    "    # (among top max_features most common words)\n",
    "    \"embedding_dim\": 100,\n",
    "    'vocabulary_path': 'data/all_vocab_clpsych_erisk_stop_20000.pkl',\n",
    "#     'embeddings_path': root_dir + '/eRisk/embeddings/finetuned_glove_clpsych_erisk_stop_normalized_20000.pkl',\n",
    "    'embeddings_path': root_dir + '/resources/glove.840B/glove.840B.300d.txt',\n",
    "    'liwc_words_cached': 'data/liwc_categories_for_vocabulary_erisk_clpsych_stop_20K.pkl',\n",
    "    \"user_level\": True, # deprecated\n",
    "    \"transfer\": transfer_type,\n",
    "    \"pretrained_model_path\": 'models/lstm_clpsych_seq52',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if transfer_type:\n",
    "    pretrained_model_path = hyperparams_features['pretrained_model_path']\n",
    "    hyperparams, hyperparams_features = load_params(hyperparams_features['pretrained_model_path'])\n",
    "    hyperparams_features['pretrained_model_path'] = pretrained_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_NRC(nrc_path):\n",
    "    word_emotions = {}\n",
    "    emotion_words = {}\n",
    "    with open(nrc_path) as in_f:\n",
    "        for line in in_f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            word, emotion, label = line.split()\n",
    "            if word not in word_emotions:\n",
    "                word_emotions[word] = set()\n",
    "            if emotion not in emotion_words:\n",
    "                emotion_words[emotion] = set()\n",
    "            label = int(label)\n",
    "            if label:\n",
    "                word_emotions[word].add(emotion)\n",
    "                emotion_words[emotion].add(word)\n",
    "    return emotion_words\n",
    "\n",
    "nrc_lexicon_path = root_dir + '/resources/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'\n",
    "nrc_lexicon = load_NRC(nrc_lexicon_path)\n",
    "emotions = list(nrc_lexicon.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_emotions(tokens, emotion_lexicon, emotions, relative=True):\n",
    "    text_len = len(tokens)\n",
    "    encoded_emotions = [0 for e in emotions]\n",
    "    for i, emotion in enumerate(emotions):\n",
    "        try:\n",
    "            emotion_words = [t for t in tokens if t in emotion_lexicon[emotion]]\n",
    "            if relative and len(tokens):\n",
    "                encoded_emotions[i] = len(emotion_words) / len(tokens)\n",
    "            else:\n",
    "                encoded_emotions[i] = len(emotion_words)\n",
    "        except ValueError:\n",
    "            print(\"Emotion not found.\")\n",
    "    return encoded_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from liwc_readDict import readDict\n",
    "\n",
    "liwc_dict = {}\n",
    "for (w, c) in readDict(root_dir + '/resources/liwc.dic'):\n",
    "    if c not in liwc_dict:\n",
    "        liwc_dict[c] = []\n",
    "    liwc_dict[c].append(w)\n",
    "\n",
    "categories = set(liwc_dict.keys())\n",
    "len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_categories=[c for c in categories]# if c in writings_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_words_for_categories = pickle.load(open(\"data/liwc_categories_for_vocabulary_erisk_clpsych_stop_20K.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"liwc_categories.txt\",\"w+\") as f:\n",
    "    for cat in liwc_dict:\n",
    "        f.write(\"%s: %s\\n\" % (cat, \",\".join(liwc_dict[cat])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nrc_emotions.txt\",\"w+\") as f:\n",
    "    for cat in nrc_lexicon:\n",
    "        f.write(\"%s: %s\\n\" % (cat, \",\".join(nrc_lexicon[cat])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Char n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(tokens):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Personal pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_person_pronouns = {\"i\", \"me\", \"my\", \"mine\", \"myself\"}\n",
    "def encode_pronouns(tokens, pronouns={\"i\", \"me\", \"my\", \"mine\", \"myself\"}, relative=True):\n",
    "    if not tokens:\n",
    "        return 0\n",
    "    text_len = len(tokens)\n",
    "    nr_pronouns = len([t for t in tokens if t in pronouns])\n",
    "    if relative and text_len:\n",
    "        return nr_pronouns/text_len\n",
    "    else:\n",
    "        return nr_pronouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words(\"english\")\n",
    "def encode_stopwords(tokens, stopwords=stopword_list):\n",
    "    encoded_stopwords = [0 for s in stopword_list]\n",
    "    if not tokens:\n",
    "        return encoded_stopwords\n",
    "    for i, stopword in enumerate(stopwords):\n",
    "        if stopword in tokens:\n",
    "            encoded_stopwords[i] += 1\n",
    "    return encoded_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_hub'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-5fb51e9c55d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbert_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFullTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_hub'"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "from bert.tokenization import FullTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "    Args:\n",
    "      guid: Unique id for the example.\n",
    "      text_a: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "        Only must be specified for sequence pair tasks.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text_for_bert(tokenizer, example, max_seq_length=256):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "#     if isinstance(example, PaddingInputExample):\n",
    "#         input_ids = [0] * max_seq_length\n",
    "#         input_mask = [0] * max_seq_length\n",
    "#         segment_ids = [0] * max_seq_length\n",
    "#         label = 0\n",
    "#         return input_ids, input_mask, segment_ids, label\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    if len(tokens_a) > max_seq_length - 2:\n",
    "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
    "\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    return input_ids, input_mask, segment_ids, example.label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer_from_hub_module(sess):\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    bert_module =  hub.Module(bert_path)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    vocab_file, do_lower_case = sess.run(\n",
    "        [\n",
    "            tokenization_info[\"vocab_file\"],\n",
    "            tokenization_info[\"do_lower_case\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_sess(sess=None):\n",
    "    if not sess:\n",
    "        sess_config = tf.ConfigProto(\n",
    "            device_count={ 'GPU' : 1, 'CPU': 4 },\n",
    "            intra_op_parallelism_threads = 0,\n",
    "            inter_op_parallelism_threads = 4,\n",
    "            allow_soft_placement=False,\n",
    "            log_device_placement=True,\n",
    "        )\n",
    "        sess_config.gpu_options.allow_growth = True\n",
    "        sess_config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "        sess = tf.Session(config=sess_config)\n",
    "\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_model(model_path, hyperparams):\n",
    "    metrics_class = Metrics(threshold=hyperparams['threshold'])\n",
    "    dependencies = {\n",
    "    'f1_m': metrics_class.f1_m,\n",
    "    'auc': metrics_class.auc,\n",
    "    'precision_m': metrics_class.precision_m,\n",
    "    'recall_m': metrics_class.recall_m,\n",
    "    'binary_crossentropy_custom': binary_crossentropy_custom,\n",
    "    'BertLayer': BertLayer\n",
    "    }\n",
    "#     loaded_model = load_model(model_path + \"_model.h5\", custom_objects=dependencies)\n",
    "    loaded_model = load_model(model_path + \"_model.model\", custom_objects=dependencies)\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def load_erisk_data(writings_df, voc_size, emotion_lexicon, emotions =  \n",
    "                    ['anger', 'anticipation', 'disgust', 'fear', 'joy', \n",
    "                     'negative', 'positive', 'sadness', 'surprise', 'trust'],\n",
    "                    liwc_categories = categories, by_subset=True,\n",
    "                    pronouns = [\"i\", \"me\", \"my\", \"mine\", \"myself\"],\n",
    "                    train_prop=0.7, valid_prop=0.3, test_slice=2,\n",
    "                    nr_slices=5,\n",
    "                    min_post_len=3, min_word_len=1, \n",
    "                    user_level=True, vocabulary=None, labelcol='label',\n",
    "                   logger=logger):\n",
    "    logger.debug(\"Loading data...\\n\")\n",
    "    \n",
    "    ## Build vocabulary\n",
    "    vocabulary_all = {}\n",
    "    word_freqs = Counter()\n",
    "    \n",
    "    for words in writings_df.tokenized_text:\n",
    "        word_freqs.update(words)\n",
    "    if 'tokenized_title' in writings_df.columns:\n",
    "        for words in writings_df.tokenized_title:\n",
    "            word_freqs.update(words)\n",
    "    i = 1\n",
    "    for w, f in word_freqs.most_common(voc_size-2): # keeping voc_size-1 for unk\n",
    "        if len(w) < min_word_len:\n",
    "            continue\n",
    "        vocabulary_all[w] = i\n",
    "        i += 1\n",
    "    if not vocabulary:\n",
    "        vocabulary = vocabulary_all\n",
    "    else:\n",
    "        logger.info(\"Words not found in the vocabulary: %d\\n\" % len(set(vocabulary_all.keys()).difference(\n",
    "            set(vocabulary.keys()))))\n",
    "\n",
    "    if labelcol != 'label':\n",
    "        label_index = {}\n",
    "        l = 0\n",
    "        for label in set(writings_df[labelcol]):\n",
    "            label_index[label] = l\n",
    "            l += 1\n",
    "    print(\"Label index\", label_index)\n",
    "   \n",
    "    if by_subset and 'subset' in writings_df.columns:\n",
    "        training_subjects = list(set(writings_df[writings_df['subset']=='train'].subject))\n",
    "        test_subjects = list(set(writings_df[writings_df['subset']=='test'].subject))\n",
    "    else:\n",
    "        all_subjects = sorted(list(set(writings_df.subject)))\n",
    "        training_subjects_size = int(len(all_subjects) * train_prop)\n",
    "        test_subjects_size = len(all_subjects) - training_subjects_size\n",
    "        logger.info(\"%d training subjects, %d test subjects\\n\" % (training_subjects_size, test_subjects_size))\n",
    "        # Cross-validation, with fixed slice as input\n",
    "        test_prop = 1-train_prop\n",
    "        test_slice = min(test_slice, nr_slices)\n",
    "        logger.debug(\"start index: %f, from %f\\n\" % (\n",
    "            len(all_subjects)*(1/nr_slices)*test_slice, test_prop*test_slice))\n",
    "        start_slice = int(len(all_subjects)*(1/nr_slices)*test_slice)\n",
    "        test_subjects = all_subjects[start_slice: start_slice+test_subjects_size]\n",
    "        training_subjects = [s for s in all_subjects if s not in test_subjects]\n",
    "    training_subjects = sorted(training_subjects) # ensuring reproducibility\n",
    "    valid_subjects_size = int(len(training_subjects) * valid_prop)\n",
    "    valid_subjects = training_subjects[:valid_subjects_size]\n",
    "    training_subjects = training_subjects[valid_subjects_size:]\n",
    "    categories = [c for c in liwc_categories if c in writings_df.columns]\n",
    "    logger.debug(\"%d training users, %d validation users, %d test users.\" % (\n",
    "        len(training_subjects), \n",
    "          len(valid_subjects),\n",
    "          len(test_subjects)))\n",
    "    subjects_split = {'train': training_subjects, \n",
    "                      'valid': valid_subjects, \n",
    "                      'test': test_subjects}\n",
    "\n",
    "    user_level_texts = {}\n",
    "    for row in writings_df.sort_values(by='date').itertuples():\n",
    "        words = []\n",
    "        raw_text = \"\"\n",
    "        if hasattr(row, 'tokenized_title'):\n",
    "            if row.tokenized_title:\n",
    "                words.extend(row.tokenized_title)\n",
    "                raw_text += row.title\n",
    "        if hasattr(row, 'tokenized_text'):\n",
    "            if row.tokenized_text:\n",
    "                words.extend(row.tokenized_text)\n",
    "                raw_text += row.text\n",
    "        if not words or len(words)<min_post_len:\n",
    "#             logger.debug(row.subject)\n",
    "            continue\n",
    "        if labelcol == 'label':\n",
    "            label = row.label\n",
    "        else:\n",
    "            label = label_index[getattr(row, labelcol)]\n",
    "        liwc_categs = [getattr(row, categ) for categ in categories]\n",
    "        if row.subject not in user_level_texts.keys():\n",
    "            user_level_texts[row.subject] = {}\n",
    "            user_level_texts[row.subject]['texts'] = [words]\n",
    "            user_level_texts[row.subject]['label'] = label\n",
    "            user_level_texts[row.subject]['liwc'] = [liwc_categs]\n",
    "            user_level_texts[row.subject]['raw'] = [raw_text]\n",
    "        else:\n",
    "            user_level_texts[row.subject]['texts'].append(words)\n",
    "            user_level_texts[row.subject]['liwc'].append(liwc_categs)\n",
    "            user_level_texts[row.subject]['raw'].append(raw_text)\n",
    "            \n",
    "    return user_level_texts, subjects_split, vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del writings_df['tokenized_title']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-12 12:07:15,694;DEBUG;Loading data...\n",
      "\n",
      "2021-01-12 12:07:24,103;INFO;Words not found in the vocabulary: 2022\n",
      "\n",
      "Label index {'selfharm': 0, 'anorexia': 1, 'depression': 2}\n",
      "2021-01-12 12:07:25,058;DEBUG;894 training users, 383 validation users, 1924 test users.\n"
     ]
    }
   ],
   "source": [
    "vocabulary_list = pickle.load(open(hyperparams_features['vocabulary_path'], 'rb'))\n",
    "vocabulary_dict={}\n",
    "for i,w in enumerate(vocabulary_list):\n",
    "    vocabulary_dict[w] = i\n",
    "user_level_data, subjects_split, vocabulary = load_erisk_data(writings_df, \n",
    "                                                            voc_size=hyperparams_features['max_features'],\n",
    "                                                           emotion_lexicon=nrc_lexicon,\n",
    "                                                           emotions=emotions,\n",
    "                                                           user_level=hyperparams_features['user_level'],\n",
    "                                                                                logger=logger,\n",
    "#                                                            vocabulary=pickle.load(open('vocabulary_40K_all.pkl', 'rb')),\n",
    "#                                                            vocabulary=pickle.load(open('vocab_clpsych_10000.pkl', 'rb')),\n",
    "                                                              vocabulary=vocabulary_dict,\n",
    "                                                              by_subset=True,\n",
    "                                                              labelcol = 'source'\n",
    "                                                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df['subset'] = writings_df['subject'].apply(lambda s: 'test' if s in subjects_split['test'] else 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [thing, is, these, are, shown, on, irish, tv, ...\n",
       "1                                           [spot, on]\n",
       "2                                   [all, 3, of, them]\n",
       "3    [oh, man, i, love, rottweilers, they, re, awes...\n",
       "4    [another, reason, why, i, love, the, yogscast,...\n",
       "Name: tokenized_text, dtype: object"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df['tokenized_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "writings_df.text_len.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(writings_df, open('writings_df_selfharm_liwc_subsets', 'wb+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bookstore', 0),\n",
       " ('weeds', 1),\n",
       " ('matty', 2),\n",
       " ('manufacture', 3),\n",
       " ('exchanges', 4),\n",
       " ('lag', 5),\n",
       " ('huge', 6),\n",
       " ('psychopath', 7),\n",
       " ('unnecessarily', 8),\n",
       " ('melania', 9),\n",
       " ('naturally', 10),\n",
       " ('perspectives', 11),\n",
       " ('legendary', 12),\n",
       " ('barbarian', 13),\n",
       " ('fatalities', 14),\n",
       " ('related', 15),\n",
       " ('inflated', 16),\n",
       " ('landslide', 17),\n",
       " ('winter', 18),\n",
       " ('legality', 19),\n",
       " ('upstairs', 20),\n",
       " ('papa', 21),\n",
       " ('blatantly', 22),\n",
       " ('oregon', 23),\n",
       " ('offend', 24),\n",
       " ('spears', 25),\n",
       " ('boom', 26),\n",
       " ('lrt', 27),\n",
       " ('snarky', 28),\n",
       " ('shallow', 29),\n",
       " ('illustrator', 30),\n",
       " ('cups', 31),\n",
       " ('airlines', 32),\n",
       " ('l', 33),\n",
       " ('twilight', 34),\n",
       " ('espresso', 35),\n",
       " ('creek', 36),\n",
       " ('swearing', 37),\n",
       " ('abomination', 38),\n",
       " ('boosie', 39),\n",
       " ('snowman', 40),\n",
       " ('recipes', 41),\n",
       " ('wiretap', 42),\n",
       " ('fapping', 43),\n",
       " ('intoxicated', 44),\n",
       " ('phrased', 45),\n",
       " ('oppression', 46),\n",
       " ('remix', 47),\n",
       " ('sailor', 48),\n",
       " ('lewis', 49),\n",
       " ('they', 50),\n",
       " ('shin', 51),\n",
       " ('airconsole', 52),\n",
       " ('accused', 53),\n",
       " ('spike', 54),\n",
       " ('delay', 55),\n",
       " ('skeleton', 56),\n",
       " ('unfollow', 57),\n",
       " ('admired', 58),\n",
       " ('informing', 59),\n",
       " ('crazies', 60),\n",
       " ('handle', 61),\n",
       " ('warmer', 62),\n",
       " ('resembles', 63),\n",
       " ('recommendations', 64),\n",
       " ('galactic', 65),\n",
       " ('palin', 66),\n",
       " ('elses', 67),\n",
       " ('ecuador', 68),\n",
       " ('rnc', 69),\n",
       " ('hay', 70),\n",
       " ('logos', 71),\n",
       " ('performances', 72),\n",
       " ('tripping', 73),\n",
       " ('diets', 74),\n",
       " ('joshua', 75),\n",
       " ('hide', 76),\n",
       " ('all', 77),\n",
       " ('ain', 78),\n",
       " ('moustache', 79),\n",
       " ('celeb', 80),\n",
       " ('safari', 81),\n",
       " ('emotions', 82),\n",
       " ('giddy', 83),\n",
       " ('opposing', 84),\n",
       " ('guardians', 85),\n",
       " ('negotiations', 86),\n",
       " ('teased', 87),\n",
       " ('officially', 88),\n",
       " ('nom', 89),\n",
       " ('bbs', 90),\n",
       " ('gist', 91),\n",
       " ('ohhhhh', 92),\n",
       " ('nahh', 93),\n",
       " ('inception', 94),\n",
       " ('drugged', 95),\n",
       " ('historic', 96),\n",
       " ('reef', 97),\n",
       " ('paula', 98),\n",
       " ('fv', 99),\n",
       " ('interactive', 100),\n",
       " ('sap', 101),\n",
       " ('preliminary', 102),\n",
       " ('smirk', 103),\n",
       " ('gunfire', 104),\n",
       " ('prejudice', 105),\n",
       " ('crates', 106),\n",
       " ('nostalgic', 107),\n",
       " ('trivial', 108),\n",
       " ('apt', 109),\n",
       " ('lasting', 110),\n",
       " ('awesomeness', 111),\n",
       " ('calming', 112),\n",
       " ('reputation', 113),\n",
       " ('monroe', 114),\n",
       " ('walls', 115),\n",
       " ('truths', 116),\n",
       " ('illusion', 117),\n",
       " ('variability', 118),\n",
       " ('nodding', 119),\n",
       " ('ninja', 120),\n",
       " ('spirituality', 121),\n",
       " ('marketplace', 122),\n",
       " ('credible', 123),\n",
       " ('sjw', 124),\n",
       " ('nathan', 125),\n",
       " ('buzzing', 126),\n",
       " ('rockford', 127),\n",
       " ('uphill', 128),\n",
       " ('billy', 129),\n",
       " ('contaminated', 130),\n",
       " ('baths', 131),\n",
       " ('psychologically', 132),\n",
       " ('newark', 133),\n",
       " ('squirrels', 134),\n",
       " ('dropped', 135),\n",
       " ('logs', 136),\n",
       " ('functions', 137),\n",
       " ('strategy', 138),\n",
       " ('dealer', 139),\n",
       " ('grassroots', 140),\n",
       " ('reopen', 141),\n",
       " ('protocols', 142),\n",
       " ('colombian', 143),\n",
       " ('raleigh', 144),\n",
       " ('explicit', 145),\n",
       " ('exile', 146),\n",
       " ('hawkeye', 147),\n",
       " ('mention', 148),\n",
       " ('ego', 149),\n",
       " ('goin', 150),\n",
       " ('monumental', 151),\n",
       " ('eww', 152),\n",
       " ('margot', 153),\n",
       " ('thts', 154),\n",
       " ('apollo', 155),\n",
       " ('placebo', 156),\n",
       " ('cancels', 157),\n",
       " ('startups', 158),\n",
       " ('yemen', 159),\n",
       " ('mango', 160),\n",
       " ('combination', 161),\n",
       " ('locker', 162),\n",
       " ('lmaoooo', 163),\n",
       " ('penn', 164),\n",
       " ('paused', 165),\n",
       " ('fn', 166),\n",
       " ('derogatory', 167),\n",
       " ('wid', 168),\n",
       " ('magically', 169),\n",
       " ('uc', 170),\n",
       " ('going', 171),\n",
       " ('watcher', 172),\n",
       " ('coat', 173),\n",
       " ('always', 174),\n",
       " ('calculator', 175),\n",
       " ('pistons', 176),\n",
       " ('um', 177),\n",
       " ('glanced', 178),\n",
       " ('mona', 179),\n",
       " ('manor', 180),\n",
       " ('dumbass', 181),\n",
       " ('articulate', 182),\n",
       " ('sciences', 183),\n",
       " ('chomsky', 184),\n",
       " ('gloomy', 185),\n",
       " ('elaborate', 186),\n",
       " ('culturally', 187),\n",
       " ('hyper', 188),\n",
       " ('babysit', 189),\n",
       " ('tweeting', 190),\n",
       " ('shark', 191),\n",
       " ('contradiction', 192),\n",
       " ('gamble', 193),\n",
       " ('spat', 194),\n",
       " ('cartel', 195),\n",
       " ('prefers', 196),\n",
       " ('annoy', 197),\n",
       " ('devos', 198),\n",
       " ('owl', 199),\n",
       " ('tldr', 200),\n",
       " ('nationalists', 201),\n",
       " ('octane', 202),\n",
       " ('traffic', 203),\n",
       " ('mankind', 204),\n",
       " ('speculation', 205),\n",
       " ('primaries', 206),\n",
       " ('gullible', 207),\n",
       " ('compulsive', 208),\n",
       " ('supporters', 209),\n",
       " ('peeled', 210),\n",
       " ('suggests', 211),\n",
       " ('timeout', 212),\n",
       " ('isis', 213),\n",
       " ('mac', 214),\n",
       " ('coherent', 215),\n",
       " ('mall', 216),\n",
       " ('mumbai', 217),\n",
       " ('flee', 218),\n",
       " ('attitudes', 219),\n",
       " ('silk', 220),\n",
       " ('proposing', 221),\n",
       " ('only', 222),\n",
       " ('quad', 223),\n",
       " ('tourism', 224),\n",
       " ('shuttle', 225),\n",
       " ('selection', 226),\n",
       " ('clarity', 227),\n",
       " ('ahhhh', 228),\n",
       " ('donna', 229),\n",
       " ('phrase', 230),\n",
       " ('weaken', 231),\n",
       " ('famu', 232),\n",
       " ('command', 233),\n",
       " ('skate', 234),\n",
       " ('refrain', 235),\n",
       " ('sideline', 236),\n",
       " ('sb', 237),\n",
       " ('roi', 238),\n",
       " ('sobs', 239),\n",
       " ('sighting', 240),\n",
       " ('calcium', 241),\n",
       " ('ounce', 242),\n",
       " ('sticker', 243),\n",
       " ('futuristic', 244),\n",
       " ('antibiotics', 245),\n",
       " ('exiting', 246),\n",
       " ('booth', 247),\n",
       " ('smith', 248),\n",
       " ('settlers', 249),\n",
       " ('completion', 250),\n",
       " ('terran', 251),\n",
       " ('borders', 252),\n",
       " ('fuck', 253),\n",
       " ('childbirth', 254),\n",
       " ('function', 255),\n",
       " ('permanent', 256),\n",
       " ('spit', 257),\n",
       " ('kirby', 258),\n",
       " ('kann', 259),\n",
       " ('scorpio', 260),\n",
       " ('employers', 261),\n",
       " ('metallica', 262),\n",
       " ('flips', 263),\n",
       " ('idols', 264),\n",
       " ('thus', 265),\n",
       " ('execution', 266),\n",
       " ('wip', 267),\n",
       " ('streaks', 268),\n",
       " ('bieber', 269),\n",
       " ('strengths', 270),\n",
       " ('lone', 271),\n",
       " ('funk', 272),\n",
       " ('hiphop', 273),\n",
       " ('preseason', 274),\n",
       " ('endeavor', 275),\n",
       " ('tropical', 276),\n",
       " ('applaud', 277),\n",
       " ('essentials', 278),\n",
       " ('claiming', 279),\n",
       " ('appearances', 280),\n",
       " ('coworker', 281),\n",
       " ('viral', 282),\n",
       " ('writing', 283),\n",
       " ('average', 284),\n",
       " ('rejection', 285),\n",
       " ('betting', 286),\n",
       " ('forever', 287),\n",
       " ('unsubscribe', 288),\n",
       " ('homosexuals', 289),\n",
       " ('wines', 290),\n",
       " ('mansion', 291),\n",
       " ('junction', 292),\n",
       " ('flirt', 293),\n",
       " ('drug', 294),\n",
       " ('relive', 295),\n",
       " ('warriors', 296),\n",
       " ('bubbles', 297),\n",
       " ('lamps', 298),\n",
       " ('report', 299),\n",
       " ('insulted', 300),\n",
       " ('brewery', 301),\n",
       " ('chewing', 302),\n",
       " ('instability', 303),\n",
       " ('insidious', 304),\n",
       " ('overly', 305),\n",
       " ('ortiz', 306),\n",
       " ('leggings', 307),\n",
       " ('showman', 308),\n",
       " ('hes', 309),\n",
       " ('galleries', 310),\n",
       " ('job', 311),\n",
       " ('uhhhh', 312),\n",
       " ('flavia', 313),\n",
       " ('ummmm', 314),\n",
       " ('fir', 315),\n",
       " ('norse', 316),\n",
       " ('nr', 317),\n",
       " ('stalks', 318),\n",
       " ('diamond', 319),\n",
       " ('shortage', 320),\n",
       " ('simmons', 321),\n",
       " ('jack', 322),\n",
       " ('kya', 323),\n",
       " ('labs', 324),\n",
       " ('culinary', 325),\n",
       " ('noobs', 326),\n",
       " ('css', 327),\n",
       " ('fob', 328),\n",
       " ('messaged', 329),\n",
       " ('projectile', 330),\n",
       " ('raider', 331),\n",
       " ('spawning', 332),\n",
       " ('nurses', 333),\n",
       " ('showing', 334),\n",
       " ('so', 335),\n",
       " ('assume', 336),\n",
       " ('battalion', 337),\n",
       " ('investigating', 338),\n",
       " ('champs', 339),\n",
       " ('grabbing', 340),\n",
       " ('clerk', 341),\n",
       " ('stabs', 342),\n",
       " ('homebrew', 343),\n",
       " ('drag', 344),\n",
       " ('staggering', 345),\n",
       " ('flipping', 346),\n",
       " ('crossover', 347),\n",
       " ('swore', 348),\n",
       " ('audi', 349),\n",
       " ('afterlife', 350),\n",
       " ('teddy', 351),\n",
       " ('gunshot', 352),\n",
       " ('assets', 353),\n",
       " ('ethics', 354),\n",
       " ('consumption', 355),\n",
       " ('offence', 356),\n",
       " ('refreshed', 357),\n",
       " ('mahomies', 358),\n",
       " ('sensible', 359),\n",
       " ('separating', 360),\n",
       " ('pete', 361),\n",
       " ('friction', 362),\n",
       " ('itch', 363),\n",
       " ('square', 364),\n",
       " ('principles', 365),\n",
       " ('regions', 366),\n",
       " ('ughh', 367),\n",
       " ('scotland', 368),\n",
       " ('investigated', 369),\n",
       " ('meet', 370),\n",
       " ('buildings', 371),\n",
       " ('fig', 372),\n",
       " ('absurdly', 373),\n",
       " ('bjj', 374),\n",
       " ('knitting', 375),\n",
       " ('acclaimed', 376),\n",
       " ('presents', 377),\n",
       " ('barbie', 378),\n",
       " ('cl', 379),\n",
       " ('educated', 380),\n",
       " ('brace', 381),\n",
       " ('handsome', 382),\n",
       " ('projection', 383),\n",
       " ('channing', 384),\n",
       " ('orientation', 385),\n",
       " ('psbattle', 386),\n",
       " ('spent', 387),\n",
       " ('communicate', 388),\n",
       " ('washington', 389),\n",
       " ('entertaining', 390),\n",
       " ('patrick', 391),\n",
       " ('plz', 392),\n",
       " ('trading', 393),\n",
       " ('progressively', 394),\n",
       " ('division', 395),\n",
       " ('approximately', 396),\n",
       " ('dame', 397),\n",
       " ('widget', 398),\n",
       " ('todays', 399),\n",
       " ('muster', 400),\n",
       " ('structures', 401),\n",
       " ('honolulu', 402),\n",
       " ('wan', 403),\n",
       " ('evaluated', 404),\n",
       " ('annoying', 405),\n",
       " ('olympics', 406),\n",
       " ('llc', 407),\n",
       " ('alert', 408),\n",
       " ('demi', 409),\n",
       " ('insiders', 410),\n",
       " ('mets', 411),\n",
       " ('ringtone', 412),\n",
       " ('fov', 413),\n",
       " ('esports', 414),\n",
       " ('arab', 415),\n",
       " ('grove', 416),\n",
       " ('bankrupt', 417),\n",
       " ('dominant', 418),\n",
       " ('activate', 419),\n",
       " ('fines', 420),\n",
       " ('deprived', 421),\n",
       " ('prophets', 422),\n",
       " ('symphony', 423),\n",
       " ('pneumonia', 424),\n",
       " ('reproduce', 425),\n",
       " ('startup', 426),\n",
       " ('recovered', 427),\n",
       " ('thesis', 428),\n",
       " ('admits', 429),\n",
       " ('yesss', 430),\n",
       " ('cereal', 431),\n",
       " ('righteous', 432),\n",
       " ('wait', 433),\n",
       " ('subscribing', 434),\n",
       " ('detect', 435),\n",
       " ('robe', 436),\n",
       " ('ina', 437),\n",
       " ('latch', 438),\n",
       " ('admiral', 439),\n",
       " ('shxt', 440),\n",
       " ('clusterfuck', 441),\n",
       " ('ms', 442),\n",
       " ('tank', 443),\n",
       " ('doomsday', 444),\n",
       " ('hater', 445),\n",
       " ('suggesting', 446),\n",
       " ('morphine', 447),\n",
       " ('xl', 448),\n",
       " ('js', 449),\n",
       " ('stickers', 450),\n",
       " ('urgency', 451),\n",
       " ('ocean', 452),\n",
       " ('syrian', 453),\n",
       " ('citizen', 454),\n",
       " ('collusion', 455),\n",
       " ('jonas', 456),\n",
       " ('introverted', 457),\n",
       " ('blaze', 458),\n",
       " ('devastated', 459),\n",
       " ('staffers', 460),\n",
       " ('histories', 461),\n",
       " ('ft', 462),\n",
       " ('endo', 463),\n",
       " ('objects', 464),\n",
       " ('cia', 465),\n",
       " ('elite', 466),\n",
       " ('loner', 467),\n",
       " ('styling', 468),\n",
       " ('bladder', 469),\n",
       " ('particles', 470),\n",
       " ('deserve', 471),\n",
       " ('martinez', 472),\n",
       " ('friendzone', 473),\n",
       " ('lawsuits', 474),\n",
       " ('athens', 475),\n",
       " ('gaping', 476),\n",
       " ('carriage', 477),\n",
       " ('advert', 478),\n",
       " ('lucifer', 479),\n",
       " ('triggers', 480),\n",
       " ('emailing', 481),\n",
       " ('paramedics', 482),\n",
       " ('builds', 483),\n",
       " ('scientific', 484),\n",
       " ('studs', 485),\n",
       " ('oo', 486),\n",
       " ('adolescents', 487),\n",
       " ('stir', 488),\n",
       " ('exceptions', 489),\n",
       " ('qualifying', 490),\n",
       " ('expresses', 491),\n",
       " ('creeped', 492),\n",
       " ('seize', 493),\n",
       " ('urged', 494),\n",
       " ('inspection', 495),\n",
       " ('spectre', 496),\n",
       " ('impairment', 497),\n",
       " ('cow', 498),\n",
       " ('wonderful', 499),\n",
       " ('mtv', 500),\n",
       " ('clause', 501),\n",
       " ('pep', 502),\n",
       " ('shortcuts', 503),\n",
       " ('tummy', 504),\n",
       " ('halfway', 505),\n",
       " ('spirited', 506),\n",
       " ('spinal', 507),\n",
       " ('slammed', 508),\n",
       " ('exclusive', 509),\n",
       " ('satellite', 510),\n",
       " ('declares', 511),\n",
       " ('rails', 512),\n",
       " ('dopamine', 513),\n",
       " ('ranked', 514),\n",
       " ('clears', 515),\n",
       " ('heidi', 516),\n",
       " ('filipino', 517),\n",
       " ('stalker', 518),\n",
       " ('tumors', 519),\n",
       " ('f', 520),\n",
       " ('letters', 521),\n",
       " ('herpes', 522),\n",
       " ('frightened', 523),\n",
       " ('twitition', 524),\n",
       " ('deposits', 525),\n",
       " ('idle', 526),\n",
       " ('fudge', 527),\n",
       " ('debated', 528),\n",
       " ('would', 529),\n",
       " ('alchemist', 530),\n",
       " ('gamergate', 531),\n",
       " ('bein', 532),\n",
       " ('leadership', 533),\n",
       " ('colors', 534),\n",
       " ('kr', 535),\n",
       " ('bunch', 536),\n",
       " ('owe', 537),\n",
       " ('exp', 538),\n",
       " ('scenes', 539),\n",
       " ('constantly', 540),\n",
       " ('thousands', 541),\n",
       " ('pricks', 542),\n",
       " ('university', 543),\n",
       " ('meditation', 544),\n",
       " ('pleasant', 545),\n",
       " ('observing', 546),\n",
       " ('obligatory', 547),\n",
       " ('ughhh', 548),\n",
       " ('stinks', 549),\n",
       " ('gifting', 550),\n",
       " ('loser', 551),\n",
       " ('snuggle', 552),\n",
       " ('filling', 553),\n",
       " ('straw', 554),\n",
       " ('seminar', 555),\n",
       " ('threatens', 556),\n",
       " ('perk', 557),\n",
       " ('brave', 558),\n",
       " ('sleeve', 559),\n",
       " ('pauses', 560),\n",
       " ('catcher', 561),\n",
       " ('poised', 562),\n",
       " ('substantial', 563),\n",
       " ('ib', 564),\n",
       " ('hiiii', 565),\n",
       " ('slim', 566),\n",
       " ('clapper', 567),\n",
       " ('wen', 568),\n",
       " ('bench', 569),\n",
       " ('dirk', 570),\n",
       " ('plow', 571),\n",
       " ('referred', 572),\n",
       " ('progressing', 573),\n",
       " ('vmas', 574),\n",
       " ('categories', 575),\n",
       " ('strategies', 576),\n",
       " ('rose', 577),\n",
       " ('climb', 578),\n",
       " ('hooking', 579),\n",
       " ('png', 580),\n",
       " ('whoop', 581),\n",
       " ('pillow', 582),\n",
       " ('contagious', 583),\n",
       " ('macdonald', 584),\n",
       " ('slip', 585),\n",
       " ('wesley', 586),\n",
       " ('cracking', 587),\n",
       " ('narration', 588),\n",
       " ('heals', 589),\n",
       " ('knot', 590),\n",
       " ('hate', 591),\n",
       " ('inhale', 592),\n",
       " ('movement', 593),\n",
       " ('hobby', 594),\n",
       " ('lectures', 595),\n",
       " ('mutually', 596),\n",
       " ('rocket', 597),\n",
       " ('interact', 598),\n",
       " ('grit', 599),\n",
       " ('deterrent', 600),\n",
       " ('congregation', 601),\n",
       " ('warp', 602),\n",
       " ('fire', 603),\n",
       " ('introduction', 604),\n",
       " ('feelin', 605),\n",
       " ('fragrance', 606),\n",
       " ('aladdin', 607),\n",
       " ('defending', 608),\n",
       " ('processors', 609),\n",
       " ('uniforms', 610),\n",
       " ('salon', 611),\n",
       " ('considerate', 612),\n",
       " ('network', 613),\n",
       " ('discontinued', 614),\n",
       " ('kidneys', 615),\n",
       " ('register', 616),\n",
       " ('such', 617),\n",
       " ('podium', 618),\n",
       " ('koji', 619),\n",
       " ('hannah', 620),\n",
       " ('celestial', 621),\n",
       " ('approves', 622),\n",
       " ('shown', 623),\n",
       " ('dumbledore', 624),\n",
       " ('rear', 625),\n",
       " ('wrap', 626),\n",
       " ('startled', 627),\n",
       " ('muslim', 628),\n",
       " ('beauties', 629),\n",
       " ('need', 630),\n",
       " ('erect', 631),\n",
       " ('weirder', 632),\n",
       " ('novice', 633),\n",
       " ('speaker', 634),\n",
       " ('tailor', 635),\n",
       " ('tying', 636),\n",
       " ('disney', 637),\n",
       " ('candidates', 638),\n",
       " ('neo', 639),\n",
       " ('nas', 640),\n",
       " ('compilation', 641),\n",
       " ('diaper', 642),\n",
       " ('unanswered', 643),\n",
       " ('doughnuts', 644),\n",
       " ('eff', 645),\n",
       " ('prizes', 646),\n",
       " ('storm', 647),\n",
       " ('covering', 648),\n",
       " ('alter', 649),\n",
       " ('college', 650),\n",
       " ('tooth', 651),\n",
       " ('gotten', 652),\n",
       " ('gel', 653),\n",
       " ('agriculture', 654),\n",
       " ('models', 655),\n",
       " ('cyborg', 656),\n",
       " ('siberian', 657),\n",
       " ('anticipating', 658),\n",
       " ('reversed', 659),\n",
       " ('hahahahaha', 660),\n",
       " ('beckham', 661),\n",
       " ('midlands', 662),\n",
       " ('snap', 663),\n",
       " ('proverb', 664),\n",
       " ('phrasing', 665),\n",
       " ('purse', 666),\n",
       " ('remnant', 667),\n",
       " ('tame', 668),\n",
       " ('revoked', 669),\n",
       " ('authentic', 670),\n",
       " ('tug', 671),\n",
       " ('relegated', 672),\n",
       " ('inventor', 673),\n",
       " ('resolved', 674),\n",
       " ('admission', 675),\n",
       " ('pi', 676),\n",
       " ('shivers', 677),\n",
       " ('rcmp', 678),\n",
       " ('toward', 679),\n",
       " ('einen', 680),\n",
       " ('handgun', 681),\n",
       " ('injected', 682),\n",
       " ('supposedly', 683),\n",
       " ('fibre', 684),\n",
       " ('tibetan', 685),\n",
       " ('smash', 686),\n",
       " ('clashes', 687),\n",
       " ('beforehand', 688),\n",
       " ('patriotism', 689),\n",
       " ('submit', 690),\n",
       " ('concentrations', 691),\n",
       " ('person', 692),\n",
       " ('attain', 693),\n",
       " ('upgrade', 694),\n",
       " ('exist', 695),\n",
       " ('poland', 696),\n",
       " ('overwhelmingly', 697),\n",
       " ('imagined', 698),\n",
       " ('folds', 699),\n",
       " ('vo', 700),\n",
       " ('fentanyl', 701),\n",
       " ('purposely', 702),\n",
       " ('whistling', 703),\n",
       " ('agreement', 704),\n",
       " ('international', 705),\n",
       " ('skeptical', 706),\n",
       " ('pasadena', 707),\n",
       " ('mohammed', 708),\n",
       " ('swarm', 709),\n",
       " ('hanks', 710),\n",
       " ('fluid', 711),\n",
       " ('vigilante', 712),\n",
       " ('waves', 713),\n",
       " ('hall', 714),\n",
       " ('spur', 715),\n",
       " ('aswell', 716),\n",
       " ('plasma', 717),\n",
       " ('abnormal', 718),\n",
       " ('warmed', 719),\n",
       " ('grooming', 720),\n",
       " ('slowdown', 721),\n",
       " ('factory', 722),\n",
       " ('malibu', 723),\n",
       " ('tin', 724),\n",
       " ('burrito', 725),\n",
       " ('lotion', 726),\n",
       " ('lul', 727),\n",
       " ('glass', 728),\n",
       " ('moss', 729),\n",
       " ('sect', 730),\n",
       " ('charges', 731),\n",
       " ('scheduling', 732),\n",
       " ('boylan', 733),\n",
       " ('higher', 734),\n",
       " ('mystery', 735),\n",
       " ('change', 736),\n",
       " ('attach', 737),\n",
       " ('influential', 738),\n",
       " ('aoe', 739),\n",
       " ('particle', 740),\n",
       " ('genitals', 741),\n",
       " ('washes', 742),\n",
       " ('sensational', 743),\n",
       " ('tsa', 744),\n",
       " ('checkpoint', 745),\n",
       " ('shade', 746),\n",
       " ('slay', 747),\n",
       " ('account', 748),\n",
       " ('possession', 749),\n",
       " ('booking', 750),\n",
       " ('kaya', 751),\n",
       " ('theater', 752),\n",
       " ('kl', 753),\n",
       " ('bully', 754),\n",
       " ('uncle', 755),\n",
       " ('salts', 756),\n",
       " ('existed', 757),\n",
       " ('colorado', 758),\n",
       " ('suspect', 759),\n",
       " ('watchin', 760),\n",
       " ('marry', 761),\n",
       " ('wrath', 762),\n",
       " ('coz', 763),\n",
       " ('tags', 764),\n",
       " ('interviewing', 765),\n",
       " ('inb', 766),\n",
       " ('scratches', 767),\n",
       " ('couples', 768),\n",
       " ('sexist', 769),\n",
       " ('medical', 770),\n",
       " ('dominos', 771),\n",
       " ('vermont', 772),\n",
       " ('tmi', 773),\n",
       " ('licence', 774),\n",
       " ('replies', 775),\n",
       " ('inflicted', 776),\n",
       " ('broadway', 777),\n",
       " ('golem', 778),\n",
       " ('bring', 779),\n",
       " ('cognitive', 780),\n",
       " ('client', 781),\n",
       " ('destroys', 782),\n",
       " ('duped', 783),\n",
       " ('nailed', 784),\n",
       " ('cliches', 785),\n",
       " ('block', 786),\n",
       " ('dipping', 787),\n",
       " ('compile', 788),\n",
       " ('jokes', 789),\n",
       " ('comprised', 790),\n",
       " ('blackness', 791),\n",
       " ('ranting', 792),\n",
       " ('month', 793),\n",
       " ('envious', 794),\n",
       " ('closure', 795),\n",
       " ('shrugged', 796),\n",
       " ('largest', 797),\n",
       " ('furry', 798),\n",
       " ('potential', 799),\n",
       " ('half', 800),\n",
       " ('trippy', 801),\n",
       " ('defensive', 802),\n",
       " ('invited', 803),\n",
       " ('los', 804),\n",
       " ('rafael', 805),\n",
       " ('revolver', 806),\n",
       " ('nauseous', 807),\n",
       " ('evasion', 808),\n",
       " ('playstation', 809),\n",
       " ('bruhh', 810),\n",
       " ('program', 811),\n",
       " ('jenner', 812),\n",
       " ('speaking', 813),\n",
       " ('butter', 814),\n",
       " ('nk', 815),\n",
       " ('lush', 816),\n",
       " ('prosecute', 817),\n",
       " ('offender', 818),\n",
       " ('menace', 819),\n",
       " ('callin', 820),\n",
       " ('demonic', 821),\n",
       " ('charisma', 822),\n",
       " ('hybrid', 823),\n",
       " ('gyms', 824),\n",
       " ('blasio', 825),\n",
       " ('introduce', 826),\n",
       " ('obvious', 827),\n",
       " ('asimov', 828),\n",
       " ('sesh', 829),\n",
       " ('bath', 830),\n",
       " ('beast', 831),\n",
       " ('spring', 832),\n",
       " ('glitch', 833),\n",
       " ('fabric', 834),\n",
       " ('blessed', 835),\n",
       " ('untrue', 836),\n",
       " ('disagreed', 837),\n",
       " ('filing', 838),\n",
       " ('yveltal', 839),\n",
       " ('gtm', 840),\n",
       " ('cunning', 841),\n",
       " ('descent', 842),\n",
       " ('heater', 843),\n",
       " ('observe', 844),\n",
       " ('frankie', 845),\n",
       " ('womb', 846),\n",
       " ('remain', 847),\n",
       " ('caviar', 848),\n",
       " ('multiple', 849),\n",
       " ('fixes', 850),\n",
       " ('listens', 851),\n",
       " ('cooler', 852),\n",
       " ('wolverine', 853),\n",
       " ('complete', 854),\n",
       " ('ae', 855),\n",
       " ('donate', 856),\n",
       " ('detroit', 857),\n",
       " ('muttered', 858),\n",
       " ('puttin', 859),\n",
       " ('paragraphs', 860),\n",
       " ('wards', 861),\n",
       " ('cuties', 862),\n",
       " ('anecdote', 863),\n",
       " ('fund', 864),\n",
       " ('glee', 865),\n",
       " ('fsa', 866),\n",
       " ('intruder', 867),\n",
       " ('highlight', 868),\n",
       " ('emotion', 869),\n",
       " ('graham', 870),\n",
       " ('dps', 871),\n",
       " ('submissions', 872),\n",
       " ('hex', 873),\n",
       " ('minimum', 874),\n",
       " ('remained', 875),\n",
       " ('chic', 876),\n",
       " ('crocs', 877),\n",
       " ('fiasco', 878),\n",
       " ('annihilation', 879),\n",
       " ('gaze', 880),\n",
       " ('stated', 881),\n",
       " ('philosophy', 882),\n",
       " ('keef', 883),\n",
       " ('quran', 884),\n",
       " ('sunscreen', 885),\n",
       " ('literal', 886),\n",
       " ('reflection', 887),\n",
       " ('intern', 888),\n",
       " ('excuses', 889),\n",
       " ('fascinating', 890),\n",
       " ('wasps', 891),\n",
       " ('despicable', 892),\n",
       " ('electricity', 893),\n",
       " ('scrap', 894),\n",
       " ('feminine', 895),\n",
       " ('tablet', 896),\n",
       " ('tpp', 897),\n",
       " ('fad', 898),\n",
       " ('sins', 899),\n",
       " ('stinky', 900),\n",
       " ('carving', 901),\n",
       " ('musicians', 902),\n",
       " ('subtitle', 903),\n",
       " ('monuments', 904),\n",
       " ('repealing', 905),\n",
       " ('gegen', 906),\n",
       " ('testament', 907),\n",
       " ('create', 908),\n",
       " ('sending', 909),\n",
       " ('peter', 910),\n",
       " ('stalled', 911),\n",
       " ('principle', 912),\n",
       " ('pcpartpicker', 913),\n",
       " ('hooker', 914),\n",
       " ('ffs', 915),\n",
       " ('syndrome', 916),\n",
       " ('situation', 917),\n",
       " ('dating', 918),\n",
       " ('relieved', 919),\n",
       " ('cynical', 920),\n",
       " ('bitterness', 921),\n",
       " ('pinpoint', 922),\n",
       " ('stems', 923),\n",
       " ('intellectuals', 924),\n",
       " ('evolutionary', 925),\n",
       " ('questionnaire', 926),\n",
       " ('filthy', 927),\n",
       " ('disaster', 928),\n",
       " ('crane', 929),\n",
       " ('barney', 930),\n",
       " ('workout', 931),\n",
       " ('hits', 932),\n",
       " ('perpetrators', 933),\n",
       " ('dice', 934),\n",
       " ('reports', 935),\n",
       " ('cooked', 936),\n",
       " ('jet', 937),\n",
       " ('audrey', 938),\n",
       " ('superbiiz', 939),\n",
       " ('measles', 940),\n",
       " ('icky', 941),\n",
       " ('embarrassing', 942),\n",
       " ('bandwidth', 943),\n",
       " ('driverless', 944),\n",
       " ('shortcomings', 945),\n",
       " ('announcer', 946),\n",
       " ('cals', 947),\n",
       " ('bonus', 948),\n",
       " ('widely', 949),\n",
       " ('phillips', 950),\n",
       " ('beings', 951),\n",
       " ('rey', 952),\n",
       " ('undeniable', 953),\n",
       " ('egyptian', 954),\n",
       " ('moving', 955),\n",
       " ('bug', 956),\n",
       " ('bell', 957),\n",
       " ('soooooooo', 958),\n",
       " ('ava', 959),\n",
       " ('accepted', 960),\n",
       " ('lowe', 961),\n",
       " ('berkeley', 962),\n",
       " ('who', 963),\n",
       " ('provided', 964),\n",
       " ('noting', 965),\n",
       " ('swords', 966),\n",
       " ('chewed', 967),\n",
       " ('hathaway', 968),\n",
       " ('comment', 969),\n",
       " ('tickle', 970),\n",
       " ('implanted', 971),\n",
       " ('alec', 972),\n",
       " ('fu', 973),\n",
       " ('aren', 974),\n",
       " ('turning', 975),\n",
       " ('commodity', 976),\n",
       " ('cal', 977),\n",
       " ('savage', 978),\n",
       " ('feast', 979),\n",
       " ('fingernails', 980),\n",
       " ('referenced', 981),\n",
       " ('narcissistic', 982),\n",
       " ('quota', 983),\n",
       " ('heated', 984),\n",
       " ('slate', 985),\n",
       " ('reactionary', 986),\n",
       " ('washed', 987),\n",
       " ('spared', 988),\n",
       " ('jpg', 989),\n",
       " ('encountered', 990),\n",
       " ('elementary', 991),\n",
       " ('distorted', 992),\n",
       " ('algae', 993),\n",
       " ('ugh', 994),\n",
       " ('draws', 995),\n",
       " ('cst', 996),\n",
       " ('railroad', 997),\n",
       " ('transforming', 998),\n",
       " ('unfair', 999),\n",
       " ...]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vocabulary.items(), key=lambda t:t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['subject41530000', 'subject30040000', 'subject36390000', 'subject8730000', 'subject86170000', 'subject10990000', 'subject82570000', 'subject49450000', 'subject14240000', 'subject94640000', 'subject82590000', 'subject81560000', 'subject4760000', 'subject77590000', 'subject68380000', 'subject85440000', 'subject5130000', 'subject41320000', 'subject51940000', 'subject69220000', 'subject44790000', 'subject79000000', 'subject30660000', 'subject48730000', 'subject22820000', 'subject57700000', 'subject44070000', 'subject83010000', 'subject12480000', 'subject80670000', 'subject10510000', 'subject17620000', 'subject33470000', 'subject77850000', 'subject21840000', 'subject660000', 'subject32370000', 'subject17650000', 'subject2510000', 'subject66640000', 'subject46300000', 'subject82440000', 'subject97160000', 'subject97700000', 'subject4980000', 'subject48020000', 'subject47150000', 'subject4840000', 'subject56040000', 'subject11290000', 'subject89780000', 'subject28920000', 'subject85410000', 'subject33910000', 'subject36760000', 'subject74190000', 'subject85250000', 'subject75690000', 'subject47330000', 'subject98390000', 'subject48700000', 'subject93600000', 'subject2500000', 'subject6220000', 'subject38060000', 'subject52880000', 'subject21250000', 'subject50560000', 'subject99470000', 'subject62670000', 'subject49110000', 'subject17240000', 'subject5720000', 'subject85920000', 'subject49240000', 'subject76950000', 'subject40710000', 'subject71450000', 'subject97280000', 'subject55290000', 'subject64250000', 'subject11750000', 'subject84570000', 'subject18170000', 'subject27620000', 'subject16150000', 'subject15260000', 'subject79420000', 'subject82680000', 'subject96180000', 'subject3770000', 'subject59900000', 'subject9710000', 'subject76230000', 'subject94120000', 'subject96310000', 'subject13840000', 'subject23700000', 'subject85560000', 'subject92240000', 'subject2230000', 'subject61950000', 'subject51640000', 'subject48870000', 'subject88710000', 'subject91800000', 'subject12090000', 'subject71430000', 'subject99710000', 'subject55710000', 'subject99120000', 'subject80640000', 'subject32600000', 'subject57830000', 'subject24720000', 'subject72470000', 'subject26390000', 'subject90000000', 'subject65770000', 'subject85690000', 'subject23680000', 'subject51870000', 'subject20490000', 'subject95350000', 'subject68710000', 'subject58440000', 'subject56000000', 'subject1570000', 'subject78760000', 'subject86670000', 'subject83390000', 'subject57590000', 'subject81030000', 'subject59070000', 'subject16910000', 'subject89410000', 'subject76810000', 'subject68410000', 'subject97300000', 'subject60510000', 'subject89680000', 'subject85800000', 'subject56930000', 'subject27990000', 'subject52190000', 'subject71590000', 'subject33530000', 'subject77780000', 'subject14390000', 'subject27490000', 'subject73640000', 'subject41670000', 'subject79620000', 'subject31210000', 'subject85670000', 'subject75190000', 'subject20480000', 'subject90920000', 'subject51030000', 'subject45430000', 'subject95430000', 'subject6570000', 'subject24460000', 'subject88940000', 'subject17900000', 'subject82510000', 'subject42410000', 'subject63660000', 'subject30930000', 'subject18010000', 'subject37930000', 'subject21280000', 'subject54230000', 'subject98150000', 'subject59120000', 'subject96210000', 'subject59550000', 'subject61190000', 'subject65530000', 'subject37630000', 'subject3790000', 'subject10970000', 'subject67010000', 'subject86240000', 'subject31790000', 'subject54100000', 'subject91810000', 'subject59540000', 'subject32940000', 'subject78620000', 'subject71150000', 'subject16320000', 'subject11780000', 'subject42440000', 'subject78510000', 'subject25020000', 'subject20220000', 'subject23380000', 'subject7780000', 'subject38420000', 'subject8140000', 'subject50510000', 'subject71720000', 'subject44570000', 'subject67880000', 'subject74070000', 'subject53020000', 'subject42570000', 'subject81810000', 'subject33040000', 'subject56630000', 'subject10800000', 'subject74050000', 'subject61380000', 'subject90160000', 'subject29560000', 'subject98910000', 'subject12220000', 'subject31810000', 'subject64700000', 'subject69420000', 'subject29100000', 'subject30370000', 'subject1180000', 'subject31410000', 'subject56910000', 'subject86680000', 'subject73360000', 'subject34240000', 'subject61020000', 'subject45500000', 'subject12560000', 'subject32430000', 'subject18530000', 'subject62400000', 'subject88640000', 'subject38650000', 'subject82880000', 'subject11980000', 'subject71680000', 'subject98250000', 'subject40000', 'subject47440000', 'subject12270000', 'subject78630000', 'subject83970000', 'subject42890000', 'subject6630000', 'subject60700000', 'subject41300000', 'subject39670000', 'subject10260000', 'subject51930000', 'subject44030000', 'subject49690000', 'subject78290000', 'subject54970000', 'subject45120000', 'subject27420000', 'subject44780000', 'subject7490000', 'subject25410000', 'subject96870000', 'subject2070000', 'subject25420000', 'subject59490000', 'subject77190000', 'subject41150000', 'subject12470000', 'subject47350000', 'subject62980000', 'subject92700000', 'subject98880000', 'subject15780000', 'subject3890000', 'subject67310000', 'subject47050000', 'subject29190000', 'subject8010000', 'subject75720000', 'subject84520000', 'subject9950000', 'subject39500000', 'subject91870000', 'subject36290000', 'subject73130000', 'subject71350000', 'subject40020000', 'subject92980000', 'subject83290000', 'subject15040000', 'subject13970000', 'subject1560000', 'subject68310000', 'subject23840000', 'subject49950000', 'subject15870000', 'subject40230000', 'subject70300000', 'subject9130000', 'subject57970000', 'subject31630000', 'subject42080000', 'subject88550000', 'subject93400000', 'subject74080000', 'subject86780000', 'subject60370000', 'subject27020000', 'subject23430000', 'subject48300000', 'subject1240000', 'subject34810000', 'subject34580000', 'subject67050000', 'subject15750000', 'subject41960000', 'subject8500000', 'subject10000000', 'subject81980000', 'subject20560000', 'subject39310000', 'subject18280000', 'subject8486', 'test_subject1798', 'test_subject8391', 'subject95040000', 'subject26690000', 'test_subject436', 'subject42870000', 'test_subject8493', 'subject16580000', 'subject78100000', 'test_subject1376', 'test_subject570', 'subject81200000', 'subject73770000', 'subject82410000', 'subject46400000', 'subject68420000', 'subject37150000', 'test_subject9840', 'subject26600000', 'subject25740000', 'test_subject9446', 'subject39820000', 'subject56660000', 'test_subject4650', 'subject35220000', 'test_subject166', 'subject84960000', 'subject39870000', 'subject51700000', 'subject91390000', 'test_subject6884', 'subject51800000', 'subject83990000', 'subject94420000', 'test_subject315', 'subject3076', 'test_subject5525', 'subject55540000', 'test_subject7829', 'subject32930000', 'subject46160000', 'subject7570000', 'test_subject8880', 'subject46350000', 'test_subject1452', 'test_subject306', 'subject29050000', 'test_subject7934', 'subject61920000', 'train_subject1201', 'subject81190000', 'subject31310000', 'subject82780000', 'subject1417', 'subject1360000', 'test_subject3928', 'subject17390000', 'train_subject6150', 'subject2450000', 'subject95720000', 'subject57710000', 'subject47530000', 'subject5730000', 'subject63530000', 'subject96520000', 'train_subject9115', 'subject63710000', 'subject49500000', 'subject62920000', 'test_subject8078', 'subject5160000', 'subject1577', 'subject3859', 'test_subject3802', 'subject3763', 'subject81310000', 'test_subject8662', 'subject25300000', 'test_subject8777', 'subject99320000', 'subject50740000', 'test_subject7146', 'subject40480000', 'subject89250000', 'subject508', 'train_subject2440', 'subject62750000', 'test_subject3592', 'test_subject9112', 'subject79820000', 'test_subject4892', 'subject9589', 'train_subject5080', 'subject51560000', 'subject97960000', 'test_subject773', 'subject39210000', 'subject78260000', 'subject5838', 'test_subject4276', 'subject896', 'subject467', 'test_subject1139', 'test_subject5054', 'subject9419', 'subject79370000', 'subject82660000', 'test_subject5859', 'subject7778', 'train_subject8771', 'subject46250000', 'test_subject7503', 'test_subject5034', 'train_subject2037', 'subject75480000', 'subject1397', 'subject8986', 'subject95870000', 'subject87560000', 'train_subject4359', 'subject66450000', 'subject81450000', 'subject90130000', 'test_subject4671', 'subject98320000', 'subject77490000', 'subject15030000', 'subject5220', 'subject86530000', 'train_subject1361', 'subject78900000', 'train_subject7930', 'subject85880000', 'subject1369', 'test_subject8969', 'test_subject2454', 'subject98560000', 'test_subject201', 'subject75730000', 'test_subject6469', 'subject3813', 'subject7540000', 'train_subject6146', 'subject10150000', 'test_subject9098', 'subject13350000', 'test_subject265', 'subject5759', 'subject99290000', 'subject91320000', 'subject3313', 'subject26350000', 'subject4551', 'subject61860000', 'subject33070000', 'subject12450000', 'subject32680000', 'subject50990000', 'subject8338', 'train_subject3670', 'subject70970000', 'subject70800000', 'subject7320', 'subject47970000', 'subject18130000', 'test_subject5688', 'subject16100000', 'test_subject7776', 'subject78910000', 'subject13950000', 'subject83350000', 'test_subject6945', 'test_subject6878', 'subject9003', 'subject51360000', 'subject49410000', 'subject83070000', 'subject10010000', 'test_subject8528', 'subject83730000', 'subject11210000', 'subject84770000', 'test_subject1572', 'subject5984', 'subject75780000', 'test_subject4408', 'subject78820000', 'subject46390000', 'subject4590000', 'test_subject1062', 'subject88350000', 'subject26990000', 'subject36470000', 'subject6154', 'subject4360000', 'subject21070000', 'subject45210000', 'subject15100000', 'subject5370000', 'subject531', 'subject34140000', 'test_subject7609', 'subject10440000', 'train_subject4630', 'subject37750000', 'subject10940000', 'subject86720000', 'subject15880000', 'subject69280000', 'subject6700000', 'test_subject9879', 'test_subject7467', 'subject6947', 'test_subject1359', 'subject94220000', 'subject57960000', 'subject73720000', 'subject76730000', 'subject21220000', 'subject97260000', 'subject8411', 'subject57280000', 'subject2760000', 'subject9830000', 'test_subject6067', 'test_subject6208', 'test_subject3791', 'test_subject5956', 'train_subject2415', 'test_subject7987', 'subject4050000', 'subject52940000', 'subject74370000', 'test_subject2673', 'test_subject3702', 'subject34450000', 'subject67820000', 'subject7210000', 'subject97430000', 'subject21650000', 'subject9803', 'test_subject3919', 'subject58320000', 'subject15630000', 'test_subject8944', 'subject18560000', 'subject96070000', 'subject2159', 'test_subject9585', 'subject84230000', 'test_subject2751', 'subject7779', 'subject98590000', 'subject55910000', 'subject97610000', 'subject64120000', 'train_subject2219', 'test_subject8103', 'subject27780000', 'test_subject9246', 'subject65230000', 'test_subject5563', 'subject85790000', 'subject89930000', 'train_subject6494', 'subject74780000', 'test_subject2886', 'subject50810000', 'test_subject8773', 'subject46910000', 'subject49880000', 'test_subject9306', 'train_subject8683', 'test_subject2607', 'subject3504', 'subject2879', 'train_subject3125', 'subject1910000', 'test_subject1896', 'subject6755', 'subject66270000', 'subject898', 'subject8657', 'subject90730000', 'subject41210000', 'subject8133', 'subject88780000', 'subject79010000', 'test_subject937', 'subject80730000', 'subject58920000', 'test_subject6620', 'subject5820000', 'subject17290000', 'subject84870000', 'test_subject4499', 'subject7920000', 'subject6792', 'subject55800000', 'subject20820000', 'subject28200000', 'subject43320000', 'test_subject3008', 'subject3183', 'subject8200', 'subject67330000', 'train_subject7042', 'test_subject6558', 'subject31740000', 'subject6293', 'subject5776', 'subject7469', 'subject74510000', 'subject86280000', 'subject901', 'subject730000', 'subject3297', 'test_subject2521', 'subject11570000', 'subject86980000', 'test_subject2960', 'subject4103', 'subject67450000', 'subject15370000', 'subject14860000', 'test_subject8481', 'subject21540000', 'train_subject416', 'subject12240000', 'subject1880000', 'subject14670000', 'test_subject8981', 'test_subject4937', 'test_subject7115', 'subject2062', 'subject18090000', 'subject70810000', 'subject62940000', 'subject3247', 'subject23490000', 'subject14100000', 'subject6302', 'subject86510000', 'subject18470000', 'test_subject8878', 'test_subject1445', 'subject93490000', 'subject4110', 'train_subject5723', 'subject26220000', 'subject845', 'test_subject5603', 'subject12980000', 'test_subject8142', 'test_subject8585', 'test_subject9791', 'train_subject136', 'test_subject6894', 'test_subject3406', 'train_subject2871', 'subject47780000', 'subject25220000', 'test_subject8523', 'subject95780000', 'subject1850000', 'subject4600000', 'subject99010000', 'subject8384', 'train_subject7367', 'test_subject8710', 'test_subject5209', 'subject7831', 'test_subject4291', 'test_subject861', 'subject84340000', 'subject76470000', 'test_subject8582', 'subject12360000', 'subject54430000', 'subject58190000', 'subject9229', 'subject62830000', 'test_subject7864', 'subject96670000', 'subject2845', 'subject57570000', 'subject46800000', 'test_subject3805', 'subject38440000', 'test_subject1724', 'subject38050000', 'subject33390000', 'subject96590000', 'subject6100000', 'test_subject3026', 'train_subject1095', 'test_subject7685', 'subject94400000', 'subject30780000', 'subject57010000', 'subject85600000', 'subject28220000', 'subject366', 'test_subject9355', 'subject41500000', 'subject3278', 'subject71250000', 'subject5532', 'train_subject6101', 'subject69450000', 'test_subject2359', 'subject13340000', 'test_subject7429', 'subject17850000', 'subject41830000', 'test_subject6514', 'train_subject2712', 'train_subject2252', 'subject82340000', 'test_subject9587', 'subject85300000', 'subject78310000', 'subject44540000', 'subject16140000', 'subject27240000', 'test_subject680', 'subject79920000', 'subject354', 'subject1637', 'train_subject1605', 'subject70510000', 'subject3269', 'subject96550000', 'test_subject4679', 'test_subject2186', 'test_subject2425', 'subject78090000', 'subject1913', 'subject99270000', 'test_subject4850', 'test_subject1539', 'subject85760000', 'test_subject3655', 'subject89260000', 'train_subject896', 'subject41700000', 'subject8296', 'subject55870000', 'subject74960000', 'subject16270000', 'subject28680000', 'test_subject7513', 'subject72820000', 'test_subject3341', 'train_subject7364', 'subject53110000', 'test_subject9736', 'test_subject4255', 'test_subject4116', 'subject45550000', 'subject21620000', 'test_subject3172', 'test_subject6794', 'test_subject954', 'subject899', 'subject46930000', 'subject88840000', 'subject58670000', 'train_subject7135', 'subject32570000', 'subject12600000', 'subject19220000', 'subject15130000', 'subject73830000', 'train_subject9763', 'test_subject974', 'subject6216', 'test_subject1111', 'subject20440000', 'test_subject3214', 'subject4858', 'subject31760000', 'subject95260000', 'subject29770000', 'test_subject5099', 'subject74710000', 'test_subject1345', 'subject45330000', 'subject6310', 'subject73510000', 'subject8512', 'test_subject4534', 'subject14930000', 'subject49610000', 'subject14660000', 'test_subject1914', 'subject5127', 'test_subject2844', 'subject24160000', 'train_subject127', 'subject76610000', 'subject4930000', 'test_subject9196', 'test_subject3081', 'subject69410000', 'subject54920000', 'test_subject9766', 'subject38630000', 'test_subject6364', 'subject74970000', 'subject2337', 'subject10100000', 'subject97760000', 'subject83610000', 'subject6083', 'subject4470', 'subject7478', 'subject35150000', 'subject12580000', 'subject48880000', 'test_subject5366', 'test_subject5983', 'subject44440000', 'subject3672', 'test_subject2286', 'train_subject9683', 'subject3191', 'subject4556', 'subject70950000', 'test_subject135', 'test_subject5550', 'train_subject7142', 'subject91160000', 'subject19480000', 'subject2013', 'subject7470000', 'test_subject1830', 'test_subject2539', 'subject95530000', 'subject52270000', 'train_subject2382', 'subject3162', 'subject5095', 'subject84550000', 'subject4112', 'subject410000', 'subject17630000', 'subject35630000', 'subject27140000', 'subject35890000', 'test_subject8889', 'train_subject4879', 'test_subject3336', 'subject87640000', 'subject758', 'subject25610000', 'train_subject669', 'subject82130000', 'subject61200000', 'subject9371', 'subject77520000', 'test_subject1116', 'test_subject7073', 'subject6620000', 'subject31540000', 'test_subject2183', 'test_subject4565', 'test_subject1372', 'train_subject9951', 'test_subject2667', 'subject44040000', 'subject11450000', 'test_subject2284', 'subject7687', 'subject13700000', 'test_subject5967', 'subject8400000', 'subject82900000', 'train_subject9001', 'test_subject7464', 'subject10530000', 'subject6950000', 'subject2730000', 'train_subject3902', 'test_subject7299', 'subject66720000', 'test_subject6752', 'subject39890000', 'train_subject1879', 'subject36420000', 'subject66980000', 'subject2746', 'subject45820000', 'subject6756', 'subject7720000', 'test_subject3921', 'test_subject259', 'test_subject8159', 'test_subject1840', 'subject8562', 'subject4529', 'test_subject8498', 'subject40490000', 'test_subject145', 'test_subject3273', 'test_subject857', 'subject1272', 'subject7927', 'subject58130000', 'test_subject4729', 'test_subject6809', 'subject67370000', 'train_subject337', 'test_subject387', 'test_subject5355', 'train_subject1637', 'subject5711', 'subject45380000', 'subject62810000', 'subject93950000', 'subject34040000', 'test_subject6048', 'subject10780000', 'subject96190000', 'test_subject6092', 'subject8607', 'subject9773', 'subject5517', 'subject6670', 'test_subject3366', 'subject350', 'subject1863', 'subject87840000', 'test_subject1222', 'subject5825', 'subject5746', 'subject82110000', 'subject61070000', 'test_subject3023', 'subject6139', 'test_subject6070', 'subject3323', 'subject5196', 'subject48680000', 'test_subject8901', 'train_subject5537', 'test_subject952', 'train_subject1979', 'test_subject1586', 'test_subject9565', 'subject2000000', 'test_subject4034', 'test_subject9802', 'test_subject2154', 'test_subject6948', 'test_subject6459', 'subject73160000', 'test_subject6491', 'subject77240000', 'test_subject50', 'test_subject8195', 'subject4153', 'subject41800000', 'subject72970000', 'subject5233', 'subject96330000', 'test_subject9300', 'test_subject211', 'subject89520000', 'test_subject8293', 'subject94250000', 'subject9010000', 'subject30990000', 'test_subject2336', 'subject24180000', 'subject2338', 'train_subject8843', 'subject35780000', 'subject8561', 'subject30420000', 'subject23300000', 'test_subject7938', 'subject87480000', 'train_subject2704', 'train_subject3364', 'test_subject6246', 'test_subject6305', 'subject6892', 'test_subject2973', 'subject6525', 'subject71270000', 'test_subject881', 'test_subject4930', 'subject8340', 'subject3094', 'subject19630000', 'subject78500000', 'test_subject4982', 'test_subject2196', 'subject1210000', 'subject92720000', 'subject47270000', 'test_subject1246', 'train_subject1199', 'subject398', 'subject95020000', 'subject94270000', 'subject2577', 'test_subject2397', 'train_subject5649', 'subject2797', 'test_subject3988', 'subject6114', 'subject26920000', 'test_subject7878', 'subject87250000', 'subject77620000', 'subject5426', 'test_subject2101', 'test_subject8276', 'subject50270000', 'test_subject5566', 'test_subject74', 'test_subject6875', 'subject73820000', 'subject83790000', 'test_subject7733', 'test_subject3744', 'subject21190000', 'subject59680000', 'subject57620000', 'test_subject96', 'subject91240000', 'subject23200000', 'subject52900000', 'subject69200000', 'subject9334', 'subject322', 'subject6088', 'subject6168', 'subject8173', 'subject8450000', 'subject8057', 'subject41820000', 'subject1152', 'subject19290000', 'subject9980000', 'test_subject5880', 'subject86410000', 'subject84790000', 'test_subject8434', 'subject77460000', 'subject26130000', 'test_subject25', 'subject77800000', 'subject3132', 'train_subject7970', 'subject58730000', 'subject58860000', 'subject62380000', 'test_subject9985', 'subject99510000', 'subject5270000', 'test_subject2389', 'subject5090000', 'subject27550000', 'test_subject5489', 'subject195', 'subject3428', 'subject1550000', 'test_subject1448', 'test_subject9062', 'subject80290000', 'subject52240000', 'subject22580000', 'test_subject7101', 'test_subject143', 'test_subject1085', 'test_subject7026', 'test_subject3107', 'test_subject9450', 'subject36380000', 'subject1811', 'subject33560000', 'subject3275', 'test_subject9710', 'train_subject7326', 'train_subject374', 'test_subject499', 'subject11740000', 'subject20120000', 'subject54270000', 'test_subject6732', 'subject6807', 'subject17870000', 'subject75330000', 'test_subject5736', 'subject92800000', 'subject24330000', 'subject76480000', 'subject43420000', 'subject42990000', 'subject34320000', 'subject57870000', 'subject29570000', 'subject33520000', 'test_subject5989', 'subject63980000', 'subject21510000', 'test_subject8830', 'train_subject4493', 'subject3400000', 'test_subject8320', 'subject5103', 'test_subject9549', 'test_subject5926', 'subject77040000', 'subject13060000', 'test_subject1590', 'subject79130000', 'subject60740000', 'test_subject4891', 'subject98370000', 'test_subject6028', 'test_subject8977', 'subject5297', 'subject97040000', 'test_subject2804', 'test_subject9029', 'subject6978', 'subject7510000', 'train_subject1839', 'test_subject8349', 'test_subject8039', 'test_subject9069', 'test_subject5266', 'test_subject4851', 'test_subject2459', 'subject1604', 'test_subject2001', 'subject973', 'train_subject6760', 'subject64310000', 'train_subject7680', 'subject85930000', 'test_subject3408', 'subject35620000', 'subject54010000', 'subject3020000', 'test_subject7921', 'train_subject7133', 'subject5808', 'subject86940000', 'subject29300000', 'test_subject3986', 'subject57560000', 'test_subject2180', 'subject38870000', 'test_subject1487', 'test_subject6728', 'subject3039', 'train_subject7533', 'subject72540000', 'subject140000', 'subject8710000', 'test_subject8020', 'subject51880000', 'subject4592', 'subject7285', 'test_subject6596', 'subject44460000', 'subject20450000', 'test_subject9298', 'subject32620000', 'subject75170000', 'test_subject7075', 'test_subject4796', 'subject97670000', 'test_subject4261', 'subject4187', 'subject68020000', 'subject43230000', 'subject16480000', 'subject61960000', 'test_subject8648', 'test_subject834', 'subject55360000', 'test_subject9579', 'test_subject4494', 'subject88630000', 'subject2529', 'subject8377', 'subject3835', 'subject9660000', 'train_subject9358', 'test_subject748', 'test_subject2452', 'test_subject6498', 'subject4260000', 'subject44270000', 'subject874', 'subject60900000', 'subject48430000', 'subject74280000', 'test_subject7521', 'test_subject3651', 'train_subject7925', 'subject7617', 'subject5221', 'subject50640000', 'test_subject4435', 'subject25620000', 'subject7422', 'subject23310000', 'test_subject1483', 'test_subject3834', 'test_subject3927', 'subject99260000', 'subject95480000', 'subject98800000', 'subject2992', 'subject19670000', 'subject98890000', 'subject545', 'subject98600000', 'subject35740000', 'subject26080000', 'subject9961', 'subject61630000', 'test_subject501', 'test_subject1849', 'subject48640000', 'subject31660000', 'train_subject7157', 'subject87540000', 'test_subject2990', 'test_subject9451', 'subject98220000', 'subject6951', 'test_subject4471', 'subject49080000', 'subject42110000', 'subject46920000', 'subject4073', 'test_subject351', 'subject4371', 'subject79930000', 'subject74130000', 'subject18800000', 'subject7077', 'subject3135', 'test_subject1506', 'subject66660000', 'train_subject933', 'test_subject2476', 'test_subject4818', 'test_subject7558', 'subject3232', 'subject1441', 'subject6358', 'test_subject9085', 'subject26850000', 'test_subject2689', 'subject42240000', 'subject50690000', 'subject83380000', 'subject98030000', 'subject57860000', 'subject1772', 'subject7371', 'subject90080000', 'test_subject3185', 'subject9650000', 'subject87310000', 'subject39960000', 'subject16920000', 'test_subject8574', 'test_subject5061', 'subject60440000', 'test_subject8980', 'subject70480000', 'subject8932', 'subject31', 'subject94490000', 'subject46830000', 'subject35050000', 'test_subject4766', 'subject77790000', 'test_subject4407', 'test_subject2317', 'subject69320000', 'subject48460000', 'subject45890000', 'subject20000000', 'subject81280000', 'test_subject7585', 'test_subject752', 'test_subject9096', 'subject30460000', 'test_subject4721', 'train_subject8577', 'subject46730000', 'test_subject353', 'subject45920000', 'subject57550000', 'test_subject5288', 'subject39720000', 'subject22040000', 'test_subject3665', 'subject90630000', 'subject70210000', 'test_subject395', 'subject88300000', 'subject5969', 'subject76660000', 'subject34430000', 'subject37770000', 'subject35800000', 'test_subject3827', 'subject4916', 'test_subject8945', 'test_subject4053', 'subject31800000', 'subject3787', 'subject81800000', 'subject6877', 'subject8740', 'test_subject8049', 'subject8597', 'subject9153', 'subject32870000', 'subject18210000', 'test_subject2674', 'subject90910000', 'subject4482', 'subject27430000', 'subject3944', 'test_subject513', 'subject63450000', 'subject49340000', 'subject35080000', 'test_subject7775', 'train_subject1190', 'subject48110000', 'test_subject3892', 'test_subject2036', 'train_subject4675', 'subject34600000', 'subject56440000', 'subject51630000', 'subject19370000', 'test_subject348', 'subject96370000', 'test_subject655', 'subject69110000', 'test_subject1620', 'subject65340000', 'subject7410000', 'subject44590000', 'subject9225', 'test_subject7254', 'train_subject405', 'test_subject3120', 'subject6780000', 'test_subject6772', 'test_subject2483', 'test_subject8409', 'test_subject6308', 'subject89490000', 'subject5364', 'test_subject497', 'test_subject2985', 'test_subject9942', 'subject71410000', 'subject4304', 'test_subject771', 'test_subject8362', 'test_subject3493', 'subject10740000', 'test_subject9670', 'test_subject6267', 'test_subject5913', 'subject28570000', 'test_subject2585', 'test_subject8820', 'test_subject2027', 'subject73300000', 'subject3883', 'subject32440000', 'test_subject5689', 'subject30410000', 'train_subject9552', 'subject40190000', 'subject8255', 'test_subject6026', 'subject7416', 'subject94890000', 'test_subject9866', 'test_subject2560', 'subject96640000', 'test_subject9304', 'subject9098', 'subject2129', 'test_subject9533', 'test_subject8168', 'test_subject1919', 'test_subject1005', 'test_subject3175', 'test_subject1613', 'subject25590000', 'test_subject2749', 'test_subject1488', 'test_subject2465', 'subject47680000', 'test_subject625', 'test_subject4227', 'subject12940000', 'test_subject9527', 'train_subject7181', 'test_subject9582', 'subject23130000', 'subject68070000', 'subject86070000', 'subject98500000', 'subject31570000', 'subject8799', 'subject9150000', 'subject77700000', 'test_subject7471', 'test_subject4746', 'test_subject5413', 'subject71920000', 'test_subject4909', 'test_subject5601', 'train_subject1191', 'test_subject1730', 'subject6446', 'subject2621', 'subject14590000', 'subject41290000', 'subject23720000', 'subject52350000', 'subject27790000', 'subject8860000', 'test_subject8590', 'subject5028', 'test_subject6569', 'subject45030000', 'subject94150000', 'subject25490000', 'subject70200000', 'subject5067', 'test_subject7451', 'subject4190000', 'subject40910000', 'subject30800000', 'train_subject888', 'test_subject1878', 'subject9166', 'test_subject2034', 'subject6639', 'subject2923', 'subject48450000', 'train_subject3336', 'subject80890000', 'subject4169', 'subject94860000', 'subject74260000', 'test_subject8881', 'subject92390000', 'subject35990000', 'subject52980000', 'test_subject6771', 'test_subject3482', 'train_subject1542', 'subject11650000', 'subject49870000', 'subject97770000', 'train_subject7515', 'subject24400000', 'subject28000000', 'subject1239', 'test_subject7578', 'subject19350000', 'test_subject768', 'subject66540000', 'test_subject6582', 'subject3230000', 'subject98230000', 'subject536', 'subject99590000', 'subject48190000', 'test_subject2790', 'test_subject7756', 'subject42210000', 'test_subject594', 'subject7662', 'subject6860000', 'test_subject9635', 'subject7442', 'subject20150000', 'test_subject5005', 'subject65760000', 'train_subject1555', 'subject6837', 'subject89020000', 'subject5770000', 'subject70710000', 'test_subject326', 'test_subject1103', 'subject3090000', 'test_subject4968', 'subject88700000', 'subject40100000', 'test_subject1693', 'subject66580000', 'subject29010000', 'test_subject2093', 'test_subject9371', 'subject27400000', 'test_subject8399', 'subject4390000', 'test_subject4293', 'subject51080000', 'test_subject6974', 'subject52380000', 'test_subject2064', 'test_subject3547', 'test_subject9359', 'subject4040000', 'subject37850000', 'subject5155', 'subject50730000', 'test_subject6436', 'subject25990000', 'train_subject96', 'subject69040000', 'subject88140000', 'subject69660000', 'test_subject8967', 'test_subject3635', 'subject56590000', 'subject95540000', 'subject8580000', 'test_subject8720', 'train_subject5936', 'subject244', 'test_subject6167', 'subject10550000', 'test_subject4978', 'test_subject2285', 'subject23230000', 'test_subject5732', 'subject43990000', 'test_subject9320', 'subject40350000', 'train_subject8901', 'subject9015', 'test_subject4188', 'subject24070000', 'subject2997', 'subject83180000', 'test_subject4139', 'subject87100000', 'subject29520000', 'test_subject7730', 'train_subject7703', 'subject9730000', 'subject5854', 'subject78770000', 'subject95900000', 'subject58020000', 'test_subject9737', 'subject29280000', 'test_subject3112', 'subject74980000', 'test_subject1304', 'test_subject2277', 'subject90120000', 'test_subject6899', 'test_subject9111', 'test_subject6049', 'subject6680', 'test_subject7070', 'subject3318', 'subject6714', 'test_subject1381', 'subject42690000', 'test_subject759', 'test_subject2270', 'train_subject8741', 'train_subject1457', 'subject53890000', 'subject1101', 'subject90440000', 'subject27260000', 'subject6921', 'subject2240', 'subject331', 'subject74310000', 'subject25510000', 'subject62430000', 'subject73740000', 'subject71320000', 'subject5461', 'subject73380000', 'subject76080000', 'subject74540000', 'subject4160', 'subject11710000', 'subject48580000', 'subject48470000', 'subject45640000', 'subject80660000', 'subject94370000', 'subject5311', 'subject47210000', 'subject7639', 'subject43690000', 'subject67520000', 'subject89650000', 'subject41590000', 'subject59340000', 'subject4569', 'subject1137', 'subject67030000', 'subject3880000', 'subject86320000', 'subject6072', 'subject63480000', 'subject2423', 'subject54050000', 'subject4196', 'subject3359', 'subject77810000', 'subject55480000', 'subject73970000', 'subject6956', 'subject83120000', 'subject75130000', 'subject93140000', 'subject42880000', 'subject95250000', 'subject49150000', 'subject50700000', 'subject94090000', 'subject62000000', 'subject43060000', 'subject12030000', 'subject82140000', 'subject3339', 'subject6410', 'subject69690000', 'subject4283', 'subject36160000', 'subject88600000', 'subject31260000', 'subject9770000', 'subject83140000', 'subject3083', 'subject22520000', 'subject28510000', 'subject12080000', 'subject3065', 'subject74090000', 'subject94940000', 'subject5169', 'subject1900000', 'subject4244', 'subject3364', 'subject45960000', 'subject49210000', 'subject65390000', 'subject73980000', 'subject13200000', 'subject9744', 'subject66420000', 'subject1193', 'subject1700000', 'subject6214', 'subject24430000', 'subject3259', 'subject73120000', 'subject7597', 'subject21400000', 'subject3480000', 'subject7632', 'subject51610000', 'subject230000', 'subject6412', 'subject68760000', 'subject4588', 'subject88200000', 'subject4994', 'subject95820000', 'subject89600000', 'subject17790000', 'subject49790000', 'subject41890000', 'subject30630000', 'subject29910000', 'subject41840000', 'subject7221', 'subject71950000', 'subject89390000', 'subject7351', 'subject63030000', 'subject87620000', 'subject6210', 'subject69590000', 'subject47720000', 'subject7355', 'subject85420000', 'subject40210000', 'subject6334', 'subject74790000', 'subject7388', 'subject90380000', 'subject55180000', 'subject70030000', 'subject96910000', 'subject29510000', 'subject3330000', 'subject70260000', 'subject58590000', 'subject96800000', 'subject2850000', 'subject90510000', 'subject5015', 'subject87350000', 'subject1713', 'subject42970000', 'subject45850000', 'subject99140000', 'subject5954', 'subject16020000', 'subject45160000', 'subject67680000', 'subject5660', 'subject95570000', 'subject30000', 'subject46020000', 'subject2469', 'subject40280000', 'subject2482', 'subject9800000', 'subject8401', 'subject7940', 'subject28460000', 'subject94990000', 'subject89310000', 'subject4180000', 'subject34010000', 'subject2540000', 'subject22600000', 'subject3220', 'subject15800000', 'subject18760000', 'subject65630000', 'subject3980000', 'subject35910000', 'subject50430000', 'subject7937', 'subject9436', 'subject44760000', 'subject11000000', 'subject2037', 'subject89850000', 'subject49630000', 'subject17020000', 'subject689', 'subject46110000', 'subject6269', 'subject8720', 'subject5830', 'subject85460000', 'subject33550000', 'subject57250000', 'subject34900000', 'subject9903', 'subject84730000', 'subject1989', 'subject1953', 'subject17220000', 'subject26670000', 'subject803', 'subject31360000', 'subject55740000', 'subject7278', 'subject62620000', 'subject2780000', 'subject5452', 'subject51710000', 'subject45730000', 'subject13800000', 'subject40270000', 'subject24470000', 'subject22230000', 'subject53720000', 'subject12300000', 'subject6037', 'subject8361', 'subject7087', 'subject41070000', 'subject33870000', 'subject19020000', 'subject2865', 'subject68570000', 'subject1358', 'subject11050000', 'subject48490000', 'subject28660000', 'subject44580000', 'subject40290000', 'subject85150000', 'subject94030000', 'subject7248', 'subject70020000', 'subject49280000', 'subject82360000', 'subject8320000', 'subject10370000', 'subject6219', 'subject5325', 'subject1656', 'subject96710000', 'subject5614', 'subject5802', 'subject34370000', 'subject54830000', 'subject42660000', 'subject78110000', 'subject22130000', 'subject1496', 'subject62010000', 'subject67200000', 'subject52550000', 'subject20370000', 'subject6620', 'subject78810000', 'subject47190000', 'subject65860000', 'subject27960000', 'subject86750000', 'subject381', 'subject8754', 'subject4222', 'subject77420000', 'subject85770000', 'subject6240000', 'subject37060000', 'subject47910000', 'subject41600000', 'subject88010000', 'subject99210000', 'subject10730000', 'subject12920000', 'subject7362', 'subject47850000', 'subject49440000', 'subject37260000', 'subject89140000', 'subject82640000', 'subject44630000', 'subject785', 'subject69700000', 'subject1113', 'subject870000', 'subject11430000', 'subject41000000', 'subject51540000', 'subject9043', 'subject7001', 'subject61250000', 'subject21630000', 'subject51490000', 'subject6790', 'subject41250000', 'subject3093', 'subject14480000', 'subject22390000', 'subject2680000', 'subject95340000', 'subject4961', 'subject69230000', 'subject32190000', 'subject65250000', 'subject21', 'subject35180000', 'subject96850000', 'subject97200000', 'subject3310000', 'subject69140000', 'subject8587', 'subject4800000', 'subject25270000', 'subject20690000', 'subject31220000', 'subject77230000', 'subject1773', 'subject98850000', 'subject9978', 'subject48820000', 'subject51120000', 'subject17800000', 'subject4406', 'subject52130000', 'subject23170000', 'subject9460000', 'subject8900', 'subject65120000', 'subject811', 'subject98010000', 'subject74950000', 'subject43030000', 'subject83670000', 'subject85570000', 'subject7433', 'subject9337', 'subject6029', 'subject63880000', 'subject81350000', 'subject63960000', 'subject74180000', 'subject61780000', 'subject99310000', 'subject97870000', 'subject73410000', 'subject53880000', 'subject2901', 'subject65910000', 'subject70090000', 'subject8930000', 'subject42760000', 'subject70870000', 'subject76680000', 'subject23690000', 'subject77610000', 'subject6910', 'subject88530000', 'subject4351', 'subject4443', 'subject2525', 'subject67730000', 'subject50540000', 'subject22470000', 'subject6863', 'subject75640000', 'subject47370000', 'subject82500000', 'subject71390000', 'subject5241', 'subject3750', 'subject85000000', 'subject31860000', 'subject5780000', 'subject62570000', 'subject41100000', 'subject6462', 'subject1143', 'subject46790000', 'subject5469', 'subject18310000', 'subject6800', 'subject3594', 'subject96270000', 'subject34730000', 'subject46410000', 'subject1300000', 'subject82530000', 'subject35010000', 'subject992', 'subject36000000', 'subject72030000', 'subject53070000', 'subject67280000', 'subject6994', 'subject3420000', 'subject42280000', 'subject6394', 'subject6695', 'subject3788', 'subject43820000', 'subject95890000', 'subject2257', 'subject14090000', 'subject9873', 'subject54710000', 'subject6407', 'subject71970000', 'subject39030000', 'subject23960000', 'subject42420000', 'subject71670000', 'subject27850000', 'subject97140000', 'subject6681', 'subject2728', 'subject80680000', 'subject5250000', 'subject67490000', 'subject92460000', 'subject8050000', 'subject3640', 'subject73950000', 'subject93410000', 'subject23100000', 'subject3240', 'subject851', 'subject9244', 'subject80510000', 'subject1518', 'subject6292', 'subject19770000', 'subject21160000', 'subject48990000', 'subject96150000', 'subject1789', 'subject2419', 'subject20940000', 'subject64690000', 'subject40970000', 'subject25870000', 'subject64340000', 'subject1474', 'subject64900000', 'subject57470000', 'subject8032', 'subject65240000', 'subject19660000', 'subject97460000', 'subject2472', 'subject57430000', 'subject29220000', 'subject2519', 'subject4836', 'subject2420000', 'subject61420000', 'subject49090000', 'subject49830000', 'subject69260000', 'subject45660000', 'subject6522', 'subject72200000', 'subject43250000', 'subject5236', 'subject2777', 'subject8750', 'subject11830000', 'subject38560000', 'subject91500000', 'subject1810000', 'subject8444', 'subject73910000', 'subject64200000', 'subject37960000', 'subject51270000', 'subject2947', 'subject8016', 'subject3740000', 'subject13980000', 'subject38260000', 'subject71040000', 'subject88510000', 'subject2101', 'subject96780000', 'subject58310000', 'subject8360000', 'subject5577', 'subject63100000', 'subject83510000', 'subject1090000', 'subject54600000', 'subject18410000', 'subject69310000', 'subject57600000', 'subject51790000', 'subject1499', 'subject40540000', 'subject57730000', 'subject489', 'subject820000', 'subject66380000', 'subject59870000', 'subject15930000', 'subject2568', 'subject8236', 'subject95850000', 'subject7468', 'subject55560000', 'subject5663', 'subject1195', 'subject30730000', 'subject3394', 'subject45020000', 'subject2359', 'subject45840000', 'subject79880000', 'subject4410', 'subject92450000', 'subject71840000', 'subject83690000', 'subject6386', 'subject1563', 'subject68620000', 'subject1074', 'subject98770000', 'subject61690000', 'subject92840000', 'subject6644', 'subject80920000', 'subject57490000', 'subject86250000', 'subject99490000', 'subject49710000', 'subject68170000', 'subject14320000', 'subject69440000', 'subject87920000', 'subject51840000', 'subject1549', 'subject64640000', 'subject48270000', 'subject79560000', 'subject64300000', 'subject48600000', 'subject41520000', 'subject15840000', 'subject85430000', 'subject7130000', 'subject9928', 'subject720000', 'subject9230', 'subject35880000', 'subject83980000', 'subject28070000', 'subject49000000', 'subject84620000', 'subject9090', 'subject8371', 'subject626', 'subject94850000', 'subject3288', 'subject8053', 'subject34470000', 'subject79230000', 'subject30000000', 'subject25480000', 'subject3530', 'subject91020000', 'subject49960000', 'subject80970000', 'subject44180000', 'subject5719', 'subject34920000', 'subject9950', 'subject61660000', 'subject91400000', 'subject46670000', 'subject30890000', 'subject75820000', 'subject41260000', 'subject40830000', 'subject1404', 'subject75210000', 'subject50530000', 'subject1169', 'subject4080000', 'subject4071', 'subject63040000', 'subject7860000', 'subject20460000', 'subject5118', 'subject93660000', 'subject92730000', 'subject11530000', 'subject4707', 'subject1565', 'subject87400000', 'subject26320000', 'subject2513', 'subject23940000', 'subject86310000', 'subject4421', 'subject23150000', 'subject8392', 'subject4999', 'subject12570000', 'subject94010000', 'subject12850000', 'subject60690000', 'subject13380000', 'subject57690000', 'subject5779', 'subject11660000', 'subject58390000', 'subject27370000', 'subject65790000', 'subject61290000', 'subject7301', 'subject5115', 'subject9716', 'subject32850000', 'subject99970000', 'subject85100000', 'subject9195', 'subject99030000', 'subject3659', 'subject48780000', 'subject9789', 'subject84880000', 'subject163', 'subject54990000', 'subject65020000', 'subject41310000', 'subject61680000', 'subject3605', 'subject69300000', 'subject6461', 'subject7230000', 'subject13460000', 'subject69990000', 'subject79300000', 'subject20300000', 'subject64360000', 'subject98980000', 'subject26970000', 'subject39190000', 'subject61230000', 'subject3145', 'subject98660000', 'subject23160000', 'subject7809', 'subject90610000', 'subject54840000', 'subject11560000', 'subject37000000', 'subject3535', 'subject58900000', 'subject6075', 'subject74670000', 'subject79330000', 'subject96', 'subject39010000', 'subject94790000', 'subject51810000', 'subject43510000', 'subject26680000', 'subject4982', 'subject7529', 'subject87240000', 'subject3386', 'subject23390000', 'subject68160000', 'subject87360000', 'subject61800000', 'subject73480000', 'subject14160000', 'subject5470000', 'subject68010000', 'subject5085', 'subject11580000', 'subject53010000', 'subject3180000', 'subject21140000', 'subject78430000', 'subject18640000', 'subject3614', 'subject32020000', 'subject47420000', 'subject4940000', 'subject6333', 'subject3274', 'subject58340000', 'subject43680000', 'subject71210000', 'subject160000', 'subject18610000', 'subject83480000', 'subject85500000', 'subject7066', 'subject39420000', 'subject66510000', 'subject4061', 'subject68610000', 'subject9273', 'subject34770000', 'subject73400000', 'subject7260000', 'subject9060000', 'subject53420000', 'subject45310000', 'subject69060000', 'subject97400000', 'subject1875', 'subject18110000', 'subject13510000', 'subject559', 'subject87130000', 'subject83660000', 'subject26070000', 'subject8701', 'subject64370000', 'subject5640000', 'subject67500000', 'subject91170000', 'subject2991', 'subject3901', 'subject43170000', 'subject38610000', 'subject53470000', 'subject73460000', 'subject89990000', 'subject1497', 'subject69290000', 'subject42270000', 'subject32470000', 'subject18050000', 'subject31490000', 'subject8247', 'subject13830000', 'subject14410000', 'subject34620000', 'subject85990000', 'subject7251', 'subject77400000', 'subject40690000', 'subject78750000', 'subject27230000', 'subject19700000', 'subject6414', 'subject2840', 'subject14500000', 'subject2069', 'subject828', 'subject10790000', 'subject46980000', 'subject62020000', 'subject99580000', 'subject8220000', 'subject36980000', 'subject7710', 'subject69730000', 'subject1423', 'subject99950000', 'subject72110000', 'subject76090000', 'subject24080000', 'subject959', 'subject3910000', 'subject34710000', 'subject70560000', 'subject16280000', 'subject4790000', 'subject6758', 'subject46120000', 'subject34630000', 'subject9654', 'subject44890000', 'subject85860000', 'subject17310000', 'subject47160000', 'subject41160000', 'subject34610000', 'subject62470000', 'subject26260000', 'subject97480000', 'subject5160', 'subject2894', 'subject18710000', 'subject76240000', 'subject25210000', 'subject72880000', 'subject5530000', 'subject3415', 'subject8240', 'subject4030000', 'subject44850000', 'subject2662', 'subject46470000', 'subject55040000', 'subject52120000', 'subject1310000', 'subject74740000', 'subject78060000', 'subject84740000', 'subject14460000', 'subject96750000', 'subject29630000', 'subject5078', 'subject30450000', 'subject71650000', 'subject50770000', 'subject2418', 'subject53500000', 'subject70320000', 'subject63610000', 'subject39020000', 'subject92330000', 'subject9653', 'subject6150000', 'subject13670000', 'subject5140000', 'subject9740000', 'subject43560000', 'subject2994', 'subject37980000', 'subject90570000', 'subject5173', 'subject44400000', 'subject8167', 'subject5512', 'subject6731', 'subject7249', 'subject82190000', 'subject46570000', 'subject89400000', 'subject375', 'subject15350000', 'subject7400000', 'subject4450', 'subject44500000', 'subject86350000', 'subject8254', 'subject4225', 'subject88240000', 'subject26540000', 'subject50400000', 'subject20040000', 'subject4140000', 'subject70590000', 'subject9078', 'subject83800000', 'subject70460000', 'subject26090000', 'subject29240000', 'subject87460000', 'subject7630000', 'subject1323', 'subject14020000', 'subject65800000', 'subject71530000', 'subject14250000', 'subject89860000', 'subject78150000', 'subject5562', 'subject67960000', 'subject610000', 'subject3097', 'subject4072', 'subject27320000', 'subject57190000', 'subject4990000', 'subject6120000', 'subject96470000', 'subject24230000', 'subject78000000', 'subject26980000', 'subject97860000', 'subject34200000', 'subject350000', 'subject9430000', 'subject7616', 'subject4896', 'subject17080000', 'subject79110000', 'subject92590000', 'subject146', 'subject62300000', 'subject97070000', 'subject9981', 'subject58820000', 'subject76690000', 'subject80520000', 'subject36740000', 'subject9210000', 'subject7925', 'subject5177', 'subject43540000', 'subject94380000', 'subject22740000', 'subject6138', 'subject40900000', 'subject49780000', 'subject26140000', 'subject2167', 'subject4276', 'subject67340000', 'subject3498', 'subject4954', 'subject91890000', 'subject58030000', 'subject3772', 'subject88', 'subject79840000', 'subject46130000', 'subject41360000', 'subject3727', 'subject1410000', 'subject34330000', 'subject87320000', 'subject4838', 'subject8550000', 'subject1120', 'subject7165', 'subject16680000', 'subject32090000', 'subject93510000', 'subject2504', 'subject87190000', 'subject42930000', 'subject27330000', 'subject4114', 'subject6288', 'subject65700000', 'subject2346', 'subject10930000', 'subject9982', 'subject36910000', 'subject96880000', 'subject9597', 'subject8841', 'subject9499', 'subject36480000', 'subject59380000', 'subject4155', 'subject9218', 'subject24680000', 'subject1271', 'subject44340000', 'subject7483', 'subject48440000', 'subject8127', 'subject51140000', 'subject6030', 'subject8054', 'subject4113', 'subject64180000', 'subject19210000', 'subject91610000', 'subject51070000', 'subject81860000', 'subject11640000', 'subject98360000', 'subject22000000', 'subject49020000', 'subject51330000', 'subject55030000', 'subject70470000', 'subject31130000', 'subject1390000', 'subject90590000', 'subject98580000', 'subject7430000', 'subject86290000', 'subject81690000', 'subject29230000', 'subject85830000', 'subject85400000', 'subject49290000', 'subject810000', 'subject78440000', 'subject79940000', 'subject69500000', 'subject33230000', 'subject72390000', 'subject4890000', 'subject88030000', 'subject95150000', 'subject70080000', 'subject88120000', 'subject77510000', 'subject51980000', 'subject90320000', 'subject62050000', 'subject14280000', 'subject52140000', 'subject55530000', 'subject53620000', 'subject31580000', 'subject40040000', 'subject32400000', 'subject46380000', 'subject86270000', 'subject45620000', 'subject46840000', 'subject89330000', 'subject30490000', 'subject74750000', 'subject440000', 'subject82630000', 'subject13150000', 'subject41920000', 'subject13600000', 'subject35670000', 'subject1630000', 'subject19190000', 'subject9680000', 'subject38890000', 'subject97840000', 'subject620000', 'subject41750000', 'subject32360000', 'subject6800000', 'subject45410000', 'subject24990000', 'subject96080000', 'subject39900000', 'subject45560000', 'subject6430000', 'subject98100000', 'subject82600000', 'subject50600000', 'subject11200000', 'subject59320000', 'subject78600000', 'subject38110000', 'subject20270000', 'subject11860000', 'subject47750000', 'subject45570000', 'subject75760000', 'subject67930000', 'subject6960000', 'subject63470000', 'subject16340000', 'subject60160000', 'subject38210000', 'subject45530000', 'subject32900000', 'subject77030000', 'subject94730000', 'subject74200000', 'subject66890000', 'subject92400000', 'subject91640000', 'subject63370000', 'subject17760000', 'subject48980000', 'subject59460000', 'subject35980000', 'subject88020000', 'subject25030000', 'subject36440000', 'subject46040000', 'subject83920000', 'subject76270000', 'subject54440000', 'subject97340000', 'subject70700000', 'subject85020000', 'subject45340000', 'subject26460000', 'subject96230000', 'subject79410000', 'subject33900000', 'subject22140000', 'subject5880000', 'subject27750000', 'subject49470000', 'subject91590000', 'subject43530000', 'subject5830000', 'subject13330000', 'subject57070000', 'subject38590000', 'subject95580000', 'subject96560000', 'subject52690000', 'subject78870000', 'subject66840000', 'subject78330000', 'subject61360000', 'subject20850000', 'subject9960000', 'subject91770000', 'subject43520000', 'subject98380000', 'subject20420000', 'subject67180000', 'subject67210000', 'subject35040000', 'subject16240000', 'subject93130000', 'subject4250000', 'subject62140000', 'subject32750000', 'subject11670000', 'subject34930000', 'subject78120000', 'subject27210000', 'subject95750000', 'subject8230000', 'subject45780000', 'subject65650000', 'subject60020000', 'subject18730000', 'subject93460000', 'subject31240000', 'subject87340000', 'subject58250000', 'subject73520000', 'subject86700000', 'subject86180000', 'subject76990000', 'subject22950000', 'subject3700000', 'subject83490000', 'subject7320000', 'subject3340000', 'subject18920000', 'subject73540000', 'subject99230000', 'subject67550000', 'subject55300000', 'subject73240000', 'subject67410000', 'subject37730000', 'subject62080000', 'subject80840000', 'subject27860000', 'subject21800000', 'subject17440000', 'subject1710000', 'subject45440000', 'subject26900000', 'subject57770000', 'subject2520000', 'subject48080000', 'subject96530000', 'subject74990000', 'subject24170000', 'subject13890000', 'subject43010000', 'subject1070000', 'subject25980000', 'subject18940000', 'subject70450000', 'subject50290000', 'subject48620000', 'subject25130000', 'subject29250000', 'subject24790000', 'subject46740000', 'subject4540000', 'subject68590000', 'subject41660000', 'subject57760000', 'subject89170000', 'subject47460000', 'subject86810000', 'subject96090000', 'subject93390000', 'subject7980000', 'subject14690000', 'subject97270000', 'subject60050000', 'subject36010000', 'subject26240000', 'subject28670000', 'subject98470000', 'subject27610000', 'subject47790000', 'subject9652', 'subject5375', 'subject8978', 'subject1623', 'subject2495', 'subject1914', 'subject8001', 'subject8472', 'subject6428', 'subject4002', 'subject6013', 'subject5974', 'subject1728', 'subject3881', 'subject6035', 'subject61300000', 'subject5000', 'subject203', 'subject682', 'subject814', 'subject2605', 'subject8581', 'subject807', 'subject242', 'subject4459', 'subject3977', 'subject6665', 'subject4226', 'subject2547', 'subject7290', 'subject4247', 'subject40620000', 'subject3868', 'subject5062', 'subject2948', 'subject9492', 'subject5036', 'subject1617', 'subject6459', 'subject217', 'subject4379', 'subject6259', 'subject6146', 'subject47', 'subject4702', 'subject3725', 'subject5100', 'subject6652', 'subject1210', 'subject3737', 'subject0', 'subject00000', 'subject7435', 'subject9725', 'subject519', 'subject5699', 'subject5793', 'subject4785', 'subject5908', 'subject8081', 'subject6453', 'subject5140', 'subject4644', 'subject5148', 'subject5840', 'subject4278', 'subject7698', 'subject5409', 'subject4762', 'subject5644', 'subject747', 'subject8064', 'subject1885', 'subject4414', 'subject5282', 'subject1469', 'subject1748', 'subject8795', 'subject8802', 'subject4831', 'subject9318', 'subject8329', 'subject9260', 'subject511', 'subject9811', 'subject7898', 'subject1485', 'subject4510', 'subject3181', 'subject5878', 'subject5383', 'subject641', 'subject9917', 'subject7499', 'subject5979', 'subject3644', 'subject7692', 'subject2980', 'subject7669', 'subject8770', 'subject4570', 'subject9114', 'subject4198', 'subject1105', 'subject9039', 'subject6946', 'subject6786', 'subject8990', 'subject1793', 'subject9160', 'subject7857', 'subject7262', 'subject7581', 'subject2097', 'subject6518', 'subject733', 'subject8626', 'subject4526', 'subject2239', 'subject3667', 'subject2182', 'subject4719', 'subject8292', 'subject4333', 'subject269', 'subject3178', 'subject1093', 'subject7439', 'subject6930', 'subject92130000', 'subject501', 'subject4074', 'subject5995', 'subject4392', 'subject7462', 'subject8726', 'subject379', 'subject7326', 'subject7560', 'subject7263', 'subject53230000', 'subject1089', 'subject1055', 'subject1786', 'subject796', 'subject2446', 'subject6903', 'subject8769', 'subject6866', 'subject2922', 'subject7740', 'subject7318', 'subject6833', 'subject8721', 'subject26040000', 'subject9949', 'subject974', 'subject8565', 'subject5937', 'subject2475', 'subject2269', 'subject1064', 'subject5603', 'subject5935', 'subject7627', 'subject5833', 'subject3277', 'subject3555', 'subject7333', 'subject6041', 'subject7229', 'subject8094', 'subject187', 'subject4934', 'subject3283', 'subject8193', 'subject5112', 'subject1288', 'subject6352', 'subject89100000', 'subject2685', 'subject1947', 'subject2690', 'subject5920', 'subject8973', 'subject6322', 'subject59140000', 'subject569', 'subject15890000', 'subject4513', 'subject7377', 'subject6238', 'subject2088', 'subject6247', 'subject9393', 'subject3994', 'subject9670000', 'subject4527', 'subject855', 'subject8233', 'subject8432', 'subject6093', 'subject3635', 'subject1655', 'subject2238', 'subject68700000', 'subject5033', 'subject2857', 'subject3270', 'subject2522', 'subject5256', 'subject8933', 'subject8062', 'subject4795', 'subject6309', 'subject2181', 'subject3357', 'subject4318', 'subject7131', 'subject5622', 'subject3014', 'subject88440000', 'subject3596', 'subject2696', 'subject7489', 'subject5150', 'subject67840000', 'subject2580', 'subject835', 'subject7661', 'subject98960000', 'subject9222', 'subject3117', 'subject5342', 'subject9285', 'subject7946', 'subject1507', 'subject8882', 'subject4729', 'subject9575', 'subject8822', 'subject8481', 'subject9249', 'subject5938', 'subject7952', 'subject3674', 'subject8065', 'subject63790000', 'subject7830', 'subject37240000', 'subject2567', 'subject9381', 'subject63220000', 'subject7764', 'subject3227', 'subject1027', 'subject9156', 'subject5528', 'subject4014', 'subject6423', 'subject9411', 'subject9014', 'subject7238', 'subject2678', 'subject6668', 'subject3914', 'subject9242', 'subject3301', 'subject4505', 'subject8845', 'subject2974', 'subject7107', 'subject5003', 'subject3612', 'subject5270', 'subject4143', 'subject7338', 'subject671', 'subject93900000', 'subject7982', 'subject7637', 'subject20980000', 'subject1264', 'subject2247', 'subject8395', 'subject522', 'subject3224', 'subject51', 'subject8357', 'subject7678', 'subject7801', 'subject1824', 'subject9497', 'subject4777', 'subject91660000', 'subject5387', 'subject45510000', 'subject1545', 'subject280', 'subject3904', 'subject1950', 'subject7428', 'subject1512', 'subject3928', 'subject6284', 'subject4848', 'subject7247', 'subject1763', 'subject4479', 'subject2949', 'subject5549', 'subject9077', 'subject9918', 'subject4563', 'subject8544', 'subject2935', 'subject9829', 'subject6918', 'subject1962', 'subject6899', 'subject8225', 'subject3844', 'subject9197', 'subject4843', 'subject82160000', 'subject7777', 'subject7018', 'subject505', 'subject9095', 'subject6173', 'subject6464', 'subject78410000', 'subject6957', 'subject5456', 'subject1524', 'subject4227', 'subject6019', 'subject2996', 'subject463', 'subject9729', 'subject7316', 'subject39270000', 'subject4284', 'subject5223', 'subject6290', 'subject75540000', 'subject2670000', 'subject92320000', 'subject8920000', 'subject83540000', 'subject27450000', 'subject66990000', 'subject37320000', 'subject30810000', 'subject69530000', 'subject10180000', 'subject87370000', 'subject37710000', 'subject5440000', 'subject92680000', 'subject39940000', 'subject17750000', 'subject4200000', 'subject97590000', 'subject28370000', 'subject18580000', 'subject31750000', 'subject54420000', 'subject24190000', 'subject1150000', 'subject3500000', 'subject79770000', 'subject96300000', 'subject41440000', 'subject8340000', 'subject91190000', 'subject4330000', 'subject7550000', 'subject97540000', 'subject18060000', 'subject76700000', 'subject68330000', 'subject72130000', 'subject96040000', 'subject74340000', 'subject9330000', 'subject51830000', 'subject41540000', 'subject1970000', 'subject57080000', 'subject9970000', 'subject17500000', 'subject39580000', 'subject18190000', 'subject52840000', 'subject37620000', 'subject77530000', 'subject99650000', 'subject31000000', 'subject78030000', 'subject43700000', 'subject91280000', 'subject2990000', 'subject15650000', 'subject66670000', 'subject3470000', 'subject18160000', 'subject47470000', 'subject73650000', 'subject18720000', 'subject77200000', 'subject85220000', 'subject24090000'])"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# session_collection = {}\n",
    "# models_collection = {}\n",
    "# hyperparams_collection = {}\n",
    "user_level_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, user_level_data, subjects_split, session=None, use_bert=False, set_type='train',\n",
    "                 batch_size=32, seq_len=512, vocabulary=vocabulary,\n",
    "                 voc_size=hyperparams_features['max_features'], emotion_lexicon=nrc_lexicon,\n",
    "                 hierarchical=False, pad_value=0, padding='pre',\n",
    "                 post_groups_per_user=None, posts_per_group=10, post_offset = 0,\n",
    "                 sampling_distr_alfa=0.1, sampling_distr='exp', # 'exp', 'uniform'\n",
    "                 emotions=emotions, pronouns=[\"i\", \"me\", \"my\", \"mine\", \"myself\"], liwc_categories=liwc_categories,\n",
    "                 liwc_dict=liwc_dict, compute_liwc=False, liwc_words_for_categories=None,\n",
    "                 pad_with_duplication=False,\n",
    "                 max_posts_per_user=None, sample_seqs=True,\n",
    "                 shuffle=True, return_subjects=False, keep_last_batch=True,\n",
    "                classes=1):\n",
    "        'Initialization'\n",
    "        self.seq_len = seq_len\n",
    "        # Instantiate tokenizer\n",
    "        if session:\n",
    "            self.bert_tokenizer = create_tokenizer_from_hub_module(session)\n",
    "            session.run(tf.local_variables_initializer())\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            session.run(tf.tables_initializer())\n",
    "        else:\n",
    "            if use_bert:\n",
    "                logger.error(\"Need a session to use bert in data generation\")\n",
    "            self.bert_tokenizer = None\n",
    "        self.use_bert = use_bert\n",
    "        self.subjects_split = subjects_split\n",
    "        self.set = set_type\n",
    "        self.emotion_lexicon = emotion_lexicon\n",
    "        self.batch_size = batch_size\n",
    "        self.hierarchical = hierarchical\n",
    "        self.data = user_level_data\n",
    "        self.pad_value = pad_value\n",
    "        self.return_subjects = return_subjects\n",
    "        self.sampling_distr_alfa = sampling_distr_alfa\n",
    "        self.sampling_distr = sampling_distr\n",
    "        self.emotions = emotions\n",
    "        self.pronouns = pronouns\n",
    "        self.liwc_categories = liwc_categories\n",
    "        self.liwc_dict = liwc_dict\n",
    "        self.liwc_words_for_categories = liwc_words_for_categories\n",
    "        self.compute_liwc = compute_liwc\n",
    "        self.sample_seqs = sample_seqs\n",
    "        self.pad_with_duplication = pad_with_duplication\n",
    "        self.padding = padding\n",
    "        self.keep_last_batch = keep_last_batch\n",
    "        self.shuffle = shuffle\n",
    "        self.voc_size = voc_size\n",
    "        self.vocabulary = vocabulary\n",
    "        self.max_posts_per_user = max_posts_per_user\n",
    "        self.post_groups_per_user = post_groups_per_user\n",
    "        self.post_offset = post_offset\n",
    "        self.posts_per_group = posts_per_group\n",
    "        self.classes = classes\n",
    "        self.generated_labels = []\n",
    "        self.__post_indexes_per_user()\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    @staticmethod\n",
    "    def _random_sample(population_size, sample_size, sampling_distr, alfa=0.1, replacement=False):\n",
    "        if sampling_distr == 'exp':\n",
    "            # Exponential sampling\n",
    "            sample = sorted(np.random.choice(population_size, \n",
    "                            min(sample_size, population_size),\n",
    "                            p = DataGenerator.__generate_reverse_exponential_indices(population_size, alfa),\n",
    "                            replace=replacement))\n",
    "                                                                # if pad_with_duplication, \n",
    "                                                                # pad by adding the same post multiple times\n",
    "                                                                # if there are not enough posts\n",
    "        elif sampling_distr == 'uniform':\n",
    "            # Uniform sampling\n",
    "            sample = sorted(np.random.choice(population_size,\n",
    "                            min(sample_size, population_size),\n",
    "                            replace=replacement))\n",
    "        return sample\n",
    "    \n",
    "    @staticmethod\n",
    "    def __generate_reverse_exponential_indices(max_index, alfa=1):\n",
    "        probabilities = []\n",
    "        for x in range(max_index):\n",
    "            probabilities.append(alfa * (np.exp(alfa*x)))\n",
    "        reverse_probabilities = [p for p in probabilities]\n",
    "        sump = sum(reverse_probabilities)\n",
    "        normalized_probabilities = [p/sump for p in reverse_probabilities]\n",
    "        return normalized_probabilities\n",
    "    \n",
    "    def __post_indexes_per_user(self):\n",
    "        self.indexes_per_user = {u: [] for u in range(len(self.subjects_split[self.set]))}\n",
    "        self.indexes_with_user = []\n",
    "        for u in range(len(self.subjects_split[self.set])):\n",
    "            if self.subjects_split[self.set][u] not in self.data:\n",
    "                logger.warning(\"User %s has no posts in %s set. Ignoring.\\n\" % (\n",
    "                    self.subjects_split[self.set][u], self.set))\n",
    "                continue\n",
    "            user_posts = self.data[self.subjects_split[self.set][u]]['texts']\n",
    "            if self.max_posts_per_user:\n",
    "                user_posts = user_posts[:self.max_posts_per_user]\n",
    "            nr_post_groups = int(np.ceil(len(user_posts) / self.posts_per_group))\n",
    "            \n",
    "            if self.post_groups_per_user:\n",
    "                nr_post_groups = min(self.post_groups_per_user, nr_post_groups)\n",
    "            for i in range(nr_post_groups):\n",
    "                # Generate random ordered samples of the posts\n",
    "                if self.sample_seqs:\n",
    "                    indexes_sample = DataGenerator._random_sample(population_size=len(user_posts),\n",
    "                                                         sample_size=self.posts_per_group,\n",
    "                                                         sampling_distr=self.sampling_distr,\n",
    "                                                         alfa=self.sampling_distr_alfa,\n",
    "                                                         replacement=self.pad_with_duplication)\n",
    "                    self.indexes_per_user[u].append(indexes_sample)\n",
    "                    self.indexes_with_user.append((u, indexes_sample))\n",
    "                    # break # just generate one?\n",
    "                # Generate all subsets of the posts in order\n",
    "                # TODO: Change here if you want a sliding window\n",
    "                else:\n",
    "                    self.indexes_per_user[u].append(range(i*self.posts_per_group + self.post_offset,\n",
    "                                                        min((i+1)*self.posts_per_group + self.post_offset, len(user_posts))))\n",
    "                    self.indexes_with_user.append((u, range(i*self.posts_per_group ,\n",
    "                                                        min((i+1)*self.posts_per_group + self.post_offset, len(user_posts)))))\n",
    "\n",
    "\n",
    "    def __encode_text(self, tokens, raw_text):\n",
    "        # Using voc_size-1 value for OOV token\n",
    "        encoded_tokens = [self.vocabulary.get(w, self.voc_size-1) for w in tokens]\n",
    "        encoded_emotions = encode_emotions(tokens, self.emotion_lexicon, self.emotions)\n",
    "        encoded_pronouns = encode_pronouns(tokens, self.pronouns)\n",
    "        encoded_stopwords = encode_stopwords(tokens)\n",
    "        if not self.compute_liwc:\n",
    "            encoded_liwc = None\n",
    "        else:\n",
    "            encoded_liwc = self.__encode_liwc_categories(tokens)\n",
    "        if self.bert_tokenizer:\n",
    "            bert_ids, bert_masks, bert_segments, label = encode_text_for_bert(self.bert_tokenizer, InputExample(None, \n",
    "                                               raw_text), self.seq_len)\n",
    "        else:\n",
    "            bert_ids, bert_masks, bert_segments = [[0]*self.seq_len, [0]*self.seq_len, [0]*self.seq_len]\n",
    "        return (encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords, encoded_liwc,\n",
    "               bert_ids, bert_masks, bert_segments)\n",
    "    \n",
    "    def __encode_liwc_categories_full(self, tokens, relative=True):\n",
    "        categories_cnt = [0 for c in self.liwc_categories]\n",
    "        if not tokens:\n",
    "            return categories_cnt\n",
    "        text_len = len(tokens)\n",
    "        for i, category in enumerate(self.liwc_categories):\n",
    "            category_words = self.liwc_dict[category]\n",
    "            for t in tokens:\n",
    "                for word in category_words:\n",
    "                    if t==word or (word[-1]=='*' and t.startswith(word[:-1])) \\\n",
    "                    or (t==word.split(\"'\")[0]):\n",
    "                        categories_cnt[i] += 1\n",
    "                        break # one token cannot belong to more than one word in the category\n",
    "            if relative and text_len:\n",
    "                categories_cnt[i] = categories_cnt[i]/text_len\n",
    "        return categories_cnt\n",
    "        \n",
    "        \n",
    "    def __encode_liwc_categories(self, tokens, relative=True):\n",
    "        categories_cnt = [0 for c in self.liwc_categories]\n",
    "        if not tokens:\n",
    "            return categories_cnt\n",
    "        text_len = len(tokens)\n",
    "        for i, category in enumerate(self.liwc_categories):\n",
    "            for t in tokens:\n",
    "                if t in self.liwc_words_for_categories[category]:\n",
    "                    categories_cnt[i] += 1\n",
    "            if relative and text_len:\n",
    "                categories_cnt[i] = categories_cnt[i]/text_len\n",
    "        return categories_cnt\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        if self.keep_last_batch:\n",
    "            return int(np.ceil(len(self.indexes) / self.batch_size)) # + 1 to not discard last batch\n",
    "        return int((len(self.indexes))/self.batch_size)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Reset generated labels\n",
    "        if index == 0:\n",
    "             self.generated_labels = []\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Find users\n",
    "        user_indexes = [t[0] for t in indexes]\n",
    "        users = set([self.subjects_split[self.set][i] for i in user_indexes\n",
    "                    if self.subjects_split[self.set][i] in self.data.keys()]) # TODO: maybe needs a warning that user is missing\n",
    "        post_indexes_per_user = {u: [] for u in users}\n",
    "        # Sample post ids\n",
    "        for u, post_indexes in indexes:\n",
    "            user = self.subjects_split[self.set][u]\n",
    "            # Note: was bug here - changed it into a list\n",
    "            post_indexes_per_user[user].append(post_indexes)\n",
    "\n",
    "        # Generate data\n",
    "        if self.hierarchical:\n",
    "            X, s, y = self.__data_generation_hierarchical(users, post_indexes_per_user)\n",
    "        else:\n",
    "            X, s, y = self.__data_generation(users, post_indexes_per_user)\n",
    "\n",
    "        if self.return_subjects:\n",
    "            return X, s, y\n",
    "        else:\n",
    "            return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = self.indexes_with_user\n",
    "#         np.arange(len(self.subjects_split[self.set]))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, users, post_indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        tokens_data = []\n",
    "        categ_data = []\n",
    "        sparse_data = []\n",
    "        subjects = []\n",
    "        bert_ids_data = []\n",
    "        bert_masks_data = []\n",
    "        bert_segments_data = []\n",
    "        labels = []\n",
    "\n",
    "        for subject in users:\n",
    "\n",
    "            if 'label' in self.data[subject]:\n",
    "                label = self.data[subject]['label']\n",
    "            else:\n",
    "                label = None\n",
    "\n",
    "            \n",
    "            all_words = []\n",
    "            all_raw_texts = []\n",
    "            liwc_aggreg = []\n",
    "\n",
    "            for post_index_range in post_indexes[subject]:\n",
    "                # Sample\n",
    "                texts = [self.data[subject]['texts'][i] for i in post_index_range]\n",
    "                if 'liwc' in self.data[subject] and not self.compute_liwc:\n",
    "                    liwc_selection = [self.data[subject]['liwc'][i] for i in post_index_range]\n",
    "                raw_texts = [self.data[subject]['raw'][i] for i in post_index_range]\n",
    "\n",
    "                all_words.append(sum(texts, [])) # merge all texts in group in one list\n",
    "                if 'liwc' in self.data[subject] and not self.compute_liwc:\n",
    "                    liwc_aggreg.append(np.array(liwc_selection).mean(axis=0).tolist())\n",
    "                all_raw_texts.append(\" \".join(raw_texts))\n",
    "            for i, words in enumerate(all_words):\n",
    "                encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords, encoded_liwc, \\\n",
    "                    bert_ids, bert_masks, bert_segments = self.__encode_text(words, all_raw_texts[i])\n",
    "                try:\n",
    "                    subject_id = int(re.findall('[0-9]+', subject)[0])\n",
    "                except IndexError:\n",
    "                    subject_id = subject\n",
    "                tokens_data.append(encoded_tokens)\n",
    "                # TODO: what will be the difference between these?\n",
    "                # I think instead of averaging for the post group, it just does it correctly\n",
    "                # for the whole post group (when computing, non-lazily)\n",
    "                if 'liwc' in self.data[subject] and not self.compute_liwc:  \n",
    "                    categ_data.append(encoded_emotions + [encoded_pronouns] + liwc_aggreg[i])\n",
    "                   \n",
    "                else:\n",
    "                    categ_data.append(encoded_emotions + [encoded_pronouns] + encoded_liwc)\n",
    "                    \n",
    "                sparse_data.append(encoded_stopwords)\n",
    "                bert_ids_data.append(bert_ids)\n",
    "                bert_masks_data.append(bert_masks)\n",
    "                bert_segments_data.append(bert_segments)\n",
    "                \n",
    "                labels.append(label)\n",
    "                subjects.append(subject_id)\n",
    "\n",
    "        \n",
    "        self.generated_labels.extend(labels)\n",
    "        # using zeros for padding\n",
    "        tokens_data_padded = sequence.pad_sequences(tokens_data, maxlen=self.seq_len, \n",
    "                                                    padding=self.padding,\n",
    "                                                   truncating=self.padding)\n",
    "\n",
    "        if self.use_bert:\n",
    "            return ([np.array(tokens_data_padded), np.array(categ_data), np.array(sparse_data),\n",
    "                 np.array(bert_ids_data), np.array(bert_masks_data), np.array(bert_segments_data),\n",
    "                ],\n",
    "                np.array(subjects),\n",
    "                np.array(labels))\n",
    "        else:\n",
    "            return ([np.array(tokens_data_padded), np.array(categ_data), np.array(sparse_data),\n",
    "                ],\n",
    "                np.array(subjects),\n",
    "                np.array(labels))\n",
    "    \n",
    "    def __data_generation_hierarchical(self, users, post_indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        user_tokens = []\n",
    "        user_categ_data = []\n",
    "        user_sparse_data = []\n",
    "        user_bert_ids_data = []\n",
    "        user_bert_masks_data = []\n",
    "        user_bert_segments_data = []\n",
    "        \n",
    "        labels = []\n",
    "        subjects = []\n",
    "        for subject in users:\n",
    "             \n",
    "            all_words = []\n",
    "            all_raw_texts = []\n",
    "            liwc_scores = []\n",
    "            \n",
    "            if 'label' in self.data[subject]:\n",
    "                if self.classes==1:\n",
    "                    label = self.data[subject]['label']\n",
    "                else:\n",
    "                    label = list(np_utils.to_categorical(self.data[subject]['label'], num_classes=self.classes))\n",
    "            else:\n",
    "                label = None\n",
    "\n",
    "            for post_index_range in post_indexes[subject]:\n",
    "                # Sample\n",
    "                texts = [self.data[subject]['texts'][i] for i in post_index_range]\n",
    "                if 'liwc' in self.data[subject] and not self.compute_liwc:\n",
    "                    liwc_selection = [self.data[subject]['liwc'][i] for i in post_index_range]\n",
    "                raw_texts = [self.data[subject]['raw'][i] for i in post_index_range]\n",
    "\n",
    "                all_words.append(texts)\n",
    "                if 'liwc' in self.data[subject] and not self.compute_liwc:\n",
    "                    liwc_scores.append(liwc_selection)\n",
    "                all_raw_texts.append(raw_texts)\n",
    "            \n",
    "#             if len(texts) < self.max_posts_per_user:\n",
    "#                 # TODO: pad with zeros\n",
    "#                 pass\n",
    "\n",
    "            for i, words in enumerate(all_words):\n",
    "                tokens_data = []\n",
    "                categ_data = []\n",
    "                sparse_data = []\n",
    "                bert_ids_data = []\n",
    "                bert_masks_data = []\n",
    "                bert_segments_data = []\n",
    "                \n",
    "                raw_text = all_raw_texts[i]\n",
    "                words = all_words[i]\n",
    "                \n",
    "                for p, posting in enumerate(words): \n",
    "                    encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords, encoded_liwc, \\\n",
    "                        bert_ids, bert_masks, bert_segments = self.__encode_text(words[p], raw_text[p])\n",
    "                    if 'liwc' in self.data[subject] and not self.compute_liwc:\n",
    "                        liwc = liwc_scores[i][p]\n",
    "                    else:\n",
    "                        liwc = encoded_liwc\n",
    "                    try:\n",
    "                        subject_id = int(re.findall('[0-9]+', subject)[0])\n",
    "                    except IndexError:\n",
    "                        subject_id = subject\n",
    "                    tokens_data.append(encoded_tokens)\n",
    "                    # using zeros for padding\n",
    "                    # TODO: there is something wrong with this\n",
    "                    categ_data.append(encoded_emotions + [encoded_pronouns] + liwc)\n",
    "                    sparse_data.append(encoded_stopwords)\n",
    "                    bert_ids_data.append(bert_ids)\n",
    "                    bert_masks_data.append(bert_masks)\n",
    "                    bert_segments_data.append(bert_segments)\n",
    "                \n",
    "                # For each range\n",
    "                tokens_data_padded = np.array(sequence.pad_sequences(tokens_data, maxlen=self.seq_len,\n",
    "                                              padding=self.padding,\n",
    "                                            truncating=self.padding))\n",
    "                user_tokens.append(tokens_data_padded)\n",
    "\n",
    "                user_categ_data.append(categ_data)\n",
    "                user_sparse_data.append(sparse_data)\n",
    "\n",
    "                user_bert_ids_data.append(bert_ids_data)\n",
    "                user_bert_masks_data.append(bert_masks_data)\n",
    "                user_bert_segments_data.append(bert_segments_data)\n",
    "\n",
    "                labels.append(label)\n",
    "                subjects.append(subject_id)\n",
    "\n",
    "        user_tokens = sequence.pad_sequences(user_tokens, \n",
    "                                             maxlen=self.posts_per_group, \n",
    "                                             value=self.pad_value)\n",
    "        user_tokens = np.rollaxis(np.dstack(user_tokens), -1)\n",
    "        user_categ_data = sequence.pad_sequences(user_categ_data,  \n",
    "                                                 maxlen=self.posts_per_group, \n",
    "                                                 value=self.pad_value, dtype='float32')\n",
    "        user_categ_data = np.rollaxis(np.dstack(user_categ_data), -1)\n",
    "        \n",
    "        user_sparse_data = sequence.pad_sequences(user_sparse_data, \n",
    "                                                  maxlen=self.posts_per_group, \n",
    "                                                  value=self.pad_value)\n",
    "        user_sparse_data = np.rollaxis(np.dstack(user_sparse_data), -1)\n",
    "        \n",
    "        user_bert_ids_data = sequence.pad_sequences(user_bert_ids_data, \n",
    "                                                    maxlen=self.posts_per_group, \n",
    "                                                    value=self.pad_value)\n",
    "        user_bert_ids_data = np.rollaxis(np.dstack(user_bert_ids_data), -1)\n",
    "        \n",
    "        user_bert_masks_data = sequence.pad_sequences(user_bert_masks_data, \n",
    "                                                      maxlen=self.posts_per_group, \n",
    "                                                      value=self.pad_value)\n",
    "        user_bert_masks_data = np.rollaxis(np.dstack(user_bert_masks_data), -1)\n",
    "        \n",
    "        user_bert_segments_data = sequence.pad_sequences(user_bert_segments_data, \n",
    "                                                         maxlen=self.posts_per_group, \n",
    "                                                         value=self.pad_value)\n",
    "        user_bert_segments_data = np.rollaxis(np.dstack(user_bert_segments_data), -1)\n",
    "\n",
    "        self.generated_labels.extend(labels)\n",
    "\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        if self.use_bert:\n",
    "            return ((user_tokens, user_categ_data, user_sparse_data, \n",
    "                 user_bert_ids_data, user_bert_masks_data, uifser_bert_segments_data),\n",
    "                np.array(subjects),\n",
    "                labels)\n",
    "        else:\n",
    "            return ((user_tokens, user_categ_data, user_sparse_data), \n",
    "                np.array(subjects),\n",
    "                labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGeneratorFromText(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, texts, batch_size=32, seq_len=512, vocabulary=vocabulary,\n",
    "                 voc_size=hyperparams_features['max_features'], emotion_lexicon=nrc_lexicon,\n",
    "                 hierarchical=False, pad_value=0, padding='pre',\n",
    "                 emotions=emotions, pronouns=[\"i\", \"me\", \"my\", \"mine\", \"myself\"], liwc_categories=liwc_categories,\n",
    "                 liwc_dict=liwc_dict, compute_liwc=False, liwc_words_for_categories=None,\n",
    "                 pad_with_duplication=False,\n",
    "                 shuffle=True, keep_last_batch=True):\n",
    "        'Initialization'\n",
    "        self.seq_len = seq_len\n",
    "        # Instantiate tokenizer\n",
    "        \n",
    "        self.texts = texts\n",
    "        self.emotion_lexicon = emotion_lexicon\n",
    "        self.batch_size = batch_size\n",
    "        self.hierarchical = hierarchical\n",
    " \n",
    "        self.pad_value = pad_value\n",
    "\n",
    "        self.keep_last_batch = keep_last_batch\n",
    "        self.emotions = emotions\n",
    "        self.pronouns = pronouns\n",
    "        self.liwc_categories = liwc_categories\n",
    "        self.liwc_dict = liwc_dict\n",
    "        self.liwc_words_for_categories = liwc_words_for_categories\n",
    "        self.compute_liwc = compute_liwc\n",
    "      \n",
    "        self.pad_with_duplication = pad_with_duplication\n",
    "        self.padding = padding\n",
    "        self.shuffle = shuffle\n",
    "        self.voc_size = voc_size\n",
    "        self.vocabulary = vocabulary\n",
    "       \n",
    "        self.on_epoch_end()\n",
    "        \n",
    "   \n",
    "\n",
    "    def __encode_text(self, tokens, raw_text):\n",
    "        # Using voc_size-1 value for OOV token\n",
    "        encoded_tokens = [self.vocabulary.get(w, self.voc_size-1) for w in tokens]\n",
    "        encoded_emotions = encode_emotions(tokens, self.emotion_lexicon, self.emotions)\n",
    "        encoded_pronouns = encode_pronouns(tokens, self.pronouns)\n",
    "        encoded_stopwords = encode_stopwords(tokens)\n",
    "\n",
    "        encoded_liwc = self.__encode_liwc_categories(tokens)\n",
    "      \n",
    "        bert_ids, bert_masks, bert_segments = [[0]*self.seq_len, [0]*self.seq_len, [0]*self.seq_len]\n",
    "        return (encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords, encoded_liwc,\n",
    "               bert_ids, bert_masks, bert_segments)\n",
    "    \n",
    "    def __encode_liwc_categories_full(self, tokens, relative=True):\n",
    "        categories_cnt = [0 for c in self.liwc_categories]\n",
    "        if not tokens:\n",
    "            return categories_cnt\n",
    "        text_len = len(tokens)\n",
    "        for i, category in enumerate(self.liwc_categories):\n",
    "            category_words = self.liwc_dict[category]\n",
    "            for t in tokens:\n",
    "                for word in category_words:\n",
    "                    if t==word or (word[-1]=='*' and t.startswith(word[:-1])) \\\n",
    "                    or (t==word.split(\"'\")[0]):\n",
    "                        categories_cnt[i] += 1\n",
    "                        break # one token cannot belong to more than one word in the category\n",
    "            if relative and text_len:\n",
    "                categories_cnt[i] = categories_cnt[i]/text_len\n",
    "        return categories_cnt\n",
    "        \n",
    "        \n",
    "    def __encode_liwc_categories(self, tokens, relative=True):\n",
    "        categories_cnt = [0 for c in self.liwc_categories]\n",
    "        if not tokens:\n",
    "            return categories_cnt\n",
    "        text_len = len(tokens)\n",
    "        for i, category in enumerate(self.liwc_categories):\n",
    "            for t in tokens:\n",
    "                if t in self.liwc_words_for_categories[category]:\n",
    "                    categories_cnt[i] += 1\n",
    "            if relative and text_len:\n",
    "                categories_cnt[i] = categories_cnt[i]/text_len\n",
    "        return categories_cnt\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        if self.keep_last_batch:\n",
    "            return int(np.ceil(len(self.indexes) / self.batch_size)) # + 1 to not discard last batch\n",
    "        return int((len(self.indexes))/self.batch_size)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "       \n",
    "        # Generate data\n",
    "        if self.hierarchical:\n",
    "            X, s, y = self.__data_generation_hierarchical(indexes)\n",
    "        else:\n",
    "            X, s, y = self.__data_generation(indexes)\n",
    "      \n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.texts))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        tokens_data = []\n",
    "        categ_data = []\n",
    "        sparse_data = []\n",
    "        subjects = []\n",
    "        bert_ids_data = []\n",
    "        bert_masks_data = []\n",
    "        bert_segments_data = []\n",
    "        labels = []\n",
    "\n",
    "        label = None\n",
    "\n",
    "\n",
    "        all_words = []\n",
    "        all_raw_texts = []\n",
    "        liwc_aggreg = []\n",
    "\n",
    "        # Sample\n",
    "        all_words = [tokenize(self.texts[i]) for i in indexes]\n",
    "\n",
    "        all_raw_texts = [self.texts[i] for i in indexes]\n",
    "\n",
    "\n",
    "        for i, words in enumerate(all_words):\n",
    "            encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords, encoded_liwc, \\\n",
    "                bert_ids, bert_masks, bert_segments = self.__encode_text(words, all_raw_texts[i])\n",
    "\n",
    "            tokens_data.append(encoded_tokens)\n",
    "            # TODO: what will be the difference between these?\n",
    "            # I think instead of averaging for the post group, it just does it correctly\n",
    "            # for the whole post group (when computing, non-lazily)\n",
    "\n",
    "            categ_data.append(encoded_emotions + [encoded_pronouns] + encoded_liwc)\n",
    "\n",
    "            sparse_data.append(encoded_stopwords)\n",
    "            bert_ids_data.append(bert_ids)\n",
    "            bert_masks_data.append(bert_masks)\n",
    "            bert_segments_data.append(bert_segments)\n",
    "\n",
    "            labels.append(None)\n",
    "            subjects.append(None)\n",
    "\n",
    "        \n",
    "        # using zeros for padding\n",
    "        tokens_data_padded = sequence.pad_sequences(tokens_data, maxlen=self.seq_len, \n",
    "                                                    padding=self.padding,\n",
    "                                                   truncating=self.padding)\n",
    "\n",
    "       \n",
    "        return ([np.array(tokens_data_padded), np.array(categ_data), np.array(sparse_data),\n",
    "                ],\n",
    "                np.array(subjects),\n",
    "                np.array(labels))\n",
    "    \n",
    "    def __data_generation_hierarchical(self, users, post_indexes):\n",
    "        pass\n",
    "#         'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "#         user_tokens = []\n",
    "#         user_categ_data = []\n",
    "#         user_sparse_data = []\n",
    "#         user_bert_ids_data = []\n",
    "#         user_bert_masks_data = []\n",
    "#         user_bert_segments_data = []\n",
    "        \n",
    "#         labels = []\n",
    "#         subjects = []\n",
    "#         for subject in users:\n",
    "             \n",
    "#             all_words = []\n",
    "#             all_raw_texts = []\n",
    "#             liwc_scores = []\n",
    "            \n",
    "#             if 'label' in self.data[subject]:\n",
    "#                 label = self.data[subject]['label']\n",
    "#             else:\n",
    "#                 label = None\n",
    "\n",
    "#             for post_index_range in post_indexes[subject]:\n",
    "#                 # Sample\n",
    "#                 texts = [self.data[subject]['texts'][i] for i in post_index_range]\n",
    "#                 if 'liwc' in self.data[subject] and not self.compute_liwc:\n",
    "#                     liwc_selection = [self.data[subject]['liwc'][i] for i in post_index_range]\n",
    "#                 raw_texts = [self.data[subject]['raw'][i] for i in post_index_range]\n",
    "\n",
    "#                 all_words.append(texts)\n",
    "#                 if 'liwc' in self.data[subject] and not self.compute_liwc:\n",
    "#                     liwc_scores.append(liwc_selection)\n",
    "#                 all_raw_texts.append(raw_texts)\n",
    "            \n",
    "# #             if len(texts) < self.max_posts_per_user:\n",
    "# #                 # TODO: pad with zeros\n",
    "# #                 pass\n",
    "\n",
    "#             for i, words in enumerate(all_words):\n",
    "#                 tokens_data = []\n",
    "#                 categ_data = []\n",
    "#                 sparse_data = []\n",
    "#                 bert_ids_data = []\n",
    "#                 bert_masks_data = []\n",
    "#                 bert_segments_data = []\n",
    "                \n",
    "#                 raw_text = all_raw_texts[i]\n",
    "#                 words = all_words[i]\n",
    "                \n",
    "#                 for p, posting in enumerate(words): \n",
    "#                     encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords, encoded_liwc, \\\n",
    "#                         bert_ids, bert_masks, bert_segments = self.__encode_text(words[p], raw_text[p])\n",
    "#                     if 'liwc' in self.data[subject] and not self.compute_liwc:\n",
    "#                         liwc = liwc_scores[i][p]\n",
    "#                     else:\n",
    "#                         liwc = encoded_liwc\n",
    "#                     try:\n",
    "#                         subject_id = int(re.findall('[0-9]+', subject)[0])\n",
    "#                     except IndexError:\n",
    "#                         subject_id = subject\n",
    "#                     tokens_data.append(encoded_tokens)\n",
    "#                     # using zeros for padding\n",
    "#                     # TODO: there is something wrong with this\n",
    "#                     categ_data.append(encoded_emotions + [encoded_pronouns] + liwc)\n",
    "#                     sparse_data.append(encoded_stopwords)\n",
    "#                     bert_ids_data.append(bert_ids)\n",
    "#                     bert_masks_data.append(bert_masks)\n",
    "#                     bert_segments_data.append(bert_segments)\n",
    "                \n",
    "#                 # For each range\n",
    "#                 tokens_data_padded = np.array(sequence.pad_sequences(tokens_data, maxlen=self.seq_len,\n",
    "#                                               padding=self.padding,\n",
    "#                                             truncating=self.padding))\n",
    "#                 user_tokens.append(tokens_data_padded)\n",
    "\n",
    "#                 user_categ_data.append(categ_data)\n",
    "#                 user_sparse_data.append(sparse_data)\n",
    "\n",
    "#                 user_bert_ids_data.append(bert_ids_data)\n",
    "#                 user_bert_masks_data.append(bert_masks_data)\n",
    "#                 user_bert_segments_data.append(bert_segments_data)\n",
    "\n",
    "#                 labels.append(label)\n",
    "#                 subjects.append(subject_id)\n",
    "\n",
    "#         user_tokens = sequence.pad_sequences(user_tokens, \n",
    "#                                              maxlen=self.posts_per_group, \n",
    "#                                              value=self.pad_value)\n",
    "#         user_tokens = np.rollaxis(np.dstack(user_tokens), -1)\n",
    "#         user_categ_data = sequence.pad_sequences(user_categ_data,  \n",
    "#                                                  maxlen=self.posts_per_group, \n",
    "#                                                  value=self.pad_value, dtype='float32')\n",
    "#         user_categ_data = np.rollaxis(np.dstack(user_categ_data), -1)\n",
    "        \n",
    "#         user_sparse_data = sequence.pad_sequences(user_sparse_data, \n",
    "#                                                   maxlen=self.posts_per_group, \n",
    "#                                                   value=self.pad_value)\n",
    "#         user_sparse_data = np.rollaxis(np.dstack(user_sparse_data), -1)\n",
    "        \n",
    "#         user_bert_ids_data = sequence.pad_sequences(user_bert_ids_data, \n",
    "#                                                     maxlen=self.posts_per_group, \n",
    "#                                                     value=self.pad_value)\n",
    "#         user_bert_ids_data = np.rollaxis(np.dstack(user_bert_ids_data), -1)\n",
    "        \n",
    "#         user_bert_masks_data = sequence.pad_sequences(user_bert_masks_data, \n",
    "#                                                       maxlen=self.posts_per_group, \n",
    "#                                                       value=self.pad_value)\n",
    "#         user_bert_masks_data = np.rollaxis(np.dstack(user_bert_masks_data), -1)\n",
    "        \n",
    "#         user_bert_segments_data = sequence.pad_sequences(user_bert_segments_data, \n",
    "#                                                          maxlen=self.posts_per_group, \n",
    "#                                                          value=self.pad_value)\n",
    "#         user_bert_segments_data = np.rollaxis(np.dstack(user_bert_segments_data), -1)\n",
    "        \n",
    "#         if self.use_bert:\n",
    "#             return ((user_tokens, user_categ_data, user_sparse_data, \n",
    "#                  user_bert_ids_data, user_bert_masks_data, user_bert_segments_data),\n",
    "#                 np.array(subjects),\n",
    "#                 np.array(labels))\n",
    "#         else:\n",
    "#             return ((user_tokens, user_categ_data, user_sparse_data), \n",
    "#                 np.array(subjects),\n",
    "#                 np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[    0,     0,     0, ..., 15258, 10492,  1461],\n",
      "       [    0,     0,     0, ..., 15258, 18114,  1461]], dtype=int32), array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.16666667,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.16666667, 0.        , 0.        ,\n",
      "        0.        , 0.16666667, 0.        , 0.        , 0.83333333,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.16666667, 0.        , 0.16666667,\n",
      "        0.        , 0.16666667, 0.        , 0.16666667, 0.        ,\n",
      "        0.        , 0.16666667, 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.16666667, 0.        , 0.        , 0.        , 0.16666667],\n",
      "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.2       , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.2       , 0.        , 0.2       ,\n",
      "        0.        , 0.2       , 0.        , 0.2       , 0.8       ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.2       , 0.        , 0.2       ,\n",
      "        0.        , 0.2       , 0.        , 0.2       , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "        0.2       , 0.        , 0.        , 0.        , 0.        ]]), array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0]])]\n"
     ]
    }
   ],
   "source": [
    "for x, y in DataGeneratorFromText([\"This is the first text\", \"and this is the second text\"], liwc_words_for_categories=liwc_words_for_categories):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "[[0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "g = DataGenerator(user_level_data, subjects_split, \n",
    "                                         set_type='test', \n",
    "                                    seq_len=hyperparams_collection[key]['maxlen'],   \n",
    "#                                         batch_size=len(subjects_split['test']), # on all data at once\n",
    "                                        batch_size=hyperparams_collection[key]['batch_size'], # on all data at once\n",
    "                                       hierarchical=hyperparams_collection[key]['hierarchical'],\n",
    "                                         max_posts_per_user=None,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams_collection[key]['posts_per_group'],\n",
    "                                        post_groups_per_user=1,  compute_liwc=True,\n",
    "                                        liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                        emotions=emotions, liwc_categories=liwc_categories,\n",
    "                                         sample_seqs=False, shuffle=False, classes=classes)\n",
    "# g = DataGenerator(user_level_data, subjects_split, sample_seqs=False, max_posts_per_user=None,\n",
    "#                                           set_type='test', hierarchical=True, post_groups_per_user=None,\n",
    "#                               posts_per_group=50, shuffle=False,\n",
    "#                              sampling_distr='exp', liwc_words_for_categories=liwc_words_for_categories,\n",
    "#                              compute_liwc=True, classes=3)\n",
    "for cnt, x in enumerate(g):\n",
    "    continue\n",
    "print(cnt)\n",
    "print(g.generated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1924"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g.generated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:12,115;WARNING;User subject6472 has no posts in train set. Ignoring.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:training:User subject6472 has no posts in train set. Ignoring.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:12,743;INFO;0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:13,370;INFO;0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:13,938;INFO;0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:14,554;INFO;0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:15,158;INFO;0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:15,692;INFO;0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:16,263;INFO;0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:16,860;INFO;0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:17,425;INFO;0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:18,119;INFO;0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:18,689;INFO;0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:19,281;INFO;0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:20,000;INFO;0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:20,698;INFO;0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:21,504;INFO;0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:22,237;INFO;0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:22,966;INFO;0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:23,789;INFO;0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:24,408;INFO;0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:25,089;INFO;0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:25,792;INFO;0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:26,416;INFO;0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:27,041;INFO;0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:27,655;INFO;0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:28,313;INFO;0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:28,966;INFO;0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:29,638;INFO;0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:30,367;INFO;0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 train positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:30,996;INFO;0 valid positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 valid positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:31,613;INFO;0 valid positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 valid positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:32,249;INFO;0 valid positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 valid positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:32,813;INFO;0 valid positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 valid positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:33,436;INFO;0 valid positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 valid positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:34,156;INFO;0 valid positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 valid positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:34,732;INFO;0 valid positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 valid positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:35,249;INFO;0 valid positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 valid positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:35,771;INFO;0 valid positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 valid positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:36,388;INFO;0 valid positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 valid positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:36,941;INFO;0 valid positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 valid positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:37,484;INFO;0 valid positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 valid positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:38,095;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:38,680;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:39,398;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:39,934;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:40,517;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:41,139;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:41,958;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:42,622;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:43,232;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:43,841;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:44,477;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:45,073;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:45,595;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:46,212;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:46,865;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:47,497;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:47,995;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:48,463;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:49,032;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:49,668;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:50,532;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:51,138;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:51,665;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:52,407;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:53,171;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:53,798;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:54,394;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:55,027;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:55,574;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:56,139;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:56,714;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:57,275;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:58,022;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:58,696;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:59,349;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:57:59,943;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:58:00,505;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:58:01,095;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:58:01,718;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:58:02,426;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:58:03,019;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:58:03,661;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:58:04,194;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:58:04,817;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:58:05,415;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:58:06,069;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:58:06,632;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:58:07,255;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:58:07,813;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:58:08,434;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:58:08,958;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:58:09,443;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:58:09,994;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:58:10,535;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:58:11,116;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:58:11,696;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:58:12,180;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:58:12,666;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:58:13,219;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:58:13,838;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-13 10:58:13,909;INFO;0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:0 test positive examples\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 2s, sys: 1.17 s, total: 1min 4s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# TODO: it is skipping the last batch\n",
    "x_data = {'train': [], 'valid': [], 'test': []}\n",
    "y_data = {'train': [], 'valid': [], 'test': []}\n",
    "for set_type in ['train', 'valid', 'test']:\n",
    "    total_positive = 0\n",
    "    for x, y in DataGenerator(user_level_data, subjects_split, sample_seqs=False, max_posts_per_user=None,\n",
    "                                          set_type=set_type, hierarchical=True, post_groups_per_user=1,\n",
    "                              posts_per_group=50, shuffle=False,\n",
    "                             sampling_distr='exp', liwc_words_for_categories=liwc_words_for_categories,\n",
    "                             compute_liwc=True, classes=3):\n",
    "#         total_positive += pd.Series(y).sum()\n",
    "        x_data[set_type].append(x)\n",
    "        y_data[set_type].append(y)\n",
    "        logger.info(\"%s %s positive examples\\n\" % (total_positive, set_type))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-12 12:12:50,115;WARNING;User subject6472 has no posts in train set. Ignoring.\n",
      "\n",
      "2021-01-12 12:12:50,228;INFO;19 train positive examples\n",
      "\n",
      "2021-01-12 12:12:50,338;INFO;35 train positive examples\n",
      "\n",
      "2021-01-12 12:12:50,431;INFO;57 train positive examples\n",
      "\n",
      "2021-01-12 12:12:50,542;INFO;77 train positive examples\n",
      "\n",
      "2021-01-12 12:12:50,646;INFO;98 train positive examples\n",
      "\n",
      "2021-01-12 12:12:50,736;INFO;115 train positive examples\n",
      "\n",
      "2021-01-12 12:12:50,830;INFO;132 train positive examples\n",
      "\n",
      "2021-01-12 12:12:50,934;INFO;148 train positive examples\n",
      "\n",
      "2021-01-12 12:12:51,030;INFO;168 train positive examples\n",
      "\n",
      "2021-01-12 12:12:51,148;INFO;187 train positive examples\n",
      "\n",
      "2021-01-12 12:12:51,245;INFO;204 train positive examples\n",
      "\n",
      "2021-01-12 12:12:51,351;INFO;222 train positive examples\n",
      "\n",
      "2021-01-12 12:12:51,476;INFO;253 train positive examples\n",
      "\n",
      "2021-01-12 12:12:51,604;INFO;317 train positive examples\n",
      "\n",
      "2021-01-12 12:12:51,751;INFO;381 train positive examples\n",
      "\n",
      "2021-01-12 12:12:51,880;INFO;445 train positive examples\n",
      "\n",
      "2021-01-12 12:12:52,007;INFO;509 train positive examples\n",
      "\n",
      "2021-01-12 12:12:52,155;INFO;573 train positive examples\n",
      "\n",
      "2021-01-12 12:12:52,260;INFO;637 train positive examples\n",
      "\n",
      "2021-01-12 12:12:52,379;INFO;701 train positive examples\n",
      "\n",
      "2021-01-12 12:12:52,506;INFO;765 train positive examples\n",
      "\n",
      "2021-01-12 12:12:52,613;INFO;829 train positive examples\n",
      "\n",
      "2021-01-12 12:12:52,718;INFO;893 train positive examples\n",
      "\n",
      "2021-01-12 12:12:52,824;INFO;957 train positive examples\n",
      "\n",
      "2021-01-12 12:12:52,946;INFO;1021 train positive examples\n",
      "\n",
      "2021-01-12 12:12:53,061;INFO;1085 train positive examples\n",
      "\n",
      "2021-01-12 12:12:53,179;INFO;1149 train positive examples\n",
      "\n",
      "2021-01-12 12:12:53,316;INFO;1207 train positive examples\n",
      "\n",
      "2021-01-12 12:12:53,424;INFO;22 valid positive examples\n",
      "\n",
      "2021-01-12 12:12:53,531;INFO;40 valid positive examples\n",
      "\n",
      "2021-01-12 12:12:53,641;INFO;56 valid positive examples\n",
      "\n",
      "2021-01-12 12:12:53,736;INFO;77 valid positive examples\n",
      "\n",
      "2021-01-12 12:12:53,843;INFO;95 valid positive examples\n",
      "\n",
      "2021-01-12 12:12:53,972;INFO;119 valid positive examples\n",
      "\n",
      "2021-01-12 12:12:54,068;INFO;143 valid positive examples\n",
      "\n",
      "2021-01-12 12:12:54,154;INFO;161 valid positive examples\n",
      "\n",
      "2021-01-12 12:12:54,242;INFO;181 valid positive examples\n",
      "\n",
      "2021-01-12 12:12:54,349;INFO;196 valid positive examples\n",
      "\n",
      "2021-01-12 12:12:54,446;INFO;214 valid positive examples\n",
      "\n",
      "2021-01-12 12:12:54,535;INFO;232 valid positive examples\n",
      "\n",
      "2021-01-12 12:12:54,650;INFO;37 test positive examples\n",
      "\n",
      "2021-01-12 12:12:54,749;INFO;86 test positive examples\n",
      "\n",
      "2021-01-12 12:12:54,883;INFO;120 test positive examples\n",
      "\n",
      "2021-01-12 12:12:54,976;INFO;164 test positive examples\n",
      "\n",
      "2021-01-12 12:12:55,073;INFO;211 test positive examples\n",
      "\n",
      "2021-01-12 12:12:55,173;INFO;246 test positive examples\n",
      "\n",
      "2021-01-12 12:12:55,310;INFO;283 test positive examples\n",
      "\n",
      "2021-01-12 12:12:55,422;INFO;318 test positive examples\n",
      "\n",
      "2021-01-12 12:12:55,529;INFO;358 test positive examples\n",
      "\n",
      "2021-01-12 12:12:55,629;INFO;392 test positive examples\n",
      "\n",
      "2021-01-12 12:12:55,743;INFO;434 test positive examples\n",
      "\n",
      "2021-01-12 12:12:55,846;INFO;466 test positive examples\n",
      "\n",
      "2021-01-12 12:12:55,932;INFO;503 test positive examples\n",
      "\n",
      "2021-01-12 12:12:56,040;INFO;546 test positive examples\n",
      "\n",
      "2021-01-12 12:12:56,159;INFO;588 test positive examples\n",
      "\n",
      "2021-01-12 12:12:56,269;INFO;625 test positive examples\n",
      "\n",
      "2021-01-12 12:12:56,348;INFO;652 test positive examples\n",
      "\n",
      "2021-01-12 12:12:56,426;INFO;695 test positive examples\n",
      "\n",
      "2021-01-12 12:12:56,525;INFO;736 test positive examples\n",
      "\n",
      "2021-01-12 12:12:56,639;INFO;777 test positive examples\n",
      "\n",
      "2021-01-12 12:12:56,798;INFO;818 test positive examples\n",
      "\n",
      "2021-01-12 12:12:56,905;INFO;854 test positive examples\n",
      "\n",
      "2021-01-12 12:12:56,996;INFO;891 test positive examples\n",
      "\n",
      "2021-01-12 12:12:57,133;INFO;938 test positive examples\n",
      "\n",
      "2021-01-12 12:12:57,263;INFO;976 test positive examples\n",
      "\n",
      "2021-01-12 12:12:57,365;INFO;1015 test positive examples\n",
      "\n",
      "2021-01-12 12:12:57,466;INFO;1046 test positive examples\n",
      "\n",
      "2021-01-12 12:12:57,579;INFO;1089 test positive examples\n",
      "\n",
      "2021-01-12 12:12:57,671;INFO;1129 test positive examples\n",
      "\n",
      "2021-01-12 12:12:57,764;INFO;1168 test positive examples\n",
      "\n",
      "2021-01-12 12:12:57,863;INFO;1198 test positive examples\n",
      "\n",
      "2021-01-12 12:12:57,957;INFO;1237 test positive examples\n",
      "\n",
      "2021-01-12 12:12:58,095;INFO;1276 test positive examples\n",
      "\n",
      "2021-01-12 12:12:58,214;INFO;1321 test positive examples\n",
      "\n",
      "2021-01-12 12:12:58,331;INFO;1364 test positive examples\n",
      "\n",
      "2021-01-12 12:12:58,426;INFO;1407 test positive examples\n",
      "\n",
      "2021-01-12 12:12:58,522;INFO;1442 test positive examples\n",
      "\n",
      "2021-01-12 12:12:58,619;INFO;1480 test positive examples\n",
      "\n",
      "2021-01-12 12:12:58,725;INFO;1519 test positive examples\n",
      "\n",
      "2021-01-12 12:12:58,849;INFO;1553 test positive examples\n",
      "\n",
      "2021-01-12 12:12:58,950;INFO;1592 test positive examples\n",
      "\n",
      "2021-01-12 12:12:59,057;INFO;1626 test positive examples\n",
      "\n",
      "2021-01-12 12:12:59,143;INFO;1664 test positive examples\n",
      "\n",
      "2021-01-12 12:12:59,250;INFO;1704 test positive examples\n",
      "\n",
      "2021-01-12 12:12:59,357;INFO;1741 test positive examples\n",
      "\n",
      "2021-01-12 12:12:59,470;INFO;1777 test positive examples\n",
      "\n",
      "2021-01-12 12:12:59,563;INFO;1811 test positive examples\n",
      "\n",
      "2021-01-12 12:12:59,667;INFO;1852 test positive examples\n",
      "\n",
      "2021-01-12 12:12:59,762;INFO;1888 test positive examples\n",
      "\n",
      "2021-01-12 12:12:59,871;INFO;1921 test positive examples\n",
      "\n",
      "2021-01-12 12:12:59,961;INFO;1953 test positive examples\n",
      "\n",
      "2021-01-12 12:13:00,038;INFO;1994 test positive examples\n",
      "\n",
      "2021-01-12 12:13:00,128;INFO;2032 test positive examples\n",
      "\n",
      "2021-01-12 12:13:00,214;INFO;2061 test positive examples\n",
      "\n",
      "2021-01-12 12:13:00,311;INFO;2095 test positive examples\n",
      "\n",
      "2021-01-12 12:13:00,410;INFO;2137 test positive examples\n",
      "\n",
      "2021-01-12 12:13:00,492;INFO;2170 test positive examples\n",
      "\n",
      "2021-01-12 12:13:00,572;INFO;2202 test positive examples\n",
      "\n",
      "2021-01-12 12:13:00,664;INFO;2235 test positive examples\n",
      "\n",
      "2021-01-12 12:13:00,772;INFO;2269 test positive examples\n",
      "\n",
      "2021-01-12 12:13:00,784;INFO;2273 test positive examples\n",
      "\n",
      "CPU times: user 10.7 s, sys: 115 ms, total: 10.9 s\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TODO: it is skipping the last batch\n",
    "x_data2 = {'train': [], 'valid': [], 'test': []}\n",
    "y_data2 = {'train': [], 'valid': [], 'test': []}\n",
    "for set_type in ['train', 'valid', 'test']:\n",
    "    total_positive = 0\n",
    "    for x, y in DataGenerator(user_level_data, subjects_split, sample_seqs=False, max_posts_per_user=None,\n",
    "                                          set_type=set_type, hierarchical=False, post_groups_per_user=1,\n",
    "                              posts_per_group=50, shuffle=False,\n",
    "                             sampling_distr='exp', liwc_words_for_categories=liwc_words_for_categories,\n",
    "                             compute_liwc=False):\n",
    "        total_positive += pd.Series(y).sum()\n",
    "        x_data2[set_type].append(x)\n",
    "        y_data2[set_type].append(y)\n",
    "        logger.info(\"%d %s positive examples\\n\" % (total_positive, set_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.]], dtype=float32),\n",
       " array([[0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.]], dtype=float32),\n",
       " array([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.]], dtype=float32),\n",
       " array([[0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.]], dtype=float32),\n",
       " array([[1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.]], dtype=float32),\n",
       " array([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.]], dtype=float32),\n",
       " array([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.]], dtype=float32),\n",
       " array([[0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.]], dtype=float32),\n",
       " array([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.]], dtype=float32),\n",
       " array([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.]], dtype=float32),\n",
       " array([[0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.]], dtype=float32),\n",
       " array([[1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.]], dtype=float32),\n",
       " array([[0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]], dtype=float32),\n",
       " array([[0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.]], dtype=float32),\n",
       " array([[0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.]], dtype=float32),\n",
       " array([[0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.]], dtype=float32),\n",
       " array([[0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.]], dtype=float32),\n",
       " array([[0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.]], dtype=float32),\n",
       " array([[0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.]], dtype=float32),\n",
       " array([[0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.]], dtype=float32),\n",
       " array([[0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.]], dtype=float32),\n",
       " array([[0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.]], dtype=float32),\n",
       " array([[0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.]], dtype=float32),\n",
       " array([[0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.]], dtype=float32),\n",
       " array([[0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.]], dtype=float32),\n",
       " array([[0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.]], dtype=float32),\n",
       " array([[0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.]], dtype=float32),\n",
       " array([[0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.]], dtype=float32)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data['train'][0][1][0]\n",
    "y_data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01044776, 0.02835821, 0.00696517, 0.0159204 , 0.03084577,\n",
       "       0.02686567, 0.05223881, 0.01393035, 0.01044776, 0.0318408 ,\n",
       "       0.05373134, 0.01360285, 0.00346323, 0.00085613, 0.09020737,\n",
       "       0.06833801, 0.05137316, 0.01059074, 0.02013274, 0.01337787,\n",
       "       0.06570218, 0.04549357, 0.00443699, 0.03020042, 0.00161512,\n",
       "       0.00871018, 0.01128699, 0.12413217, 0.01024824, 0.16229404,\n",
       "       0.01496272, 0.05799883, 0.00072376, 0.00274076, 0.02532783,\n",
       "       0.10052475, 0.01769724, 0.14599014, 0.05362405, 0.01843325,\n",
       "       0.00207391, 0.21570415, 0.002     , 0.05046897, 0.51654475,\n",
       "       0.00599988, 0.0178758 , 0.00581564, 0.01667293, 0.06624023,\n",
       "       0.01182401, 0.01303094, 0.1688824 , 0.00111111, 0.01014204,\n",
       "       0.02592204, 0.15915888, 0.00483863, 0.05578277, 0.00330005,\n",
       "       0.01231227, 0.05937276, 0.        , 0.01299368, 0.10757265,\n",
       "       0.03467344, 0.00916028, 0.00343776, 0.0040479 , 0.        ,\n",
       "       0.0552525 , 0.        , 0.01527557, 0.00760621, 0.03744025])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data2['train'][0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #x_data['train'][]\n",
    "# featureindex = 1\n",
    "\n",
    "# from scipy.stats import pearsonr, spearmanr\n",
    "# for i in range(len(x_data['train'][0][featureindex])):\n",
    "#     print(spearmanr(x_data['train'][0][featureindex][i], x_data2['train'][0][featureindex][i]))    \n",
    "#     plt.scatter(x_data['train'][0][featureindex][i], x_data2['train'][0][featureindex][i])\n",
    "# #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(liwc_words_for_categories[c]) for c in categories])\n",
    "len(categories)\n",
    "len(set(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x_data['valid'][0][0].shape, x_data['valid'][0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_for_bert = encode_text_for_bert(bert_tokenizer, InputExample(None, \n",
    "#                                                \"Ana are mere\"), 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids, masks, segments, label = encoded_for_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weights = class_weight.compute_class_weight('balanced',\n",
    "#                                                  np.unique(y_data['train']),\n",
    "#                                                  y_data['train'])\n",
    "# class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 20002 word vectors.\n",
      "Words not found in embedding space 20002\n"
     ]
    }
   ],
   "source": [
    "def load_embeddings(path, embedding_dim, voc):\n",
    "    # random matrix with mean value = 0\n",
    "    embedding_matrix = np.random.random((len(voc)+2, embedding_dim)) - 0.5 # voc + unk + pad value(0)\n",
    "    cnt_inv = 0\n",
    "    f = open(path, encoding='utf8')\n",
    "    for i, line in enumerate(f):\n",
    "#         print(i)\n",
    "        values = line.split()\n",
    "        word = ''.join(values[:-hyperparams_features['embedding_dim']])\n",
    "        coefs = np.asarray(values[-hyperparams_features['embedding_dim']:], dtype='float32')\n",
    "        word_i = voc.get(word)\n",
    "        if word_i is not None:\n",
    "            embedding_matrix[word_i] = coefs\n",
    "            cnt_inv += 1\n",
    "    f.close()\n",
    "\n",
    "    print('Total %s word vectors.' % len(embedding_matrix))\n",
    "    print('Words not found in embedding space %d' % (len(embedding_matrix)-cnt_inv))\n",
    " \n",
    "    return embedding_matrix\n",
    "\n",
    "def load_embeddings2(path, embedding_dim, voc):\n",
    "    # random matrix with mean value = 0\n",
    "    embedding_matrix = np.random.random((len(voc)+2, embedding_dim)) #- 0.5 # voc + unk + pad value(0)\n",
    "    cnt_inv = 0\n",
    "    with open(path, \"rb\") as f:\n",
    "        embedding_dict = pickle.load(f)\n",
    "    for word, coefs in embedding_dict.items():\n",
    "        word_i = voc.get(word)\n",
    "        if word_i is not None:\n",
    "            embedding_matrix[word_i] = coefs\n",
    "            cnt_inv += 1\n",
    "    print('Total %s word vectors.' % len(embedding_matrix))\n",
    "    print('Words not found in embedding space %d' % (len(embedding_matrix)-cnt_inv))\n",
    " \n",
    "    return embedding_matrix\n",
    "# \n",
    "# pretrained_embeddings_path = root_dir + '/resources/glove.twitter.27B/glove.twitter.27B.%dd.txt' % hyperparams_features['embedding_dim']\n",
    "# pretrained_embeddings_path = root_dir + '/resources/glove.840B/glove.840B.%dd.txt' % hyperparams_features['embedding_dim']\n",
    "pretrained_embeddings_path = hyperparams_features['embeddings_path']#root_dir + '/eRisk/finetuned_glove_clpsych_erisk_normalized_2_20000.pkl'\n",
    "embedding_matrix = load_embeddings(pretrained_embeddings_path, hyperparams_features['embedding_dim'], \n",
    "                                    voc=vocabulary_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.609859040531987e-05"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.mean()\n",
    "# hyperparams_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics():\n",
    "    def __init__(self, threshold=0.5):\n",
    "        self.threshold=threshold\n",
    "        \n",
    "    def recall_m(self, y_true, y_pred):\n",
    "            y_labels = y_true\n",
    "            y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), self.threshold), K.floatx())        \n",
    "            possible_positives = K.sum(K.round(K.clip(y_labels, 0, 1)))\n",
    "            true_positives = K.sum(K.round(K.clip(y_labels * y_pred, 0, 1)))\n",
    "            recall = true_positives / (possible_positives + K.epsilon())\n",
    "            return recall\n",
    "\n",
    "    def precision_m(self, y_true, y_pred):\n",
    "            y_labels = y_true\n",
    "            y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), self.threshold), K.floatx())        \n",
    "            true_positives = K.sum(K.round(K.clip(y_labels * y_pred, 0, 1)))\n",
    "            predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "            precision = true_positives / (predicted_positives + K.epsilon())\n",
    "            return precision\n",
    "\n",
    "    def f1_m(self, y_true, y_pred):\n",
    "        precision = self.precision_m(y_true, y_pred)\n",
    "        recall = self.recall_m(y_true, y_pred)\n",
    "        return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "    \n",
    "    def auc2(self, y_true, y_pred):\n",
    "        auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        return auc\n",
    "\n",
    "    def auc(self, y_true, y_pred):\n",
    "#         has_true_examples = K.greater(K.cast(K.sum(y_true), K.floatx()),0)\n",
    "#         has_false_examples = K.less(K.cast(K.mean(y_true), K.floatx()),1)\n",
    "#         score = tf.cond(tf.logical_and(has_true_examples, has_false_examples), \n",
    "#                         lambda:tf.py_function(roc_auc_score, (\n",
    "#                             K.cast(y_true, K.floatx()), \n",
    "#                             K.cast(y_pred, K.floatx())), tf.float32), \n",
    "#                         lambda:0.0)\n",
    "        return 0\n",
    "        \n",
    "def binary_crossentropy_custom(y_true, y_pred):\n",
    "    y_labels = y_true\n",
    "    return K.binary_crossentropy(y_labels, \n",
    "                                 y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fine_tune_layers=10,\n",
    "        pooling=\"first\",\n",
    "        trainable=True,\n",
    "        bert_path=\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\", \n",
    "        **kwargs\n",
    "    ):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = trainable\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_path = bert_path\n",
    "        if self.pooling not in [\"first\", \"mean\"]:\n",
    "            raise NameError(\n",
    "               \"Undefined pooling type (must be either first or mean, but is %s)\" % self.pooling\n",
    "            )\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def get_config(self):\n",
    "\n",
    "        config = super(BertLayer, self).get_config().copy()\n",
    "        config.update({\n",
    "            'n_fine_tune_layers': self.n_fine_tune_layers,\n",
    "            'trainable': self.trainable,\n",
    "            'output_size': self.output_size,\n",
    "            'pooling': self.pooling,\n",
    "            'bert_path': self.bert_path,\n",
    "        })\n",
    "\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_path, trainable=self.trainable, name=\"%s_module\" % self.name\n",
    "        )\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        if self.pooling == \"first\":\n",
    "            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "            trainable_layers = [\"pooler/dense\"]\n",
    "\n",
    "        elif self.pooling == \"mean\":\n",
    "            trainable_vars = [\n",
    "                var\n",
    "                for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "            ]\n",
    "            trainable_layers = []\n",
    "        else:\n",
    "            raise NameError(\n",
    "                \"Undefined pooling type (must be either first or mean, but is %s)\" % self.pooling\n",
    "            )\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(\"encoder/layer_%s\" % str(11 - i))\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        if self.pooling == \"first\":\n",
    "            pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"pooled_output\"\n",
    "            ]\n",
    "        elif self.pooling == \"mean\":\n",
    "            result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"sequence_output\"\n",
    "            ]\n",
    "\n",
    "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
    "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
    "            input_mask = tf.cast(input_mask, tf.float32)\n",
    "            pooled = masked_reduce_mean(result, input_mask)\n",
    "        else:\n",
    "            raise NameError(\"Undefined pooling type (must be either first or mean, but is %s)\" % self.pooling)\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopwords_list,\n",
    "                liwc_categories,\n",
    "               ignore_layer=[], classes=1):\n",
    "    def attention(xin):\n",
    "        return K.sum(xin, axis=1) \n",
    "    tokens_features = Input(shape=(hyperparams['maxlen'],), name='word_seq')\n",
    "    embedding_layer = Embedding(hyperparams_features['max_features'], \n",
    "                                hyperparams_features['embedding_dim'], \n",
    "                                input_length=hyperparams['maxlen'],\n",
    "                                embeddings_regularizer=regularizers.l2(hyperparams['l2_embeddings']),\n",
    "                                weights=[embedding_matrix], \n",
    "                                trainable=hyperparams['trainable_embeddings'],\n",
    "                               name='embeddings_layer')(\n",
    "        tokens_features)\n",
    "#     if 'batchnorm' not in ignore_layer:\n",
    "#         embedding_layer_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "#                                                      name='embeddings_layer_norm')(embedding_layer)\n",
    "    embedding_layer = Dropout(hyperparams['dropout'], name='embedding_dropout')(embedding_layer)\n",
    "\n",
    "    if 'lstm' not in hyperparams['ignore_layer']:\n",
    "        if False: #tf.test.is_gpu_available():\n",
    "            lstm_layers = Bidirectional(CuDNNLSTM(\n",
    "                            hyperparams['lstm_units'], \n",
    "                                    return_sequences='attention' not in hyperparams['ignore_layer'], # only True if using attention\n",
    "                          name='LSTM_layer'), name='bilstm')(embedding_layer)\n",
    "        else:\n",
    "            lstm_layers = Bidirectional(LSTM(hyperparams['lstm_units'], \n",
    "                               return_sequences='attention' not in ignore_layer,\n",
    "                          name='LSTM_layer'), name='bilstm')(embedding_layer)\n",
    "#             lstm_layers = LSTM(hyperparams['lstm_units'], \n",
    "#                                return_sequences='attention' not in hyperparams['ignore_layer'],\n",
    "#                           name='LSTM_layer')(embedding_layer)\n",
    "            \n",
    "    elif 'cnn' not in hyperparams['ignore_layer']:\n",
    "        cnn_layers = Conv1D(hyperparams['filters'],\n",
    "                             hyperparams['kernel_size'],\n",
    "                             padding='valid',\n",
    "                             activation='relu',\n",
    "                             strides=1, name='convolution')(embedding_layer)\n",
    "        # we use max pooling:\n",
    "        cnn_layers = GlobalMaxPooling1D()(cnn_layers)\n",
    "    \n",
    "    # Attention\n",
    "    if 'attention' not in hyperparams['ignore_layer']:\n",
    "        attention = Dense(1, activation='tanh', name='attention')(lstm_layers)\n",
    "        attention = Flatten()(attention)\n",
    "        attention = Activation('softmax')(attention)\n",
    "        attention = RepeatVector(hyperparams['lstm_units']*2)(attention)\n",
    "        attention = Permute([2, 1])(attention)\n",
    "\n",
    "        sent_representation = Multiply()([lstm_layers, attention])\n",
    "        sent_representation = Lambda(lambda xin: K.sum(xin, axis=1), \n",
    "                                     output_shape=(hyperparams['lstm_units'],)\n",
    "                                    )(sent_representation)\n",
    "#         sent_representation = Lambda(attention, \n",
    "#                                          output_shape=(hyperparams['lstm_units'],\n",
    "#                                         ))(sent_representation)\n",
    "\n",
    "        \n",
    "    elif 'lstm' not in hyperparams['ignore_layer']:\n",
    "        sent_representation = lstm_layers\n",
    "    elif 'cnn' not in hyperparams['ignore_layer']:\n",
    "        sent_representation = cnn_layers\n",
    "    else:\n",
    "        sent_representation = None\n",
    "        \n",
    "    if sent_representation is not None:\n",
    "        sent_representation = Dropout(hyperparams['dropout'], name='lstm_att_dropout')(sent_representation)\n",
    "        if hyperparams['dense_sentence_units']:\n",
    "            sent_representation = Dense(units=hyperparams['dense_sentence_units'], activation='relu',\n",
    "                                       name='dense_sent_representation')(sent_representation)\n",
    "    \n",
    "    # Other features\n",
    "    numerical_features = Input(shape=(len(emotions) + 1 + len(liwc_categories),), name='numeric_input') # emotions and pronouns\n",
    "    sparse_features = Input(shape=(len(stopwords_list),), name='sparse_input') # stopwords\n",
    "\n",
    "    dense_layer_sparse = Dense(units=hyperparams['dense_bow_units'],\n",
    "                              name='sparse_feat_dense_layer', activation='relu',\n",
    "                                kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                              )(sparse_features)\n",
    "    \n",
    "    # BERT encoder\n",
    "    if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "        in_id_bert = Input(shape=(hyperparams['maxlen'],), name=\"input_ids_bert\")\n",
    "        in_mask_bert = Input(shape=(hyperparams['maxlen'],), name=\"input_masks_bert\")\n",
    "        in_segment_bert = Input(shape=(hyperparams['maxlen'],), name=\"segment_ids_bert\")\n",
    "        bert_inputs = [in_id_bert, in_mask_bert, in_segment_bert]\n",
    "\n",
    "        bert_output = BertLayer(n_fine_tune_layers=hyperparams['bert_finetune_layers'], \n",
    "                                pooling=hyperparams['bert_pooling'],\n",
    "                               trainable=hyperparams['bert_trainable'],\n",
    "                               name='bert_layer')(bert_inputs)\n",
    "        dense_bert = Dense(hyperparams['bert_dense_units'], activation='relu',\n",
    "                           kernel_regularizer=regularizers.l2(hyperparams['l2_bert']),\n",
    "                          name='bert_dense_layer')(bert_output)\n",
    "    else:\n",
    "        dense_bert = None\n",
    "\n",
    "\n",
    "    \n",
    "    # Batch normalization\n",
    "    if 'batchnorm' not in hyperparams['ignore_layer']:\n",
    "        numerical_features_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                     name='numerical_features_norm')(numerical_features)\n",
    "        sent_representation_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                      name='sent_repr_norm')(sent_representation)\n",
    "        dense_layer_sparse_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                     name='sparse_features_norm')(dense_layer_sparse)\n",
    "        if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "            if hyperparams['bert_dense_units']:\n",
    "                dense_bert_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                         name='bert_layer_norm')(dense_bert)\n",
    "            else:\n",
    "                dense_bert_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                         name='bert_layer_norm')(bert_output)\n",
    "        else:\n",
    "            dense_bert_norm = None\n",
    "        \n",
    "#     subjects = Input(shape=(1,), name='subjects')\n",
    "    \n",
    "\n",
    "    all_layers = {\n",
    "        'user_encoded': sent_representation,\n",
    "        'numerical_dense_layer': numerical_features,\n",
    "        'sparse_feat_dense_layer': dense_layer_sparse,\n",
    "        'bert_layer': dense_bert if hyperparams['bert_dense_units'] else bert_output,\n",
    "    }\n",
    "    if 'batchnorm' not in hyperparams['ignore_layer']:\n",
    "        all_layers = {\n",
    "            'user_encoded': sent_representation_norm,\n",
    "            'numerical_dense_layer': numerical_features_norm,\n",
    "            'sparse_feat_dense_layer': dense_layer_sparse_norm,\n",
    "            'bert_layer': dense_bert_norm\n",
    "        }\n",
    "    layers_to_merge = []\n",
    "    for n, l in all_layers.items():\n",
    "        if n in hyperparams['ignore_layer'] or l is None:\n",
    "            continue\n",
    "        layers_to_merge.append(l)\n",
    "        \n",
    "    if len(layers_to_merge) == 1:\n",
    "        merged_layers = layers_to_merge[0]\n",
    "    else:\n",
    "        merged_layers = concatenate(layers_to_merge)\n",
    "    output_layer = Dense(classes, activation='sigmoid' if classes==1 else 'softmax',\n",
    "                         name='output_layer',\n",
    "                        kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                        )(merged_layers)\n",
    "\n",
    "    # Compile model\n",
    "    if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "        inputs=[tokens_features, numerical_features, sparse_features, \n",
    "                              in_id_bert, in_mask_bert, in_segment_bert,\n",
    "    #                           subjects\n",
    "                             ]\n",
    "    else:\n",
    "        inputs=[tokens_features, numerical_features, sparse_features]\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "        \n",
    "\n",
    "    metrics_class = Metrics(threshold=hyperparams['threshold'])\n",
    "    model.compile(hyperparams['optimizer'], binary_crossentropy_custom,\n",
    "                  metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m,\n",
    "                          metrics_class.auc, \n",
    "                           tf.keras.metrics.AUC(name=\"AUC_keras\"), \n",
    "                           tf.keras.metrics.FalsePositives(), \n",
    "                           tf.keras.metrics.FalseNegatives()])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tl_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopwords_list,\n",
    "                liwc_categories,\n",
    "               ignore_layer=[]):\n",
    "    def attention(xin):\n",
    "        return K.sum(xin, axis=1) \n",
    "    tokens_features = Input(shape=(hyperparams['maxlen'],), name='word_seq')\n",
    "    embedding_layer = Embedding(hyperparams_features['max_features'], \n",
    "                                hyperparams_features['embedding_dim'], \n",
    "                                input_length=hyperparams['maxlen'],\n",
    "                                embeddings_regularizer=regularizers.l2(hyperparams['l2_embeddings']),\n",
    "                                weights=[embedding_matrix], \n",
    "                                trainable=False,\n",
    "                               name='embeddings_layer')(\n",
    "        tokens_features)\n",
    "#     if 'batchnorm' not in ignore_layer:\n",
    "#         embedding_layer_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "#                                                      name='embeddings_layer_norm')(embedding_layer)\n",
    "    embedding_layer = Dropout(hyperparams['dropout'], name='embedding_dropout')(embedding_layer)\n",
    "\n",
    "    if 'lstm' not in ignore_layer:\n",
    "        if False: #tf.test.is_gpu_available():\n",
    "            lstm_layers = Bidirectional(CuDNNLSTM(hyperparams['lstm_units'], trainable=False,\n",
    "                                    return_sequences='attention' not in ignore_layer, # only True if using attention\n",
    "                          name='LSTM_layer'), name='bilstm')(embedding_layer)\n",
    "        else:\n",
    "            lstm_layers = Bidirectional(LSTM(hyperparams['lstm_units'], trainable=False,\n",
    "                               return_sequences='attention' not in ignore_layer,\n",
    "                          name='LSTM_layer'), name='bilstm')(embedding_layer)\n",
    "            \n",
    "    elif 'cnn' not in ignore_layer:\n",
    "        cnn_layers = Conv1D(hyperparams['filters'],\n",
    "                             hyperparams['kernel_size'],\n",
    "                             padding='valid',\n",
    "                             activation='relu',\n",
    "                             strides=1,\n",
    "                           trainable=False,\n",
    "                           name='convolution')(embedding_layer)\n",
    "        # we use max pooling:\n",
    "        cnn_layers = GlobalMaxPooling1D(trainable=False)(cnn_layers)\n",
    "    \n",
    "    # Attention\n",
    "    if 'attention' not in ignore_layer:\n",
    "        attention = Dense(1, activation='tanh', name='attention', trainable=False)(lstm_layers)\n",
    "        attention = Flatten()(attention)\n",
    "        attention = Activation('softmax')(attention)\n",
    "        attention = RepeatVector(hyperparams['lstm_units']*2)(attention)\n",
    "        attention = Permute([2, 1])(attention)\n",
    "\n",
    "        sent_representation = Multiply()([lstm_layers, attention])\n",
    "        sent_representation = Lambda(lambda xin: K.sum(xin, axis=1), \n",
    "                                     output_shape=(hyperparams['lstm_units'],)\n",
    "                                    )(sent_representation)\n",
    "#         sent_representation = Lambda(attention, \n",
    "#                                          output_shape=(hyperparams['lstm_units'],\n",
    "#                                         ))(sent_representation)\n",
    "\n",
    "        \n",
    "    elif 'lstm' not in ignore_layer:\n",
    "        sent_representation = lstm_layers\n",
    "    elif 'cnn' not in ignore_layer:\n",
    "        sent_representation = cnn_layers\n",
    "        \n",
    "    \n",
    "    sent_representation = Dropout(hyperparams['dropout'], name='lstm_att_dropout')(sent_representation)\n",
    "    if hyperparams['dense_sentence_units']:\n",
    "        sent_representation = Dense(units=hyperparams['dense_sentence_units'], activation='relu',\n",
    "                                   name='dense_sent_representation',\n",
    "                                   trainable=False)(sent_representation)\n",
    "    \n",
    "    # Other features\n",
    "    numerical_features = Input(shape=(len(emotions) + 1 + len(liwc_categories),), name='numeric_input') # emotions and pronouns\n",
    "    sparse_features = Input(shape=(len(stopwords_list),), name='sparse_input') # stopwords\n",
    "\n",
    "    dense_layer_sparse = Dense(units=hyperparams['dense_bow_units'],\n",
    "                              name='sparse_feat_dense_layer', activation='relu',\n",
    "                                kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                               trainable=False,\n",
    "                              )(sparse_features)\n",
    "    \n",
    "    # BERT encoder\n",
    "    if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "        in_id_bert = Input(shape=(hyperparams['maxlen'],), name=\"input_ids_bert\")\n",
    "        in_mask_bert = Input(shape=(hyperparams['maxlen'],), name=\"input_masks_bert\")\n",
    "        in_segment_bert = Input(shape=(hyperparams['maxlen'],), name=\"segment_ids_bert\")\n",
    "        bert_inputs = [in_id_bert, in_mask_bert, in_segment_bert]\n",
    "\n",
    "        bert_output = BertLayer(n_fine_tune_layers=hyperparams['bert_finetune_layers'], \n",
    "                                pooling=hyperparams['bert_pooling'],\n",
    "                               trainable=False,\n",
    "                               name='bert_layer')(bert_inputs)\n",
    "        dense_bert = Dense(hyperparams['bert_dense_units'], activation='relu',\n",
    "                           kernel_regularizer=regularizers.l2(hyperparams['l2_bert']),\n",
    "                          name='bert_dense_layer',\n",
    "                          trainable=False)(bert_output)\n",
    "    else:\n",
    "        dense_bert = None\n",
    "\n",
    "\n",
    "    \n",
    "    # Batch normalization\n",
    "    if 'batchnorm' not in ignore_layer:\n",
    "        numerical_features_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                     name='numerical_features_norm')(numerical_features)\n",
    "        sent_representation_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                      name='sent_repr_norm')(sent_representation)\n",
    "        dense_layer_sparse_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                     name='sparse_features_norm')(dense_layer_sparse)\n",
    "        if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "            if hyperparams['bert_dense_units']:\n",
    "                dense_bert_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                         name='bert_layer_norm')(dense_bert)\n",
    "            else:\n",
    "                dense_bert_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                         name='bert_layer_norm')(bert_output)\n",
    "        else:\n",
    "            dense_bert_norm = None\n",
    "        \n",
    "#     subjects = Input(shape=(1,), name='subjects')\n",
    "    \n",
    "\n",
    "    all_layers = {\n",
    "        'user_encoded': sent_representation,\n",
    "        'numerical_dense_layer': numerical_features,\n",
    "        'sparse_feat_dense_layer': dense_layer_sparse,\n",
    "        'bert_layer': dense_bert if hyperparams['bert_dense_units'] else bert_output,\n",
    "    }\n",
    "    if 'batchnorm' not in ignore_layer:\n",
    "        all_layers = {\n",
    "            'user_encoded': sent_representation_norm,\n",
    "            'numerical_dense_layer': numerical_features_norm,\n",
    "            'sparse_feat_dense_layer': dense_layer_sparse_norm,\n",
    "            'bert_layer': dense_bert_norm\n",
    "        }\n",
    "    layers_to_merge = []\n",
    "    for n, l in all_layers.items():\n",
    "        if n in ignore_layer:\n",
    "            continue\n",
    "        layers_to_merge.append(l)\n",
    "        \n",
    "    if len(layers_to_merge) == 1:\n",
    "        merged_layers = layers_to_merge[0]\n",
    "    else:\n",
    "        merged_layers = concatenate(layers_to_merge)\n",
    "        \n",
    "    TL_layer = Dense(hyperparams['transfer_units'], activation='sigmoid',\n",
    "                         name='tl_layer',\n",
    "                        kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                         trainable=True\n",
    "                        )(merged_layers)\n",
    "    output_layer = Dense(1, activation='sigmoid',\n",
    "                         name='output_layer_tl',\n",
    "                        kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                         trainable=True,\n",
    "                        )(TL_layer)\n",
    "\n",
    "    # Compile model\n",
    "    if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "        inputs=[tokens_features, numerical_features, sparse_features, \n",
    "                              in_id_bert, in_mask_bert, in_segment_bert,\n",
    "    #                           subjects\n",
    "                             ]\n",
    "    else:\n",
    "        inputs=[tokens_features, numerical_features, sparse_features]\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    if classes==1:\n",
    "        metrics_class = Metrics(threshold=hyperparams['threshold'])\n",
    "        model.compile(hyperparams['optimizer'], binary_crossentropy_custom,\n",
    "                      metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m,\n",
    "                              metrics_class.auc, \n",
    "                               tf.keras.metrics.AUC(name=\"AUC_keras\"), \n",
    "                               tf.keras.metrics.FalsePositives(), \n",
    "                               tf.keras.metrics.FalseNegatives()])\n",
    "    else:\n",
    "        model.compile(hyperparams['optimizer'], K.categorical_crossentropy,\n",
    "                     metrics=[tf.keras.metrics.CategoricalAccuracy(name='cat_acc'),\n",
    "#                              tf.keras.metrics.Precision(name='prec'), tf.keras.metrics.Recall(name='rec'), \n",
    "                              ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hierarchical_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopwords_list,\n",
    "                liwc_categories,\n",
    "               ignore_layer=[], activations=None, classes=1):\n",
    "    def attention(xin):\n",
    "        return K.sum(xin, axis=1) \n",
    "\n",
    "\n",
    "    # Post/sentence representation - word sequence\n",
    "    tokens_features = Input(shape=(hyperparams['maxlen'],), name='word_seq')\n",
    "    embedding_layer = Embedding(hyperparams_features['max_features'], \n",
    "                                hyperparams_features['embedding_dim'], \n",
    "                                input_length=hyperparams['maxlen'],\n",
    "                                embeddings_regularizer=regularizers.l2(hyperparams['l2_embeddings']),\n",
    "                                weights=[embedding_matrix], \n",
    "                                trainable=hyperparams['trainable_embeddings'],\n",
    "                               name='embeddings_layer')(\n",
    "        tokens_features)\n",
    "    embedding_layer = Dropout(hyperparams['dropout'], name='embedding_dropout')(embedding_layer)\n",
    "\n",
    "    \n",
    "    if 'lstm' not in ignore_layer:\n",
    "        if False: #tf.test.is_gpu_available():\n",
    "            lstm_layers = CuDNNLSTM(hyperparams['lstm_units'], \n",
    "                                    return_sequences='attention' not in ignore_layer, # only True if using attention\n",
    "                          name='LSTM_layer')(embedding_layer)\n",
    "        else:\n",
    "            lstm_layers = LSTM(hyperparams['lstm_units'], \n",
    "                               return_sequences='attention' not in ignore_layer,\n",
    "                          name='LSTM_layer')(embedding_layer)\n",
    "\n",
    "        # Attention\n",
    "        if 'attention' not in ignore_layer:\n",
    "            attention_layer = Dense(1, activation='tanh', name='attention')\n",
    "            attention = attention_layer(lstm_layers)\n",
    "            attention = Flatten()(attention)\n",
    "            attention_output = Activation('softmax')(attention)\n",
    "            attention = RepeatVector(hyperparams['lstm_units'])(attention_output)\n",
    "            attention = Permute([2, 1])(attention)\n",
    "\n",
    "            sent_representation = Multiply()([lstm_layers, attention])\n",
    "            sent_representation = Lambda(lambda xin: K.sum(xin, axis=1), \n",
    "                                     output_shape=(hyperparams['lstm_units'],)\n",
    "                                    )(sent_representation)\n",
    "\n",
    "#             sent_representation = Lambda(attention, \n",
    "#                                          output_shape=(hyperparams['lstm_units'],\n",
    "#                                         ))(sent_representation)\n",
    "        else:\n",
    "            sent_representation = lstm_layers\n",
    "\n",
    "    elif 'cnn' not in ignore_layer:\n",
    "        cnn_layers = Conv1D(hyperparams['filters'],\n",
    "                             hyperparams['kernel_size'],\n",
    "                             padding='valid',\n",
    "                             activation='relu',\n",
    "                             strides=1)(embedding_layer)\n",
    "        # we use max pooling:\n",
    "        sent_representation = GlobalMaxPooling1D()(cnn_layers)\n",
    "    \n",
    "    \n",
    "    if 'batchnorm' not in ignore_layer:\n",
    "        sent_representation = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                          name='sent_repr_norm')(sent_representation)\n",
    "    sent_representation = Dropout(hyperparams['dropout'], name='sent_repr_dropout')(sent_representation)\n",
    "\n",
    "            # Other features \n",
    "    numerical_features_history = Input(shape=(\n",
    "            hyperparams['posts_per_group'],\n",
    "            len(emotions) + 1 + len(liwc_categories)\n",
    "        ), name='numeric_input_hist') # emotions and pronouns\n",
    "    sparse_features_history = Input(shape=(\n",
    "            hyperparams['posts_per_group'],\n",
    "            len(stopwords_list)\n",
    "        ), name='sparse_input_hist') # stopwords\n",
    "\n",
    "\n",
    "\n",
    "    if activations == 'attention':\n",
    "#         sent_representation = Flatten()(attention_layer.output)\n",
    "        sent_representation = attention_output\n",
    "\n",
    "\n",
    "    posts_history_input = Input(shape=(hyperparams['posts_per_group'], \n",
    "                                     hyperparams['maxlen']\n",
    "                                          ), name='hierarchical_word_seq_input')\n",
    "\n",
    "    # Hierarchy\n",
    "    sentEncoder = Model(inputs=tokens_features, \n",
    "                        outputs=sent_representation)\n",
    "    sentEncoder.summary()\n",
    "\n",
    "    user_encoder = TimeDistributed(sentEncoder, name='user_encoder')(posts_history_input)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    if activations != 'attention':\n",
    "        \n",
    "        \n",
    "        # BERT encoder\n",
    "        if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "            in_id_bert = Input(shape=(hyperparams['maxlen'],), name=\"input_ids_bert\")\n",
    "            in_mask_bert = Input(shape=(hyperparams['maxlen'],), name=\"input_masks_bert\")\n",
    "            in_segment_bert = Input(shape=(hyperparams['maxlen'],), name=\"segment_ids_bert\")\n",
    "            bert_inputs = [in_id_bert, in_mask_bert, in_segment_bert]\n",
    "\n",
    "            bert_output = BertLayer(n_fine_tune_layers=hyperparams['bert_finetune_layers'], \n",
    "                                    pooling=hyperparams['bert_pooling'],\n",
    "                                   trainable=hyperparams['bert_trainable'],\n",
    "                                   name='bert_layer')(bert_inputs)\n",
    "            dense_bert = Dense(hyperparams['bert_dense_units'], \n",
    "                               activation='relu',\n",
    "                              kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                              name='bert_dense_layer')(bert_output)\n",
    "\n",
    "            bertSentEncoder = Model(bert_inputs, dense_bert)\n",
    "\n",
    "\n",
    "            in_id_bert_history = Input(shape=(hyperparams['posts_per_group'],\n",
    "                                                              hyperparams['maxlen'],), name=\"input_ids_bert_hist\")\n",
    "            in_mask_bert_history = Input(shape=(hyperparams['posts_per_group'],\n",
    "                                                                hyperparams['maxlen'],), name=\"input_masks_bert_hist\")\n",
    "            in_segment_bert_history = Input(shape=(hyperparams['posts_per_group'],\n",
    "                                                                   hyperparams['maxlen'],), name=\"segment_ids_bert_hist\")\n",
    "            bert_inputs_history = [in_id_bert_history, in_mask_bert_history, in_segment_bert_history]\n",
    "            bert_inputs_concatenated = concatenate(bert_inputs_history)\n",
    "            inputs_indices = [hyperparams['maxlen']*i for i in range(3)]\n",
    "            # slice the input in equal slices on the last dimension\n",
    "            bert_encoder_layer = TimeDistributed(Lambda(lambda x: bertSentEncoder([x[:,inputs_indices[0]:inputs_indices[1]], \n",
    "                                                                          x[:,inputs_indices[1]:inputs_indices[2]],\n",
    "                                                                                  x[:,inputs_indices[2]:]])),\n",
    "                                                name='bert_distributed_layer')(\n",
    "                                bert_inputs_concatenated)\n",
    "            bertUserEncoder = Model(bert_inputs_history, bert_encoder_layer)\n",
    "            bertUserEncoder.summary()\n",
    "\n",
    "            bert_user_encoder = bertUserEncoder(bert_inputs_history)\n",
    "        else:\n",
    "            bert_user_encoder = None\n",
    "\n",
    "\n",
    "        dense_layer_sparse = Dense(units=hyperparams['dense_bow_units'],\n",
    "                                  name='sparse_feat_dense_layer', activation='relu',\n",
    "                                    kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                                  )\n",
    "        dense_layer_sparse_user = TimeDistributed(dense_layer_sparse,\n",
    "                                                 name='sparse_dense_layer_user')(sparse_features_history)\n",
    "\n",
    "\n",
    "        # Concatenate features\n",
    "        if 'batchnorm' not in ignore_layer:\n",
    "            numerical_features_history_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                         name='numerical_features_norm')(numerical_features_history)\n",
    "            dense_layer_sparse_user = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                         name='sparse_features_norm')(dense_layer_sparse_user)\n",
    "        all_layers = {\n",
    "            'user_encoded': user_encoder,\n",
    "            'bert_layer': bert_user_encoder,\n",
    "            'numerical_dense_layer': numerical_features_history if 'batchnorm' in ignore_layer \\\n",
    "                        else numerical_features_history_norm,\n",
    "            'sparse_feat_dense_layer': dense_layer_sparse_user,\n",
    "        }\n",
    "\n",
    "        layers_to_merge = [l for n,l in all_layers.items() if n not in ignore_layer]\n",
    "        if len(layers_to_merge) == 1:\n",
    "            merged_layers = layers_to_merge[0]\n",
    "        else:\n",
    "            merged_layers = concatenate(layers_to_merge)\n",
    "\n",
    "        if 'lstm_user' not in ignore_layer:\n",
    "\n",
    "            if False:#tf.test.is_gpu_available():\n",
    "                lstm_user_layers = CuDNNLSTM(hyperparams['lstm_units_user'], \n",
    "                                        return_sequences='attention_user' not in ignore_layer, # only True if using attention\n",
    "                              name='LSTM_layer_user')(merged_layers)\n",
    "            else:\n",
    "                lstm_user_layers = LSTM(hyperparams['lstm_units_user'], \n",
    "                                   return_sequences='attention_user' not in ignore_layer,\n",
    "                              name='LSTM_layer_user')(merged_layers)\n",
    "\n",
    "            # Attention\n",
    "            if 'attention_user' not in ignore_layer:\n",
    "                attention_user_layer = Dense(1, activation='tanh', name='attention_user')\n",
    "                attention_user = attention_user_layer(lstm_user_layers)\n",
    "                attention_user = Flatten()(attention_user)\n",
    "                attention_user_output = Activation('softmax')(attention_user)\n",
    "                attention_user = RepeatVector(hyperparams['lstm_units_user'])(attention_user_output)\n",
    "                attention_user = Permute([2, 1])(attention_user)\n",
    "\n",
    "                user_representation = Multiply()([lstm_user_layers, attention_user])\n",
    "                user_representation = Lambda(lambda xin: K.sum(xin, axis=1), \n",
    "                                             output_shape=(hyperparams['lstm_units_user'],))(user_representation)\n",
    "    #             user_representation = Lambda(attention, \n",
    "    #                                          output_shape=(hyperparams['lstm_units_user'],\n",
    "    #                                         ))(user_representation)\n",
    "            else:\n",
    "                user_representation = lstm_user_layers\n",
    "\n",
    "\n",
    "        elif 'cnn_user' not in ignore_layer:\n",
    "            cnn_layers_user = Conv1D(hyperparams['filters_user'],\n",
    "                                 hyperparams['kernel_size_user'],\n",
    "                                 padding='valid',\n",
    "                                 activation='relu',\n",
    "                                 strides=1)(merged_layers)\n",
    "            # we use max pooling:\n",
    "            user_representation = GlobalMaxPooling1D()(cnn_layers_user)\n",
    "    #         user_representation = Flatten()(user_representation)\n",
    "\n",
    "\n",
    "        user_representation = Dropout(hyperparams['dropout'], name='user_repr_dropout')(user_representation)\n",
    "\n",
    "\n",
    "        if hyperparams['dense_user_units']:\n",
    "            user_representation = Dense(units=hyperparams['dense_user_units'], activation='relu',\n",
    "                                       name='dense_user_representation')(user_representation)\n",
    "\n",
    "        output_layer = Dense(classes, activation='sigmoid' if classes==1 else 'softmax',\n",
    "                             name='output_layer',\n",
    "                            kernel_regularizer=regularizers.l2(hyperparams['l2_dense'])\n",
    "                            )(user_representation)\n",
    "\n",
    "    # Compile model\n",
    "\n",
    "#     elif activations == 'attention':\n",
    "#         outputs = attention_layer.output\n",
    "    if activations == 'attention':\n",
    "        outputs = user_encoder\n",
    "\n",
    "        \n",
    "    elif activations == 'attention_user':\n",
    "#         outputs = attention_user.output\n",
    "        outputs = attention_user_output\n",
    "\n",
    "    \n",
    "    else:\n",
    "        outputs = output_layer\n",
    "    if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "\n",
    "        hierarchical_model = Model(inputs=[posts_history_input, \n",
    "                                       numerical_features_history, sparse_features_history,\n",
    "                                      in_id_bert_history, in_mask_bert_history, in_segment_bert_history], \n",
    "                  outputs=outputs)\n",
    "    else:\n",
    "        hierarchical_model = Model(inputs=[posts_history_input, \n",
    "                                       numerical_features_history, sparse_features_history,\n",
    "                                      ], \n",
    "                  outputs=outputs)\n",
    "    hierarchical_model.summary()\n",
    "    \n",
    "    if classes==1:\n",
    "        metrics_class = Metrics(threshold=hyperparams['threshold'])\n",
    "        hierarchical_model.compile(hyperparams['optimizer'], binary_crossentropy_custom,\n",
    "                      metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m,\n",
    "                              metrics_class.auc])\n",
    "    else:\n",
    "        \n",
    "        hierarchical_model.compile(hyperparams['optimizer'], K.categorical_crossentropy,\n",
    "                     metrics=[ tf.keras.metrics.CategoricalAccuracy(name='cat_acc'),\n",
    "#                              tf.keras.metrics.Precision(name='prec'), tf.keras.metrics.Recall(name='rec'), \n",
    "                              ])\n",
    "    return hierarchical_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_tl_model(pretrained_model, hyperparams, units=100,  emotions=emotions, stopwords_list=stopword_list,\n",
    "#                 liwc_categories=liwc_categories):\n",
    "#     tokens_features = Input(shape=(hyperparams['maxlen'],), name='word_seq_tl')\n",
    "#     numerical_features = Input(shape=(len(emotions) + 1 + len(liwc_categories),), name='numeric_input_tl') # emotions and pronouns\n",
    "#     sparse_features = Input(shape=(len(stopwords_list),), name='sparse_input_tl') # stopwords\n",
    "#     pretrained_model.summary()\n",
    "#     print(\"inputs\", tokens_features, numerical_features, sparse_features)\n",
    "#     pretrained_output = pretrained_model(inputs=[tokens_features, numerical_features, sparse_features])\n",
    "#     # TODO: set trainable to false\n",
    "#     tl_layer = Dense(units=units, activation='relu',\n",
    "#                                    name='tl_layer', trainable=False,\n",
    "#                         kernel_regularizer=regularizers.l2(hyperparams['l2_dense'])\n",
    "#                     )(pretrained_output)\n",
    "#     output_layer = Dense(1, activation='sigmoid',\n",
    "#                          name='tl_output_layer',\n",
    "#                         kernel_regularizer=regularizers.l2(hyperparams['l2_dense'])\n",
    "#                         )(tl_layer)\n",
    "# #     pretrained_model.outputs = output_layer ## I DUNNO\n",
    "#     tl_model = Model(inputs=[tokens_features, numerical_features, sparse_features], \n",
    "#           outputs=output_layer)\n",
    "#     metrics_class = Metrics(threshold=hyperparams['threshold'])\n",
    "#     tl_model.compile(hyperparams['optimizer'], binary_crossentropy_custom,\n",
    "#                   metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m,\n",
    "#                           metrics_class.auc, \n",
    "#                            tf.keras.metrics.AUC(name=\"AUC_keras\"), \n",
    "#                            tf.keras.metrics.FalsePositives(), \n",
    "#                            tf.keras.metrics.FalseNegatives()])\n",
    "\n",
    "#     return tl_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopword_list,\n",
    "#                     liwc_categories=[c for c in categories if c in writings_df.columns]\n",
    "# ,\n",
    "#                    ignore_layer=hyperparams['ignore_layer'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchical_model = build_hierarchical_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopword_list,\n",
    "#                     liwc_categories=[c for c in categories if c in writings_df.columns]\n",
    "# ,\n",
    "#                    ignore_layer=hyperparams['ignore_layer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(model, 'models/sequential_bert_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize_vars(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightsHistory(callbacks.Callback):\n",
    "    def __init__(self, logs={}):\n",
    "        super(WeightsHistory, self).__init__()\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.log_weights(0)\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % 10 == 0:\n",
    "            self.log_weights(epoch)\n",
    "    def log_weights(self, step):\n",
    "        for layer in self.model.layers:\n",
    "            try:\n",
    "                experiment.log_histogram_3d(layer.get_weights()[0], \n",
    "                                            name=layer.name + \"_weight\", step=step)\n",
    "            except Exception as e:\n",
    "#                 logger.debug(\"Logging weights error: \" + layer.name + \"; \" + str(e) + \"\\n\")\n",
    "                # Layer probably does not exist\n",
    "                pass\n",
    "\n",
    "class OutputsHistory(callbacks.Callback):\n",
    "    def __init__(self, logs={}, generator=None, generator_type=\"\"):\n",
    "        super(OutputsHistory, self).__init__()\n",
    "        self.generator_type = generator_type\n",
    "        if generator:\n",
    "            self.generator = generator\n",
    "        elif generator_type:\n",
    "            self.generator = DataGenerator(user_level_data, subjects_split, \n",
    "                                     set_type=generator_type, \n",
    "                                   hierarchical=hyperparams['hierarchical'],\n",
    "                                seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
    "                                     max_posts_per_user=None,\n",
    "                                   pad_with_duplication=False,\n",
    "                                    posts_per_group=hyperparams['posts_per_group'],\n",
    "                                    post_groups_per_user=None, \n",
    "                                     sample_seqs=False, shuffle=False)\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.log_outputs(0)\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % 10 == 0:\n",
    "            self.log_outputs(epoch)\n",
    "    def log_outputs(self, step):\n",
    "        try:\n",
    "            experiment.log_histogram_3d(self.model.predict(self.generator,  verbose=1, steps=2),\n",
    "                                        name='output_%s' % self.generator_type, step=step)\n",
    "        except Exception as e:\n",
    "            logger.debug(\"Logging outputs error: \" + str(e) + \"\\n\")\n",
    "#                 Layer probably does not exist\n",
    "            pass\n",
    "\n",
    "class ActivationsAttention(callbacks.Callback):\n",
    "    def __init__(self, logs={}, generator=None, generator_type=\"\"):\n",
    "        super(ActivationsAttention, self).__init__()\n",
    "        self.generator_type = generator_type\n",
    "        if generator:\n",
    "            self.generator = generator\n",
    "        elif generator_type:\n",
    "            self.generator = DataGenerator(user_level_data, subjects_split, \n",
    "                                     set_type=generator_type, \n",
    "                                   hierarchical=hyperparams['hierarchical'],\n",
    "                                seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
    "                                     max_posts_per_user=None,\n",
    "                                   pad_with_duplication=False,\n",
    "                                    posts_per_group=hyperparams['posts_per_group'],\n",
    "                                    post_groups_per_user=None, \n",
    "                                     sample_seqs=False, shuffle=False)\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.log_outputs(0)\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % 10 == 0:\n",
    "            self.log_outputs(epoch)\n",
    "    def log_outputs(self, step):\n",
    "        try:\n",
    "            experiment.log_histogram_3d(self.model.get_layer('attention_user').output.eval())\n",
    "        except Exception as e:\n",
    "            logger.debug(\"Logging activations error: \" + str(e) + \"\\n\")\n",
    "            pass\n",
    "\n",
    "class LRHistory(callbacks.Callback):\n",
    "    def __init__(self, logs={}):\n",
    "        super(LRHistory, self).__init__()\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.log_lr()\n",
    "        \n",
    "    def log_lr(self):\n",
    "        lr = K.eval(self.model.optimizer.lr)\n",
    "        logger.debug(\"Learning rate is %f...\\n\" % lr)\n",
    "        experiment.log_parameter('lr', lr)\n",
    "\n",
    "class FreezeLayer(callbacks.Callback):\n",
    "    def __init__(self, logs={}, patience=5, layer={'user_encoder':'embeddings_layer'}, verbose=1, set_to=False):\n",
    "        super(FreezeLayer, self).__init__()\n",
    "        self.freeze_epoch = patience\n",
    "        self.freeze_layer = layer\n",
    "        self.verbose = verbose\n",
    "        self.set_to = set_to\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        if type(self.freeze_layer)==dict:\n",
    "            submodel = self.model.get_layer(list(self.freeze_layer.keys())[0])\n",
    "        else:\n",
    "            submodel = self.model\n",
    "        logging.debug(\"Trainable embeddings\", submodel.get_layer(self.freeze_layer).trainable)\n",
    "        if epoch == self.freeze_epoch:\n",
    "            try:\n",
    "                layer = submodel.get_layer(self.freeze_layer)\n",
    "                old_value = layer.trainable\n",
    "                layer.trainable = self.set_to\n",
    "                # TODO: does this reset the optimizer? should I also compile the top-level model?\n",
    "                self.model.compile(hyperparams['optimizer'], binary_crossentropy_custom,\n",
    "                  metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m])\n",
    "                if self.verbose:\n",
    "                    logging.debug(\"Setting %s layer from %s to trainable=%s...\\n\" % (layer.name, old_value,\n",
    "                                                                   submodel.get_layer(self.freeze_layer).trainable))\n",
    "            except Exception as e:\n",
    "                # layer probably does not exist\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, hyperparams,\n",
    "                data_generator_train, data_generator_valid,\n",
    "                epochs, class_weight, start_epoch=0, workers=4,\n",
    "                callback_list = [],\n",
    "                \n",
    "                model_path='/tmp/model',\n",
    "                validation_set='valid',\n",
    "               verbose=1):\n",
    "    \n",
    "    logger.info(\"Initializing callbacks...\\n\")\n",
    "    # Initialize callbacks\n",
    "    freeze_layer = FreezeLayer(patience=hyperparams['freeze_patience'], set_to=not hyperparams['trainable_embeddings'])\n",
    "    weights_history = WeightsHistory()\n",
    "    outputs_history_valid = OutputsHistory(generator_type=validation_set)\n",
    "    outputs_history_train = OutputsHistory(generator_type='train')\n",
    "    activations_history_train = ActivationsAttention(generator_type='train')\n",
    "    lr_history = LRHistory()\n",
    "\n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=hyperparams['reduce_lr_factor'],\n",
    "                              patience=hyperparams['reduce_lr_patience'], min_lr=0.000001, verbose=1)\n",
    "    lr_schedule = callbacks.LearningRateScheduler(lambda epoch, lr: \n",
    "                                                  lr if (epoch+1)%hyperparams['scheduled_reduce_lr_freq']!=0 else\n",
    "                                                  lr*hyperparams['scheduled_reduce_lr_factor'], verbose=1)\n",
    "    callbacks_dict = {'freeze_layer': freeze_layer, 'weights_history': weights_history,\n",
    "           'outputs_history_valid': outputs_history_valid, 'outputs_history_train': outputs_history_train,\n",
    "           'lr_history': lr_history,\n",
    "            'activations': activations_history_train,\n",
    "           'reduce_lr_plateau': reduce_lr,\n",
    "            'lr_schedule': lr_schedule}\n",
    "\n",
    "    \n",
    "    logging.info('Train...')\n",
    "\n",
    "\n",
    "    history = model.fit_generator(data_generator_train,\n",
    "                steps_per_epoch=100,\n",
    "              epochs=epochs, initial_epoch=start_epoch, \n",
    "              class_weight=class_weight,\n",
    "              validation_data=data_generator_valid,\n",
    "                        verbose=verbose,\n",
    "#               validation_split=0.3,\n",
    "                       workers=workers,\n",
    "            callbacks = [\n",
    "                callbacks.ModelCheckpoint(filepath='%s_best.h5' % model_path, verbose=1, \n",
    "                                          save_best_only=True, save_weights_only=True),\n",
    "#                 callbacks.EarlyStopping(patience=hyperparams['early_stopping_patience'],\n",
    "#                                        restore_best_weights=True)\n",
    "            ] + [\n",
    "                callbacks_dict[c] for c in [\n",
    "#                     'weights_history', \n",
    "#                     'outputs_history_valid', 'outputs_history_train', \n",
    "#                     'reduce_lr_plateau', \n",
    "#                     'lr_schedule', \n",
    "#                     'activations'\n",
    "                ]])\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network_type(hyperparams):\n",
    "    if 'lstm' in hyperparams['ignore_layer']:\n",
    "        network_type = 'cnn'\n",
    "    else:\n",
    "        network_type = 'lstm'\n",
    "    if 'user_encoded' in hyperparams['ignore_layer']:\n",
    "        if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "            network_type = 'bert'\n",
    "        else:\n",
    "            network_type = 'extfeatures'\n",
    "    if hyperparams['hierarchical']:\n",
    "        hierarch_type = 'hierarchical'\n",
    "    else:\n",
    "        hierarch_type = 'seq'\n",
    "    return network_type, hierarch_type\n",
    "\n",
    "def initialize_experiment(hyperparams, nrc_lexicon_path, emotions, pretrained_embeddings_path, \n",
    "                          dataset_type, transfer_type):\n",
    "\n",
    "    experiment = Experiment(api_key=\"eoBdVyznAhfg3bK9pZ58ZSXfv\",\n",
    "                            project_name=\"mental\", workspace=\"ananana\", disabled=False)\n",
    "\n",
    "    experiment.log_parameters(hyperparams_features)\n",
    "\n",
    "    experiment.log_parameter('emotion_lexicon', nrc_lexicon_path)\n",
    "    experiment.log_parameter('emotions', emotions)\n",
    "    experiment.log_parameter('embeddings_path', pretrained_embeddings_path)\n",
    "    experiment.log_parameter('dataset_type', dataset_type)\n",
    "    experiment.log_parameter('transfer_type', transfer_type)\n",
    "    experiment.add_tag(dataset_type)\n",
    "    experiment.log_parameters(hyperparams)\n",
    "    network_type, hierarch_type = get_network_type(hyperparams)\n",
    "    experiment.add_tag(network_type)\n",
    "    experiment.add_tag(hierarch_type)\n",
    "    \n",
    "    return experiment\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_datasets(user_level_data, subjects_split, hyperparams, hyperparams_features, \n",
    "                        validation_set, emotions, liwc_categories, session=None, classes=1):\n",
    "    liwc_words_for_categories = pickle.load(open(hyperparams_features['liwc_words_cached'], 'rb'))\n",
    "    data_generator_train = DataGenerator(user_level_data, subjects_split, set_type='train',\n",
    "                                        seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
    "                                        sample_seqs=hyperparams['sample_seqs'], sampling_distr=hyperparams['sampling_distr'],\n",
    "                                        posts_per_group=hyperparams['posts_per_group'], post_groups_per_user=hyperparams['post_groups_per_user'],\n",
    "                                        max_posts_per_user=hyperparams['posts_per_user'], \n",
    "                                         hierarchical=hyperparams['hierarchical'], \n",
    "                                         use_bert='bert_layer' not in hyperparams['ignore_layer'],\n",
    "                                         compute_liwc=True, liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                         emotions=emotions, liwc_categories=liwc_categories,\n",
    "                                        session=session, classes=classes)\n",
    "    data_generator_valid = DataGenerator(user_level_data, subjects_split, set_type=validation_set,\n",
    "                                        seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
    "                                        posts_per_group=hyperparams['posts_per_group'], \n",
    "                                         post_groups_per_user=1,#hyperparams['post_groups_per_user'],\n",
    "                                        max_posts_per_user=None, \n",
    "                                        sample_seqs=False, shuffle=False, hierarchical=hyperparams['hierarchical'],\n",
    "                                         use_bert='bert_layer' not in hyperparams['ignore_layer'],\n",
    "                                         compute_liwc=True, liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                        emotions=emotions, liwc_categories=liwc_categories,\n",
    "                                        session=session, classes=classes)\n",
    "\n",
    "    return data_generator_train, data_generator_valid\n",
    "\n",
    "def initialize_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopword_list,\n",
    "                    liwc_categories, session=None, transfer=False, classes=1):\n",
    "\n",
    "    logger.info(\"Initializing model...\\n\")\n",
    "    # Initialize model\n",
    "    if hyperparams['hierarchical']:\n",
    "        model = build_hierarchical_model(hyperparams, hyperparams_features, embedding_matrix, \n",
    "                                         emotions, stopword_list, liwc_categories,\n",
    "                       ignore_layer=hyperparams['ignore_layer'], classes=classes)\n",
    "    else:\n",
    "        model = build_model(hyperparams, hyperparams_features, embedding_matrix, \n",
    "                                        emotions, stopword_list, liwc_categories,\n",
    "                       ignore_layer=hyperparams['ignore_layer'], classes=classes)\n",
    "    if transfer:\n",
    "        model = build_tl_model(hyperparams, hyperparams_features, embedding_matrix, \n",
    "                                        emotions, stopword_list, liwc_categories,\n",
    "                       ignore_layer=hyperparams['ignore_layer'])\n",
    "        model.load_weights(hyperparams_features['pretrained_model_path'] + '_weights.h5', by_name=True)\n",
    "    # Needed just for bert\n",
    "    if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "        initialize_sess(session)                  \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(user_level_data, subjects_split, \n",
    "          hyperparams, hyperparams_features, \n",
    "          embedding_matrix, emotions, stopword_list, liwc_categories,\n",
    "          experiment, validation_set='valid',\n",
    "          version=0, epochs=50, start_epoch=0,\n",
    "         session=None, model=None, transfer=False,\n",
    "         classes=1):\n",
    "    network_type, hierarch_type = get_network_type(hyperparams)\n",
    "    model_path='models/%s_%s_%s%d' % (network_type, dataset_type, hierarch_type, version)\n",
    "    \n",
    "    # Ablate emotions or LIWC\n",
    "    if 'emotions' in hyperparams['ignore_layer']:\n",
    "        emotions = []\n",
    "    if 'LIWC' in hyperparams['ignore_layer']:\n",
    "        liwc_categories = []\n",
    "\n",
    "    logger.info(\"Initializing datasets...\\n\")\n",
    "    data_generator_train, data_generator_valid = initialize_datasets(user_level_data, subjects_split, \n",
    "                                                                     hyperparams,hyperparams_features,\n",
    "                                                                     validation_set=validation_set,\n",
    "                                                                     emotions=emotions, liwc_categories=liwc_categories, \n",
    "                                                                    session=session, classes=classes)\n",
    "    if not model:\n",
    "        if transfer:\n",
    "            logger.info(\"Initializing pretrained model...\\n\")\n",
    "        else:\n",
    "            logger.info(\"Initializing model...\\n\")\n",
    "        model = initialize_model(hyperparams, hyperparams_features, embedding_matrix, \n",
    "                                 emotions, stopword_list, liwc_categories, session=session, transfer=transfer,\n",
    "                                classes=classes)\n",
    "\n",
    "       \n",
    "    print(model_path)\n",
    "    logger.info(\"Training model...\\n\")\n",
    "    model, history = train_model(model, hyperparams,\n",
    "                                 data_generator_train, data_generator_valid,\n",
    "                       epochs=epochs, start_epoch=start_epoch,\n",
    "                      class_weight={0:1, 1:hyperparams['positive_class_weight']} if classes==1 else None,\n",
    "                      callback_list = [\n",
    "                          'weights_history',\n",
    "                          'lr_history',\n",
    "                          'outputs_history_valid',\n",
    "                          'outputs_history_train',\n",
    "                          'reduce_lr_plateau',\n",
    "                          'lr_schedule'\n",
    "                                      ],\n",
    "                      model_path=model_path, workers=4,\n",
    "                                validation_set=validation_set)\n",
    "    logger.info(\"Saving model...\\n\")\n",
    "    try:\n",
    "        save_model_and_params(model, model_path, hyperparams, hyperparams_features)\n",
    "        experiment.log_parameter(\"model_path\", model_path)\n",
    "    except:\n",
    "        logger.error(\"Could not save model.\\n\")\n",
    "\n",
    "    return model, history\n",
    "# except Exception as e:# tf.errors.ResourceExhaustedError:\n",
    "#     print(e)\n",
    "#     sess.close()\n",
    "#     sess = tf.Session(config=sess_config)\n",
    "#     initialize_vars(sess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'lstm_seq'\n",
    "# # non-duplicate key\n",
    "# while key in models_collection.keys():\n",
    "#     key=key + \"1\"\n",
    "# # models_collection[key] = cur_model\n",
    "# # session.close()\n",
    "# session = initialize_sess()\n",
    "# # all_sessions.append(session)\n",
    "# session_collection[key] = session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_collection = {}\n",
    "hyperparams_collection = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(liwc_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    # Network parmeters\n",
    "    \n",
    "    # Sequential + hierarchical layers\n",
    "    'trainable_embeddings': True,\n",
    "\n",
    "    'lstm_units': 128,\n",
    "#     'lstm_units': 256,\n",
    "    \n",
    "    'dense_bow_units': 20,\n",
    "    'dense_sentence_units': 0,\n",
    "    \n",
    "    # CNN\n",
    "    'filters': 100,\n",
    "    'kernel_size': 5,\n",
    "    \n",
    "    # Just hierarchical layers\n",
    "    'lstm_units_user': 32,\n",
    "    'dense_user_units': 0,\n",
    "    \n",
    "    'filters_user': 10,\n",
    "    'kernel_size_user': 3,\n",
    "        \n",
    "    # BERT layers\n",
    "    'bert_dense_units': 256,\n",
    "    'bert_finetune_layers': 0,\n",
    "    'bert_trainable': False ,\n",
    "    'bert_pooling': 'first', # mean, first\n",
    "    \n",
    "    'transfer_units': 20,\n",
    "\n",
    "    # Regularization etc\n",
    "    'dropout': 0.1,\n",
    "#     'dropout': 0,\n",
    "    'l2_dense': 0.00011,\n",
    "    'l2_embeddings': 0.0000001,\n",
    "    'l2_bert': 0.0001,\n",
    "    'norm_momentum': 0.1,\n",
    "    \n",
    "    'ignore_layer': [\n",
    "#         'lstm', 'attention', \n",
    "#         'cnn',\n",
    "#         'user_encoded',\n",
    "#         'lstm_user', 'attention_user', \n",
    "#                      'batchnorm',\n",
    "#                      'user_encoded', # remove LSTM/CNN\n",
    "                     'bert_layer',\n",
    "#                      'numerical_dense_layer', \n",
    "#         'sparse_feat_dense_layer', # remove extracted features\n",
    "#         'LIWC',\n",
    "#         'emotions'\n",
    "                    ],\n",
    "\n",
    "    # Learning parameters\n",
    "    'optimizer': None, #'adam',\n",
    "    'decay': 0.001,\n",
    "#     'lr': 0.01,\n",
    "#     'lr': 0.00005,#     'lr': 0.01,\n",
    "#     'lr': 0.01,#     'lr': 0.01,\n",
    "    'lr': 0.0005,\n",
    "    \"reduce_lr_factor\": 0.5,\n",
    "    \"reduce_lr_patience\": 55,\n",
    "    'scheduled_reduce_lr_freq': 95,\n",
    "    'scheduled_reduce_lr_factor': 0.5,\n",
    "    \"freeze_patience\": 2000,\n",
    "    'threshold': 0.5,\n",
    "    'early_stopping_patience': 20,\n",
    "    'positive_class_weight': 2,\n",
    "#     'positive_class_weight': 2,\n",
    "    \n",
    "    # Generator parameters\n",
    "    \n",
    "    # Note: average text length in eRisk: 300\n",
    "    #       average text length in CLPsych: 13\n",
    "#     \"maxlen\": 512,\n",
    "#     \"maxlen\": 128,\n",
    "#     \"maxlen\": 512,\n",
    "    \"maxlen\": 256,\n",
    "    \"posts_per_user\": None, # if you want to limit total nr of posts considered per user\n",
    "    \"post_groups_per_user\": None, # if you want a fixed number of post groups per user\n",
    "                                  # to even out user weights\n",
    "    \"posts_per_group\": 50, # how long are the \"batches\" of posts. maxlen/avglen~=posts_per_group\n",
    "    \"batch_size\": 32,\n",
    "    \"padding\": \"pre\",\n",
    "    \"hierarchical\": True,\n",
    "    'sample_seqs': False,\n",
    "    'sampling_distr': 'exp',\n",
    "\n",
    "}\n",
    "if not hyperparams['optimizer']:\n",
    "    hyperparams['optimizer'] = optimizers.Adam(lr=hyperparams['lr'], #beta_1=0.9, beta_2=0.999, epsilon=0.0001,\n",
    "                                   decay=hyperparams['decay'])\n",
    "    \n",
    "if transfer_type:\n",
    "    hyperparams, _ = load_params(hyperparams_features['pretrained_model_path'])\n",
    "    if 'optimizer' not in hyperparams:\n",
    "        hyperparams['optimizer'] = optimizers.Adam(lr=hyperparams['lr'], #beta_1=0.9, beta_2=0.999, epsilon=0.0001,\n",
    "                                       decay=hyperparams['decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_collection[key] = hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'combined'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# session_collection['cnn_hierarch'].close()\n",
    "# K.clear_session()\n",
    "# transfer_type=None\n",
    "dataset_type\n",
    "# transfer_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.adam.Adam at 0x7f27dc3b1310>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams['optimizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/ananana/mental/b8656c3d65284a3482cd957753cc3709\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     batch_cat_acc [20]         : (0.15625, 0.5702160596847534)\n",
      "COMET INFO:     batch_loss [20]            : (0.8128575086593628, 1.2390024662017822)\n",
      "COMET INFO:     cat_acc [2]                : (0.557812511920929, 0.5649999976158142)\n",
      "COMET INFO:     epoch_duration [2]         : (82.16225676611066, 89.9363702738192)\n",
      "COMET INFO:     loss [2]                   : (0.9478838968276978, 0.9821135902404785)\n",
      "COMET INFO:     val_cat_acc [2]            : (0.0, 0.12271540611982346)\n",
      "COMET INFO:     val_loss [2]               : (1.3295733233292897, 1.758111149072647)\n",
      "COMET INFO:     validate_batch_cat_acc [4] : (0.0, 0.125)\n",
      "COMET INFO:     validate_batch_loss [4]    : (1.3958113193511963, 1.8319497108459473)\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     trainable_params : 2154989\n",
      "COMET INFO:   Parameters [count]:\n",
      "COMET INFO:     Adam_amsgrad               : 1\n",
      "COMET INFO:     Adam_beta_1                : 0.9\n",
      "COMET INFO:     Adam_beta_2                : 0.999\n",
      "COMET INFO:     Adam_decay                 : 0.001\n",
      "COMET INFO:     Adam_epsilon               : 1e-07\n",
      "COMET INFO:     Adam_learning_rate         : 0.0005\n",
      "COMET INFO:     Adam_name                  : Adam\n",
      "COMET INFO:     batch_size [2]             : 2\n",
      "COMET INFO:     bert_dense_units           : 256\n",
      "COMET INFO:     bert_finetune_layers       : 1\n",
      "COMET INFO:     bert_pooling               : first\n",
      "COMET INFO:     bert_trainable             : 1\n",
      "COMET INFO:     dataset_type               : combined\n",
      "COMET INFO:     decay                      : 0.001\n",
      "COMET INFO:     dense_bow_units            : 20\n",
      "COMET INFO:     dense_sentence_units       : 1\n",
      "COMET INFO:     dense_user_units           : 1\n",
      "COMET INFO:     dropout                    : 0.1\n",
      "COMET INFO:     early_stopping_patience    : 20\n",
      "COMET INFO:     embedding_dim              : 100\n",
      "COMET INFO:     embeddings_path            : /home/anasab//resources/glove.840B/glove.840B.300d.txt\n",
      "COMET INFO:     emotion_lexicon            : /home/anasab//resources/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\n",
      "COMET INFO:     emotions                   : ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'negative', 'positive', 'sadness', 'surprise', 'trust']\n",
      "COMET INFO:     epochs                     : 5\n",
      "COMET INFO:     filters                    : 100\n",
      "COMET INFO:     filters_user               : 10\n",
      "COMET INFO:     freeze_patience            : 2000\n",
      "COMET INFO:     hierarchical               : True\n",
      "COMET INFO:     ignore_layer               : ['bert_layer']\n",
      "COMET INFO:     kernel_size                : 5\n",
      "COMET INFO:     kernel_size_user           : 3\n",
      "COMET INFO:     l2_bert                    : 0.0001\n",
      "COMET INFO:     l2_dense                   : 0.00011\n",
      "COMET INFO:     l2_embeddings              : 1e-07\n",
      "COMET INFO:     liwc_words_cached          : data/liwc_categories_for_vocabulary_erisk_clpsych_stop_20K.pkl\n",
      "COMET INFO:     lr                         : 0.0005\n",
      "COMET INFO:     lstm_units                 : 128\n",
      "COMET INFO:     lstm_units_user            : 32\n",
      "COMET INFO:     max_features               : 20002\n",
      "COMET INFO:     maxlen                     : 256\n",
      "COMET INFO:     model_path                 : models/lstm_combined_hierarchical56\n",
      "COMET INFO:     norm_momentum              : 0.1\n",
      "COMET INFO:     optimizer                  : <tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f27dc3b1310>\n",
      "COMET INFO:     padding                    : pre\n",
      "COMET INFO:     positive_class_weight      : 2\n",
      "COMET INFO:     post_groups_per_user       : 1\n",
      "COMET INFO:     posts_per_group            : 50\n",
      "COMET INFO:     posts_per_user             : 1\n",
      "COMET INFO:     pretrained_model_path      : models/lstm_clpsych_seq52\n",
      "COMET INFO:     reduce_lr_factor           : 0.5\n",
      "COMET INFO:     reduce_lr_patience         : 55\n",
      "COMET INFO:     sample_seqs                : 1\n",
      "COMET INFO:     samples                    : 100\n",
      "COMET INFO:     sampling_distr             : exp\n",
      "COMET INFO:     scheduled_reduce_lr_factor : 0.5\n",
      "COMET INFO:     scheduled_reduce_lr_freq   : 95\n",
      "COMET INFO:     steps                      : 100\n",
      "COMET INFO:     threshold                  : 0.5\n",
      "COMET INFO:     trainable_embeddings       : True\n",
      "COMET INFO:     transfer                   : 1\n",
      "COMET INFO:     transfer_type              : 1\n",
      "COMET INFO:     transfer_units             : 20\n",
      "COMET INFO:     user_level                 : True\n",
      "COMET INFO:     vocabulary_path            : data/all_vocab_clpsych_erisk_stop_20000.pkl\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (162 KB)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     model graph              : 1\n",
      "COMET INFO:     os packages              : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/ananana/mental/f108d41656e44141af5440581d60dd69\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-12 21:15:46,537;INFO;Initializing datasets...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:Initializing datasets...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-12 21:15:46,548;WARNING;User subject6472 has no posts in train set. Ignoring.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:training:User subject6472 has no posts in train set. Ignoring.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-12 21:15:46,564;INFO;Initializing model...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:Initializing model...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-12 21:15:46,565;INFO;Initializing model...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:Initializing model...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "word_seq (InputLayer)           [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embeddings_layer (Embedding)    (None, 256, 100)     2000200     word_seq[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_dropout (Dropout)     (None, 256, 100)     0           embeddings_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "LSTM_layer (LSTM)               (None, 256, 128)     117248      embedding_dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "attention (Dense)               (None, 256, 1)       129         LSTM_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 256)          0           attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 256)          0           flatten_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_16 (RepeatVector) (None, 128, 256)     0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "permute_16 (Permute)            (None, 256, 128)     0           repeat_vector_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "multiply_16 (Multiply)          (None, 256, 128)     0           LSTM_layer[0][0]                 \n",
      "                                                                 permute_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 128)          0           multiply_16[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sent_repr_norm (BatchNormalizat (None, 128)          512         lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sent_repr_dropout (Dropout)     (None, 128)          0           sent_repr_norm[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 2,118,089\n",
      "Trainable params: 2,117,833\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sparse_input_hist (InputLayer)  [(None, 50, 179)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hierarchical_word_seq_input (In [(None, 50, 256)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "numeric_input_hist (InputLayer) [(None, 50, 75)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sparse_dense_layer_user (TimeDi (None, 50, 20)       3600        sparse_input_hist[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "user_encoder (TimeDistributed)  (None, 50, 128)      2118089     hierarchical_word_seq_input[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "numerical_features_norm (BatchN (None, 50, 75)       200         numeric_input_hist[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "sparse_features_norm (BatchNorm (None, 50, 20)       200         sparse_dense_layer_user[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 50, 223)      0           user_encoder[0][0]               \n",
      "                                                                 numerical_features_norm[0][0]    \n",
      "                                                                 sparse_features_norm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "LSTM_layer_user (LSTM)          (None, 50, 32)       32768       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_user (Dense)          (None, 50, 1)        33          LSTM_layer_user[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 50)           0           attention_user[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 50)           0           flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_17 (RepeatVector) (None, 32, 50)       0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "permute_17 (Permute)            (None, 50, 32)       0           repeat_vector_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "multiply_17 (Multiply)          (None, 50, 32)       0           LSTM_layer_user[0][0]            \n",
      "                                                                 permute_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 32)           0           multiply_17[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "user_repr_dropout (Dropout)     (None, 32)           0           lambda_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 3)            99          user_repr_dropout[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 2,154,989\n",
      "Trainable params: 2,154,533\n",
      "Non-trainable params: 456\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sparse_input_hist (InputLayer)  [(None, 50, 179)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hierarchical_word_seq_input (In [(None, 50, 256)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "numeric_input_hist (InputLayer) [(None, 50, 75)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sparse_dense_layer_user (TimeDi (None, 50, 20)       3600        sparse_input_hist[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "user_encoder (TimeDistributed)  (None, 50, 128)      2118089     hierarchical_word_seq_input[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "numerical_features_norm (BatchN (None, 50, 75)       200         numeric_input_hist[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "sparse_features_norm (BatchNorm (None, 50, 20)       200         sparse_dense_layer_user[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 50, 223)      0           user_encoder[0][0]               \n",
      "                                                                 numerical_features_norm[0][0]    \n",
      "                                                                 sparse_features_norm[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "LSTM_layer_user (LSTM)          (None, 50, 32)       32768       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_user (Dense)          (None, 50, 1)        33          LSTM_layer_user[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 50)           0           attention_user[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 50)           0           flatten_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_17 (RepeatVector) (None, 32, 50)       0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "permute_17 (Permute)            (None, 50, 32)       0           repeat_vector_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "multiply_17 (Multiply)          (None, 50, 32)       0           LSTM_layer_user[0][0]            \n",
      "                                                                 permute_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 32)           0           multiply_17[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "user_repr_dropout (Dropout)     (None, 32)           0           lambda_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 3)            99          user_repr_dropout[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 2,154,989\n",
      "Trainable params: 2,154,533\n",
      "Non-trainable params: 456\n",
      "__________________________________________________________________________________________________\n",
      "models/lstm_combined_hierarchical56\n",
      "2021-01-12 21:15:47,368;INFO;Training model...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:Training model...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-12 21:15:47,370;INFO;Initializing callbacks...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:Initializing callbacks...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-12 21:15:47,378;WARNING;User subject6472 has no posts in train set. Ignoring.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:training:User subject6472 has no posts in train set. Ignoring.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-12 21:15:47,392;WARNING;User subject6472 has no posts in train set. Ignoring.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:training:User subject6472 has no posts in train set. Ignoring.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 100 steps, validate for 12 steps\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tfconda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/anasab/tfconda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/100 [============================>.] - ETA: 0s - loss: 0.9999 - cat_acc: 0.5398\n",
      "Epoch 00001: val_loss improved from inf to 1.44979, saving model to models/lstm_combined_hierarchical56_best.h5\n",
      "100/100 [==============================] - 84s 843ms/step - loss: 1.0001 - cat_acc: 0.5394 - val_loss: 1.4498 - val_cat_acc: 0.0000e+00\n",
      "Epoch 2/15\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.9589 - cat_acc: 0.5633\n",
      "Epoch 00002: val_loss improved from 1.44979 to 1.39826, saving model to models/lstm_combined_hierarchical56_best.h5\n",
      "100/100 [==============================] - 86s 862ms/step - loss: 0.9586 - cat_acc: 0.5642 - val_loss: 1.3983 - val_cat_acc: 0.0313\n",
      "Epoch 3/15\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.9325 - cat_acc: 0.5786\n",
      "Epoch 00003: val_loss did not improve from 1.39826\n",
      "100/100 [==============================] - 82s 820ms/step - loss: 0.9315 - cat_acc: 0.5794 - val_loss: 1.5010 - val_cat_acc: 0.0444\n",
      "Epoch 4/15\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.8680 - cat_acc: 0.6175\n",
      "Epoch 00004: val_loss did not improve from 1.39826\n",
      "100/100 [==============================] - 81s 806ms/step - loss: 0.8683 - cat_acc: 0.6176 - val_loss: 1.4076 - val_cat_acc: 0.1332\n",
      "Epoch 5/15\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.8066 - cat_acc: 0.6531\n",
      "Epoch 00005: val_loss did not improve from 1.39826\n",
      "100/100 [==============================] - 88s 882ms/step - loss: 0.8084 - cat_acc: 0.6516 - val_loss: 1.4433 - val_cat_acc: 0.1880\n",
      "Epoch 6/15\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.7175 - cat_acc: 0.6984\n",
      "Epoch 00006: val_loss did not improve from 1.39826\n",
      "100/100 [==============================] - 82s 818ms/step - loss: 0.7159 - cat_acc: 0.7002 - val_loss: 1.7404 - val_cat_acc: 0.1802\n",
      "Epoch 7/15\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.6403 - cat_acc: 0.7447\n",
      "Epoch 00007: val_loss did not improve from 1.39826\n",
      "100/100 [==============================] - 88s 879ms/step - loss: 0.6395 - cat_acc: 0.7457 - val_loss: 1.8301 - val_cat_acc: 0.2010\n",
      "Epoch 8/15\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.5960 - cat_acc: 0.7656\n",
      "Epoch 00008: val_loss did not improve from 1.39826\n",
      "100/100 [==============================] - 82s 817ms/step - loss: 0.5963 - cat_acc: 0.7661 - val_loss: 1.8514 - val_cat_acc: 0.2324\n",
      "Epoch 9/15\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.5553 - cat_acc: 0.7819\n",
      "Epoch 00009: val_loss did not improve from 1.39826\n",
      "100/100 [==============================] - 82s 824ms/step - loss: 0.5566 - cat_acc: 0.7816 - val_loss: 1.7748 - val_cat_acc: 0.2689\n",
      "Epoch 10/15\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.5257 - cat_acc: 0.7992\n",
      "Epoch 00010: val_loss did not improve from 1.39826\n",
      "100/100 [==============================] - 88s 884ms/step - loss: 0.5242 - cat_acc: 0.8000 - val_loss: 2.1417 - val_cat_acc: 0.2245\n",
      "Epoch 11/15\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.4908 - cat_acc: 0.8163\n",
      "Epoch 00011: val_loss did not improve from 1.39826\n",
      "100/100 [==============================] - 82s 823ms/step - loss: 0.4905 - cat_acc: 0.8166 - val_loss: 1.5737 - val_cat_acc: 0.3525\n",
      "Epoch 12/15\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.4426 - cat_acc: 0.8354\n",
      "Epoch 00012: val_loss did not improve from 1.39826\n",
      "100/100 [==============================] - 81s 806ms/step - loss: 0.4434 - cat_acc: 0.8349 - val_loss: 2.0170 - val_cat_acc: 0.2689\n",
      "Epoch 13/15\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.4080 - cat_acc: 0.8494\n",
      "Epoch 00013: val_loss did not improve from 1.39826\n",
      "100/100 [==============================] - 89s 891ms/step - loss: 0.4067 - cat_acc: 0.8503 - val_loss: 2.3544 - val_cat_acc: 0.2611\n",
      "Epoch 14/15\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3945 - cat_acc: 0.8579\n",
      "Epoch 00014: val_loss did not improve from 1.39826\n",
      "100/100 [==============================] - 81s 814ms/step - loss: 0.3954 - cat_acc: 0.8575 - val_loss: 1.8659 - val_cat_acc: 0.3473\n",
      "Epoch 15/15\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.3596 - cat_acc: 0.8792\n",
      "Epoch 00015: val_loss did not improve from 1.39826\n",
      "100/100 [==============================] - 81s 814ms/step - loss: 0.3596 - cat_acc: 0.8791 - val_loss: 2.6826 - val_cat_acc: 0.2219\n",
      "2021-01-12 21:36:47,061;INFO;Saving model...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:Saving model...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27min 42s, sys: 48.1 s, total: 28min 30s\n",
      "Wall time: 21min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# transfer_type=\"depression\"\n",
    "transfer_type=None\n",
    "classes=3\n",
    "# transfer_type=\"depression\"\n",
    "# with session_collection[key].as_default():\n",
    "#     with session_collection[key].graph.as_default():\n",
    "hyperparams_collection[key] = hyperparams\n",
    "\n",
    "\n",
    "if transfer_type:\n",
    "    # TODO: try to compile? with proper optimizer?\n",
    "    hyperparams_collection[key]['trainable_embeddings'] = False\n",
    "    hyperparams_collection[key]['transfer_units'] = 20\n",
    "    hyperparams_collection[key]['positive_class_weight'] = 5\n",
    "    hyperparams_collection[key]['lr'] = 0.0001\n",
    "    hyperparams_collection[key]['optimizer'] = optimizers.Adam(lr=hyperparams_collection[key]['lr'], #beta_1=0.9, beta_2=0.999, epsilon=0.0001,\n",
    "                                   decay=hyperparams_collection[key]['decay'])\n",
    "#     hyperparams_collection[key]['lr'] = 0.01\n",
    "\n",
    "#     models_collection[key] = load_saved_model(hyperparams_features['pretrained_model_path'], \n",
    "#                                               hyperparams_collection[key])\n",
    "#     metrics_class = Metrics(threshold=hyperparams_collection[key]['threshold'])\n",
    "#     models_collection[key].compile(hyperparams_collection[key]['optimizer'], binary_crossentropy_custom,\n",
    "#                   metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m,\n",
    "#                           metrics_class.auc, \n",
    "#                            tf.keras.metrics.AUC(name=\"AUC_keras\"), \n",
    "#                            tf.keras.metrics.FalsePositives(), \n",
    "#                            tf.keras.metrics.FalseNegatives()])\n",
    "\n",
    "    hyperparams_features['pretrained_model_path'] = 'models/lstm_combined_depr_seq52'\n",
    "\n",
    "    experiment = initialize_experiment(hyperparams_collection[key], nrc_lexicon_path, emotions, pretrained_embeddings_path, \n",
    "                                  dataset_type, transfer_type)\n",
    "    models_collection[key], history = train(user_level_data2, subjects_split2, \n",
    "              hyperparams_collection[key], hyperparams_features, \n",
    "              embedding_matrix, emotions, stopword_list, liwc_categories,\n",
    "                  experiment,\n",
    "                  validation_set='valid',\n",
    "              version=54, epochs=90, start_epoch=80, transfer=True,\n",
    "#                     session=session_collection[key],\n",
    "#                 model=models_collection[key]\n",
    "                                           )\n",
    "else:\n",
    "    experiment = initialize_experiment(hyperparams, nrc_lexicon_path, emotions, pretrained_embeddings_path, \n",
    "                          dataset_type, transfer_type)\n",
    "    models_collection[key], history = train(user_level_data, subjects_split, \n",
    "              hyperparams_collection[key], hyperparams_features, \n",
    "              embedding_matrix, emotions, stopword_list, liwc_categories,\n",
    "                  experiment,\n",
    "                  validation_set='valid',\n",
    "              version=56, epochs=15, start_epoch=0, classes=classes,\n",
    "#                     session=session_collection[key],\n",
    "#                     model=models_collection[key]\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'anticipation', 'disgust', 'fear', 'joy', 'negative', 'positive', 'sadness', 'surprise', 'trust']\n",
      "['negemo', 'sexual', 'health', 'ppron', 'affect', 'i', 'cause', 'discrep', 'body', 'social', 'tentat', 'swear', 'excl', 'friend', 'shehe', 'money', 'relativ', 'they', 'cogmech', 'certain', 'space', 'family', 'death', 'percept', 'future', 'bio', 'pronoun', 'posemo', 'achieve', 'anx', 'verb', 'we', 'time', 'funct', 'humans', 'you', 'inhib', 'assent', 'adverb', 'motion', 'see', 'present', 'nonfl', 'number', 'quant', 'auxverb', 'anger', 'ipron', 'sad', 'negate', 'conj', 'filler', 'leisure', 'preps', 'past', 'work', 'hear', 'relig', 'home', 'article', 'ingest', 'insight', 'feel', 'incl']\n"
     ]
    }
   ],
   "source": [
    "print(emotions)\n",
    "print(liwc_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"models/lstm_%s_hierarchical53\" % dataset_type\n",
    "# models_collection[key] = initialize_model(hyperparams_collection[key], \n",
    "#                                           hyperparams_features, embedding_matrix, emotions, stopword_list,\n",
    "#                     liwc_categories)\n",
    "# models_collection[key].load_weights(model_path + \"_weights.h5\")\n",
    "\n",
    "\n",
    "# embedding_matrix.shape\n",
    "# models_collection[key].load_weights(hyperparams_features['pretrained_model_path'] + \"_weights.h5\", by_name=True)\n",
    "# metrics_class = Metrics(threshold=hyperparams_collection[key]['threshold'])\n",
    "# models_collection[key].compile(hyperparams_collection[key]['optimizer'], binary_crossentropy_custom,\n",
    "#               metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m,\n",
    "#                       metrics_class.auc, \n",
    "#                        tf.keras.metrics.AUC(name=\"AUC_keras\"), \n",
    "#                        tf.keras.metrics.FalsePositives(), \n",
    "#                        tf.keras.metrics.FalseNegatives()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_collection[key] = initialize_model(hyperparams_collection[key], \n",
    "#                                           hyperparams_features, embedding_matrix, emotions, stopword_list,\n",
    "#                     liwc_categories)\n",
    "# models_collection[key] = load_weights(hyperparams_features['pretrained_model_path'] + \"_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_collection[key] = depression_model\n",
    "# hyperparams_collection[key]\n",
    "# classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating only on last group (1)...\n",
      "Evaluating...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 34s 565ms/step - loss: 1.8325 - cat_acc: 0.4297 - prec: 0.4365 - rec: 0.4156\n",
      "Predicting\n",
      "Confusion Matrix\n",
      "(1920,) (1920,)\n",
      "[[ 54  57 300]\n",
      " [ 39 133 577]\n",
      " [ 33  89 638]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    selfharm       0.43      0.13      0.20       411\n",
      "    anorexia       0.48      0.18      0.26       749\n",
      "  depression       0.42      0.84      0.56       760\n",
      "\n",
      "    accuracy                           0.43      1920\n",
      "   macro avg       0.44      0.38      0.34      1920\n",
      "weighted avg       0.44      0.43      0.37      1920\n",
      "\n",
      "CPU times: user 1min 30s, sys: 1.58 s, total: 1min 31s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pickle\n",
    "\n",
    "# # Evaluate on entire posts history, final F1-score\n",
    "# print(\"Evaluating on same nr of groups as train (%d)...\" % hyperparams['post_groups_per_user'] if \n",
    "#       hyperparams['post_groups_per_user'] else 0)\n",
    "# cur_model.evaluate_generator(DataGenerator(user_level_data, subjects_split, \n",
    "#                                          set_type='test', \n",
    "#                                     seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
    "#                                        hierarchical=hyperparams['hierarchical'],\n",
    "#                                          max_posts_per_user=None,\n",
    "#                                        pad_with_duplication=False,\n",
    "#                                         posts_per_group=hyperparams['posts_per_group'],\n",
    "#                                         post_groups_per_user=hyperparams['post_groups_per_user'], \n",
    "#                                          sample_seqs=False, shuffle=False), verbose=1)\n",
    "# print(\"Evaluating on entire posts history...\")\n",
    "# cur_model.evaluate_generator(DataGenerator(user_level_data, subjects_split, \n",
    "#                                          set_type='test', \n",
    "#                                     seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
    "#                                        hierarchical=hyperparams['hierarchical'],\n",
    "#                                          max_posts_per_user=None,\n",
    "#                                        pad_with_duplication=False,\n",
    "#                                         posts_per_group=hyperparams['posts_per_group'],\n",
    "#                                         post_groups_per_user=hyperparams['post_groups_per_user'], \n",
    "#                                          sample_seqs=False, shuffle=False), verbose=1)\n",
    "# hyperparams_collection[key] = hyperparams\n",
    "\n",
    "\n",
    "liwc_words_for_categories = pickle.load(open(hyperparams_features['liwc_words_cached'], 'rb'))\n",
    "if 'emotions' in hyperparams_collection[key]['ignore_layer']:\n",
    "    emotions = []\n",
    "if 'LIWC' in hyperparams_collection[key]['ignore_layer']:\n",
    "    liwc_categories = []\n",
    "# # \n",
    "# with session_collection[key].as_default():\n",
    "#     with session_collection[key].graph.as_default():\n",
    "print(\"Evaluating only on last group (1)...\")\n",
    "if classes==1:\n",
    "    metrics_class = Metrics(threshold=0.5)\n",
    "    models_collection[key].compile(hyperparams_collection[key]['optimizer'], binary_crossentropy_custom,\n",
    "                      metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m,\n",
    "                              metrics_class.auc, \n",
    "                               tf.keras.metrics.AUC(name=\"AUC_keras\"), \n",
    "                               tf.keras.metrics.FalsePositives(), \n",
    "                               tf.keras.metrics.FalseNegatives()])\n",
    "else:\n",
    "    models_collection[key].compile(hyperparams_collection[key]['optimizer'], K.categorical_crossentropy,\n",
    "                     metrics=[tf.keras.metrics.CategoricalAccuracy(name='cat_acc'),\n",
    "                             tf.keras.metrics.Precision(name='prec'), tf.keras.metrics.Recall(name='rec'), \n",
    "                              ])\n",
    "\n",
    "test_generator = DataGenerator(user_level_data, subjects_split, \n",
    "                                         set_type='test', \n",
    "                                    seq_len=hyperparams_collection[key]['maxlen'],   \n",
    "#                                         batch_size=len(subjects_split['test']), # on all data at once\n",
    "                                        batch_size=hyperparams_collection[key]['batch_size'], # on all data at once\n",
    "                                       hierarchical=hyperparams_collection[key]['hierarchical'],\n",
    "                                         max_posts_per_user=None,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams_collection[key]['posts_per_group'],\n",
    "                                        post_groups_per_user=1,  compute_liwc=True,\n",
    "                                        liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                        emotions=emotions, liwc_categories=liwc_categories, keep_last_batch=False,\n",
    "                                         sample_seqs=False, shuffle=False, classes=classes)\n",
    "print(\"Evaluating...\")\n",
    "results1 = models_collection[key].evaluate_generator(test_generator, verbose=1)\n",
    "\n",
    "if classes>1:\n",
    "    test_labels_sparse = list(test_generator.generated_labels) # need to get these after iterating\n",
    "    \n",
    "    print(\"Predicting\")\n",
    "    Y_pred = models_collection[key].predict_generator(test_generator)#, num_of_test_samples // batch_size+1)\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "    print('Confusion Matrix')\n",
    "    # TODO: check\n",
    "    # keeping as many batches as we have for predictions, hopefully they match... I got one more\n",
    "    # TODO: no, can't assume it, have to fix it\n",
    "\n",
    "    test_labels = np.argmax(test_labels_sparse, axis=1) \n",
    "    print(test_labels.shape, y_pred.shape)\n",
    "    print(confusion_matrix(test_labels, y_pred))\n",
    "    print('Classification Report')\n",
    "    target_names = ['selfharm', 'anorexia', 'depression']\n",
    "    print(classification_report(test_labels, y_pred, target_names=target_names))\n",
    "    # experiment.log_metric(\"test_f1\", results1[1])\n",
    "# experiment.log_metric(\"test_prec\", results1[2])\n",
    "# experiment.log_metric(\"test_recall\", results1[3])\n",
    "# experiment.log_metric(\"test_auc\", results1[4])\n",
    "\n",
    "\n",
    "#     Optimal thresh\n",
    "if classes==1:\n",
    "    print(\"Finding optimal threshold...\")\n",
    "    f1s = {}\n",
    "    for thresh in range(0, 10, 2):\n",
    "        print(thresh)\n",
    "        # model1.load_weights(model_path + \"_weights\", by_name=True)\n",
    "        metrics_class = Metrics(threshold=thresh/10)\n",
    "        models_collection[key].compile(hyperparams_collection[key]['optimizer'], binary_crossentropy_custom,\n",
    "                          metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m,\n",
    "                                metrics_class.auc, \n",
    "                               tf.keras.metrics.AUC(name=\"AUC_keras\"), \n",
    "                               tf.keras.metrics.FalsePositives(), \n",
    "                               tf.keras.metrics.FalseNegatives()])\n",
    "    \n",
    "        results = models_collection[key].evaluate_generator(DataGenerator(user_level_data, subjects_split, \n",
    "                                             set_type='valid', compute_liwc=True,\n",
    "                                            liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                            emotions=emotions, liwc_categories=liwc_categories,\n",
    "                                        seq_len=hyperparams_collection[key]['maxlen'], \n",
    "                                            batch_size=hyperparams_collection[key]['batch_size'],\n",
    "                                           hierarchical=hyperparams_collection[key]['hierarchical'],\n",
    "                                             max_posts_per_user=None,\n",
    "                                           pad_with_duplication=False,\n",
    "                                            posts_per_group=hyperparams_collection[key]['posts_per_group'],\n",
    "                                            post_groups_per_user=1, \n",
    "                                                     sample_seqs=False, shuffle=False), verbose=1)\n",
    "        f1s[thresh] = results[1]\n",
    "    best_f1 = 0\n",
    "    best_thresh = 0.5\n",
    "    for t, f1 in f1s.items():\n",
    "        if f1 >= best_f1:\n",
    "            best_thresh = t\n",
    "            best_f1 = f1\n",
    "    print(f1s, best_thresh)\n",
    "    hyperparams_features['best_thresh'] = best_thresh/10\n",
    "    # experiment.log_metric(\"best_thresh\", best_thresh/10)\n",
    "\n",
    "    print(\"Computing with best threshold...\")\n",
    "    metrics_class = Metrics(best_thresh/10)\n",
    "    models_collection[key].compile(hyperparams_collection[key]['optimizer'], binary_crossentropy_custom,\n",
    "                          metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m,\n",
    "                                metrics_class.auc, \n",
    "                               tf.keras.metrics.AUC(name=\"AUC_keras\"), \n",
    "                               tf.keras.metrics.FalsePositives(), \n",
    "                               tf.keras.metrics.FalseNegatives()])\n",
    "\n",
    "    results = models_collection[key].evaluate_generator(DataGenerator(user_level_data, subjects_split, \n",
    "                                             set_type='test', compute_liwc=True,\n",
    "                                            liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                            emotions = emotions, liwc_categories=liwc_categories,\n",
    "\n",
    "                                        seq_len=hyperparams_collection[key]['maxlen'], \n",
    "                                           batch_size=hyperparams_collection[key]['batch_size'],\n",
    "                                           hierarchical=hyperparams_collection[key]['hierarchical'],\n",
    "                                             max_posts_per_user=None,\n",
    "                                           pad_with_duplication=False,\n",
    "                                            posts_per_group=hyperparams_collection[key]['posts_per_group'],\n",
    "                                            post_groups_per_user=1, \n",
    "                                                     sample_seqs=False, shuffle=False), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n",
      "(32, 50, 256)\n"
     ]
    }
   ],
   "source": [
    "cnt=0\n",
    "alll = []\n",
    "g = DataGenerator(user_level_data, subjects_split, \n",
    "                                         set_type='test', \n",
    "                                    seq_len=hyperparams_collection[key]['maxlen'],   \n",
    "#                                         batch_size=len(subjects_split['test']), # on all data at once\n",
    "                                        batch_size=hyperparams_collection[key]['batch_size'], # on all data at once\n",
    "                                       hierarchical=hyperparams_collection[key]['hierarchical'],\n",
    "                                         max_posts_per_user=None,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams_collection[key]['posts_per_group'],\n",
    "                                        post_groups_per_user=1,  compute_liwc=True,\n",
    "                                        liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                        emotions=emotions, liwc_categories=liwc_categories, keep_last_batch=False,\n",
    "                                         sample_seqs=False, shuffle=False, classes=classes)\n",
    "for x, y in g:\n",
    "    cnt += 1\n",
    "    print(x[0].shape)\n",
    "    alll.extend(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1952 1920 60 1920\n"
     ]
    }
   ],
   "source": [
    "print(len(test_generator.generated_labels), len(alll), cnt, len(Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 1500\n",
    "\n",
    "# Evaluate on partial post history, simulating stream\n",
    "print(\"Evaluating on partial posts history...\")\n",
    "cnt+=1f1_per_iteration = []\n",
    "auc_per_iteration = []\n",
    "precision_per_iteration = []\n",
    "recall_per_iteration = []\n",
    "predictions_per_iteration = []\n",
    "for iteration in range(0, iterations, hyperparams_collection[key]['posts_per_group']):\n",
    "    print(\"Iteration\", iteration)\n",
    "    results = models_collection[key].evaluate_generator(DataGenerator(user_level_data, subjects_split, \n",
    "                                         set_type='test', \n",
    "                                       hierarchical=hyperparams_collection[key]['hierarchical'],\n",
    "                                        seq_len=hyperparams['maxlen'], \n",
    "#                                         batch_size=len(subjects_split['test'])//2, # on all data at once\n",
    "                                        batch_size=hyperparams_collection[key]['batch_size'], # on all data at once\n",
    "                                         max_posts_per_user=iteration,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams_collection[key]['posts_per_group'],\n",
    "                                        post_groups_per_user=1, \n",
    "                                        post_offset=iteration,\n",
    "                                        compute_liwc=True,\n",
    "                                        liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                        emotions=emotions, liwc_categories=liwc_categories,\n",
    "                                         sample_seqs=False, shuffle=False), verbose=1)\n",
    "    f1_per_iteration.append(results[1])\n",
    "    precision_per_iteration.append(results[2])\n",
    "    recall_per_iteration.append(results[3])\n",
    "    auc_per_iteration.append(results[5])\n",
    "    predictions = models_collection[key].predict(DataGenerator(user_level_data, subjects_split, \n",
    "                                         set_type='test', \n",
    "                                       hierarchical=hyperparams_collection[key]['hierarchical'],\n",
    "                                        seq_len=hyperparams['maxlen'], \n",
    "#                                         batch_size=len(subjects_split['test'])//2, # on all data at once\n",
    "                                        batch_size=hyperparams_collection[key]['batch_size'], # on all data at once\n",
    "                                         max_posts_per_user=iteration,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams_collection[key]['posts_per_group'],\n",
    "                                        post_groups_per_user=1, \n",
    "                                        post_offset=iteration,\n",
    "                                        compute_liwc=True,\n",
    "                                        liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                        emotions=emotions, liwc_categories=liwc_categories,\n",
    "                                         sample_seqs=False, shuffle=False), verbose=1)\n",
    "    predictions_per_iteration.extend(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"average precision across %d iterations: %f;\n",
    "average recall across %d iterations: %f;\n",
    "average f1 across %d iterations: %f;\n",
    "average auc across %d iterations: %f\"\"\" % (\n",
    "    iterations, np.mean(precision_per_iteration[:iterations]),\n",
    "    iterations, np.mean(recall_per_iteration[:iterations]),\n",
    "    iterations, np.mean(f1_per_iteration[:iterations]),\n",
    "    iterations, np.mean(auc_per_iteration[:iterations])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(range(0, len(auc_per_iteration)*hyperparams_collection[key]['posts_per_group'], hyperparams_collection[key]['posts_per_group']), \n",
    "#          auc_per_iteration)\n",
    "pd.Series(auc_per_iteration, \n",
    "          index=range(0, len(auc_per_iteration)*hyperparams_collection[key]['posts_per_group'], hyperparams_collection[key]['posts_per_group'])\n",
    "                                                                    ).rolling(20).mean().plot()\n",
    "plt.xlabel(\"Posts offset\")\n",
    "plt.ylabel(\"AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([p[0] for p in predictions_per_iteration]).hist(bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(range(0, iteration*hyperparams_collection[key]['posts_per_group'], \n",
    "#                hyperparams_collection[key]['posts_per_group']), \n",
    "#          predictions_per_iteration)\n",
    "pd.Series([p[0] for p in predictions_per_iteration]).rolling(5000).std().plot()\n",
    "plt.xlabel(\"Posts offset\")\n",
    "plt.ylabel(\"Predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_user = [v for v in models_collection[key].get_layer('attention_user').get_weights()[0].flatten()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = models_collection[key].get_layer('user_encoder').get_weights()[-2]\n",
    "assert(attention_layer.shape == (128, 1))\n",
    "pd.Series([v for v in attention_layer]).rolling(30).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(auc_per_iteration, open(\"auc_per_iteration_selfharm53.pkl\", \"wb+\"))\n",
    "# pickle.dump(f1_per_iteration, open(\"f1_per_iteration_selfharm53.pkl\", \"wb+\"))\n",
    "# pickle.dump(predictions_per_iteration, open(\"predictions_per_iteration_selfharm53.pkl\", \"wb+\"))\n",
    "# pickle.dump(attention_user, open(\"attention_user_selfharm53.pkl\", \"wb+\"))\n",
    "# pickle.dump(attention_layer, open(\"attention_post_selfharm53.pkl\", \"wb+\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_saved_model(\"models/lstm_depression_hierarchical52\", hyperparams_collection[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0, len(scores_per_iteration[:15])*hyperparams_collection[key]['posts_per_group'], hyperparams_collection[key]['posts_per_group']), \n",
    "         scores_per_iteration[:15])\n",
    "plt.xlabel(\"Posts offset\")\n",
    "plt.ylabel(\"AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_user = [v for v in models_collection[key].get_layer('attention_user').get_activations()[0].flatten()]\n",
    "l = models_collection[key].get_layer('attention_user')\n",
    "# l.activation(samplegenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_per_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in nrc_lexicon:\n",
    "    print(e)\n",
    "    print(nrc_lexicon[e].intersection(set(tokenize(text.lower()))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model_and_params(cur_model, 'models/lstm_selfharm_seq16_2', hyperparams, hyperparams_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_collection = {}\n",
    "# hyperparams_collection = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([v for v in models_collection[key].get_layer('attention_user').get_weights()[0].flatten()]).rolling(10\n",
    "                                                                                                             ).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([v for v in models_collection[key].get_layer('attention_user').get_weights()[0].flatten()]).abs().rolling(10\n",
    "                                                                                                             ).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series([v for v in models_collection[key].get_layer('attention_user').get_weights()[0].flatten()]).to_csv(\n",
    "#     'attention_user_weights_lstm_selfharm_erisk_all_1group.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([v for v in models_collection[key].get_layer('attention').get_weights()[0].flatten()]).rolling(5).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([abs(v) for v in models_collection[key].get_layer('output_layer').get_weights()[0].flatten()]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1, d2 = initialize_datasets(user_level_data, subjects_split, hyperparams, hyperparams_features, \n",
    "                        validation_set='valid', session=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_level_data2, subjects_split2, vocabulary = load_erisk_data(writings_df[writings_df['source']=='reddit'], \n",
    "                                                            voc_size=hyperparams_features['max_features'],\n",
    "                                                           emotion_lexicon=nrc_lexicon,\n",
    "                                                           emotions=emotions,\n",
    "                                                           user_level=hyperparams_features['user_level'],\n",
    "                                                                                logger=logger,\n",
    "#                                                            vocabulary=pickle.load(open('vocabulary_40K_all.pkl', 'rb')),\n",
    "#                                                            vocabulary=pickle.load(open('vocab_clpsych_10000.pkl', 'rb')),\n",
    "                                                              vocabulary=vocabulary_dict,\n",
    "                                                              by_subset=False\n",
    "                                                                               )\n",
    "results2 = models_collection[key].evaluate_generator(DataGenerator(user_level_data2, subjects_split2, \n",
    "                                         set_type='test', \n",
    "                                    seq_len=hyperparams_collection[key]['maxlen'],   \n",
    "#                                         batch_size=len(subjects_split['test']), # on all data at once\n",
    "                                        batch_size=hyperparams_collection[key]['batch_size'], # on all data at once\n",
    "                                       hierarchical=hyperparams_collection[key]['hierarchical'],\n",
    "                                         max_posts_per_user=None,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams_collection[key]['posts_per_group'],\n",
    "                                        post_groups_per_user=1,  compute_liwc=True,\n",
    "                                        liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                         sample_seqs=False, shuffle=False), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df.source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention and predictions analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nr = 53\n",
    "model_name = \"models/lstm_%s_hierarchical53\" % dataset_type\n",
    "hyperparams, _ = load_params(model_name)\n",
    "model = load_saved_model(model_name, hyperparams)\n",
    "auc_per_iteration = pickle.load(open(\"auc_per_iteration_depression53.pkl\", \"rb\"))\n",
    "f1_per_iteration = pickle.load(open(\"f1_per_iteration_depression53.pkl\", \"rb\"))\n",
    "predictions_per_iteration = pickle.load(open(\"predictions_per_iteration_depression53.pkl\", \"rb\"))\n",
    "attention_user = pickle.load(open(\"attention_user_depression53.pkl\", \"rb\"))\n",
    "attention_layer = pickle.load(open(\"attention_post_depression53.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(attention_user).rolling(10).mean().plot()\n",
    "plt.ylabel(\"Attention weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([abs(l[0]) for l in attention_layer]).rolling(50).mean().plot()\n",
    "plt.ylabel(\"Attention weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = range(0, min(len(auc_per_iteration)*hyperparams['posts_per_group'], 2000), \n",
    "                      hyperparams['posts_per_group'])\n",
    "pd.Series(auc_per_iteration, \n",
    "          index=index\n",
    "                                                                    ).rolling(5).mean().plot()\n",
    "plt.xlabel(\"Posts offset\")\n",
    "plt.ylabel(\"AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[l.shape for l in model.get_layer('user_encoder').get_weights()]\n",
    "# hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.get_layer('attention_user').get_weights()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(l.name, len(l.get_weights()), len(l.get_weights()[0]) if len(l.get_weights()) else 0) for l in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(len(l), len(l[0]) if len(l) and type(l[0])==list else 0) for \n",
    " l in model.get_layer('user_encoder').get_weights()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userl = model.get_layer('user_encoder').layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userl.get_layer('lstm_user').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_weights = userl.get_layer(index=3).get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_weights[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'optimizer' not in hyperparams:\n",
    "    hyperparams['optimizer'] = optimizers.Adam(lr=hyperparams['lr'], #beta_1=0.9, beta_2=0.999, epsilon=0.0001,\n",
    "                                   decay=hyperparams['decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nr=55\n",
    "original_model = build_hierarchical_model(hyperparams, hyperparams_features, embedding_matrix, \n",
    "                                         emotions, stopword_list, liwc_categories,\n",
    "                       ignore_layer=hyperparams['ignore_layer'], activations=False)\n",
    "# original_model.load_weights(\"models/lstm_%s_hierarchical%d\" % (dataset_type, model_nr) + '_weights.h5', by_name=True)\n",
    "\n",
    "activation_model_user = build_hierarchical_model(hyperparams, hyperparams_features, embedding_matrix, \n",
    "                                         emotions, stopword_list, liwc_categories,\n",
    "                       ignore_layer=hyperparams['ignore_layer'], activations='attention_user')\n",
    "# activation_model_user.load_weights(\"models/lstm_%s_hierarchical%d\"  % (dataset_type, model_nr) + '_weights.h5', by_name=True)\n",
    "\n",
    "activation_model = build_hierarchical_model(hyperparams, hyperparams_features, embedding_matrix, \n",
    "                                         emotions, stopword_list, liwc_categories,\n",
    "                       ignore_layer=hyperparams['ignore_layer'], activations='attention')\n",
    "# activation_model.load_weights(\"models/lstm_%s_hierarchical%d\" % (dataset_type, model_nr) + '_weights.h5', by_name=True)\n",
    "\n",
    "\n",
    "\n",
    "# activation_outputs = [layer.output for layer in original_model.layers]\n",
    "# activation_model = tf.keras.Model(inputs, activation_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_subjects = list(set(writings_df[writings_df['label']==1].subject))\n",
    "negative_subjects = list(set(writings_df[writings_df['label']==0].subject))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subji = 15\n",
    "subjects_split_test_positive = {'test':\n",
    "                                 [s for s in subjects_split['test'] if s in positive_subjects][:]}\n",
    "subjects_split_test_negative = {'test':\n",
    "                                 [s for s in subjects_split['test'] if s in negative_subjects][:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplegenerator_positive = DataGenerator(user_level_data, subjects_split_test_positive, \n",
    "                                     set_type='test', \n",
    "                                   hierarchical=True,\n",
    "                                seq_len=hyperparams['maxlen'], #batch_size=hyperparams['batch_size'],\n",
    "                                batch_size=1,\n",
    "                                max_posts_per_user=None,\n",
    "                                   pad_with_duplication=False,\n",
    "                                    posts_per_group=hyperparams['posts_per_group'],\n",
    "                                    post_groups_per_user=None, \n",
    "                                     sample_seqs=False, shuffle=False)\n",
    "samplegenerator_negative = DataGenerator(user_level_data, subjects_split_test_negative, \n",
    "                                     set_type='test', \n",
    "                                   hierarchical=True,\n",
    "                                seq_len=hyperparams['maxlen'], #batch_size=hyperparams['batch_size'],\n",
    "                                batch_size=1,\n",
    "                                max_posts_per_user=None,\n",
    "                                   pad_with_duplication=False,\n",
    "                                    posts_per_group=hyperparams['posts_per_group'],\n",
    "                                    post_groups_per_user=None, \n",
    "                                     sample_seqs=False, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_words_positive = activation_model.predict(samplegenerator_positive)\n",
    "attention_words_negative = activation_model.predict(samplegenerator_negative)\n",
    "predictions_positive = original_model.predict(samplegenerator_positive)\n",
    "predictions_negative = original_model.predict(samplegenerator_negative)\n",
    "attention_positive_user = activation_model_user.predict(samplegenerator_positive)\n",
    "attention_negative_user = activation_model_user.predict(samplegenerator_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_negative.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(samplegenerator_positive), len(samplegenerator_negative), \n",
    "      predictions_positive.shape, predictions_negative.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df[writings_df['subject']==subjects_split_test_positive['test'][0]][['text']].head()\n",
    "# writings_df[writings_df['subject']==subjects_split_test_negative['test'][0]][['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions_negative.shape, attention_words_negative.shape, attention_negative_user.shape)\n",
    "print(predictions_negative.sum(), attention_words_negative.sum(axis=2), attention_negative_user.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_positive_user.shape\n",
    "# len(writings_df[writings_df['subject']==subjects_split_test_positive['test'][2]])\n",
    "len(predictions_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_scores(writings_df, subjects, generator, predictions, attention_user, attention_words, \n",
    "                      words_scores=None, nr_words=None, users=None):\n",
    "    unk = \" \"\n",
    "    if not words_scores:\n",
    "        words_scores = {t:0 for t in vocabulary_list}\n",
    "        words_scores_all = {t:[] for t in vocabulary_list}\n",
    "        nr_words = {t:0 for t in vocabulary_list}\n",
    "        words_scores[unk] = 0\n",
    "        words_scores_all[unk] = []\n",
    "        nr_words[unk] = 0\n",
    "    highlighted_texts = []\n",
    "    # NOTE: I am considering only first post group for each subject?\n",
    "    for i in range(len(predictions)):\n",
    "        if users and i not in users:\n",
    "            continue\n",
    "#         if predictions[i]<0.8:\n",
    "#             continue\n",
    "#         if i%57!=0:\n",
    "#             continue\n",
    "    #     if predictions_positive_user[i].mean() < 0.02:\n",
    "    #         continue\n",
    "#         sampletexts = writings_df[writings_df['subject']==subjects[i]][['tokenized_text']].values\n",
    "        post_lengths = []\n",
    "        for p in range(50):\n",
    "#             if  abs(attention_user[i][p]) < 0.05:\n",
    "#                 continue\n",
    "            print(i, p)\n",
    "            post_tokens = generator[i][0][0][0][p]\n",
    "            post_len = sum(post_tokens!=0)\n",
    "            post_lengths.append(post_len)\n",
    "            nr_posts = 50\n",
    "            if post_len == 0:\n",
    "                nr_posts -= 1\n",
    "                continue\n",
    "#             print(\"post len\", post_len, list(enumerate([vocabulary_list[j] if (j!=0 and j<len(vocabulary_list)) else \" \"\n",
    "#                           for j in post_tokens ])))\n",
    "            post_with_weights = [(k, tok, round(attention_words[i][p][k], 10)) \n",
    "                                 for (k, tok) in list(\n",
    "                enumerate([vocabulary_list[j] if (j!=0 and j<len(vocabulary_list)) else unk\n",
    "                          for j in post_tokens ]))]\n",
    "            highlighted_text = [(\">>>\", attention_user[i][p])]\n",
    "            for (pos, tok, att) in post_with_weights:\n",
    "                score = att * attention_user[i][p] * post_len\n",
    "                words_scores[tok] += score\n",
    "                words_scores_all[tok].append(score)\n",
    "                highlighted_text.append((tok,score))\n",
    "                nr_words[tok] += 1\n",
    "#             print(post_with_weights)\n",
    "            highlighted_texts.append(highlighted_text)\n",
    "            \n",
    "            print(\"post len\", post_len, \"nr posts\", nr_posts)\n",
    "        #     print([list(enumerate([t for t in sampletexts[-3:]])))\n",
    "            print(\"user attention\", attention_user[i][p], \"avg word attention\", attention_words[i][p].mean())\n",
    "            print(\"prediction\", predictions[i])\n",
    "            \n",
    "            pd.Series(attention_words[i][p]).plot(label=\"%d_%d\" %(i, p))\n",
    "#             pd.Series(attention_positive_user[i]).plot()\n",
    "#         plt.scatter(np.exp(attention_user[i]), post_lengths)\n",
    "#         plt.scatter(attention_words[i].mean(axis=1), post_lengths)\n",
    "    words_scores = {w: s/nr_words[tok] for (w,s) in words_scores.items()}\n",
    "    plt.legend()\n",
    "    return words_scores, words_scores_all, highlighted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "words_scores_positive, words_scores_positive_all, highlighted_texts = build_word_scores(writings_df, subjects_split_test_positive['test'], samplegenerator_positive,\n",
    "                                 predictions_positive, attention_positive_user, attention_words_positive,\n",
    "                                                            users=[2,3,5,7,88,99,44,66])\n",
    "\n",
    "words_scores_negative, words_scores_negative_all, highlighted_texts_negative = build_word_scores(writings_df, subjects_split_test_negative['test'], samplegenerator_negative,\n",
    "                                 predictions_negative, attention_negative_user, attention_words_negative,\n",
    "                                                                     users=[2,4,7])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_highlighted_html(texts):\n",
    "    htmltext = \"\"\"\"<html> \"\"\"\n",
    "    for text in texts:\n",
    "        for i, (tok, score) in enumerate(text):\n",
    "            if i==0:\n",
    "                htmltext += '<span style=\"background-color: rgba(200,0,0,%f)\"> %s </span>' % (20*abs(score), tok)\n",
    "            else:\n",
    "                htmltext += '<span style=\"background-color: rgba(0,0,200,%f)\"> %s </span>' % (10*abs(score), tok)\n",
    "        \n",
    "#         htmltext += \"</span>  <br> \\n\"\n",
    "        htmltext += \" <br> \\n\"\n",
    "    htmltext += \"\"\"</html>\"\"\"\n",
    "    return htmltext\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%html\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(generate_highlighted_html(highlighted_texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(words_scores_negative.items(), key=lambda i: -abs(i[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'i'\n",
    "print(pd.Series(words_scores_positive_all[word]).describe(), pd.Series(words_scores_negative_all[word]).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(words_scores_negative_all[word]).hist(bins=20, label=\"not self-harming\", alpha=0.7)\n",
    "pd.Series(words_scores_positive_all[word]).hist(bins=20, label=\"self-harming\", alpha=0.7)\n",
    "plt.xlabel('attention weights')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.concatenate([attention_positive_user,attention_negative_user]).mean(axis=0)).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.concatenate([attention_positive_user]).mean(axis=0)).plot(label=\"depressed\")\n",
    "pd.Series(np.concatenate([attention_negative_user]).mean(axis=0)).plot(label=\"not depressed\")\n",
    "plt.ylabel(\"attention weights for user encoder\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(attention_words_positive.mean(axis=(1,0))).plot(label=\"depressed\")\n",
    "pd.Series(attention_words_negative.mean(axis=(1,0))).plot(label=\"not depressed\")\n",
    "plt.ylabel(\"attention weights for post encoder\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in vocabulary_list:\n",
    "    if not words_scores_negative[word]:\n",
    "#         if words_scores_positive[word]:\n",
    "#             print(word)\n",
    "        continue\n",
    "    if not words_scores_positive[word]:\n",
    "        continue\n",
    "    if pd.Series(words_scores_positive_all[word]).mean()/pd.Series(words_scores_negative_all[word]).mean() > 5:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> <mark> text </mark> more\n",
    "\n",
    "<span style=\"background-color: rgba(0,0,200,0.2)\">This text is highlighted in yellow.</span>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_collection[key] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_collection[key].get_output_at?\n",
    "# combined_model = models_collection[key]\n",
    "writings_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "all_predictions = {}\n",
    "for subject in set(writings_df[writings_df['subset']=='test'].subject.values):\n",
    "    all_predictions[subject] = {'true_label': None, 'predicted_label': None,\n",
    "                               'predicted_score': None, 'texts': None}\n",
    "    user_level_data_subject = {subject: user_level_data[subject]}\n",
    "    true_label = writings_df[writings_df['subject']==subject].label.values[0]\n",
    "    print(subject, true_label)\n",
    "    prediction = models_collection[key].predict(DataGenerator(user_level_data_subject, {'test':[subject]}, \n",
    "                                         set_type='test', \n",
    "                                    seq_len=hyperparams_collection[key]['maxlen'],   \n",
    "#                                         batch_size=len(subjects_split['test']), # on all data at once\n",
    "                                        batch_size=hyperparams_collection[key]['batch_size'], # on all data at once\n",
    "                                       hierarchical=hyperparams_collection[key]['hierarchical'],\n",
    "                                         max_posts_per_user=None,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams_collection[key]['posts_per_group'],\n",
    "                                        post_groups_per_user=1,  compute_liwc=True,\n",
    "                                        liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                         sample_seqs=False, shuffle=False), verbose=1)\n",
    "    model_prediction = int(prediction[0][0]>=0.5)\n",
    "    print(prediction[0][0], model_prediction)\n",
    "    # TODO: is this aggregation of texts correct?\n",
    "    if (true_label!=model_prediction):\n",
    "        print(\"Misclassification:\", len(\n",
    "            user_level_data_subject[subject]['raw']))\n",
    "    all_predictions[subject]['true_label'] = true_label\n",
    "    all_predictions[subject]['predicted_label'] = model_prediction\n",
    "    all_predictions[subject]['predicted_score'] = prediction[0][0]\n",
    "    all_predictions[subject]['texts'] = \"\\n\".join(\n",
    "            user_level_data_subject[subject]['raw'][:hyperparams_collection[key]['posts_per_group']])\n",
    "    all_predictions[subject]['liwc'] = user_level_data_subject[subject]['liwc'][\n",
    "        :hyperparams_collection[key]['posts_per_group']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_level_data_subject[subject].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame.from_dict(all_predictions, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df['misclass'] = abs(predictions_df['true_label'] - predictions_df['predicted_label'])\n",
    "predictions_df[predictions_df['misclass']==1].predicted_score.describe()\n",
    "# predictions_df[predictions_df['misclass']==0].predicted_score.describe()\n",
    "predictions_df[predictions_df['misclass']==1].mean()\n",
    "predictions_df[predictions_df['misclass']==0].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(Counter(tokenize(\"\\n\".join(predictions_df[predictions_df['misclass']==1].texts.values))).items(), key=lambda t: -t[1])[:50])\n",
    "print(sorted(Counter(tokenize(\"\\n\".join(predictions_df[predictions_df['misclass']==0].texts.values))).items(), key=lambda t: -t[1])[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "misclassified_texts = \"\\n\".join(predictions_df[predictions_df['misclass']==1].texts.values)\n",
    "correct_texts = \"\\n\".join(predictions_df[predictions_df['misclass']==0].texts.values)\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize_tweets)\n",
    "term_doc=vectorizer.fit_transform([misclassified_texts, correct_texts])\n",
    "ch2 = SelectKBest(chi2, \"all\")\n",
    "X_train = ch2.fit_transform(term_doc, [1,0])\n",
    "\n",
    "scores = ch2.scores_\n",
    "words = vectorizer.get_feature_names()\n",
    "for i, s in enumerate(scores):\n",
    "    if s>0.005:\n",
    "        print(words[i], s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(scores)[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df['liwc_avg'] = predictions_df['liwc'].apply(lambda x: np.mean(x, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, c in enumerate(categories):\n",
    "    predictions_df[c] = predictions_df['liwc_avg'].apply(lambda x: x[i])\n",
    "# TODO: compute emotions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df=predictions_df[~predictions_df.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pickle.load(open('predictions_df_depression_clpsych.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_words_for_categories = {}\n",
    "for categ in liwc_dict.keys():\n",
    "    liwc_words_for_categories[categ] = set()\n",
    "    for word in liwc_dict[categ]:\n",
    "        for t in vocabulary:\n",
    "            if t==word or (word[-1]=='*' and t.startswith(word[:-1])) \\\n",
    "                or (t==word.split(\"'\")[0]):\n",
    "                    liwc_words_for_categories[categ].add(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_words_for_categories.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df['all_tokens'] = predictions_df['texts'].apply(tokenize)\n",
    "\n",
    "# TODO: include the title\n",
    "def extract_emotions(tokens, emotion, relative=True):\n",
    "    if not tokens:\n",
    "        return None\n",
    "    emotion_words = [t for t in tokens \n",
    "                     if t in nrc_lexicon[emotion]]\n",
    "    if relative and len(tokens):\n",
    "        return len(emotion_words) / len(tokens)\n",
    "    else:\n",
    "        return len(emotion_words)\n",
    "    \n",
    "    return encoded_emotions\n",
    "\n",
    "from functools import partial\n",
    "for emotion in emotions:\n",
    "    predictions_df[emotion] = predictions_df['all_tokens'].apply(partial(extract_emotions, emotion=emotion, \n",
    "                                                                   relative=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_liwc_categories(tokens, category_words, relative=True):\n",
    "    category_cnt = 0\n",
    "    if not tokens:\n",
    "        return None\n",
    "    text_len = len(tokens)\n",
    "    for t in tokens:\n",
    "        for word in category_words:\n",
    "            if t==word or (word[-1]=='*' and t.startswith(word[:-1])) \\\n",
    "            or (t==word.split(\"'\")[0]):\n",
    "                category_cnt += 1\n",
    "                break # one token cannot belong to more than one word in the category\n",
    "    if relative:\n",
    "        return category_cnt/text_len\n",
    "    else:\n",
    "        return category_cnt\n",
    "\n",
    "for categ in liwc_dict.keys():\n",
    "#     if categ in predictions_df.columns:\n",
    "#         continue\n",
    "    print(\"Encoding for category %s...\" % categ)\n",
    "    predictions_df[categ] = predictions_df['all_tokens'].apply(partial(encode_liwc_categories, \n",
    "                                                                   category_words=liwc_dict[categ], \n",
    "                                                                   relative=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in categories:\n",
    "    print(c, 'misclass', predictions_df[predictions_df['misclass']==1][c].mean())\n",
    "    print(c, 'not misclass', predictions_df[predictions_df['misclass']==0][c].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in emotions:\n",
    "    print(c, 'misclass', predictions_df[predictions_df['misclass']==1][c].mean())\n",
    "    print(c, 'not misclass', predictions_df[predictions_df['misclass']==0][c].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, ttest_ind\n",
    "for c in categories:\n",
    "    print(c)\n",
    "    print('\\tmisclass', predictions_df[predictions_df['misclass']==1][c].mean())\n",
    "    print('\\tnot misclass', predictions_df[predictions_df['misclass']==0][c].mean())    \n",
    "    ttest = ttest_ind(predictions_df[predictions_df['misclass']==0][c],\n",
    "                                  predictions_df[predictions_df['misclass']==1][c])\n",
    "    print('\\tttest', ttest)\n",
    "    print('\\tpvalue', ttest[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, ttest_ind\n",
    "for c in emotions:\n",
    "    print(c)\n",
    "    print('\\tmisclass', predictions_df[predictions_df['misclass']==1][c].mean())\n",
    "    print('\\tnot misclass', predictions_df[predictions_df['misclass']==0][c].mean())    \n",
    "    ttest = ttest_ind(predictions_df[predictions_df['misclass']==0][c],\n",
    "                                  predictions_df[predictions_df['misclass']==1][c])\n",
    "    print('\\tttest', ttest)\n",
    "    print('\\tpvalue', ttest[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, ttest_ind\n",
    "for c in categories:\n",
    "    print(c)\n",
    "    print('\\tmisclass', predictions_df[predictions_df['true_label']==1][c].mean())\n",
    "    print('\\tnot misclass', predictions_df[predictions_df['true_label']==0][c].mean())    \n",
    "    ttest = ttest_ind(predictions_df[predictions_df['true_label']==0][c],\n",
    "                                  predictions_df[predictions_df['true_label']==1][c])\n",
    "    print('\\tttest', ttest)\n",
    "    print('\\tpvalue', ttest[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, ttest_ind\n",
    "for c in categories:\n",
    "    print(c)\n",
    "    print('\\tmisclass', predictions_df[predictions_df['predicted_label']==1][c].mean())\n",
    "    print('\\tnot misclass', predictions_df[predictions_df['predicted_label']==0][c].mean())    \n",
    "    ttest = ttest_ind(predictions_df[predictions_df['predicted_label']==0][c],\n",
    "                                  predictions_df[predictions_df['predicted_label']==1][c])\n",
    "    print('\\tttest', ttest)\n",
    "    print('\\tpvalue', ttest[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(predictions_df, open('predictions_df_depression_clpsych.pkl', 'wb+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_df = pickle.load(open('predictions_df_depression_erisk.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_saliency(loaded_model, pure_txt, text_class, pred_labels, text_sequence):\n",
    "    text_class = 'Positive' if text_class==1 else 'Negative'\n",
    "    pred_labels = 'Positive' if pred_labels==1 else 'Negative'\n",
    "\n",
    "    input_tensors = [loaded_model.input, K.learning_phase()]\n",
    "    model_input = loaded_model.layers[2].input # the input for convolution layer\n",
    "    model_output = loaded_model.output[0][1]\n",
    "    gradients = loaded_model.optimizer.get_gradients(model_output,model_input)\n",
    "    compute_gradients = K.function(inputs=input_tensors, outputs=gradients)\n",
    "    matrix = compute_gradients([text_sequence.reshape(1,30), text_class])[0][0]\n",
    "    matrix = matrix[:len(pure_txt),:]\n",
    "\n",
    "    matrix_magnify=np.zeros((matrix.shape[0]*10,matrix.shape[1]))\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(10):\n",
    "            matrix_magnify[i*10+j,:]=matrix[i,:]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.imshow(normalize_array(np.absolute(matrix_magnify)), interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.yticks(np.arange(5, matrix.shape[0]*10, 10), pure_txt, weight='bold',fontsize=24)\n",
    "    plt.xticks(np.arange(0, matrix.shape[1], 50), weight='bold',fontsize=24)\n",
    "    plt.title('True Label: \"{}\" Predicted Label: \"{}\"'.format(text_class,pred_labels), weight='bold',fontsize=24)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_saliency(models_collection[key], \"this is some text to classify\", 0, 0, [\"this\", \"is\", \"some\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_level_data3, subjects_split2, vocabulary = load_erisk_data(writings_df, \n",
    "                                                            voc_size=hyperparams_features['max_features'],\n",
    "                                                           emotion_lexicon=nrc_lexicon,\n",
    "                                                           emotions=[],\n",
    "                                                                liwc_categories=[],\n",
    "                                                           user_level=hyperparams_features['user_level'],\n",
    "                                                                                logger=logger,\n",
    "#                                                            vocabulary=pickle.load(open('vocabulary_40K_all.pkl', 'rb')),\n",
    "#                                                            vocabulary=pickle.load(open('vocab_clpsych_10000.pkl', 'rb')),\n",
    "                                                              vocabulary=vocabulary_dict,\n",
    "                                                              by_subset=False\n",
    "                                                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import lime.lime_text as lt\n",
    "import keras\n",
    "# my self written library\n",
    "import numpy as np\n",
    "\n",
    "# This is the text that I want to explain\n",
    "\n",
    "\n",
    "#\n",
    "# recodeText = pl.generate_one_hot(text=inputText, alphabet=alphabet, maxChars=maxChars)\n",
    "\n",
    "model = models_collection[key]\n",
    "\n",
    "\n",
    "\n",
    "subjects = ['subject14410000', 'subject68010000', 'test_subject315', 'test_subject2270', \n",
    "            'test_subject9306', 'test_subject9359']\n",
    "# subjects = [s for s in set(writings_df.subject.values) if s in user_level_data3]\n",
    "labels = [writings_df[writings_df['subject']==subject].label.values[0] for subject in subjects]\n",
    "overall_importance_vocabulary = {}\n",
    "\n",
    "def textForSubject(subject):\n",
    "    user_level_data_subject = user_level_data3[subject]\n",
    "    text = \"\\n\".join([\" \".join(t) for t in user_level_data_subject['texts'][\n",
    "        :hyperparams_collection[key]['posts_per_group']]])\n",
    "    return text\n",
    "    \n",
    "# pipeline-like function\n",
    "# takes raw text as input, converts it to one-hot character vectors via the alphabet\n",
    "# feeds it into keras' model.predict() function to receive predictions\n",
    "# works for lists of text as well as for single strings\n",
    "# TODO: need to make this actually work on the text input, not dummy. it needs to feed new things\n",
    "def predictFromText(inputTexts):\n",
    "    if type(inputTexts) != list:\n",
    "        inputTexts = [inputTexts]\n",
    "    \n",
    "    # list for predictions\n",
    "    predStorage = []\n",
    "    # loop through input list and predict\n",
    "\n",
    "    predictions = models_collection[key].predict(DataGeneratorFromText(inputTexts,\n",
    "                                        seq_len=hyperparams_collection[key]['maxlen'],\n",
    "                                        liwc_words_for_categories=liwc_words_for_categories,\n",
    "#                                         emotions=[], liwc_categories=[]\n",
    "                                                             ), verbose=1)\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        pred = int(prediction>=0.5)\n",
    "\n",
    "#         recodeText = pl.generate_one_hot(text=textInput, alphabet=alphabet, maxChars=maxChars)\n",
    "#         pred = model.predict(recodeText.transpose())\n",
    "        # control output of function\n",
    "#         print(\"TEXT\", str(inputTexts[i][:100]), \"PRED\", prediction, \"\\n\")\n",
    "        predStorage.append((1-prediction[0], prediction[0]))\n",
    "    return(np.asarray(predStorage))\n",
    "\n",
    "inputTexts = [textForSubject(subject) for subject in subjects]\n",
    "\n",
    "print(len(inputTexts))\n",
    "print(labels)\n",
    "\n",
    "# print(inputTexts)\n",
    "# print(predictFromText(inputTexts))\n",
    "# Lime Explainer\n",
    "# bow controls if words are perturbed or overwritten with UNKWORDZ\n",
    "# False makes sense, if location of words is important as in this classifier\n",
    "explainer = lt.LimeTextExplainer(\n",
    "#     kernel_width=25, \n",
    "    verbose=False, class_names=['not depressed', 'depressed'],\n",
    "#                            feature_selection=\"auto\", split_expression=\" \", bow=False\n",
    ")\n",
    "for i, inputText in enumerate(inputTexts):\n",
    "    print(\"EXPLAINING...\", i)\n",
    "    exp = explainer.explain_instance(text_instance=inputText, labels=(0,1), \n",
    "#                                      labels=[labels[i]],\n",
    "                         classifier_fn=predictFromText, \n",
    "                                     num_features=100, num_samples=1000\n",
    "                                    )\n",
    "    for word, weight in exp.as_list():\n",
    "        if word not in overall_importance_vocabulary:\n",
    "            overall_importance_vocabulary[word] = []\n",
    "        overall_importance_vocabulary[word].append(weight)\n",
    "    # this works, yields an array with probabilities for both classes\n",
    "    # print(predictFromText(textInputList = listTexts))\n",
    "    # print(len(predictFromText(inputTexts[0])), len(inputTexts))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(user_level_data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.explain_instance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.explain_instance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_importance_vocabulary_averages = {word: np.mean(overall_importance_vocabulary[word]) \n",
    "                                          for word in overall_importance_vocabulary \n",
    "                                          if len(overall_importance_vocabulary[word])>=2}\n",
    "overall_importance_vocabulary_averages_abs = {word: np.abs(np.mean(overall_importance_vocabulary[word])) \n",
    "                                              for word in overall_importance_vocabulary\n",
    "                                             if len(overall_importance_vocabulary[word])>=2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(overall_importance_vocabulary_averages_abs.items(), key=lambda t: t[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(t[0], np.mean(t[1]), len(t[1])) for t in \n",
    " sorted(overall_importance_vocabulary.items(), key=lambda t: len(t[1]), reverse=True)[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, weight in sorted(overall_importance_vocabulary_averages_abs.items(), key=lambda t: t[1], reverse=True)[:20]:\n",
    "    print(word, overall_importance_vocabulary[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(overall_importance_vocabulary, open('overall_importance_vocabulary_depression.pkl', 'wb+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall_importance_vocabulary = pickle.load(open('overall_importance_vocabulary_selfharm.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([len(overall_importance_vocabulary[w]) for w in overall_importance_vocabulary]).hist(log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.explain_instance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in DataGenerator(user_level_data3, {'test':[subject]}, \n",
    "                                         set_type='test', \n",
    "                                    seq_len=hyperparams_collection[key]['maxlen'],   \n",
    "#                                         batch_size=len(subjects_split['test']), # on all data at once\n",
    "                                        batch_size=hyperparams_collection[key]['batch_size'], # on all data at once\n",
    "                                       hierarchical=hyperparams_collection[key]['hierarchical'],\n",
    "                                         max_posts_per_user=None,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams_collection[key]['posts_per_group'],\n",
    "                                        post_groups_per_user=1,  compute_liwc=True,\n",
    "                                        liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                         sample_seqs=False, shuffle=False,\n",
    "                                        emotions=[], liwc_categories=[]\n",
    "                                                             ):\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D visualization of emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def normalize_date(date):\n",
    "        return \" \".join(date.strip().split())\n",
    "writings_df['datetime'] = writings_df.date.apply(lambda d: datetime.datetime.strptime(\n",
    "            normalize_date(d), '%Y-%m-%d %H:%M:%S'))\n",
    "date_cutoff1 = datetime.datetime.strptime('2010','%Y').date()\n",
    "date_cutoff2 = datetime.datetime.strptime('2020','%Y').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_emotions = writings_df[\n",
    "    writings_df['datetime'].between(date_cutoff1, date_cutoff2)\n",
    "].groupby('subject').mean()[list(categories) + emotions + ['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "emotions_dict=users_emotions.dropna().to_dict(orient='index')\n",
    "subjects = list(emotions_dict.keys())\n",
    "emotions_matrix=[list(emotions_dict[s].values())[:-1] for s in subjects]\n",
    "emotions_labels=[emotions_dict[s]['label'] for s in subjects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(np.array(emotions_matrix).transpose())\n",
    "col = plt.scatter(pca.components_[0], pca.components_[1], c=['r' if l==1 else 'b' for l in emotions_labels],\n",
    "           s=[0.5 for s in emotions_labels])\n",
    "\n",
    "ax = col.axes\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "# manually define a new patch \n",
    "handles.append(mpatches.Patch(color='red', label='anorexic users'))\n",
    "handles.append(mpatches.Patch(color='blue', label='non anorexic users'))\n",
    "\n",
    "\n",
    "# plot the legend\n",
    "plt.legend(handles=handles, loc='upper center')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = initialize_experiment(hyperparams, nrc_lexicon_path, emotions, pretrained_embeddings_path, \n",
    "                          dataset_type, transfer_type)\n",
    "experiment.add_tag(\"cross-validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 5\n",
    "\n",
    "f1_scores = []\n",
    "for test_slice in range(folds): \n",
    "    logger.info(\"Testing on slice %d, training on the rest...\\n\" % test_slice)\n",
    "    user_level_data, subjects_split, vocabulary = load_erisk_data(writings_df, \n",
    "                                                            voc_size=hyperparams_features['max_features'],\n",
    "                                                           emotion_lexicon=nrc_lexicon,\n",
    "                                                           emotions=emotions,\n",
    "                                                           user_level=hyperparams_features['user_level'],\n",
    "                                                                                logger=logger,\n",
    "                                                           vocabulary=pickle.load(open('all_vocab_clpsych_erisk_2000_dict.pkl', 'rb')),\n",
    "                                                              by_subset=False,\n",
    "                                                              nr_slices=folds,\n",
    "                                                                valid_prop=0.1,\n",
    "                                                                  test_slice=test_slice\n",
    "                                                                 )\n",
    "    model, history = train(user_level_data, subjects_split, \n",
    "              hyperparams, hyperparams_features, \n",
    "              embedding_matrix, emotions, stopword_list, liwc_categories,\n",
    "                  experiment,\n",
    "                validation_set='test',\n",
    "              version=8, epochs=50,\n",
    "    )\n",
    "    f1_scores.append(sum(history.history['val_f1_m'][-6:-1])/5)\n",
    "experiment.add_metric(\"average_f1\", sum(f1_scores)/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on server data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize_vars(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"models/lstm_selfharm_seq16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams1, hyperparams_features1 = load_params(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams_features1['embeddings_path']=[\n",
    "#     '/home/anasab//eRisk/embeddings/finetuned_glove_clpsych_erisk_stop_20000_2.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_features1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_saved_model(model_path, hyperparams1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_list = pickle.load(open(hyperparams_features1['vocabulary_path'], 'rb'))\n",
    "vocabulary_dict={}\n",
    "for i,w in enumerate(vocabulary_list):\n",
    "    vocabulary_dict[w] = i\n",
    "embedding_matrix = load_embeddings2(hyperparams_features1['embeddings_path'], \n",
    "                                    hyperparams_features1['embedding_dim'], vocabulary_dict)\n",
    "liwc_words_for_categories = pickle.load(open(hyperparams_features1['liwc_words_cached'], 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'optimizer' not in hyperparams1:\n",
    "#         hyperparams1['optimizer'] = optimizers.Adam(lr=hyperparams1['lr'], #beta_1=0.9, beta_2=0.999, epsilon=0.0001,\n",
    "#                                        decay=hyperparams1['decay'])\n",
    "# loaded_model = initialize_model(hyperparams1, hyperparams_features1, embedding_matrix, emotions, stopword_list,\n",
    "#                     liwc_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_weights(model_path + \"_weights.h5\", by_name=True)\n",
    "# metrics_class = Metrics(threshold=0.5)\n",
    "# loaded_model.compile(hyperparams1['optimizer'], binary_crossentropy_custom,\n",
    "#                   metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_runs = {\n",
    "    'bert': 0,\n",
    "    'lstm_seq': 1,\n",
    "    'cnn_hierarch': 2,\n",
    "    'transfer': 3,\n",
    "    'ensemble': 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_key = 'lstm_seq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def load_erisk_server_data(datarounds_json, voc_size, emotion_lexicon, tokenizer,\n",
    "                           liwc_words_for_categories,\n",
    "                           emotions =  \n",
    "                    ['anger', 'anticipation', 'disgust', 'fear', 'joy', \n",
    "                     'negative', 'positive', 'sadness', 'surprise', 'trust'],\n",
    "                    liwc_categories = liwc_categories, \n",
    "                    pronouns = [\"i\", \"me\", \"my\", \"mine\", \"myself\"],\n",
    "                    user_level=True, vocabulary=vocabulary_dict,\n",
    "                   logger=logger):\n",
    "    def __encode_liwc_categories(tokens, liwc_words_for_categories, relative=True):\n",
    "        categories_cnt = [0 for c in liwc_categories]\n",
    "        if not tokens:\n",
    "            return categories_cnt\n",
    "        text_len = len(tokens)\n",
    "        for i, category in enumerate(liwc_categories):\n",
    "            for t in tokens:\n",
    "                if t in liwc_words_for_categories[category]:\n",
    "                    categories_cnt[i] += 1\n",
    "            if relative and text_len:\n",
    "                categories_cnt[i] = categories_cnt[i]/text_len\n",
    "        return categories_cnt\n",
    "    logger.debug(\"Loading data...\\n\")\n",
    "     \n",
    "    subjects_split = {'test': []}\n",
    " \n",
    "    user_level_texts = {}\n",
    "    for datapoints_json in datarounds_json:\n",
    "        for datapoint in datapoints_json:\n",
    "            words = []\n",
    "            raw_text = \"\"\n",
    "            if \"title\" in datapoint:\n",
    "                tokenized_title = tokenizer.tokenize(datapoint[\"title\"])\n",
    "                words.extend(tokenized_title)\n",
    "                raw_text += datapoint[\"title\"]\n",
    "            if \"content\" in datapoint:\n",
    "                tokenized_text = tokenizer.tokenize(datapoint[\"content\"])\n",
    "                words.extend(tokenized_text)\n",
    "                raw_text += datapoint[\"content\"]\n",
    "            \n",
    "            liwc_categs = __encode_liwc_categories(words, liwc_words_for_categories)\n",
    "            if datapoint[\"nick\"] not in user_level_texts.keys():\n",
    "                user_level_texts[datapoint[\"nick\"]] = {}\n",
    "                user_level_texts[datapoint[\"nick\"]]['texts'] = [words]\n",
    "                user_level_texts[datapoint[\"nick\"]]['liwc'] = [liwc_categs]\n",
    "                user_level_texts[datapoint[\"nick\"]]['raw'] = [raw_text]\n",
    "                subjects_split['test'].append(datapoint['nick'])\n",
    "            else:\n",
    "                user_level_texts[datapoint[\"nick\"]]['texts'].append(words)\n",
    "                user_level_texts[datapoint[\"nick\"]]['liwc'].append(liwc_categs)\n",
    "                user_level_texts[datapoint[\"nick\"]]['raw'].append(raw_text)\n",
    "            \n",
    "    return user_level_texts, subjects_split, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_datapoint(jlpath):\n",
    "    datapoints = []\n",
    "    with open(jlpath) as f:\n",
    "        for line in f:\n",
    "            datapoints.append(json.loads(line))\n",
    "    return datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# erisk_server_data, erisk_server_subjects_split, vocabulary = load_erisk_server_data(data,\n",
    "#                        tokenizer=regtokenizer,\n",
    "#                        liwc_words_for_categories=liwc_words_for_categories,\n",
    "#                     voc_size=hyperparams_features['max_features'],\n",
    "#                     emotion_lexicon=nrc_lexicon,\n",
    "#                     emotions=emotions,\n",
    "#                     user_level=hyperparams_features['user_level'],\n",
    "#                        vocabulary=vocabulary_dict,\n",
    "#     #                                                            vocabulary=pickle.load(open('vocabulary20K_selfharm.pkl', 'rb'))\n",
    "#                     logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erisk_server_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#TODO: predict only on last 50 posts\n",
    "\n",
    "with session_collection[model_key].as_default():\n",
    "    with session_collection[model_key].graph.as_default():\n",
    "        server_erisk_predictions = models_collection[model_key].predict(DataGenerator(erisk_server_data, erisk_server_subjects_split, \n",
    "                                             set_type='test', vocabulary=vocabulary_dict, \n",
    "                                           hierarchical=hyperparams_collection[model_key]['hierarchical'],\n",
    "                                        seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
    "                                             max_posts_per_user=None,\n",
    "                                           pad_with_duplication=False,\n",
    "                                            posts_per_group=hyperparams['posts_per_group'],\n",
    "                                            post_groups_per_user=None, \n",
    "                                             sample_seqs=False, shuffle=False,\n",
    "                                                    compute_liwc=False)\n",
    "                                                                       )\n",
    "        pd.Series(server_erisk_predictions.flatten()).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(server_erisk_predictions.flatten()).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seding results to server!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects = [d['nick'] for d in read_json_datapoint(\"client/data0.jl\")]\n",
    "# results = {s: 0 for s in subjects}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_next_data(rnd, results):\n",
    "#     # TODO: send results to get data\n",
    "#     response = build_response(results)\n",
    "#     # Send response\n",
    "#     data = {\"...\"}\n",
    "#     # Make sure it's the correct round\n",
    "#     assert data['number'] == rnd\n",
    "#     serialize_data(data)\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_next_data_dummy(rnd, results):\n",
    "#     return read_json_datapoint(\"client/data0.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for round and model\n",
    "results = {key: {} for key in models_runs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_chunk(rnds):\n",
    "    # Send same results for a chunk of rounds to get new posts\n",
    "    data = [read_json_datapoint(\"data_server/data%d.jl\" % i) for i in rnds]\n",
    "#     data_chunks = []\n",
    "#     for rnd in rnds:\n",
    "#         # TODO: REPLACE THIS WITH THE CORRECT ONE\n",
    "#         data = get_next_data_dummy(rnd, results)\n",
    "#         data_chunks.append(data)\n",
    "#         all_data[rnd] = data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def predict_for_round_chunk(model, hyperparams, hyperparams_features, vocabulary, data_chunk, subjects=[],\n",
    "                           model_key='', cache_round=None): \n",
    "    # preload for subjects not occurring in the round with results from previous round\n",
    "#     results = {s: 0 for s in subjects}\n",
    "    if cache_round:\n",
    "        results = load_results(model_key, cache_round)\n",
    "    else:\n",
    "        results = {s: 0 for s in subjects}\n",
    "        \n",
    "    \n",
    "    erisk_server_data, erisk_server_subjects_split, vocabulary = load_erisk_server_data(data_chunk,\n",
    "                       tokenizer=regtokenizer,\n",
    "                       liwc_words_for_categories=liwc_words_for_categories,\n",
    "                    voc_size=hyperparams_features['max_features'],\n",
    "                    emotion_lexicon=nrc_lexicon,\n",
    "                    emotions=emotions,\n",
    "                    user_level=1,\n",
    "                       vocabulary=vocabulary,\n",
    "                    logger=logger)\n",
    "\n",
    "    for features, subjects, _ in DataGenerator(erisk_server_data, erisk_server_subjects_split, \n",
    "                                         set_type='test', vocabulary=vocabulary, \n",
    "                                       hierarchical=hyperparams['hierarchical'],\n",
    "                                    seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
    "                                         max_posts_per_user=None,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams['posts_per_group'],\n",
    "                                        post_groups_per_user=None, \n",
    "                                         sample_seqs=False, shuffle=False,\n",
    "                                      return_subjects=True):\n",
    "        predictions = model.predict_on_batch(features)\n",
    "        print(len(features[0]), len(subjects), len(predictions), len(results))\n",
    "        for i,s in enumerate(subjects):\n",
    "            results[\"subject\" + str(s)] = predictions[i].item()\n",
    "    return(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_response(results, decision_thresh=0.5, model_name='', rnd=0):\n",
    "    response = []\n",
    "    for subject, score in results.items():\n",
    "        prediction = 1 if score >= decision_thresh else 0\n",
    "        response.append({'nick': subject, 'score': score, 'decision': prediction})\n",
    "    json.dump(response, open(\"data_server/response_run%s_rnd%d.json\" % (model_name, rnd), 'w+'))\n",
    "    return response\n",
    "# build_response(results, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_ensemble_results(rnd, all_results, model_keys_to_average=['lstm_seq', 'cnn_hierarch']):\n",
    "#     subjects = [s for s in all_results[model_keys_to_average[0]][rnd]]\n",
    "#     results_ensemble = {}\n",
    "#     for sub in subjects:\n",
    "#         s = 0\n",
    "#         for k in model_keys_to_average:\n",
    "#             s += all_results[k][rnd][sub]\n",
    "#         results_ensemble[sub] = s/len(model_keys_to_average)\n",
    "#     return results_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble_results(results_to_average):\n",
    "    subjects = [s for s in results_to_average[0]]\n",
    "    results_ensemble = {}\n",
    "    for sub in subjects:\n",
    "        s = 0\n",
    "        for res in results_to_average:\n",
    "            s += res[sub]\n",
    "        results_ensemble[sub] = s/len(results_to_average)\n",
    "    return results_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transfer_results(rnd, all_results, model_to_average='lstm_seq', rounds_back=100):\n",
    "    subjects = [s for s in all_results[model_to_average][rnd]]\n",
    "    results_ensemble = {}\n",
    "    for sub in subjects:\n",
    "        s = 0\n",
    "        existing_rounds = 0\n",
    "        for prev_rnd in range(rnd-rounds_back, rnd+1):\n",
    "#             print(\"rolling rnds\", prev_rnd)\n",
    "            if prev_rnd in all_results[model_to_average]:\n",
    "                s += all_results[model_to_average][prev_rnd][sub]\n",
    "                existing_rounds += 1\n",
    "        results_ensemble[sub] = s/existing_rounds\n",
    "#         print(\"Have found a rolling window of %d for the transfer model\" % existing_rounds)\n",
    "    return results_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(model_key, rnd):\n",
    "    results = {}\n",
    "    with open(\"data_server/response_run%s_rnd%d.json\" % (models_runs[model_key], rnd)) as f:\n",
    "        response = json.loads(f.read())\n",
    "        for line in response:\n",
    "            results[line['nick']] = line['score']\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results['lstm_seq'][20] = load_results('lstm_seq', 20)\n",
    "# results['lstm_seq'][40] = \n",
    "results# results['bert'][40] = load_results('bert', 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnds = range(500,600)\n",
    "decision_thresh = 0.5\n",
    "data_chunks = get_data_chunk(rnds)\n",
    "subjects = [d['nick'] for d in read_json_datapoint(\"data_server/data0.jl\")]\n",
    "# for key in ['transfer', 'ensemble']:\n",
    "for model_key in [\n",
    "                  'lstm_seq',\n",
    "                    'bert',\n",
    "                  'cnn_hierarch', \n",
    "                  'transfer', \n",
    "                  'ensemble',\n",
    "                ]:\n",
    "    print(model_key)\n",
    "    end_rnd = rnds[-1]\n",
    "#     if model_key=='lstm_seq':\n",
    "#         results[model_key][end_rnd]=load_results('lstm_seq', 20)\n",
    "    if model_key=='cnn_hierarch':\n",
    "        results[model_key][end_rnd]=load_results('cnn_hierarch', 40)\n",
    "    elif model_key=='bert':\n",
    "        results[model_key][end_rnd]=load_results('bert', end_rnd)\n",
    "    elif model_key=='ensemble':\n",
    "        model_keys_to_average=['bert', 'cnn_hierarch', 'lstm_seq']\n",
    "        missing_models = [m for m in model_keys_to_average if not results[m]]\n",
    "        if len(missing_models)!=0:\n",
    "            print(\"Missing models! cannot compute ensemble results\", missing_models)\n",
    "            continue\n",
    "        results_to_average = [results[m][end_rnd] for m in model_keys_to_average]\n",
    "#         results[model_key][end_rnd] = get_ensemble_results(rnd, results, \n",
    "#                                                 model_keys_to_average)\n",
    "        results[model_key][end_rnd] = get_ensemble_results(results_to_average)\n",
    "    ## For now\n",
    "    elif model_key=='transfer':\n",
    "        results[model_key][end_rnd]=get_transfer_results(\n",
    "            end_rnd, results, model_to_average='lstm_seq', rounds_back=60)\n",
    "#         results[model_key][end_rnd]=results['lstm_seq'][end_rnd]\n",
    "    ##\n",
    "    else:\n",
    "        with session_collection[model_key].as_default():\n",
    "            with session_collection[model_key].graph.as_default():\n",
    "                results[model_key][end_rnd] = predict_for_round_chunk(models_collection[model_key], \n",
    "                                              hyperparams_collection[model_key], hyperparams_features, \n",
    "                                              vocabulary_dict, \n",
    "                                          data_chunks, subjects=subjects, model_key=model_key, cache_round=499)\n",
    "\n",
    "    \n",
    "    print(len(results[model_key][end_rnd].values()), \"positive:\", \n",
    "      len([r for r in results[model_key][end_rnd].values() if r >=0.5]))\n",
    "    response1 = build_response(results[model_key][end_rnd], rnd=end_rnd, \n",
    "                               model_name=models_runs[model_key], decision_thresh=decision_thresh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['lstm_seq'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(list(results['transfer'][180].values())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "pearsonr(list(results['bert'][220].values()), list(results['bert'][180].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(list(results['lstm_seq'][160].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on eRisk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = {}\n",
    "# labels = {}\n",
    "# featuresall = {}\n",
    "# with session_collection['lstm_seq2'].as_default():\n",
    "#     with session_collection['lstm_seq2'].graph.as_default():\n",
    "        # for features, subjects, lbls in DataGenerator(user_level_data, subjects_split, \n",
    "        #                                          set_type='train', vocabulary=vocabulary_dict,\n",
    "        #                                        hierarchical=hyperparams1['hierarchical'],\n",
    "        #                                     seq_len=hyperparams1['maxlen'], batch_size=hyperparams1['batch_size'],\n",
    "        #                                          max_posts_per_user=None,\n",
    "        #                                        pad_with_duplication=False,\n",
    "        #                                         posts_per_group=hyperparams1['posts_per_group'],\n",
    "        #                                         post_groups_per_user=None, \n",
    "        #                                          sample_seqs=False, shuffle=False,\n",
    "        #                                                return_subjects=True):\n",
    "\n",
    "        #     predictions = loaded_model.predict_on_batch(features)\n",
    "        #     print(len(features[0]), len(subjects), len(predictions), len(labels), len(results))\n",
    "        #     for i,s in enumerate(subjects):\n",
    "        #         if s not in results:\n",
    "        #             results[s] = []\n",
    "        #             featuresall[s] = []\n",
    "        #         results[s].append(predictions[i].item())\n",
    "        #         featuresall[s].append([features[j][i] for j in range(len(features))])\n",
    "        #         labels[s] = lbls[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in results:\n",
    "    if not labels[subject]:\n",
    "        if np.std(results[subject])>0.0:\n",
    "            print(subject), print(results[subject][0], results[subject][-1]-results[subject][0])\n",
    "            pd.Series(results[subject]).rolling(window=5).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[featuresall[4278][i][0].sum() for i in range(len(featuresall[4278]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(user_level_data['subject4278']['raw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, s, y in DataGenerator(user_level_data, subjects_split, \n",
    "                                         set_type='test', vocabulary=vocabulary_dict,\n",
    "                                       hierarchical=hyperparams1['hierarchical'],\n",
    "                                    seq_len=hyperparams1['maxlen'], batch_size=hyperparams1['batch_size'],\n",
    "                                         max_posts_per_user=None,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams1['posts_per_group'],\n",
    "                                        post_groups_per_user=None, \n",
    "                                         sample_seqs=False, shuffle=False,\n",
    "                                               return_subjects=True):\n",
    "    print(\"subject\", s, \"features\", x[0].sum(axis=1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_key='lstm_seq'\n",
    "with session_collection[model_key].as_default():\n",
    "    with session_collection[model_key].graph.as_default():\n",
    "        res = models_collection[model_key].evaluate_generator(DataGenerator(user_level_data, subjects_split, \n",
    "                                              liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                         set_type='test', vocabulary=vocabulary_dict,\n",
    "                                       hierarchical=hyperparams_collection[model_key]['hierarchical'],\n",
    "                                    seq_len=hyperparams_collection[model_key]['maxlen'], \n",
    "                                    batch_size=hyperparams['batch_size'],\n",
    "                                         max_posts_per_user=None,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams_collection[model_key]['posts_per_group'],\n",
    "                                        post_groups_per_user=1,#None, \n",
    "                                         sample_seqs=False, shuffle=False,\n",
    "                                             compute_liwc=False))\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams_collection['cnn_bert'] = {'hierarchical': False, 'maxlen': 512, 'posts_per_group': 50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    (e, 'nrc') for e in emotions] + ['pers_pronouns'] + [\n",
    "    (c, 'liwc') for c in list(categories) if c in writings_df.columns] + [\n",
    "(st, 'stopword') for st in stopword_list]\n",
    "weights = model.get_layer('output_layer').get_weights()[0].tolist()[-(len(features)):]\n",
    "\n",
    "print(len(weights), len(features))\n",
    "feature_importance = {}\n",
    "for (i, f) in enumerate(features):\n",
    "    feature_importance[f] = weights[i][0]\n",
    "\n",
    "sorted(feature_importance.items(), key=lambda t: abs(t[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_point(subject, voc, hyperparams_features=hyperparams_features, nrc_lexicon=nrc_lexicon,\n",
    "                      emotions=emotions):\n",
    "    eval_writings_df = writings_df[writings_df['subject']==subject]\n",
    "    correct_label = eval_writings_df.label.values[0]\n",
    "    (x_train, y_train), (x_valid, y_valid), (x_test, y_test), voc = load_erisk_data(eval_writings_df,\n",
    "                        seq_len=hyperparams_features['maxlen'],\n",
    "                        voc_size=hyperparams_features['max_features'],\n",
    "                        emotion_lexicon=nrc_lexicon,\n",
    "                        emotions=emotions, user_level=False,\n",
    "                        train_prop=0.0, vocabulary=voc)\n",
    "    return x_test, y_test, correct_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_per_user(writings_df, majority_prop=0.2, train_prop=0.7, majority_nr=0, validate=False, voc=None,\n",
    "                    random=False, nr_slices=5, test_slice=2):\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    thresh=0.5\n",
    "    majority_proportion=majority_prop\n",
    "    valid_prop = 0.3\n",
    "    \n",
    "    if 'subset' in writings_df.columns:\n",
    "        training_subjects = list(set(writings_df[writings_df['subset']=='train'].subject))\n",
    "        test_subjects = list(set(writings_df[writings_df['subset']=='test'].subject))\n",
    "    else:\n",
    "        all_subjects = sorted(list(set(writings_df.subject)))\n",
    "        training_subjects_size = int(len(all_subjects) * train_prop)\n",
    "        test_subjects_size = len(all_subjects) - training_subjects_size\n",
    "        # Cross-validation, with fixed slice as input\n",
    "        test_prop = 1-train_prop\n",
    "        test_slice = min(test_slice, nr_slices)\n",
    "        logger.debug(\"start index: %f, from %f\\n\" % (\n",
    "            len(all_subjects)*(1/nr_slices)*test_slice, test_prop*test_slice))\n",
    "        start_slice = int(len(all_subjects)*(1/nr_slices)*test_slice)\n",
    "        test_subjects = all_subjects[start_slice: start_slice+test_subjects_size]\n",
    "        training_subjects = [s for s in all_subjects if s not in test_subjects]\n",
    "    training_subjects = sorted(training_subjects) # ensuring reproducibility\n",
    "    valid_subjects_size = int(len(training_subjects) * valid_prop)\n",
    "    valid_subjects = training_subjects[:valid_subjects_size]\n",
    "    training_subjects = training_subjects[valid_subjects_size:]\n",
    "    \n",
    "    if validate:\n",
    "        subjects = valid_subjects\n",
    "    else:\n",
    "        subjects = test_subjects\n",
    "    for subject in subjects:\n",
    "        x_test_user, y_test_user, label = get_data_for_point(subject, voc=voc)\n",
    "        outputs = model.predict(x_test_user)\n",
    "        if random:\n",
    "            sigma = np.std(outputs)\n",
    "            mu = np.mean(outputs)\n",
    "            print(\"generating random outputs with sigma\", sigma, \"and mu\", mu)\n",
    "            outputs = sigma*np.random.randn(len(outputs))+mu\n",
    "        positive_pred = sum(outputs>=thresh)\n",
    "        negative_pred = sum(outputs<thresh)\n",
    "        majority_pred = 0\n",
    "        if majority_proportion and positive_pred >= majority_proportion*negative_pred:\n",
    "            majority_pred = 1\n",
    "        if majority_nr and positive_pred>=majority_nr:\n",
    "            majority_pred = 1\n",
    "        if label == 1:\n",
    "            if majority_pred == 1:\n",
    "                tp+=1\n",
    "            else:\n",
    "                fn+=1\n",
    "        else:\n",
    "            if majority_pred == 0:\n",
    "                tn+=1\n",
    "            else:\n",
    "                fp+=1\n",
    "        print(negative_pred, positive_pred, majority_pred)\n",
    "        all_predictions.append(majority_pred)\n",
    "        all_labels.append(label)\n",
    "    def prec_recall_f1(tp, fp, tn, fn):\n",
    "        recall = tp/(tp+fn+0.0000001)\n",
    "        precision = tp/(tp+fp+0.0000001)\n",
    "        f1 = 2*precision*recall/(precision+recall+0.0000001)\n",
    "        print(\"Recall\", recall, \"Precision\", precision, \"F1\", f1)\n",
    "    if majority_prop:\n",
    "        print(\"Vote proportion\", majority_prop)\n",
    "    if majority_nr:\n",
    "        print(\"Vote points\", majority_nr)\n",
    "    prec_recall_f1(tp, fp, tn, fn)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_per_user(writings_df=writings_df, voc=voc, majority_prop=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_per_slice = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_slices=5\n",
    "logger.setLevel(logging.INFO)\n",
    "for tslice in range(nr_slices): \n",
    "    (x_train, y_train), (x_valid, y_valid), (x_test, y_test), voc = load_erisk_data(writings_df, \n",
    "                                                                seq_len=hyperparams_features['maxlen'],\n",
    "                                                                voc_size=hyperparams_features['max_features'],\n",
    "                                                               emotion_lexicon=nrc_lexicon,\n",
    "                                                               emotions=emotions,\n",
    "                                                               user_level=hyperparams_features['user_level'],\n",
    "                                                                                    test_slice=tslice,\n",
    "                                                                                    nr_slices=nr_slices,\n",
    "    #                                                            vocabulary=pickle.load(open('vocabulary20K_selfharm.pkl', 'rb'))\n",
    "                                                                                   logger=logger)\n",
    "    model, history = train_model(model, x_train, y_train, x_valid, y_valid,\n",
    "           epochs=200, batch_size=hyperparams['batch_size'],\n",
    "                      class_weight={0:0.5, 1:5}, start_epoch=0,\n",
    "                      callback_list = [freeze_layer, weights_history, reduce_lr],\n",
    "                      workers=2, verbose=0)\n",
    "    results_per_slice[tslice] = model.evaluate(x_test, y_test)\n",
    "    logger.info(\"Results for slice %d: %s\\n\" % (tslice, results_per_slice[tslice]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average F1 score: \", np.array([results_per_slice[s][1] for s in results_per_slice.keys()]).mean(),\n",
    "     \"all F1 scores: \", {s: v[1] for (s,v) in results_per_slice.items()} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tokens(row):\n",
    "    tokens = []\n",
    "    if row.tokenized_text:\n",
    "        tokens += row.tokenized_text\n",
    "    if row.tokenized_title:\n",
    "        tokens += row.tokenized_title\n",
    "    return tokens\n",
    "writings_df['all_tokens'] = writings_df.apply (lambda row: merge_tokens(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: include the title\n",
    "def extract_emotions(tokens, emotion, relative=True):\n",
    "    if not tokens:\n",
    "        return None\n",
    "    emotion_words = [t for t in tokens \n",
    "                     if t in nrc_lexicon[emotion]]\n",
    "    if relative and len(tokens):\n",
    "        return len(emotion_words) / len(tokens)\n",
    "    else:\n",
    "        return len(emotion_words)\n",
    "    \n",
    "    return encoded_emotions\n",
    "\n",
    "from functools import partial\n",
    "for emotion in emotions:\n",
    "    writings_df[emotion] = writings_df['all_tokens'].apply(partial(extract_emotions, emotion=emotion, \n",
    "                                                                   relative=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['pronouns'] = writings_df['all_tokens'].apply(partial(encode_pronouns, relative=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len'] + emotions].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len'] + emotions].groupby('label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentAnalyzer, SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid.polarity_scores(\"We are here today happiness is all around\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['neg_vader'] = writings_df.text.apply(lambda t: sid.polarity_scores(t)['neg']\n",
    "                                                 if type(t)==str else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['pos_vader'] = writings_df.text.apply(lambda t: sid.polarity_scores(t)['pos']\n",
    "                                                 if type(t)==str else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len', 'neg_vader', 'pos_vader'] + emotions].groupby('label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len', 'neg_vader', 'pos_vader'] + emotions].corr('spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from liwc_readDict import readDict\n",
    "\n",
    "liwc = readDict('/home/ana/resources/FakeOrFact/features/LIWC/LIWC/liwc.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [c for (w,c) in liwc]\n",
    "set(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_dict = {}\n",
    "for (w, c) in liwc:\n",
    "    if c not in liwc_dict:\n",
    "        liwc_dict[c] = []\n",
    "    liwc_dict[c].append(w)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_dict['pronoun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_liwc_categories(tokens, category_words, relative=True):\n",
    "    category_cnt = 0\n",
    "    if not tokens:\n",
    "        return None\n",
    "    text_len = len(tokens)\n",
    "    for t in tokens:\n",
    "        for word in category_words:\n",
    "            if t==word or (word[-1]=='*' and t.startswith(word[:-1])) \\\n",
    "            or (t==word.split(\"'\")[0]):\n",
    "                category_cnt += 1\n",
    "                break # one token cannot belong to more than one word in the category\n",
    "    if relative:\n",
    "        return category_cnt/text_len\n",
    "    else:\n",
    "        return category_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __encode_liwc_categories(self, tokens, relative=True):\n",
    "        categories_cnt = [0 for c in self.liwc_categories]\n",
    "        if not tokens:\n",
    "            return None\n",
    "        text_len = len(tokens)\n",
    "        for i, category in enumerate(self.liwc_categories):\n",
    "            for t in tokens:\n",
    "                print(\"token\", t)\n",
    "                if t in self.liwc_words_for_categories[category]:\n",
    "                    categories_cnt[i] += 1\n",
    "            if relative:\n",
    "                categories_cnt[i] = categories_cnt[i]/text_len\n",
    "        return categories_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from functools import partial\n",
    "# for categ in ['negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']:#liwc_dict.keys():\n",
    "for categ in liwc_dict.keys():\n",
    "    if categ in writings_df.columns:\n",
    "        continue\n",
    "    print(\"Encoding for category %s...\" % categ)\n",
    "    writings_df[categ] = writings_df['all_tokens'].apply(partial(encode_liwc_categories, \n",
    "                                                                   category_words=liwc_dict[categ], \n",
    "                                                                   relative=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# categories for all words in vocabulary\n",
    "# vocabulary = pickle.load(open(\"all_vocab_clpsych_erisk_50000.pkl\", \"rb\"))\n",
    "liwc_words_for_categories = {}\n",
    "for categ in liwc_dict.keys():\n",
    "    liwc_words_for_categories[categ] = set()\n",
    "    for word in liwc_dict[categ]:\n",
    "        for t in vocabulary:\n",
    "            if t==word or (word[-1]=='*' and t.startswith(word[:-1])) \\\n",
    "                or (t==word.split(\"'\")[0]):\n",
    "                    liwc_words_for_categories[categ].add(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(liwc_words_for_categories, open(\"data/liwc_categories_for_vocabulary_erisk_clpsych_stop_40K.pkl\", \"wb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df.groupby('subject').mean()[['label', 'negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df[['label', 'negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']].groupby('label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df.groupby('subject').mean()[['label'] + categories].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare your hyperparameters search:\n",
    "tune_epochs=150\n",
    "config = {\n",
    "      \"algorithm\": \"bayes\",\n",
    "      \"parameters\": {\n",
    "          \"lstm_units\": {\"type\": \"integer\", \"min\": 32, \"max\": 300},\n",
    "          \"lstm_units_user\": {\"type\": \"integer\", \"min\": 5, \"max\": 40},\n",
    "          \"dense_bow_units\": {\"type\": \"integer\", \"min\": 5, \"max\": 35},\n",
    "          \"filters\": {\"type\": \"integer\", \"min\": 30, \"max\": 250},\n",
    "          \"kernel_size\": {\"type\": \"integer\", \"min\": 3, \"max\": 7},\n",
    "          \"dense_sentence_units\": {\"type\": \"integer\", \"min\": 0, \"max\": 10},\n",
    "          \"dense_user_units\": {\"type\": \"integer\", \"min\": 0, \"max\": 0},\n",
    "          \"bert_dense_units\": {\"type\": \"integer\", \"min\": 10, \"max\": 200},\n",
    "          \"bert_finetune_layers\": {\"type\": \"integer\", \"min\": 0, \"max\": 2},\n",
    "          \"lr\": {\"type\": \"float\", \"min\": 0.0005, \"max\": 0.2, \"scalingType\": \"loguniform\"},\n",
    "          \"l2_dense\": {\"type\": \"float\", \"min\": 0.0000001, \"max\": 0.1, \"scalingType\": \"loguniform\"},\n",
    "          \"l2_embeddings\": {\"type\": \"float\", \"min\": 0.00000001, \"max\": 0.2, \"scalingType\": \"loguniform\"},\n",
    "          \"l2_bert\": {\"type\": \"float\", \"min\": 0.00000001, \"max\": 0.2, \"scalingType\": \"loguniform\"},\n",
    "          \"dropout\": {\"type\": \"float\", \"min\": 0, \"max\": 0.5, \"scalingType\": \"uniform\"},\n",
    "          \"norm_momentum\": {\"type\": \"float\", \"min\": 0, \"max\": 0.99, \"scalingType\": \"uniform\"},\n",
    "          \"optimizer\": {\"type\": \"categorical\", \"values\": [\"\"]},#\"adam\", \"adagrad\", \"\"]},\n",
    "          \"batch_size\": {\"type\": \"integer\", \"min\": 5, \"max\": 128, \"scalingType\": \"loguniform\"},\n",
    "          \"positive_class_weight\": {\"type\": \"integer\", \"min\": 3, \"max\": 10},\n",
    "          \"trainable_embeddings\": {\"type\": \"discrete\", \"values\": [True, False]},\n",
    "          \"sample_seqs\": {\"type\": \"discrete\", \"values\": [True, False]},\n",
    "          \"bert_trainable\": {\"type\": \"discrete\", \"values\": [True, False]},\n",
    "          \"bert_pooling\": {\"type\": \"categorical\", \"values\": ['first', 'mean']},\n",
    "#           \"hierarchical\": {\"type\": \"discrete\", \"values\": [True, False]},\n",
    "          \"freeze_patience\": {\"type\": \"integer\", \"min\": 2, \"max\": tune_epochs+1},\n",
    "          \"lr_reduce_factor\": {\"type\": \"float\", \"min\": 0.0001, \"max\": 0.8},\n",
    "          \"scheduled_lr_reduce_factor\": {\"type\": \"float\", \"min\": 0.0001, \"max\": 0.8},\n",
    "          \"lr_reduce_patience\": {\"type\": \"integer\", \"min\": 2, \"max\": tune_epochs+1},\n",
    "          \"scheduled_lr_reduce_patience\": {\"type\": \"integer\", \"min\": 2, \"max\": tune_epochs+1},\n",
    "          \"early_stopping_patience\": {\"type\": \"integer\", \"min\": 2, \"max\": tune_epochs+1},\n",
    "          \"decay\": {\"type\": \"float\", \"min\": 0.00000001, \"max\": 0.5, \"scalingType\": \"loguniform\"},\n",
    "#           \"ignore_layers_values\": {\"type\": \"categorical\", \"values\": [\"attention\", \"batchnorm\", \"bert_layer\"]},\n",
    "          \"sampling_distr\": {\"type\": \"categorical\", \"values\": [\"exp\", \"uniform\"]},\n",
    "          \"posts_per_group\": {\"type\": \"integer\", \"min\": 10, \"max\": 100},\n",
    "          \"post_groups_per_user\": {\"type\": \"integer\", \"min\": 1, \"max\": 50},\n",
    "#           \"posts_per_user\": {\"type\": \"integer\", \"min\": 0, \"max\": 1000},\n",
    "          \"maxlen\": {\"type\": \"integer\", \"min\":64, \"max\": 512},\n",
    "      },\n",
    "      \"spec\": {\n",
    "          \"metric\": \"loss\",\n",
    "          \"objective\": \"minimize\",\n",
    "      },\n",
    "  }\n",
    "optimizer = Optimizer(config, api_key=\"eoBdVyznAhfg3bK9pZ58ZSXfv\")\n",
    "\n",
    "hyperparams_config = hyperparams\n",
    "\n",
    "for experiment in optimizer.get_experiments(project_name=\"mental\"):\n",
    "    experiment.add_tag(\"tune\")\n",
    "    experiment.add_tag(dataset_type)\n",
    "    \n",
    "    print(hyperparams_config)\n",
    "    \n",
    "\n",
    "    data_generator_train = DataGenerator(user_level_data, subjects_split, set_type='train',\n",
    "                                         seq_len=hyperparams_config[\"maxlen\"],\n",
    "                                                     sample_seqs=hyperparams_config['sample_seqs'],\n",
    "                                                     sampling_distr=hyperparams_config['sampling_distr'],\n",
    "                                                    posts_per_group=hyperparams_config['posts_per_group'],\n",
    "                                                    post_groups_per_user=1,#hyperparams_config['post_groups_per_user'],\n",
    "                                                    max_posts_per_user=hyperparams_config['posts_per_user'],\n",
    "                                    hierarchical=hyperparams['hierarchical'])\n",
    "    data_generator_valid = DataGenerator(user_level_data, subjects_split, set_type='valid',\n",
    "                                         seq_len=hyperparams_config[\"maxlen\"],\n",
    "                                         \n",
    "                                                    posts_per_group=hyperparams_config['posts_per_group'],\n",
    "                                                    post_groups_per_user=hyperparams_config['post_groups_per_user'],\n",
    "                                                    max_posts_per_user=None,\n",
    "                                                    sample_seqs=False, \n",
    "                                                     shuffle=False,\n",
    "                                                hierarchical=hyperparams['hierarchical'])\n",
    "    try:\n",
    "        if hyperparams['hierarchical']:\n",
    "            model = build_hierarchical_model(hyperparams_config, hyperparams_features, embedding_matrix, emotions, stopword_list,\n",
    "                            liwc_categories=[c for c in categories if c in writings_df.columns]\n",
    "        ,\n",
    "                           ignore_layer=hyperparams_config['ignore_layer'])\n",
    "        else:\n",
    "            model = build_model(hyperparams_config, hyperparams_features, embedding_matrix, emotions, stopword_list,\n",
    "                            liwc_categories=[c for c in categories if c in writings_df.columns]\n",
    "        ,\n",
    "                           ignore_layer=hyperparams_config['ignore_layer'])\n",
    "        model.summary()\n",
    "\n",
    "        freeze_layer = FreezeLayer(model, patience=hyperparams['freeze_patience'], set_to=not hyperparams['trainable_embeddings'])\n",
    "        weights_history = WeightsHistory()\n",
    "        lr_history = LRHistory()\n",
    "        reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=hyperparams['reduce_lr_factor'],\n",
    "                                  patience=hyperparams['reduce_lr_patience'], min_lr=0.000001, verbose=1)\n",
    "        lr_schedule = callbacks.LearningRateScheduler(lambda epoch, lr: \n",
    "                                                      lr if (epoch+1)%hyperparams['scheduled_reduce_lr_freq']!=0 else\n",
    "                                                      lr*hyperparams['scheduled_reduce_lr_factor'], verbose=1)\n",
    "\n",
    "        model, history = train_model(model, hyperparams, data_generator_train, data_generator_valid,\n",
    "                           epochs=tune_epochs,\n",
    "                          class_weight={0:1, 1:experiment.get_parameter('positive_class_weight')}, \n",
    "                                     start_epoch=0,\n",
    "                          callback_list = [\n",
    "    #                                  weights_history, \n",
    "                                           reduce_lr, \n",
    "    #                                        lr_history, \n",
    "                                           lr_schedule\n",
    "                                          ],\n",
    "                          model_path='models/experiment', workers=4)\n",
    "    except tf.errors.ResourceExhaustedError as e:\n",
    "        print(e)\n",
    "        sess.close()\n",
    "        sess = tf.Session(config=sess_config)\n",
    "        initialize_vars(sess)\n",
    "\n",
    "    \n",
    "    loss = history.history['loss'][-1]\n",
    "    \n",
    "    \n",
    "    # Test the model\n",
    "    for param in config['parameters'].keys():    \n",
    "        hyperparams_config[param] = experiment.get_parameter(param)\n",
    "    if not hyperparams_config['optimizer']:\n",
    "        hyperparams_config['optimizer'] = optimizers.Adam(lr=hyperparams_config['lr'], \n",
    "                                   decay=hyperparams_config['decay'])\n",
    "#     hyperparams_config[\"ignore_layer\"] = []\n",
    "#     if hyperparams_config[\"ignore_layers_values\"]:\n",
    "#         hyperparams_config[\"ignore_layer\"] = [hyperparams_config[\"ignore_layers_values\"]]\n",
    "    hyperparams_config[\"ignore_layer\"] = [\"bert_layer\", \"batchnorm\"]\n",
    "        \n",
    "#     freeze_layer = FreezeLayer(model, patience=experiment.get_parameter('freeze_patience'),\n",
    "#                               set_to=not experiment.get_parameter('trainable_embeddings'))\n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            factor=experiment.get_parameter('lr_reduce_factor'),\n",
    "                                            patience=experiment.get_parameter('lr_reduce_patience'), \n",
    "                                            min_lr=0.00000001, verbose=1)\n",
    "    \n",
    "    # Report the loss, if not auto-logged:\n",
    "    experiment.log_metric(\"loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tfconda)",
   "language": "python",
   "name": "tfconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "362px",
    "width": "218px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
