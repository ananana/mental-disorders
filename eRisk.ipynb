{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob, os\n",
    "import numpy as np\n",
    "from comet_ml import Experiment, Optimizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_KERAS'] = '1'\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, Input, concatenate\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import callbacks, optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# only reserve 1 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_subject_writings(subject_file):\n",
    "    writings = []\n",
    "    with open(subject_file) as sf:\n",
    "        contents = sf.read()\n",
    "        root = ET.fromstring(contents)\n",
    "        try:\n",
    "            subject = root.findall('ID')[0].text.strip()\n",
    "        except Exception:\n",
    "            print('Cannot extract ID', contents[:500], '\\n-------\\n')        \n",
    "        for w in root.iter('WRITING'):\n",
    "            subject_writings = {'subject': subject}\n",
    "            for title in w.findall('TITLE'):\n",
    "                subject_writings['title'] = title.text\n",
    "            for text in w.findall('TEXT'):\n",
    "                subject_writings['text'] = text.text\n",
    "            for date in w.findall('DATE'):\n",
    "                subject_writings['date'] = date.text\n",
    "            writings.append(subject_writings)\n",
    "    return writings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dir = '/home/bighanem/ana_data/' \n",
    "root_dir = '/home/ana/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eRisk 2020 T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir_T1 = root_dir + '/eRisk/data/eRisk2020_T1_train/eRISK2020_T1_training_data/eRISK2020_training_data/data/'\n",
    "labels_file_T1 = root_dir + '/eRisk/data//eRisk2020_T1_train/eRISK2020_T1_training_data/eRISK2020_training_data/golden_truth.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_2020(datadir_T1, labels_file_T1):\n",
    "    writings = []\n",
    "    for subject_file in os.listdir(datadir_T1):\n",
    "        print(subject_file)\n",
    "        writings.extend(read_subject_writings(os.path.join(datadir_T1, subject_file)))\n",
    "    writings_df = pd.DataFrame(writings)\n",
    "\n",
    "    labels_T1 = pd.read_csv(labels_file_T1, delimiter=' ', names=['subject', 'label'])\n",
    "    labels_T1 = labels_T1.set_index('subject')\n",
    "\n",
    "    writings_df['label'] = writings_df['subject'].apply(\n",
    "    lambda s: labels_T1.loc[s, 'label'])\n",
    "    \n",
    "    return writings_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eRisk 2019 T1 (Anorexia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadirs_T1_2019 = {\n",
    "    'train': ['2018 test/', '2018 train/positive_examples/', '2018 train/negative_examples/'],\n",
    "    'test': ['data/']\n",
    "}\n",
    "datadir_root_T1_2019 = {\n",
    "    'train': root_dir + '/eRisk/data/past/eRisk2019_T1/training data - t1/',\n",
    "    'test': root_dir + '/eRisk/data/past/eRisk2019_T1/test data - T1/'\n",
    "}\n",
    "    \n",
    "labels_files_T1_2019 = {\n",
    "    'train': ['2018 train/risk_golden_truth.txt', '2018 test/risk-golden-truth-test.txt'],\n",
    "    'test': ['T1_erisk_golden_truth.txt']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_2019(datadir_root_T1_2019,\n",
    "                   datadirs_T1_2019,\n",
    "                   labels_files_T1_2019,\n",
    "                   test_suffix='0000'):\n",
    "    writings = {'train': [], 'test': []}\n",
    "    writings_df = pd.DataFrame()\n",
    "    labels_df = pd.DataFrame()\n",
    "\n",
    "    for subset in ('train', 'test'):\n",
    "        for subdir in [os.path.join(datadir_root_T1_2019[subset], subp) for subp in datadirs_T1_2019[subset]]:\n",
    "            if subset=='train':\n",
    "                chunkdirs = [os.path.join(datadir_root_T1_2019[subset], subdir, chunkdir) \n",
    "                             for chunkdir in os.listdir(subdir)]\n",
    "            else:\n",
    "                chunkdirs = [os.path.join(datadir_root_T1_2019[subset], subdir)]\n",
    "                \n",
    "            for chunkdir in chunkdirs:\n",
    "                if not os.path.isdir(chunkdir):\n",
    "                    continue\n",
    "                for subject_file in os.listdir(chunkdir):\n",
    "                    writings[subset].extend(read_subject_writings(os.path.join(chunkdir, subject_file)))\n",
    "        writings_df_part = pd.DataFrame(writings[subset])\n",
    "        # add a suffix for users in the test -- the numbers are duplicated with the ones in train\n",
    "        if subset=='test':\n",
    "            writings_df_part['subject'] = writings_df_part['subject'].apply(lambda s: s+test_suffix)\n",
    "            print(subset, writings_df_part.subject)\n",
    "        writings_df_part['subset'] = subset\n",
    "        writings_df = pd.concat([writings_df, writings_df_part])\n",
    "        writings_df.reindex()\n",
    "\n",
    "        for label_file in labels_files_T1_2019[subset]:\n",
    "            labels = pd.read_csv(os.path.join(datadir_root_T1_2019[subset], label_file), \n",
    "                                 delimiter='\\s+', names=['subject', 'label'])\n",
    "            # add a suffix for users in the test -- the numbers are duplicated with the ones in train\n",
    "            if subset=='test':\n",
    "                labels['subject'] = labels['subject'].apply(lambda s: s+test_suffix)\n",
    "            labels_df = pd.concat([labels_df, labels])\n",
    "    labels_df = labels_df.drop_duplicates()\n",
    "    labels_df = labels_df.set_index('subject')\n",
    "\n",
    "    writings_df = writings_df.drop_duplicates()\n",
    "    \n",
    "    writings_df = writings_df.join(labels_df, on='subject')\n",
    "    \n",
    "    return writings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject8292.xml\n",
      "subject6644.xml\n",
      "subject7982.xml\n",
      "subject9260.xml\n",
      "subject9918.xml\n",
      "subject4284.xml\n",
      "subject9829.xml\n",
      "subject7661.xml\n",
      "subject8361.xml\n",
      "subject4831.xml\n",
      "subject2181.xml\n",
      "subject9077.xml\n",
      "subject2922.xml\n",
      "subject2238.xml\n",
      "subject4513.xml\n",
      "subject269.xml\n",
      "subject2678.xml\n",
      "subject9197.xml\n",
      "subject4143.xml\n",
      "subject2605.xml\n",
      "subject4226.xml\n",
      "subject7627.xml\n",
      "subject5150.xml\n",
      "subject4510.xml\n",
      "subject2182.xml\n",
      "subject280.xml\n",
      "subject1105.xml\n",
      "subject187.xml\n",
      "subject8001.xml\n",
      "subject9285.xml\n",
      "subject2621.xml\n",
      "subject4414.xml\n",
      "subject2685.xml\n",
      "subject9961.xml\n",
      "subject8065.xml\n",
      "subject8225.xml\n",
      "subject6866.xml\n",
      "subject9949.xml\n",
      "subject1507.xml\n",
      "subject8329.xml\n",
      "subject9411.xml\n",
      "subject7857.xml\n",
      "subject1545.xml\n",
      "subject9811.xml\n",
      "subject5000.xml\n",
      "subject4843.xml\n",
      "subject569.xml\n",
      "subject51.xml\n",
      "subject9156.xml\n",
      "subject6453.xml\n",
      "subject1210.xml\n",
      "subject5528.xml\n",
      "subject1485.xml\n",
      "subject5935.xml\n",
      "subject4527.xml\n",
      "subject3301.xml\n",
      "subject4074.xml\n",
      "subject6093.xml\n",
      "subject2088.xml\n",
      "subject8990.xml\n",
      "subject6459.xml\n",
      "subject7830.xml\n",
      "subject8395.xml\n",
      "subject4247.xml\n",
      "subject3667.xml\n",
      "subject5003.xml\n",
      "subject992.xml\n",
      "subject5644.xml\n",
      "subject242.xml\n",
      "subject7764.xml\n",
      "subject3283.xml\n",
      "subject6322.xml\n",
      "subject7678.xml\n",
      "subject6668.xml\n",
      "subject4333.xml\n",
      "subject1288.xml\n",
      "subject8200.xml\n",
      "subject5383.xml\n",
      "subject9039.xml\n",
      "subject7698.xml\n",
      "subject9652.xml\n",
      "subject5223.xml\n",
      "subject9725.xml\n",
      "subject1512.xml\n",
      "subject3994.xml\n",
      "subject7018.xml\n",
      "subject3644.xml\n",
      "subject1786.xml\n",
      "subject1027.xml\n",
      "subject8094.xml\n",
      "subject974.xml\n",
      "subject2947.xml\n",
      "subject9575.xml\n",
      "subject4570.xml\n",
      "subject5062.xml\n",
      "subject4729.xml\n",
      "subject5100.xml\n",
      "subject5177.xml\n",
      "subject505.xml\n",
      "subject5974.xml\n",
      "subject7499.xml\n",
      "subject1264.xml\n",
      "subject4071.xml\n",
      "subject7740.xml\n",
      "subject8721.xml\n",
      "subject9222.xml\n",
      "subject8432.xml\n",
      "subject2547.xml\n",
      "subject5995.xml\n",
      "subject6930.xml\n",
      "subject8472.xml\n",
      "subject6918.xml\n",
      "subject4198.xml\n",
      "subject501.xml\n",
      "subject7777.xml\n",
      "subject5375.xml\n",
      "subject7229.xml\n",
      "subject4762.xml\n",
      "subject5622.xml\n",
      "subject7637.xml\n",
      "subject47.xml\n",
      "subject1962.xml\n",
      "subject8795.xml\n",
      "subject4785.xml\n",
      "subject5840.xml\n",
      "subject3014.xml\n",
      "subject6464.xml\n",
      "subject522.xml\n",
      "subject5984.xml\n",
      "subject641.xml\n",
      "subject7326.xml\n",
      "subject4227.xml\n",
      "subject7428.xml\n",
      "subject203.xml\n",
      "subject6946.xml\n",
      "subject4563.xml\n",
      "subject682.xml\n",
      "subject9014.xml\n",
      "subject7435.xml\n",
      "subject8626.xml\n",
      "subject4459.xml\n",
      "subject733.xml\n",
      "subject7238.xml\n",
      "subject6428.xml\n",
      "subject7262.xml\n",
      "subject0.xml\n",
      "subject2269.xml\n",
      "subject8233.xml\n",
      "subject2522.xml\n",
      "subject5456.xml\n",
      "subject1064.xml\n",
      "subject8822.xml\n",
      "subject5033.xml\n",
      "subject1089.xml\n",
      "subject3277.xml\n",
      "subject5549.xml\n",
      "subject6352.xml\n",
      "subject6652.xml\n",
      "subject7669.xml\n",
      "subject5833.xml\n",
      "subject4795.xml\n",
      "subject4002.xml\n",
      "subject5878.xml\n",
      "subject1524.xml\n",
      "subject3928.xml\n",
      "subject9318.xml\n",
      "subject2935.xml\n",
      "subject1093.xml\n",
      "subject6786.xml\n",
      "subject3612.xml\n",
      "subject9114.xml\n",
      "subject4719.xml\n",
      "subject7439.xml\n",
      "subject1623.xml\n",
      "subject6290.xml\n",
      "subject8973.xml\n",
      "subject3844.xml\n",
      "subject7898.xml\n",
      "subject3605.xml\n",
      "subject2097.xml\n",
      "subject9381.xml\n",
      "subject3178.xml\n",
      "subject5908.xml\n",
      "subject3191.xml\n",
      "subject4196.xml\n",
      "subject8882.xml\n",
      "subject8845.xml\n",
      "subject5256.xml\n",
      "subject7318.xml\n",
      "subject4777.xml\n",
      "subject6309.xml\n",
      "subject4479.xml\n",
      "subject9393.xml\n",
      "subject4961.xml\n",
      "subject6247.xml\n",
      "subject1055.xml\n",
      "subject4644.xml\n",
      "subject7338.xml\n",
      "subject6284.xml\n",
      "subject5699.xml\n",
      "subject2580.xml\n",
      "subject2446.xml\n",
      "subject5409.xml\n",
      "subject1914.xml\n",
      "subject7263.xml\n",
      "subject5148.xml\n",
      "subject1793.xml\n",
      "subject9729.xml\n",
      "subject7952.xml\n",
      "subject9917.xml\n",
      "subject3868.xml\n",
      "subject5793.xml\n",
      "subject4934.xml\n",
      "subject3674.xml\n",
      "subject6019.xml\n",
      "subject2974.xml\n",
      "subject2857.xml\n",
      "subject855.xml\n",
      "subject5937.xml\n",
      "subject671.xml\n",
      "subject4318.xml\n",
      "subject5112.xml\n",
      "subject9249.xml\n",
      "subject7107.xml\n",
      "subject2996.xml\n",
      "subject5603.xml\n",
      "subject511.xml\n",
      "subject6518.xml\n",
      "subject5140.xml\n",
      "subject3737.xml\n",
      "subject9095.xml\n",
      "subject3227.xml\n",
      "subject7355.xml\n",
      "subject1617.xml\n",
      "subject6670.xml\n",
      "subject5387.xml\n",
      "subject3883.xml\n",
      "subject6146.xml\n",
      "subject2949.xml\n",
      "subject1763.xml\n",
      "subject2980.xml\n",
      "subject8933.xml\n",
      "subject6833.xml\n",
      "subject8802.xml\n",
      "subject8657.xml\n",
      "subject6259.xml\n",
      "subject1947.xml\n",
      "subject3635.xml\n",
      "subject8978.xml\n",
      "subject6423.xml\n",
      "subject1748.xml\n",
      "subject4702.xml\n",
      "subject8062.xml\n",
      "subject3555.xml\n",
      "subject2577.xml\n",
      "subject2475.xml\n",
      "subject8357.xml\n",
      "subject9492.xml\n",
      "subject3914.xml\n",
      "subject2495.xml\n",
      "subject7581.xml\n",
      "subject3725.xml\n",
      "subject6173.xml\n",
      "subject2247.xml\n",
      "subject8481.xml\n",
      "subject7946.xml\n",
      "subject7131.xml\n",
      "subject4848.xml\n",
      "subject747.xml\n",
      "subject5270.xml\n",
      "subject5979.xml\n",
      "subject6041.xml\n",
      "subject1950.xml\n",
      "subject7333.xml\n",
      "subject7247.xml\n",
      "subject814.xml\n",
      "subject5938.xml\n",
      "subject9160.xml\n",
      "subject6238.xml\n",
      "subject6957.xml\n",
      "subject8770.xml\n",
      "subject9497.xml\n",
      "subject807.xml\n",
      "subject6899.xml\n",
      "subject4014.xml\n",
      "subject2696.xml\n",
      "subject1885.xml\n",
      "subject8064.xml\n",
      "subject8081.xml\n",
      "subject2690.xml\n",
      "subject7462.xml\n",
      "subject8193.xml\n",
      "subject4526.xml\n",
      "subject7316.xml\n",
      "subject7290.xml\n",
      "subject463.xml\n",
      "subject4379.xml\n",
      "subject3181.xml\n",
      "subject5920.xml\n",
      "subject1728.xml\n",
      "subject2567.xml\n",
      "subject3904.xml\n",
      "subject4392.xml\n",
      "subject8581.xml\n",
      "subject9242.xml\n",
      "subject379.xml\n",
      "subject3881.xml\n",
      "subject8565.xml\n",
      "subject4505.xml\n",
      "subject3977.xml\n",
      "subject7489.xml\n",
      "subject2948.xml\n",
      "subject5342.xml\n",
      "subject8544.xml\n",
      "subject6903.xml\n",
      "subject7377.xml\n",
      "subject8769.xml\n",
      "subject3270.xml\n",
      "subject3224.xml\n",
      "subject2239.xml\n",
      "subject7801.xml\n",
      "subject3596.xml\n",
      "subject1469.xml\n",
      "subject4278.xml\n",
      "subject5282.xml\n",
      "subject3357.xml\n",
      "subject6013.xml\n",
      "subject5036.xml\n",
      "subject796.xml\n",
      "subject7692.xml\n",
      "subject7560.xml\n",
      "subject6035.xml\n",
      "subject1824.xml\n",
      "subject8726.xml\n",
      "subject6665.xml\n",
      "subject835.xml\n",
      "subject3117.xml\n",
      "subject519.xml\n",
      "subject1655.xml\n",
      "subject217.xml\n"
     ]
    }
   ],
   "source": [
    "writings_df = read_texts_2020(datadir_T1, labels_file_T1)\n",
    "# writings_df = read_texts_2019(datadir_root_T1_2019,\n",
    "#                    datadirs_T1_2019,\n",
    "#                    labels_files_T1_2019)\n",
    "# writings_df = pickle.load(open('writings_df_liwc3', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# writings_df[writings_df['subset']=='test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ana/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>If anyone could help with which sub to put thi...</td>\n",
       "      <td>2016-08-02 09:22:12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>I'm literally never gonna stop waiting...</td>\n",
       "      <td>2016-08-05 09:35:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>This is a really interesting study! Makes sens...</td>\n",
       "      <td>2016-08-05 21:36:24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>Mostly always me during this whole charade</td>\n",
       "      <td>2016-08-09 08:39:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>What is your best advice to a healthy, success...</td>\n",
       "      <td>2016-11-02 05:33:33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170664</th>\n",
       "      <td>subject217</td>\n",
       "      <td>scary</td>\n",
       "      <td>2018-06-24 14:26:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170668</th>\n",
       "      <td>subject217</td>\n",
       "      <td>rescuing man after his car got stuck on Rub' a...</td>\n",
       "      <td>2018-07-05 15:31:29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170680</th>\n",
       "      <td>subject217</td>\n",
       "      <td>Gorilla in streets of Riyadh,KSA</td>\n",
       "      <td>2018-07-24 22:22:48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170681</th>\n",
       "      <td>subject217</td>\n",
       "      <td>Gorilla in streets of Riyadh,KSA</td>\n",
       "      <td>2018-07-24 22:46:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170696</th>\n",
       "      <td>subject217</td>\n",
       "      <td>WHAT DID YOU SAY TO ME?</td>\n",
       "      <td>2018-08-20 10:54:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42757 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            subject                                              title  \\\n",
       "0       subject8292  If anyone could help with which sub to put thi...   \n",
       "1       subject8292          I'm literally never gonna stop waiting...   \n",
       "2       subject8292  This is a really interesting study! Makes sens...   \n",
       "4       subject8292         Mostly always me during this whole charade   \n",
       "7       subject8292  What is your best advice to a healthy, success...   \n",
       "...             ...                                                ...   \n",
       "170664   subject217                                              scary   \n",
       "170668   subject217  rescuing man after his car got stuck on Rub' a...   \n",
       "170680   subject217                   Gorilla in streets of Riyadh,KSA   \n",
       "170681   subject217                   Gorilla in streets of Riyadh,KSA   \n",
       "170696   subject217                            WHAT DID YOU SAY TO ME?   \n",
       "\n",
       "                       date text  label  \n",
       "0       2016-08-02 09:22:12  NaN      0  \n",
       "1       2016-08-05 09:35:55  NaN      0  \n",
       "2       2016-08-05 21:36:24  NaN      0  \n",
       "4       2016-08-09 08:39:41  NaN      0  \n",
       "7       2016-11-02 05:33:33  NaN      0  \n",
       "...                     ...  ...    ...  \n",
       "170664  2018-06-24 14:26:01  NaN      0  \n",
       "170668  2018-07-05 15:31:29  NaN      0  \n",
       "170680  2018-07-24 22:22:48  NaN      0  \n",
       "170681  2018-07-24 22:46:11  NaN      0  \n",
       "170696  2018-08-20 10:54:11  NaN      0  \n",
       "\n",
       "[42757 rows x 5 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[writings_df['text'].isna()][~writings_df['title'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f237f104d10>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "writings_df.label.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>If anyone could help with which sub to put thi...</td>\n",
       "      <td>2016-08-02 09:22:12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>I'm literally never gonna stop waiting...</td>\n",
       "      <td>2016-08-05 09:35:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>This is a really interesting study! Makes sens...</td>\n",
       "      <td>2016-08-05 21:36:24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>The only thing Frank is building ...</td>\n",
       "      <td>2016-08-07 23:35:23</td>\n",
       "      <td>... Is hype. Think about it, every time he wor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>Mostly always me during this whole charade</td>\n",
       "      <td>2016-08-09 08:39:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170693</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 11:29:21</td>\n",
       "      <td>this is my personal experience ,it may not ref...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170694</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 16:17:34</td>\n",
       "      <td>stop looking at 20 million saudis as one entit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170695</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 20:00:31</td>\n",
       "      <td>i am aware of stats now and then. i was just s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170696</th>\n",
       "      <td>subject217</td>\n",
       "      <td>WHAT DID YOU SAY TO ME?</td>\n",
       "      <td>2018-08-20 10:54:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170697</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-20 12:07:44</td>\n",
       "      <td>me smellz fish,me find no fish!...what the fuc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170698 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            subject                                              title  \\\n",
       "0       subject8292  If anyone could help with which sub to put thi...   \n",
       "1       subject8292          I'm literally never gonna stop waiting...   \n",
       "2       subject8292  This is a really interesting study! Makes sens...   \n",
       "3       subject8292               The only thing Frank is building ...   \n",
       "4       subject8292         Mostly always me during this whole charade   \n",
       "...             ...                                                ...   \n",
       "170693   subject217                                                NaN   \n",
       "170694   subject217                                                NaN   \n",
       "170695   subject217                                                NaN   \n",
       "170696   subject217                            WHAT DID YOU SAY TO ME?   \n",
       "170697   subject217                                                NaN   \n",
       "\n",
       "                       date  \\\n",
       "0       2016-08-02 09:22:12   \n",
       "1       2016-08-05 09:35:55   \n",
       "2       2016-08-05 21:36:24   \n",
       "3       2016-08-07 23:35:23   \n",
       "4       2016-08-09 08:39:41   \n",
       "...                     ...   \n",
       "170693  2018-08-19 11:29:21   \n",
       "170694  2018-08-19 16:17:34   \n",
       "170695  2018-08-19 20:00:31   \n",
       "170696  2018-08-20 10:54:11   \n",
       "170697  2018-08-20 12:07:44   \n",
       "\n",
       "                                                     text  label  \n",
       "0                                                     NaN      0  \n",
       "1                                                     NaN      0  \n",
       "2                                                     NaN      0  \n",
       "3       ... Is hype. Think about it, every time he wor...      0  \n",
       "4                                                     NaN      0  \n",
       "...                                                   ...    ...  \n",
       "170693  this is my personal experience ,it may not ref...      0  \n",
       "170694  stop looking at 20 million saudis as one entit...      0  \n",
       "170695  i am aware of stats now and then. i was just s...      0  \n",
       "170696                                                NaN      0  \n",
       "170697  me smellz fish,me find no fish!...what the fuc...      0  \n",
       "\n",
       "[170698 rows x 5 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def tokenize(t):\n",
    "    return tokenizer.tokenize(t.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'wasn', 't', 'ready', 'to', 'leave', 'buh', 'buw', 'dd', 'sasa']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"I wasn't ready to leave! buh-buw(dd). Sasa .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['tokenized_title'] = writings_df['title'].apply(lambda t: tokenize(t) if type(t)==str and t else None)\n",
    "writings_df['title_len'] = writings_df['tokenized_title'].apply(lambda t: len(t) if type(t)==list and t else None)\n",
    "writings_df['tokenized_text'] = writings_df['text'].apply(lambda t: tokenize(t) if type(t)==str and t else None)\n",
    "writings_df['text_len'] = writings_df['tokenized_text'].apply(lambda t: len(t) if type(t)==list and t else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    127604.00000\n",
       "mean         32.35415\n",
       "std          82.68303\n",
       "min           1.00000\n",
       "25%           6.00000\n",
       "50%          13.00000\n",
       "75%          31.00000\n",
       "max        7201.00000\n",
       "Name: text_len, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.text_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    49752.000000\n",
       "mean        10.701922\n",
       "std          9.282147\n",
       "min          1.000000\n",
       "25%          4.000000\n",
       "50%          8.000000\n",
       "75%         14.000000\n",
       "max        149.000000\n",
       "Name: title_len, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.title_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title_len</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subject0</th>\n",
       "      <td>0</td>\n",
       "      <td>20.285714</td>\n",
       "      <td>31.711712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1027</th>\n",
       "      <td>0</td>\n",
       "      <td>7.769231</td>\n",
       "      <td>1.190476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1055</th>\n",
       "      <td>0</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>79.983193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1064</th>\n",
       "      <td>1</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>68.410256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1089</th>\n",
       "      <td>0</td>\n",
       "      <td>9.823529</td>\n",
       "      <td>13.254902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9917</th>\n",
       "      <td>1</td>\n",
       "      <td>8.983607</td>\n",
       "      <td>95.806897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9918</th>\n",
       "      <td>0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.900901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject992</th>\n",
       "      <td>0</td>\n",
       "      <td>5.872928</td>\n",
       "      <td>19.914122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9949</th>\n",
       "      <td>0</td>\n",
       "      <td>10.609756</td>\n",
       "      <td>42.346979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9961</th>\n",
       "      <td>0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>26.389313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>340 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             label  title_len   text_len\n",
       "subject                                 \n",
       "subject0         0  20.285714  31.711712\n",
       "subject1027      0   7.769231   1.190476\n",
       "subject1055      0  16.666667  79.983193\n",
       "subject1064      1  13.000000  68.410256\n",
       "subject1089      0   9.823529  13.254902\n",
       "...            ...        ...        ...\n",
       "subject9917      1   8.983607  95.806897\n",
       "subject9918      0   5.000000  11.900901\n",
       "subject992       0   5.872928  19.914122\n",
       "subject9949      0  10.609756  42.346979\n",
       "subject9961      0   5.000000  26.389313\n",
       "\n",
       "[340 rows x 3 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.groupby('subject').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title_len</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>299</td>\n",
       "      <td>296</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>40</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date  title_len  text_len\n",
       "label                           \n",
       "0       299        296       299\n",
       "1        41         40        41"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.groupby('subject').max().groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of posts per user 146.35882352941175\n",
      "Average number of comments per user 376.2970588235294\n"
     ]
    }
   ],
   "source": [
    "print(\"Average number of posts per user\", writings_df.groupby('subject').count().title.mean())\n",
    "print(\"Average number of comments per user\", writings_df.groupby('subject').count().text.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>title_len</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>Okay friends so I messed up and posted to do a...</td>\n",
       "      <td>2017-04-25 22:37:57</td>\n",
       "      <td>Sorry for that, I truly didn't think it was go...</td>\n",
       "      <td>0</td>\n",
       "      <td>[okay, friends, so, i, messed, up, and, posted...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>[sorry, for, that, i, truly, didn, t, think, i...</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-09-16 06:29:13</td>\n",
       "      <td>You've got plenty of time to fix that. You can...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you, ve, got, plenty, of, time, to, fix, that...</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-11-24 01:33:22</td>\n",
       "      <td>LCD, Glass animals, Kendrick, The Weeknd, Jack...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[lcd, glass, animals, kendrick, the, weeknd, j...</td>\n",
       "      <td>127.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>Getting that coachella bod</td>\n",
       "      <td>2018-01-09 00:54:06</td>\n",
       "      <td>First I want to say whatever skin is your skin...</td>\n",
       "      <td>0</td>\n",
       "      <td>[getting, that, coachella, bod]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[first, i, want, to, say, whatever, skin, is, ...</td>\n",
       "      <td>149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-03-12 17:14:03</td>\n",
       "      <td>Not the same but me and my wife saw a man and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[not, the, same, but, me, and, my, wife, saw, ...</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170652</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-05-28 12:23:00</td>\n",
       "      <td>/r/keto /r/ketorecipes /r/ketodessert all are ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[r, keto, r, ketorecipes, r, ketodessert, all,...</td>\n",
       "      <td>197.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170653</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-05-28 12:32:36</td>\n",
       "      <td>its okay dont worry . as long as you don't exc...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[its, okay, dont, worry, as, long, as, you, do...</td>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170662</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-06-20 00:33:57</td>\n",
       "      <td>the national number is :1919 here are more com...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[the, national, number, is, 1919, here, are, m...</td>\n",
       "      <td>115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170693</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 11:29:21</td>\n",
       "      <td>this is my personal experience ,it may not ref...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[this, is, my, personal, experience, it, may, ...</td>\n",
       "      <td>153.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170695</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 20:00:31</td>\n",
       "      <td>i am aware of stats now and then. i was just s...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[i, am, aware, of, stats, now, and, then, i, w...</td>\n",
       "      <td>198.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7655 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            subject                                              title  \\\n",
       "122     subject8292  Okay friends so I messed up and posted to do a...   \n",
       "390     subject8292                                                NaN   \n",
       "498     subject8292                                                NaN   \n",
       "752     subject8292                         Getting that coachella bod   \n",
       "904     subject8292                                                NaN   \n",
       "...             ...                                                ...   \n",
       "170652   subject217                                                NaN   \n",
       "170653   subject217                                                NaN   \n",
       "170662   subject217                                                NaN   \n",
       "170693   subject217                                                NaN   \n",
       "170695   subject217                                                NaN   \n",
       "\n",
       "                       date  \\\n",
       "122     2017-04-25 22:37:57   \n",
       "390     2017-09-16 06:29:13   \n",
       "498     2017-11-24 01:33:22   \n",
       "752     2018-01-09 00:54:06   \n",
       "904     2018-03-12 17:14:03   \n",
       "...                     ...   \n",
       "170652  2018-05-28 12:23:00   \n",
       "170653  2018-05-28 12:32:36   \n",
       "170662  2018-06-20 00:33:57   \n",
       "170693  2018-08-19 11:29:21   \n",
       "170695  2018-08-19 20:00:31   \n",
       "\n",
       "                                                     text  label  \\\n",
       "122     Sorry for that, I truly didn't think it was go...      0   \n",
       "390     You've got plenty of time to fix that. You can...      0   \n",
       "498     LCD, Glass animals, Kendrick, The Weeknd, Jack...      0   \n",
       "752     First I want to say whatever skin is your skin...      0   \n",
       "904     Not the same but me and my wife saw a man and ...      0   \n",
       "...                                                   ...    ...   \n",
       "170652  /r/keto /r/ketorecipes /r/ketodessert all are ...      0   \n",
       "170653  its okay dont worry . as long as you don't exc...      0   \n",
       "170662  the national number is :1919 here are more com...      0   \n",
       "170693  this is my personal experience ,it may not ref...      0   \n",
       "170695  i am aware of stats now and then. i was just s...      0   \n",
       "\n",
       "                                          tokenized_title  title_len  \\\n",
       "122     [okay, friends, so, i, messed, up, and, posted...       34.0   \n",
       "390                                                  None        NaN   \n",
       "498                                                  None        NaN   \n",
       "752                       [getting, that, coachella, bod]        4.0   \n",
       "904                                                  None        NaN   \n",
       "...                                                   ...        ...   \n",
       "170652                                               None        NaN   \n",
       "170653                                               None        NaN   \n",
       "170662                                               None        NaN   \n",
       "170693                                               None        NaN   \n",
       "170695                                               None        NaN   \n",
       "\n",
       "                                           tokenized_text  text_len  \n",
       "122     [sorry, for, that, i, truly, didn, t, think, i...     120.0  \n",
       "390     [you, ve, got, plenty, of, time, to, fix, that...     104.0  \n",
       "498     [lcd, glass, animals, kendrick, the, weeknd, j...     127.0  \n",
       "752     [first, i, want, to, say, whatever, skin, is, ...     149.0  \n",
       "904     [not, the, same, but, me, and, my, wife, saw, ...     151.0  \n",
       "...                                                   ...       ...  \n",
       "170652  [r, keto, r, ketorecipes, r, ketodessert, all,...     197.0  \n",
       "170653  [its, okay, dont, worry, as, long, as, you, do...     109.0  \n",
       "170662  [the, national, number, is, 1919, here, are, m...     115.0  \n",
       "170693  [this, is, my, personal, experience, it, may, ...     153.0  \n",
       "170695  [i, am, aware, of, stats, now, and, then, i, w...     198.0  \n",
       "\n",
       "[7655 rows x 9 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[(~writings_df['text_len'].isnull()) & (writings_df['text_len'] > 100)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features and encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_features = {\n",
    "    \"max_features\": 20000,\n",
    "    # cut texts after this number of words\n",
    "    # (among top max_features most common words)\n",
    "    \"maxlen\": 100,\n",
    "    \"embedding_dim\": 100,\n",
    "    \"user_level\": True,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_NRC(nrc_path):\n",
    "    word_emotions = {}\n",
    "    emotion_words = {}\n",
    "    with open(nrc_path) as in_f:\n",
    "        for line in in_f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            word, emotion, label = line.split()\n",
    "            if word not in word_emotions:\n",
    "                word_emotions[word] = set()\n",
    "            if emotion not in emotion_words:\n",
    "                emotion_words[emotion] = set()\n",
    "            label = int(label)\n",
    "            if label:\n",
    "                word_emotions[word].add(emotion)\n",
    "                emotion_words[emotion].add(word)\n",
    "    return emotion_words\n",
    "\n",
    "nrc_lexicon_path = root_dir + '/resources/NRC-Sentiment-Emotion-Lexicons/NRC-Sentiment-Emotion-Lexicons/NRC-Emotion-Lexicon-v0.92/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'\n",
    "nrc_lexicon = load_NRC(nrc_lexicon_path)\n",
    "emotions = list(nrc_lexicon.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_emotions(tokens, emotion_lexicon, emotions, relative=True):\n",
    "    text_len = len(tokens)\n",
    "    encoded_emotions = [0 for e in emotions]\n",
    "    for i, emotion in enumerate(emotions):\n",
    "        try:\n",
    "            emotion_words = [t for t in tokens if t in emotion_lexicon[emotion]]\n",
    "            if relative:\n",
    "                encoded_emotions[i] = len(emotion_words) / len(tokens)\n",
    "            else:\n",
    "                encoded_emotions[i] = len(emotion_words)\n",
    "        except ValueError:\n",
    "            print(\"Emotion not found.\")\n",
    "    return encoded_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from liwc_readDict import readDict\n",
    "\n",
    "liwc = readDict(root_dir + '/resources/liwc.dic')\n",
    "\n",
    "categories = set([c for (w,c) in liwc])\n",
    "len(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Char n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(tokens):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Personal pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_person_pronouns = {\"i\", \"me\", \"my\", \"mine\", \"myself\"}\n",
    "def encode_pronouns(tokens, pronouns={\"i\", \"me\", \"my\", \"mine\", \"myself\"}, relative=True):\n",
    "    if not tokens:\n",
    "        return np.nan\n",
    "    text_len = len(tokens)\n",
    "    nr_pronouns = len([t for t in tokens if t in pronouns])\n",
    "    if relative:\n",
    "        return nr_pronouns/text_len\n",
    "    else:\n",
    "        return nr_pronouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words(\"english\")\n",
    "def encode_stopwords(tokens, stopwords=stopword_list):\n",
    "    encoded_stopwords = [0 for s in stopword_list]\n",
    "    if not tokens:\n",
    "        return encoded_stopwords\n",
    "    for i, stopword in enumerate(stopwords):\n",
    "        if stopword in tokens:\n",
    "            encoded_stopwords[i] += 1\n",
    "    return encoded_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def load_erisk_data(writings_df, voc_size, emotion_lexicon, seq_len, emotions =  \n",
    "                    ['anger', 'anticipation', 'disgust', 'fear', 'joy', \n",
    "                     'negative', 'positive', 'sadness', 'surprise', 'trust'],\n",
    "                    liwc_categories = categories,\n",
    "                    pronouns = [\"i\", \"me\", \"my\", \"mine\", \"myself\"],\n",
    "                    train_prop=0.7, valid_prop=0.3, min_post_len=3, min_word_len=1, \n",
    "                    user_level=True, vocabulary=None):\n",
    "    print(\"Loading data...\")\n",
    "    if not vocabulary:\n",
    "        vocabulary = {}\n",
    "        word_freqs = Counter()\n",
    "        for words in writings_df.tokenized_text:\n",
    "            word_freqs.update(words)\n",
    "        for words in writings_df.tokenized_title:\n",
    "            word_freqs.update(words)\n",
    "        i = 1\n",
    "        for w, f in word_freqs.most_common(voc_size-2): # keeping voc_size-1 for unk\n",
    "            if len(w) < min_word_len:\n",
    "                continue\n",
    "            vocabulary[w] = i\n",
    "            i += 1\n",
    "    tokens_data_train = []\n",
    "    categ_data_train = []\n",
    "    sparse_data_train = []\n",
    "    tokens_data_valid = []\n",
    "    categ_data_valid = []\n",
    "    sparse_data_valid = []\n",
    "    tokens_data_test = []\n",
    "    categ_data_test = []\n",
    "    sparse_data_test = []\n",
    "    labels_train = []\n",
    "    users_train = []\n",
    "    labels_valid = []\n",
    "    users_valid = []\n",
    "    users_test = []\n",
    "    labels_test = []\n",
    "    if 'subset' in writings_df.columns:\n",
    "        training_subjects = list(set(writings_df[writings_df['subset']=='train'].subject))\n",
    "        test_subjects = list(set(writings_df[writings_df['subset']=='test'].subject))\n",
    "    else:\n",
    "        all_subjects = sorted(list(set(writings_df.subject)))\n",
    "        training_subjects_size = int(len(all_subjects) * train_prop)\n",
    "        training_subjects = all_subjects[:training_subjects_size]\n",
    "        test_subjects = all_subjects[training_subjects_size:]\n",
    "    training_subjects = sorted(training_subjects) # ensuring reproducibility\n",
    "    valid_subjects_size = int(len(training_subjects) * valid_prop)\n",
    "    valid_subjects = training_subjects[:valid_subjects_size]\n",
    "    training_subjects = training_subjects[valid_subjects_size:]\n",
    "    categories = [c for c in liwc_categories if c in writings_df.columns]\n",
    "    print(len(training_subjects), \"training users, \", \n",
    "          len(valid_subjects), \"validation users, \",\n",
    "          len(test_subjects), \" test users.\")\n",
    "#     training_rows = writings_df[writings_df['subject'].isin(training_subjects)].sample(frac=1) # shuffling\n",
    "#     test_rows = writings_df[~writings_df['subject'].isin(training_subjects)].sample(frac=1)\n",
    "#     positive_training_users = training_rows.groupby('subject').max().groupby('label').count().date[1]\n",
    "#     positive_test_users = test_rows.groupby('subject').max().groupby('label').count().date[1]\n",
    "#     print(\"Positive training users: \", positive_training_users, \", positive test users: \", positive_test_users)\n",
    "    def encode_text(tokens):\n",
    "        # Using voc_size-1 value for OOV token\n",
    "        encoded_tokens = [vocabulary.get(w, voc_size-1) for w in tokens]\n",
    "        encoded_emotions = encode_emotions(tokens, emotion_lexicon, emotions)\n",
    "        encoded_pronouns = encode_pronouns(tokens, pronouns)\n",
    "        encoded_stopwords = encode_stopwords(tokens)\n",
    "        return (encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords)\n",
    "    user_level_texts = {}\n",
    "    for row in writings_df.itertuples():\n",
    "        words = []\n",
    "        if row.tokenized_title:\n",
    "            words.extend(row.tokenized_title)\n",
    "        if row.tokenized_text:\n",
    "            words.extend(row.tokenized_text)\n",
    "        if not words or len(words)<min_post_len:\n",
    "            continue\n",
    "        label = row.label\n",
    "        liwc_categs = [getattr(row, categ) for categ in categories]\n",
    "        if row.subject not in user_level_texts.keys():\n",
    "            user_level_texts[row.subject] = {}\n",
    "            user_level_texts[row.subject]['texts'] = [words]\n",
    "            user_level_texts[row.subject]['label'] = label\n",
    "            user_level_texts[row.subject]['liwc'] = [liwc_categs]\n",
    "        else:\n",
    "            user_level_texts[row.subject]['texts'].append(words) # TODO: sort datapoints chronologically\n",
    "            user_level_texts[row.subject]['liwc'].append(liwc_categs)\n",
    "\n",
    "    for subject in user_level_texts.keys():\n",
    "        texts = user_level_texts[subject]['texts']\n",
    "        label = user_level_texts[subject]['label']\n",
    "        if user_level:\n",
    "            all_words = [sum(texts, [])] # merge all texts in one list\n",
    "            liwc_aggreg = [np.array(user_level_texts[subject]['liwc']).mean(axis=0).tolist()]\n",
    "        else:\n",
    "            all_words = texts\n",
    "            liwc_aggreg = user_level_texts[subject]['liwc']\n",
    "        for i, words in enumerate(all_words):\n",
    "            encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords = encode_text(words)\n",
    "            subject_id = int(subject.split('t')[1])\n",
    "            if subject in training_subjects:\n",
    "                tokens_data_train.append(encoded_tokens)\n",
    "                categ_data_train.append(encoded_emotions + [encoded_pronouns] + liwc_aggreg[i])\n",
    "                sparse_data_train.append(encoded_stopwords)\n",
    "                labels_train.append(label)\n",
    "                users_train.append(subject_id)\n",
    "            elif subject in valid_subjects:\n",
    "                tokens_data_valid.append(encoded_tokens)\n",
    "                categ_data_valid.append(encoded_emotions + [encoded_pronouns]  + liwc_aggreg[i])\n",
    "                sparse_data_valid.append(encoded_stopwords)\n",
    "                labels_valid.append(label)\n",
    "                users_valid.append(subject_id)\n",
    "            else:\n",
    "                tokens_data_test.append(encoded_tokens)\n",
    "                categ_data_test.append(encoded_emotions + [encoded_pronouns] + liwc_aggreg[i])\n",
    "                sparse_data_test.append(encoded_stopwords)\n",
    "                labels_test.append(label)\n",
    "                users_test.append(subject_id)\n",
    "        \n",
    "    # using zeros for padding\n",
    "    tokens_data_train_padded = sequence.pad_sequences(tokens_data_train, maxlen=seq_len)\n",
    "    tokens_data_valid_padded = sequence.pad_sequences(tokens_data_valid, maxlen=seq_len)\n",
    "    tokens_data_test_padded = sequence.pad_sequences(tokens_data_test, maxlen=seq_len)\n",
    "        \n",
    "    return ([np.array(tokens_data_train_padded), np.array(categ_data_train), np.array(sparse_data_train),\n",
    "            np.array(users_train)],\n",
    "            np.array(labels_train)), \\\n",
    "            ([np.array(tokens_data_valid_padded), np.array(categ_data_valid), np.array(sparse_data_valid),\n",
    "            np.array(users_valid)],\n",
    "            np.array(labels_valid)), \\\n",
    "            ([np.array(tokens_data_test_padded), np.array(categ_data_test), np.array(sparse_data_test),\n",
    "             np.array(users_test)],\n",
    "             np.array(labels_test)), vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "166 training users,  71 validation users,  103  test users.\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_valid, y_valid), (x_test, y_test), voc = load_erisk_data(writings_df, \n",
    "                                                            seq_len=hyperparams_features['maxlen'],\n",
    "                                                            voc_size=hyperparams_features['max_features'],\n",
    "                                                           emotion_lexicon=nrc_lexicon,\n",
    "                                                           emotions=emotions,\n",
    "                                                           user_level=hyperparams_features['user_level'],\n",
    "#                                                            vocabulary=pickle.load(open('vocabulary20K_selfharm.pkl', 'rb'))\n",
    "                                                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166 train sequences\n",
      "71 train sequences\n",
      "103 test sequences\n"
     ]
    }
   ],
   "source": [
    "x_train_seq, x_train_categ, x_train_sparse, x_train_users = x_train\n",
    "x_valid_seq, x_valid_categ, x_valid_sparse, x_valid_users = x_valid\n",
    "x_test_seq, x_test_categ, x_test_sparse, x_test_users = x_test\n",
    "print(len(x_train_seq), 'train sequences')\n",
    "print(len(x_valid_seq), 'train sequences')\n",
    "print(len(x_test_seq), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 positive training examples\n",
      "5 positive validation examples\n",
      "17 positive test examples\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(y_train).sum(), \"positive training examples\")\n",
    "print(pd.Series(y_valid).sum(), \"positive validation examples\")\n",
    "\n",
    "print(pd.Series(y_test).sum(), \"positive test examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 5537,   185,     7, ...,    64,    11, 11012],\n",
       "        [    2,   121,     4, ...,  1495,   436, 19999],\n",
       "        [  250,     3,    89, ..., 19999,   829,   207],\n",
       "        ...,\n",
       "        [   27,    32,    21, ...,   240,  1425,  2193],\n",
       "        [  230,     7,     8, ...,     3,   340,    10],\n",
       "        [    2,    32,    15, ...,     4,   324, 19999]], dtype=int32),\n",
       " array([[0.01368948, 0.02103506, 0.00834725, ..., 0.01068447, 0.02838063,\n",
       "         0.02337229],\n",
       "        [0.01486989, 0.04089219, 0.01115242, ..., 0.01115242, 0.04460967,\n",
       "         0.05576208],\n",
       "        [0.01768878, 0.02106219, 0.01037972, ..., 0.01851051, 0.0350532 ,\n",
       "         0.01448837],\n",
       "        ...,\n",
       "        [0.01091779, 0.019756  , 0.00713989, ..., 0.00849161, 0.02526688,\n",
       "         0.05580202],\n",
       "        [0.01725149, 0.01764255, 0.00809181, ..., 0.00976131, 0.0382932 ,\n",
       "         0.00753531],\n",
       "        [0.01476301, 0.02797203, 0.01087801, ..., 0.01709402, 0.03807304,\n",
       "         0.05749806]]),\n",
       " array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 0, 1, ..., 0, 1, 0],\n",
       "        [1, 1, 1, ..., 0, 1, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 0, 1, 0],\n",
       "        [1, 1, 1, ..., 0, 1, 0],\n",
       "        [1, 1, 1, ..., 0, 1, 0]]),\n",
       " array([6644, 4284, 4831, 4513, 4143, 4226, 5150, 4510, 4414, 6866, 5000,\n",
       "        4843,  569,   51, 6453, 5528, 5935, 4527, 3301, 4074, 6093, 6459,\n",
       "        4247, 3667, 5003, 5644, 3283, 6322, 6668, 4333, 5383, 5223, 3994,\n",
       "        7018, 3644, 4570, 5062, 4729, 5100, 5177,  505, 5974, 4071, 5995,\n",
       "        6930, 6918, 4198,  501, 5375, 7229, 4762, 5622,   47, 4785, 5840,\n",
       "        6464,  522, 5984,  641, 7326, 4227, 6946, 4563,  682, 4459, 7238,\n",
       "        6428, 7262, 5456, 5033, 3277, 5549, 6352, 6652, 5833, 4795, 4002,\n",
       "        5878, 3928, 6786, 3612, 4719, 6290, 3844, 3605, 5908, 3191, 4196,\n",
       "        5256, 7318, 4777, 6309, 4479, 4961, 6247, 4644, 6284, 5699, 5409,\n",
       "        7263, 5148, 3868, 5793, 4934, 3674, 6019, 5937,  671, 4318, 5112,\n",
       "        7107, 5603,  511, 6518, 5140, 3737, 3227, 6670, 5387, 3883, 6146,\n",
       "        6833, 6259, 3635, 6423, 4702, 3555, 3914, 3725, 6173, 7131, 4848,\n",
       "        5270, 5979, 6041, 7247, 5938, 6238, 6957, 6899, 4014, 4526, 7316,\n",
       "        7290,  463, 4379, 5920, 3904, 4392,  379, 3881, 4505, 3977, 5342,\n",
       "        6903, 3270, 3224, 3596, 4278, 5282, 3357, 6013, 5036, 6035, 6665,\n",
       "         519])]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.56462585, 4.36842105])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 20000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "def load_embeddings(path, embedding_dim, voc):\n",
    "    # random matrix with mean value = 0\n",
    "    embedding_matrix = np.random.random((len(voc)+2, embedding_dim)) - 0.5 # voc + unk + pad value(0)\n",
    "#     embedding_matrix = np.zeros((len(voc)+1, embedding_dim))\n",
    "\n",
    "    f = open(path)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_i = voc.get(word)\n",
    "        if word_i is not None:\n",
    "            embedding_matrix[word_i] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print('Total %s word vectors.' % len(embedding_matrix))\n",
    "\n",
    " \n",
    "    return embedding_matrix\n",
    "\n",
    "# pretrained_embeddings_path = '/home/ana/resources/glove.6B/glove.6B.%dd.txt' % hyperparams_features['embedding_dim']\n",
    "pretrained_embeddings_path = root_dir + '/resources/glove.twitter.27B/glove.twitter.27B.%dd.txt' % hyperparams_features['embedding_dim']\n",
    "embedding_matrix = load_embeddings(pretrained_embeddings_path, hyperparams_features['embedding_dim'], voc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166, 179)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'lstm_units': 100,\n",
    "    'dense_bow_units': 5,\n",
    "    'dropout': 0.14,\n",
    "    'l2_dense': 0.00011,\n",
    "    'l2_embeddings': 0.00001,\n",
    "    'optimizer': 'adam',\n",
    "    'decay': 0.0001,\n",
    "    'lr': 0.001,\n",
    "    \"batch_size\": 32,\n",
    "    \"trainable_embeddings\": False,\n",
    "    \"reduce_lr_factor\": 0.02,\n",
    "    \"reduce_lr_patience\": 50,\n",
    "    \"freeze_patience\": 50,\n",
    "    'threshold': 0.5,\n",
    "\n",
    "\n",
    "}\n",
    "if not hyperparams['optimizer']:\n",
    "    hyperparams['optimizer'] = optimizers.Adam(lr=hyperparams['lr'], #beta_1=0.9, beta_2=0.999, epsilon=0.0001,\n",
    "                                   decay=hyperparams['decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics():\n",
    "    def __init__(self, threshold=0.5):\n",
    "        self.threshold=threshold\n",
    "        \n",
    "    def recall_m(self, y_true, y_pred):\n",
    "            y_labels = y_true#tf.reshape(y_true[:], (-1,1))\n",
    "    #         y_pred = tf.reshape(y_pred, (-1,1))\n",
    "            y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), self.threshold), K.floatx())        \n",
    "            possible_positives = K.sum(K.round(K.clip(y_labels, 0, 1)))\n",
    "            true_positives = K.sum(K.round(K.clip(y_labels * y_pred, 0, 1)))\n",
    "            recall = true_positives / (possible_positives + K.epsilon())\n",
    "            return recall\n",
    "\n",
    "    def precision_m(self, y_true, y_pred):\n",
    "            y_labels = y_true#tf.reshape(y_true[:], (-1,1))\n",
    "    #         y_pred = tf.reshape(y_pred, (-1,1))\n",
    "            y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), self.threshold), K.floatx())        \n",
    "            true_positives = K.sum(K.round(K.clip(y_labels * y_pred, 0, 1)))\n",
    "            predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "            precision = true_positives / (predicted_positives + K.epsilon())\n",
    "            return precision\n",
    "\n",
    "    def f1_m(self, y_true, y_pred):\n",
    "        precision = self.precision_m(y_true, y_pred)\n",
    "        recall = self.recall_m(y_true, y_pred)\n",
    "        return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def binary_crossentropy_custom(y_true, y_pred):\n",
    "    y_labels = y_true#tf.reshape(y_true[0],(1,-1))\n",
    "    return K.binary_crossentropy(y_labels, \n",
    "                                 y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopwords_list,\n",
    "                liwc_categories,\n",
    "               ignore_layer=[]):\n",
    "\n",
    "    tokens_features = Input(shape=(hyperparams_features['maxlen'],), name='word_seq')\n",
    "    embedding_layer = Embedding(hyperparams_features['max_features'], \n",
    "                                hyperparams_features['embedding_dim'], \n",
    "                                input_length=hyperparams_features['maxlen'],\n",
    "                                mask_zero=True,\n",
    "                                embeddings_regularizer=regularizers.l2(hyperparams['l2_embeddings']),\n",
    "                                weights=[embedding_matrix], \n",
    "                                trainable=hyperparams['trainable_embeddings'],\n",
    "                               name='embeddings_layer')(\n",
    "        tokens_features)\n",
    "#     lstm_layers = Bidirectional(LSTM(hyperparams['lstm_units']))(embedding_layer)\n",
    "    lstm_layers = LSTM(hyperparams['lstm_units'], dropout=hyperparams['dropout'],\n",
    "                      recurrent_dropout=hyperparams['dropout'],\n",
    "                      name='LSTM_layer')(embedding_layer)\n",
    "    numerical_features = Input(shape=(len(emotions) + 1 + len(liwc_categories),), name='numeric_input') # emotions and pronouns\n",
    "    dense_layer = Dense(units=1,\n",
    "                        kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                        name='numerical_dense_layer',\n",
    "                       )(numerical_features)\n",
    "    sparse_features = Input(shape=(len(stopwords_list),), name='sparse_input') # stopwords\n",
    "    subjects = Input(shape=(1,), name='subjects')\n",
    "    dense_layer_sparse = Dense(units=hyperparams['dense_bow_units'],\n",
    "                              name='sparse_feat_dense_layer',\n",
    "                                kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                              )(sparse_features)\n",
    "#     # TODO: this is getting out of hand. Refactor this ablation part.\n",
    "    all_layers = {\n",
    "        'lstm_layers': lstm_layers,\n",
    "        'numerical_dense_layer': numerical_features,# dense_layer,\n",
    "        'sparse_feat_dense_layer': sparse_features#dense_layer_sparse,\n",
    "    }\n",
    "    layers_to_merge = []\n",
    "    for n, l in all_layers.items():\n",
    "        if n in ignore_layer:\n",
    "            continue\n",
    "        layers_to_merge.append(l)\n",
    "        \n",
    "    if len(layers_to_merge) == 1:\n",
    "        merged_layers = layers_to_merge[0]\n",
    "    else:\n",
    "        merged_layers = concatenate(layers_to_merge)\n",
    "    output_layer = Dense(1, activation='sigmoid',\n",
    "                         name='output_layer',\n",
    "                        kernel_regularizer=regularizers.l2(hyperparams['l2_dense']))(merged_layers)\n",
    "\n",
    "    # Compile model\n",
    "    model = Model(inputs=[tokens_features, numerical_features, sparse_features, subjects], \n",
    "                  outputs=output_layer)\n",
    "    metrics_class = Metrics(threshold=hyperparams['threshold'])\n",
    "    model.compile(hyperparams['optimizer'], binary_crossentropy_custom,\n",
    "                  metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "numeric_input (InputLayer)      (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sparse_input (InputLayer)       (None, 179)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 190)          0           numeric_input[0][0]              \n",
      "                                                                 sparse_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 1)            191         concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 191\n",
      "Trainable params: 191\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopword_list,\n",
    "                    liwc_categories=[c for c in categories if c in writings_df.columns]\n",
    ",\n",
    "                   ignore_layer=['lstm_layers'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, 'models/lstm_plus_ablated3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary:\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     url: https://www.comet.ml/ananana/mental/d007b8f7ce144e99b7556f23cc15d5df\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     sys.cpu.percent.01 [2] : (12.4, 26.9)\n",
      "COMET INFO:     sys.cpu.percent.02 [2] : (12.0, 13.3)\n",
      "COMET INFO:     sys.cpu.percent.03 [2] : (12.0, 13.8)\n",
      "COMET INFO:     sys.cpu.percent.04 [2] : (12.5, 13.8)\n",
      "COMET INFO:     sys.cpu.percent.avg [2]: (12.225, 16.95)\n",
      "COMET INFO:     sys.gpu.0.total_memory : (1073414144.0, 1073414144.0)\n",
      "COMET INFO:     sys.load.avg [2]       : (1.22, 1.23)\n",
      "COMET INFO:     sys.ram.total [2]      : (8277307392.0, 8277307392.0)\n",
      "COMET INFO:     sys.ram.used [2]       : (3954827264.0, 3989647360.0)\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     git-patch: 1\n",
      "COMET INFO: ----------------------------\n",
      "COMET INFO: old comet version (3.0.2) detected. current: 3.0.3 please update your comet lib with command: `pip install --no-cache-dir --upgrade comet_ml`\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/ananana/mental/be350999b7a24760a3cfdad7240a257f\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(api_key=\"eoBdVyznAhfg3bK9pZ58ZSXfv\",\n",
    "                        project_name=\"mental\", workspace=\"ananana\")\n",
    "\n",
    "experiment.log_parameters(hyperparams_features)\n",
    "\n",
    "experiment.log_parameter('emotion_lexicon', nrc_lexicon_path)\n",
    "experiment.log_parameter('emotions', emotions)\n",
    "experiment.log_parameter('embeddings_path', pretrained_embeddings_path)\n",
    "\n",
    "experiment.log_parameters(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'i': 2,\n",
       " 'to': 3,\n",
       " 'a': 4,\n",
       " 'and': 5,\n",
       " 'of': 6,\n",
       " 'it': 7,\n",
       " 'in': 8,\n",
       " 'you': 9,\n",
       " 'that': 10,\n",
       " 'is': 11,\n",
       " 's': 12,\n",
       " 'for': 13,\n",
       " 'this': 14,\n",
       " 't': 15,\n",
       " 'on': 16,\n",
       " 'with': 17,\n",
       " 'but': 18,\n",
       " 'my': 19,\n",
       " '8217': 20,\n",
       " 'be': 21,\n",
       " 'was': 22,\n",
       " 'have': 23,\n",
       " 'are': 24,\n",
       " 'not': 25,\n",
       " 'they': 26,\n",
       " 'as': 27,\n",
       " 'if': 28,\n",
       " 'so': 29,\n",
       " 'just': 30,\n",
       " 'what': 31,\n",
       " 'can': 32,\n",
       " 'like': 33,\n",
       " 'he': 34,\n",
       " 'or': 35,\n",
       " 'at': 36,\n",
       " 'we': 37,\n",
       " 'me': 38,\n",
       " 'from': 39,\n",
       " 'your': 40,\n",
       " 'm': 41,\n",
       " 'do': 42,\n",
       " 'com': 43,\n",
       " 'all': 44,\n",
       " 'about': 45,\n",
       " 'an': 46,\n",
       " 'one': 47,\n",
       " 'there': 48,\n",
       " 'would': 49,\n",
       " 'out': 50,\n",
       " 'up': 51,\n",
       " 'when': 52,\n",
       " 'more': 53,\n",
       " 'get': 54,\n",
       " 'don': 55,\n",
       " 'people': 56,\n",
       " 'by': 57,\n",
       " 'will': 58,\n",
       " 'no': 59,\n",
       " 'how': 60,\n",
       " 'https': 61,\n",
       " 'gt': 62,\n",
       " 'has': 63,\n",
       " 'them': 64,\n",
       " 'his': 65,\n",
       " 'time': 66,\n",
       " 'some': 67,\n",
       " 're': 68,\n",
       " 'know': 69,\n",
       " 'think': 70,\n",
       " 'who': 71,\n",
       " 'their': 72,\n",
       " 'because': 73,\n",
       " 'had': 74,\n",
       " 'she': 75,\n",
       " 'here': 76,\n",
       " 'good': 77,\n",
       " 'really': 78,\n",
       " 'www': 79,\n",
       " 'r': 80,\n",
       " 'now': 81,\n",
       " 've': 82,\n",
       " 'been': 83,\n",
       " 'only': 84,\n",
       " 'her': 85,\n",
       " 'also': 86,\n",
       " 'were': 87,\n",
       " 'than': 88,\n",
       " 'see': 89,\n",
       " 'any': 90,\n",
       " 'http': 91,\n",
       " 'even': 92,\n",
       " 'make': 93,\n",
       " 'other': 94,\n",
       " 'then': 95,\n",
       " '128056': 96,\n",
       " 'much': 97,\n",
       " '1': 98,\n",
       " 'which': 99,\n",
       " 'him': 100,\n",
       " 'could': 101,\n",
       " 'go': 102,\n",
       " '2': 103,\n",
       " 'first': 104,\n",
       " 'd': 105,\n",
       " 'want': 106,\n",
       " 'new': 107,\n",
       " 'why': 108,\n",
       " 'well': 109,\n",
       " 'did': 110,\n",
       " 'too': 111,\n",
       " 'right': 112,\n",
       " 'way': 113,\n",
       " 'into': 114,\n",
       " 'very': 115,\n",
       " 'after': 116,\n",
       " 'being': 117,\n",
       " 'over': 118,\n",
       " 'back': 119,\n",
       " 'still': 120,\n",
       " 'got': 121,\n",
       " 'most': 122,\n",
       " 'reddit': 123,\n",
       " 'should': 124,\n",
       " 'something': 125,\n",
       " 'post': 126,\n",
       " 'going': 127,\n",
       " 'll': 128,\n",
       " '3': 129,\n",
       " 'our': 130,\n",
       " 'these': 131,\n",
       " 'thanks': 132,\n",
       " 'never': 133,\n",
       " '8220': 134,\n",
       " 'need': 135,\n",
       " '8221': 136,\n",
       " 'say': 137,\n",
       " 'where': 138,\n",
       " 'us': 139,\n",
       " 'use': 140,\n",
       " 'am': 141,\n",
       " 'love': 142,\n",
       " 'work': 143,\n",
       " 'day': 144,\n",
       " 'same': 145,\n",
       " 'off': 146,\n",
       " 'sure': 147,\n",
       " 'game': 148,\n",
       " 'before': 149,\n",
       " 'said': 150,\n",
       " 'didn': 151,\n",
       " '10': 152,\n",
       " 'lot': 153,\n",
       " 'those': 154,\n",
       " 'thing': 155,\n",
       " 'years': 156,\n",
       " 'someone': 157,\n",
       " 'does': 158,\n",
       " 'take': 159,\n",
       " 'best': 160,\n",
       " 'made': 161,\n",
       " 'feel': 162,\n",
       " 'great': 163,\n",
       " 'two': 164,\n",
       " 'actually': 165,\n",
       " 'better': 166,\n",
       " 'look': 167,\n",
       " '_': 168,\n",
       " 'its': 169,\n",
       " '5': 170,\n",
       " 'every': 171,\n",
       " 'down': 172,\n",
       " 'year': 173,\n",
       " 'pretty': 174,\n",
       " 'life': 175,\n",
       " 'doesn': 176,\n",
       " 'amp': 177,\n",
       " 'things': 178,\n",
       " 'many': 179,\n",
       " 'though': 180,\n",
       " '4': 181,\n",
       " 'while': 182,\n",
       " 'last': 183,\n",
       " 'always': 184,\n",
       " 'around': 185,\n",
       " 'man': 186,\n",
       " 'imgur': 187,\n",
       " 'thank': 188,\n",
       " 'world': 189,\n",
       " 'give': 190,\n",
       " 'find': 191,\n",
       " 'little': 192,\n",
       " 'help': 193,\n",
       " 'anything': 194,\n",
       " 'u': 195,\n",
       " 'through': 196,\n",
       " 'trump': 197,\n",
       " 'long': 198,\n",
       " 'used': 199,\n",
       " 'o': 200,\n",
       " 'yeah': 201,\n",
       " 'may': 202,\n",
       " 'few': 203,\n",
       " 'ever': 204,\n",
       " 'thought': 205,\n",
       " 'since': 206,\n",
       " 'yes': 207,\n",
       " 'doing': 208,\n",
       " 'getting': 209,\n",
       " 'shit': 210,\n",
       " 'anyone': 211,\n",
       " 'comments': 212,\n",
       " 'probably': 213,\n",
       " 'let': 214,\n",
       " 'point': 215,\n",
       " 'person': 216,\n",
       " 'bad': 217,\n",
       " 'both': 218,\n",
       " 'watch': 219,\n",
       " 'another': 220,\n",
       " 'again': 221,\n",
       " 'own': 222,\n",
       " 'might': 223,\n",
       " 'isn': 224,\n",
       " 'looking': 225,\n",
       " 'maybe': 226,\n",
       " 'everyone': 227,\n",
       " 'old': 228,\n",
       " 'come': 229,\n",
       " 'put': 230,\n",
       " 'mean': 231,\n",
       " 'without': 232,\n",
       " 'try': 233,\n",
       " 'please': 234,\n",
       " 'looks': 235,\n",
       " 'keep': 236,\n",
       " 'trying': 237,\n",
       " 'big': 238,\n",
       " 'video': 239,\n",
       " 'different': 240,\n",
       " 'show': 241,\n",
       " 'guy': 242,\n",
       " 'next': 243,\n",
       " 'e': 244,\n",
       " 'part': 245,\n",
       " 'play': 246,\n",
       " 'using': 247,\n",
       " 'makes': 248,\n",
       " 'already': 249,\n",
       " 'enough': 250,\n",
       " '2018': 251,\n",
       " 'found': 252,\n",
       " 'money': 253,\n",
       " 'oh': 254,\n",
       " 'real': 255,\n",
       " 'such': 256,\n",
       " '9685': 257,\n",
       " 'high': 258,\n",
       " 'end': 259,\n",
       " 'between': 260,\n",
       " '12388': 261,\n",
       " '6': 262,\n",
       " 'done': 263,\n",
       " 'read': 264,\n",
       " 'link': 265,\n",
       " 'lol': 266,\n",
       " 'nice': 267,\n",
       " 'having': 268,\n",
       " 'away': 269,\n",
       " 'tell': 270,\n",
       " 'nothing': 271,\n",
       " 'kind': 272,\n",
       " 'making': 273,\n",
       " 'question': 274,\n",
       " 'live': 275,\n",
       " 'fuck': 276,\n",
       " 'v': 277,\n",
       " 'start': 278,\n",
       " 'else': 279,\n",
       " 'team': 280,\n",
       " 'today': 281,\n",
       " 'hard': 282,\n",
       " '0': 283,\n",
       " 'against': 284,\n",
       " 'once': 285,\n",
       " 'days': 286,\n",
       " 'seen': 287,\n",
       " 'far': 288,\n",
       " 'bit': 289,\n",
       " 'able': 290,\n",
       " 'org': 291,\n",
       " 'place': 292,\n",
       " 'week': 293,\n",
       " 'everything': 294,\n",
       " 'each': 295,\n",
       " 'free': 296,\n",
       " 'wrong': 297,\n",
       " 'school': 298,\n",
       " 'least': 299,\n",
       " 'times': 300,\n",
       " 'less': 301,\n",
       " 'edit': 302,\n",
       " 'source': 303,\n",
       " 'idea': 304,\n",
       " 'top': 305,\n",
       " 'went': 306,\n",
       " 'during': 307,\n",
       " 'name': 308,\n",
       " 'believe': 309,\n",
       " 'until': 310,\n",
       " 'won': 311,\n",
       " 'state': 312,\n",
       " 'definitely': 313,\n",
       " 'seems': 314,\n",
       " 'guys': 315,\n",
       " 'saying': 316,\n",
       " 'home': 317,\n",
       " 'hope': 318,\n",
       " 'title': 319,\n",
       " 'story': 320,\n",
       " '7': 321,\n",
       " 'youtube': 322,\n",
       " 'fucking': 323,\n",
       " 'small': 324,\n",
       " 'says': 325,\n",
       " 'full': 326,\n",
       " 'rep': 327,\n",
       " 'n': 328,\n",
       " 'yet': 329,\n",
       " 'case': 330,\n",
       " 'sorry': 331,\n",
       " 'stop': 332,\n",
       " 'twitter': 333,\n",
       " 'friend': 334,\n",
       " 'under': 335,\n",
       " 'stuff': 336,\n",
       " 'reason': 337,\n",
       " 'ago': 338,\n",
       " 'support': 339,\n",
       " 'change': 340,\n",
       " '8': 341,\n",
       " 'job': 342,\n",
       " 'however': 343,\n",
       " 'house': 344,\n",
       " 'wasn': 345,\n",
       " 'family': 346,\n",
       " 'left': 347,\n",
       " 'second': 348,\n",
       " 'friends': 349,\n",
       " 'etc': 350,\n",
       " 'guess': 351,\n",
       " 'comment': 352,\n",
       " 'true': 353,\n",
       " 'problem': 354,\n",
       " 'power': 355,\n",
       " 'gets': 356,\n",
       " 'myself': 357,\n",
       " 'called': 358,\n",
       " 'news': 359,\n",
       " 'buy': 360,\n",
       " 'either': 361,\n",
       " 'rules': 362,\n",
       " 'sub': 363,\n",
       " 'set': 364,\n",
       " 'wanted': 365,\n",
       " 'happy': 366,\n",
       " 'w': 367,\n",
       " 'water': 368,\n",
       " 'fun': 369,\n",
       " 'white': 370,\n",
       " 'almost': 371,\n",
       " 'fact': 372,\n",
       " 'wiki': 373,\n",
       " 'experience': 374,\n",
       " 'open': 375,\n",
       " 'call': 376,\n",
       " 'whole': 377,\n",
       " 'started': 378,\n",
       " 'understand': 379,\n",
       " 'wouldn': 380,\n",
       " '20': 381,\n",
       " 'night': 382,\n",
       " '9': 383,\n",
       " 'article': 384,\n",
       " 'h': 385,\n",
       " 'side': 386,\n",
       " '100': 387,\n",
       " 'check': 388,\n",
       " 'came': 389,\n",
       " 'dog': 390,\n",
       " 'took': 391,\n",
       " 'net': 392,\n",
       " 'possible': 393,\n",
       " 'months': 394,\n",
       " 'jpg': 395,\n",
       " 'hours': 396,\n",
       " 'face': 397,\n",
       " 'food': 398,\n",
       " 'saw': 399,\n",
       " 'playing': 400,\n",
       " 'doi': 401,\n",
       " 'care': 402,\n",
       " '12': 403,\n",
       " 'internet': 404,\n",
       " 'study': 405,\n",
       " 'talking': 406,\n",
       " 'remember': 407,\n",
       " 'content': 408,\n",
       " 'mind': 409,\n",
       " 'de': 410,\n",
       " 'ask': 411,\n",
       " 'level': 412,\n",
       " 'told': 413,\n",
       " 'instead': 414,\n",
       " 'season': 415,\n",
       " 'god': 416,\n",
       " 'cool': 417,\n",
       " 'thinking': 418,\n",
       " 'working': 419,\n",
       " 'must': 420,\n",
       " 'human': 421,\n",
       " 'based': 422,\n",
       " 'country': 423,\n",
       " 'talk': 424,\n",
       " '225': 425,\n",
       " 'p': 426,\n",
       " 'movie': 427,\n",
       " 'aren': 428,\n",
       " 'run': 429,\n",
       " 'body': 430,\n",
       " '233': 431,\n",
       " 'agree': 432,\n",
       " 'posted': 433,\n",
       " '227': 434,\n",
       " 'black': 435,\n",
       " '65039': 436,\n",
       " 'awesome': 437,\n",
       " 'party': 438,\n",
       " 'wait': 439,\n",
       " 'games': 440,\n",
       " 'journal': 441,\n",
       " 'others': 442,\n",
       " 'group': 443,\n",
       " 'likely': 444,\n",
       " 'exactly': 445,\n",
       " 'worth': 446,\n",
       " 'government': 447,\n",
       " 'gonna': 448,\n",
       " 'que': 449,\n",
       " 'coming': 450,\n",
       " 'women': 451,\n",
       " 'means': 452,\n",
       " 'tried': 453,\n",
       " 'information': 454,\n",
       " '30': 455,\n",
       " 'pay': 456,\n",
       " 'haven': 457,\n",
       " 'head': 458,\n",
       " 'future': 459,\n",
       " '2017': 460,\n",
       " 'hey': 461,\n",
       " 'c': 462,\n",
       " 'taking': 463,\n",
       " 'single': 464,\n",
       " 'literally': 465,\n",
       " 'hear': 466,\n",
       " 'hate': 467,\n",
       " 'super': 468,\n",
       " 'self': 469,\n",
       " 'goes': 470,\n",
       " 'health': 471,\n",
       " 'amazing': 472,\n",
       " 'hand': 473,\n",
       " 'message': 474,\n",
       " 'within': 475,\n",
       " 'issue': 476,\n",
       " 'comes': 477,\n",
       " 'happened': 478,\n",
       " 'sounds': 479,\n",
       " 'system': 480,\n",
       " 'sense': 481,\n",
       " 'car': 482,\n",
       " 'couple': 483,\n",
       " 'type': 484,\n",
       " 'half': 485,\n",
       " 'social': 486,\n",
       " 'usually': 487,\n",
       " 'facebook': 488,\n",
       " 'order': 489,\n",
       " '3901': 490,\n",
       " '8211': 491,\n",
       " 'close': 492,\n",
       " '3900': 493,\n",
       " 'comic': 494,\n",
       " 'book': 495,\n",
       " 'three': 496,\n",
       " 'children': 497,\n",
       " 'needs': 498,\n",
       " 'interesting': 499,\n",
       " 'futurology': 500,\n",
       " 'number': 501,\n",
       " 'past': 502,\n",
       " 'data': 503,\n",
       " 'girl': 504,\n",
       " 'quite': 505,\n",
       " 'low': 506,\n",
       " 'kids': 507,\n",
       " '128514': 508,\n",
       " 'reference': 509,\n",
       " 'course': 510,\n",
       " 'subreddit': 511,\n",
       " 'dont': 512,\n",
       " 'ok': 513,\n",
       " 'heard': 514,\n",
       " 'weeks': 515,\n",
       " 'yourself': 516,\n",
       " '000': 517,\n",
       " 'together': 518,\n",
       " 'front': 519,\n",
       " 'especially': 520,\n",
       " 'war': 521,\n",
       " 'important': 522,\n",
       " 'control': 523,\n",
       " 'picture': 524,\n",
       " 'happen': 525,\n",
       " 'sometimes': 526,\n",
       " 'fine': 527,\n",
       " 'eat': 528,\n",
       " 'parents': 529,\n",
       " 'list': 530,\n",
       " 'rather': 531,\n",
       " 'line': 532,\n",
       " 'law': 533,\n",
       " 'thread': 534,\n",
       " 'opinion': 535,\n",
       " 'release': 536,\n",
       " 'often': 537,\n",
       " 'public': 538,\n",
       " 'works': 539,\n",
       " 'seem': 540,\n",
       " '8216': 541,\n",
       " '50': 542,\n",
       " 'later': 543,\n",
       " 'vote': 544,\n",
       " 'non': 545,\n",
       " 'american': 546,\n",
       " 'matter': 547,\n",
       " 'favorite': 548,\n",
       " 'posts': 549,\n",
       " 'phone': 550,\n",
       " '15': 551,\n",
       " 'wish': 552,\n",
       " 'original': 553,\n",
       " 'deal': 554,\n",
       " 'b': 555,\n",
       " 'hit': 556,\n",
       " 'google': 557,\n",
       " '8212': 558,\n",
       " 'leave': 559,\n",
       " 'win': 560,\n",
       " 'completely': 561,\n",
       " 'im': 562,\n",
       " 'add': 563,\n",
       " 'damn': 564,\n",
       " 'ones': 565,\n",
       " 'example': 566,\n",
       " 'space': 567,\n",
       " 'due': 568,\n",
       " 'cat': 569,\n",
       " 'lost': 570,\n",
       " 'entire': 571,\n",
       " 'fight': 572,\n",
       " 'absolutely': 573,\n",
       " 'kill': 574,\n",
       " 'basically': 575,\n",
       " 'whether': 576,\n",
       " '11': 577,\n",
       " 'image': 578,\n",
       " 'haha': 579,\n",
       " 'history': 580,\n",
       " 'removed': 581,\n",
       " 'early': 582,\n",
       " 'cause': 583,\n",
       " 'easy': 584,\n",
       " 'die': 585,\n",
       " 'room': 586,\n",
       " 'op': 587,\n",
       " 'couldn': 588,\n",
       " 'media': 589,\n",
       " 'become': 590,\n",
       " 'neutrality': 591,\n",
       " 'players': 592,\n",
       " 'death': 593,\n",
       " 'baby': 594,\n",
       " 'men': 595,\n",
       " 'okay': 596,\n",
       " 'minutes': 597,\n",
       " 'similar': 598,\n",
       " 'answer': 599,\n",
       " 'age': 600,\n",
       " 'company': 601,\n",
       " 'child': 602,\n",
       " 'sound': 603,\n",
       " 'gif': 604,\n",
       " 'music': 605,\n",
       " 'huge': 606,\n",
       " 'move': 607,\n",
       " 'whatever': 608,\n",
       " 'results': 609,\n",
       " 'weight': 610,\n",
       " 'dude': 611,\n",
       " 'page': 612,\n",
       " 'city': 613,\n",
       " 'questions': 614,\n",
       " 'pain': 615,\n",
       " 'hell': 616,\n",
       " 'online': 617,\n",
       " 'ban': 618,\n",
       " 'woman': 619,\n",
       " 'community': 620,\n",
       " 'status': 621,\n",
       " 'soon': 622,\n",
       " 'fire': 623,\n",
       " 'month': 624,\n",
       " 'turn': 625,\n",
       " 'class': 626,\n",
       " 'dead': 627,\n",
       " 'red': 628,\n",
       " 'stay': 629,\n",
       " 'quality': 630,\n",
       " 'watching': 631,\n",
       " 'police': 632,\n",
       " 'simply': 633,\n",
       " 'per': 634,\n",
       " 'business': 635,\n",
       " 'president': 636,\n",
       " 'asked': 637,\n",
       " 'general': 638,\n",
       " 'en': 639,\n",
       " 'weird': 640,\n",
       " 'x': 641,\n",
       " 'chance': 642,\n",
       " 'wants': 643,\n",
       " 'research': 644,\n",
       " 'act': 645,\n",
       " 'seeing': 646,\n",
       " 'area': 647,\n",
       " 'higher': 648,\n",
       " 'court': 649,\n",
       " 'song': 650,\n",
       " 'linked': 651,\n",
       " 'reading': 652,\n",
       " 'press': 653,\n",
       " 'dad': 654,\n",
       " 'light': 655,\n",
       " 'rest': 656,\n",
       " 'advice': 657,\n",
       " 'price': 658,\n",
       " 'wife': 659,\n",
       " 'photo': 660,\n",
       " 'shot': 661,\n",
       " 'mine': 662,\n",
       " 'episode': 663,\n",
       " 'project': 664,\n",
       " 'amount': 665,\n",
       " 'sex': 666,\n",
       " 'finally': 667,\n",
       " 'large': 668,\n",
       " 'mods': 669,\n",
       " 'outside': 670,\n",
       " 'shows': 671,\n",
       " 'honestly': 672,\n",
       " 'series': 673,\n",
       " 'funny': 674,\n",
       " 'account': 675,\n",
       " 'taken': 676,\n",
       " 'risk': 677,\n",
       " 'save': 678,\n",
       " 'states': 679,\n",
       " 'given': 680,\n",
       " 'knew': 681,\n",
       " 'mom': 682,\n",
       " 'felt': 683,\n",
       " 'sleep': 684,\n",
       " 'perfect': 685,\n",
       " 'word': 686,\n",
       " 'player': 687,\n",
       " 'issues': 688,\n",
       " 'behind': 689,\n",
       " 'j': 690,\n",
       " 'hot': 691,\n",
       " 'played': 692,\n",
       " 'fan': 693,\n",
       " 'across': 694,\n",
       " 'wow': 695,\n",
       " 'running': 696,\n",
       " 'copy': 697,\n",
       " 'blue': 698,\n",
       " 'cut': 699,\n",
       " 'anyway': 700,\n",
       " 'normal': 701,\n",
       " '2016': 702,\n",
       " 'share': 703,\n",
       " 'uk': 704,\n",
       " 'store': 705,\n",
       " 'feeling': 706,\n",
       " 'current': 707,\n",
       " 'young': 708,\n",
       " 'points': 709,\n",
       " 'abstract': 710,\n",
       " 'kid': 711,\n",
       " 'takes': 712,\n",
       " 'vs': 713,\n",
       " 'short': 714,\n",
       " 'actual': 715,\n",
       " 'g': 716,\n",
       " 'rule': 717,\n",
       " 'although': 718,\n",
       " 'looked': 719,\n",
       " 'recently': 720,\n",
       " 'site': 721,\n",
       " 'xbox': 722,\n",
       " 'bring': 723,\n",
       " 'moment': 724,\n",
       " 'enjoy': 725,\n",
       " 'late': 726,\n",
       " 'f': 727,\n",
       " 'specific': 728,\n",
       " '18': 729,\n",
       " 'happens': 730,\n",
       " 'related': 731,\n",
       " 'stupid': 732,\n",
       " 'version': 733,\n",
       " 'words': 734,\n",
       " 'middle': 735,\n",
       " 'imagine': 736,\n",
       " 'sign': 737,\n",
       " 'bill': 738,\n",
       " 'test': 739,\n",
       " 'along': 740,\n",
       " 'longer': 741,\n",
       " 'poor': 742,\n",
       " 'break': 743,\n",
       " '25': 744,\n",
       " 'tv': 745,\n",
       " 'garfield': 746,\n",
       " 'giving': 747,\n",
       " 'crazy': 748,\n",
       " 'college': 749,\n",
       " 'near': 750,\n",
       " 'unless': 751,\n",
       " 'situation': 752,\n",
       " 'eating': 753,\n",
       " 'million': 754,\n",
       " 'needed': 755,\n",
       " 'learn': 756,\n",
       " 'joke': 757,\n",
       " 'follow': 758,\n",
       " 'living': 759,\n",
       " 'themselves': 760,\n",
       " 'known': 761,\n",
       " 'instagram': 762,\n",
       " 'blood': 763,\n",
       " 'l': 764,\n",
       " 'pick': 765,\n",
       " 'inside': 766,\n",
       " 'totally': 767,\n",
       " 'star': 768,\n",
       " 'evidence': 769,\n",
       " 'common': 770,\n",
       " 'discussion': 771,\n",
       " 'beautiful': 772,\n",
       " 'png': 773,\n",
       " 'album': 774,\n",
       " 'personal': 775,\n",
       " 'serious': 776,\n",
       " 'bought': 777,\n",
       " 'interested': 778,\n",
       " '16': 779,\n",
       " 'currently': 780,\n",
       " 'build': 781,\n",
       " 'plan': 782,\n",
       " 'gave': 783,\n",
       " 'effect': 784,\n",
       " 'killed': 785,\n",
       " 'clear': 786,\n",
       " 'gay': 787,\n",
       " 'pkk': 788,\n",
       " 'character': 789,\n",
       " 'asking': 790,\n",
       " 'worked': 791,\n",
       " 'fast': 792,\n",
       " 'pass': 793,\n",
       " 'relationship': 794,\n",
       " 'local': 795,\n",
       " 'office': 796,\n",
       " 'air': 797,\n",
       " 'league': 798,\n",
       " 'national': 799,\n",
       " 'political': 800,\n",
       " 'several': 801,\n",
       " 'size': 802,\n",
       " 'main': 803,\n",
       " 'gun': 804,\n",
       " 'process': 805,\n",
       " 'brain': 806,\n",
       " 'report': 807,\n",
       " 'difference': 808,\n",
       " 'above': 809,\n",
       " 'sort': 810,\n",
       " 'explain': 811,\n",
       " 'fair': 812,\n",
       " 'major': 813,\n",
       " 'science': 814,\n",
       " 'term': 815,\n",
       " 'certain': 816,\n",
       " 'lose': 817,\n",
       " 'film': 818,\n",
       " 'html': 819,\n",
       " 'boy': 820,\n",
       " '11088': 821,\n",
       " 'eyes': 822,\n",
       " 'strong': 823,\n",
       " 'response': 824,\n",
       " 'anymore': 825,\n",
       " 'himself': 826,\n",
       " 'glad': 827,\n",
       " 'problems': 828,\n",
       " 'obviously': 829,\n",
       " 'associated': 830,\n",
       " 'worst': 831,\n",
       " 'morning': 832,\n",
       " 'knows': 833,\n",
       " 'allowed': 834,\n",
       " 'sell': 835,\n",
       " 'trade': 836,\n",
       " 'congress': 837,\n",
       " 'positive': 838,\n",
       " 'alone': 839,\n",
       " 'fat': 840,\n",
       " 'hold': 841,\n",
       " 'attack': 842,\n",
       " 'card': 843,\n",
       " 'action': 844,\n",
       " 'form': 845,\n",
       " 'credit': 846,\n",
       " 'kinda': 847,\n",
       " 'hour': 848,\n",
       " 'recommend': 849,\n",
       " 'mod': 850,\n",
       " 'starting': 851,\n",
       " 'service': 852,\n",
       " 'decided': 853,\n",
       " '99': 854,\n",
       " 'ass': 855,\n",
       " 'piece': 856,\n",
       " 'special': 857,\n",
       " 'personally': 858,\n",
       " 'effects': 859,\n",
       " 'worse': 860,\n",
       " 'paste': 861,\n",
       " 'average': 862,\n",
       " 'market': 863,\n",
       " 'seriously': 864,\n",
       " 'appreciate': 865,\n",
       " 'heart': 866,\n",
       " 'y': 867,\n",
       " 'dark': 868,\n",
       " 'earth': 869,\n",
       " 'key': 870,\n",
       " 'drop': 871,\n",
       " '160': 872,\n",
       " 'til': 873,\n",
       " 'hi': 874,\n",
       " 'skin': 875,\n",
       " 'event': 876,\n",
       " 'including': 877,\n",
       " 'product': 878,\n",
       " 'wikipedia': 879,\n",
       " 'thoughts': 880,\n",
       " 'simple': 881,\n",
       " 'review': 882,\n",
       " 'nature': 883,\n",
       " 'united': 884,\n",
       " 'luck': 885,\n",
       " 'hands': 886,\n",
       " 'bed': 887,\n",
       " '40': 888,\n",
       " 'ideas': 889,\n",
       " 'send': 890,\n",
       " 'gone': 891,\n",
       " 'value': 892,\n",
       " 'view': 893,\n",
       " 'posting': 894,\n",
       " 'safe': 895,\n",
       " 'extra': 896,\n",
       " 'gives': 897,\n",
       " 'hair': 898,\n",
       " 'attention': 899,\n",
       " 'according': 900,\n",
       " 'drive': 901,\n",
       " 'meme': 902,\n",
       " 'gold': 903,\n",
       " '13': 904,\n",
       " 'info': 905,\n",
       " 'drug': 906,\n",
       " 'mother': 907,\n",
       " 'green': 908,\n",
       " 'except': 909,\n",
       " 'itself': 910,\n",
       " 'k': 911,\n",
       " 'rights': 912,\n",
       " 'race': 913,\n",
       " 'spend': 914,\n",
       " 'ball': 915,\n",
       " 'provide': 916,\n",
       " 'option': 917,\n",
       " 'america': 918,\n",
       " 'iamhoneydill': 919,\n",
       " 'cost': 920,\n",
       " 'app': 921,\n",
       " 'son': 922,\n",
       " 'academic': 923,\n",
       " 'mostly': 924,\n",
       " 'stand': 925,\n",
       " 'force': 926,\n",
       " 'decision': 927,\n",
       " 'avoid': 928,\n",
       " 'available': 929,\n",
       " 'potential': 930,\n",
       " 'create': 931,\n",
       " 'final': 932,\n",
       " 'members': 933,\n",
       " 'energy': 934,\n",
       " 'meant': 935,\n",
       " 'website': 936,\n",
       " '14': 937,\n",
       " 'videos': 938,\n",
       " 'shouldn': 939,\n",
       " 'sad': 940,\n",
       " 'walk': 941,\n",
       " 'fake': 942,\n",
       " 'quick': 943,\n",
       " 'total': 944,\n",
       " 'fit': 945,\n",
       " 'proof': 946,\n",
       " 'lower': 947,\n",
       " 'straight': 948,\n",
       " '24': 949,\n",
       " 'wonder': 950,\n",
       " 'holy': 951,\n",
       " 'ready': 952,\n",
       " 'offers': 953,\n",
       " 'feels': 954,\n",
       " 'king': 955,\n",
       " 'art': 956,\n",
       " 'campaign': 957,\n",
       " 'user': 958,\n",
       " '17': 959,\n",
       " 'countries': 960,\n",
       " 'users': 961,\n",
       " 'compared': 962,\n",
       " 'majority': 963,\n",
       " 'recent': 964,\n",
       " 'park': 965,\n",
       " 'access': 966,\n",
       " 'allow': 967,\n",
       " 'note': 968,\n",
       " 'language': 969,\n",
       " 'clinton': 970,\n",
       " 'correct': 971,\n",
       " 'search': 972,\n",
       " 'south': 973,\n",
       " 'spent': 974,\n",
       " 'changed': 975,\n",
       " 'choice': 976,\n",
       " 'pro': 977,\n",
       " 'damage': 978,\n",
       " 'lead': 979,\n",
       " 'figure': 980,\n",
       " 'paper': 981,\n",
       " 'following': 982,\n",
       " 'plus': 983,\n",
       " 'cannot': 984,\n",
       " 'co': 985,\n",
       " 'turned': 986,\n",
       " '8201': 987,\n",
       " 'multiple': 988,\n",
       " 'four': 989,\n",
       " 'welcome': 990,\n",
       " 'further': 991,\n",
       " 'ability': 992,\n",
       " 'legal': 993,\n",
       " 'official': 994,\n",
       " 'popular': 995,\n",
       " 'expect': 996,\n",
       " 'offer': 997,\n",
       " 'lives': 998,\n",
       " 'upvote': 999,\n",
       " 'added': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightsHistory(callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.log_weights(0)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.log_weights(epoch)\n",
    "        \n",
    "    def log_weights(self, step):\n",
    "        for layer_name in ['LSTM_layer', 'embeddings_layer', 'sparse_feat_dense_layer', 'output_layer']:\n",
    "            try:\n",
    "                layer = self.model.get_layer(layer_name)\n",
    "                experiment.log_histogram_3d(layer.get_weights()[0], \n",
    "                                            name=layer_name,\n",
    "                                   step=step)  \n",
    "            except Exception as e:\n",
    "                print(\"Logging weights\", e)\n",
    "                # layer probably does not exist\n",
    "                pass\n",
    "        \n",
    "class FreezeLayer(callbacks.Callback):\n",
    "    def __init__(self, logs={}, patience=5, layer='embeddings_layer', verbose=1, set_to=False):\n",
    "        super(FreezeLayer, self).__init__()\n",
    "        self.freeze_epoch = patience\n",
    "        self.freeze_layer = layer\n",
    "        self.verbose = verbose\n",
    "        self.set_to = set_to\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        if epoch == self.freeze_epoch:\n",
    "            try:\n",
    "                layer = model.get_layer(self.freeze_layer)\n",
    "                old_value = layer.trainable\n",
    "                layer.trainable = self.set_to\n",
    "                model.compile()\n",
    "                if self.verbose:\n",
    "                    print(\"Setting %s layer from %s to trainable=%s...\" % (layer.name, old_value,\n",
    "                                                                   model.get_layer(self.freeze_layer).trainable))\n",
    "            except Exception as e:\n",
    "                # layer probably does not exist\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, \n",
    "                x_train, y_train, x_test, y_test, \n",
    "                batch_size, epochs, class_weight, start_epoch=0, workers=4,\n",
    "                callback_list = [],\n",
    "                model_path='/tmp/model'):\n",
    "    print('Train...')\n",
    "    experiment.log_parameter('class_weight', class_weight.values())\n",
    "    experiment.log_parameter('callbacks', callbacks)\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs, initial_epoch=start_epoch, \n",
    "              class_weight=class_weight,\n",
    "              validation_data=[x_test, y_test],\n",
    "#               validation_split=0.3,\n",
    "                       workers=workers,\n",
    "            callbacks = [\n",
    "#                 callbacks.ModelCheckpoint(filepath='%s_best' % model_path, verbose=1, \n",
    "#                                           save_best_only=True, save_weights_only=True),\n",
    "                callbacks.EarlyStopping(patience=500), *callback_list\n",
    "            ])\n",
    "    model.save(model_path)#, save_weights_only=True)\n",
    "    experiment.log_parameter('model_path', model_path)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 166 samples, validate on 71 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Ignoring automatic log_parameter('verbose') because 'keras:verbose' is in COMET_LOGGING_PARAMETERS_IGNORE\n",
      "COMET INFO: Ignoring automatic log_parameter('do_validation') because 'keras:do_validation' is in COMET_LOGGING_PARAMETERS_IGNORE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Ignoring automatic log_metric('batch_batch') because 'keras:batch_batch' is in COMET_LOGGING_METRICS_IGNORE\n",
      "COMET INFO: Ignoring automatic log_metric('batch_size') because 'keras:batch_size' is in COMET_LOGGING_METRICS_IGNORE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/166 [====>.........................] - ETA: 4s - loss: 0.9732 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 1s 9ms/sample - loss: 1.1048 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 0.4405 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.9459 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 318us/sample - loss: 0.8632 - f1_m: 0.0370 - precision_m: 0.0278 - recall_m: 0.0556 - val_loss: 0.6217 - val_f1_m: 0.0989 - val_precision_m: 0.0611 - val_recall_m: 0.4167\n",
      "Epoch 3/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6949 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 256us/sample - loss: 0.7342 - f1_m: 0.1323 - precision_m: 0.0847 - recall_m: 0.4139 - val_loss: 0.8361 - val_f1_m: 0.1911 - val_precision_m: 0.1121 - val_recall_m: 0.6667\n",
      "Epoch 4/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6071 - f1_m: 0.2143 - precision_m: 0.1200 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 270us/sample - loss: 0.6735 - f1_m: 0.2267 - precision_m: 0.1317 - recall_m: 0.8917 - val_loss: 1.0267 - val_f1_m: 0.0954 - val_precision_m: 0.0525 - val_recall_m: 0.5556\n",
      "Epoch 5/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.7240 - f1_m: 0.1818 - precision_m: 0.1071 - recall_m: 0.6000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 247us/sample - loss: 0.6681 - f1_m: 0.1733 - precision_m: 0.0989 - recall_m: 0.7667 - val_loss: 1.1507 - val_f1_m: 0.1825 - val_precision_m: 0.1083 - val_recall_m: 0.5833\n",
      "Epoch 6/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.7528 - f1_m: 0.1818 - precision_m: 0.1000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 213us/sample - loss: 0.6657 - f1_m: 0.2194 - precision_m: 0.1316 - recall_m: 0.9167 - val_loss: 1.1284 - val_f1_m: 0.0860 - val_precision_m: 0.0494 - val_recall_m: 0.3333\n",
      "Epoch 7/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.7342 - f1_m: 0.1290 - precision_m: 0.0690 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 315us/sample - loss: 0.6516 - f1_m: 0.2147 - precision_m: 0.1232 - recall_m: 0.9722 - val_loss: 1.1168 - val_f1_m: 0.0952 - val_precision_m: 0.0528 - val_recall_m: 0.5000\n",
      "Epoch 8/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5282 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 349us/sample - loss: 0.6754 - f1_m: 0.2159 - precision_m: 0.1259 - recall_m: 0.9444 - val_loss: 1.0928 - val_f1_m: 0.1159 - val_precision_m: 0.0638 - val_recall_m: 0.6667\n",
      "Epoch 9/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4369 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 380us/sample - loss: 0.6468 - f1_m: 0.2142 - precision_m: 0.1229 - recall_m: 0.9667 - val_loss: 1.0626 - val_f1_m: 0.1166 - val_precision_m: 0.0641 - val_recall_m: 0.6667\n",
      "Epoch 10/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5379 - f1_m: 0.2424 - precision_m: 0.1379 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 410us/sample - loss: 0.6183 - f1_m: 0.2154 - precision_m: 0.1233 - recall_m: 0.9722 - val_loss: 1.0300 - val_f1_m: 0.1127 - val_precision_m: 0.0633 - val_recall_m: 0.6667\n",
      "Epoch 11/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5414 - f1_m: 0.3784 - precision_m: 0.2333 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 402us/sample - loss: 0.6440 - f1_m: 0.1738 - precision_m: 0.1007 - recall_m: 0.7500 - val_loss: 1.0140 - val_f1_m: 0.1803 - val_precision_m: 0.0999 - val_recall_m: 1.0000\n",
      "Epoch 12/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6357 - f1_m: 0.1818 - precision_m: 0.1000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 314us/sample - loss: 0.6139 - f1_m: 0.2179 - precision_m: 0.1247 - recall_m: 0.9583 - val_loss: 0.9271 - val_f1_m: 0.0833 - val_precision_m: 0.0476 - val_recall_m: 0.3333\n",
      "Epoch 13/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.7778 - f1_m: 0.0690 - precision_m: 0.0357 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 418us/sample - loss: 0.6311 - f1_m: 0.2673 - precision_m: 0.1722 - recall_m: 0.9667 - val_loss: 0.8845 - val_f1_m: 0.0961 - val_precision_m: 0.0545 - val_recall_m: 0.5833\n",
      "Epoch 14/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5784 - f1_m: 0.2581 - precision_m: 0.1481 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 393us/sample - loss: 0.5960 - f1_m: 0.2257 - precision_m: 0.1292 - recall_m: 0.9583 - val_loss: 0.9049 - val_f1_m: 0.1010 - val_precision_m: 0.0595 - val_recall_m: 0.3333\n",
      "Epoch 15/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5683 - f1_m: 0.1818 - precision_m: 0.1000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 360us/sample - loss: 0.6050 - f1_m: 0.1732 - precision_m: 0.0996 - recall_m: 0.7778 - val_loss: 0.9286 - val_f1_m: 0.1803 - val_precision_m: 0.0999 - val_recall_m: 1.0000\n",
      "Epoch 16/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4456 - f1_m: 0.3784 - precision_m: 0.2333 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 422us/sample - loss: 0.6079 - f1_m: 0.2132 - precision_m: 0.1235 - recall_m: 0.9444 - val_loss: 0.8838 - val_f1_m: 0.1778 - val_precision_m: 0.0991 - val_recall_m: 1.0000\n",
      "Epoch 17/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5653 - f1_m: 0.2857 - precision_m: 0.1724 - recall_m: 0.8333Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 346us/sample - loss: 0.5856 - f1_m: 0.2179 - precision_m: 0.1259 - recall_m: 0.9722 - val_loss: 0.8762 - val_f1_m: 0.1143 - val_precision_m: 0.0627 - val_recall_m: 0.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5374 - f1_m: 0.3030 - precision_m: 0.1852 - recall_m: 0.8333Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 351us/sample - loss: 0.5914 - f1_m: 0.2662 - precision_m: 0.1711 - recall_m: 0.9722 - val_loss: 0.8645 - val_f1_m: 0.1149 - val_precision_m: 0.0632 - val_recall_m: 0.6667\n",
      "Epoch 19/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5165 - f1_m: 0.2000 - precision_m: 0.1111 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 381us/sample - loss: 0.5669 - f1_m: 0.1846 - precision_m: 0.1048 - recall_m: 0.7917 - val_loss: 0.8782 - val_f1_m: 0.2063 - val_precision_m: 0.1222 - val_recall_m: 0.6667\n",
      "Epoch 20/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5080 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 422us/sample - loss: 0.5732 - f1_m: 0.1791 - precision_m: 0.1027 - recall_m: 0.7917 - val_loss: 0.8444 - val_f1_m: 0.2674 - val_precision_m: 0.1768 - val_recall_m: 0.6667\n",
      "Epoch 21/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5679 - f1_m: 0.2424 - precision_m: 0.1379 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 359us/sample - loss: 0.5585 - f1_m: 0.2564 - precision_m: 0.1532 - recall_m: 0.9444 - val_loss: 0.7969 - val_f1_m: 0.1892 - val_precision_m: 0.1062 - val_recall_m: 1.0000\n",
      "Epoch 22/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5799 - f1_m: 0.2500 - precision_m: 0.1481 - recall_m: 0.8000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 418us/sample - loss: 0.5577 - f1_m: 0.2498 - precision_m: 0.1470 - recall_m: 0.9667 - val_loss: 0.8200 - val_f1_m: 0.1244 - val_precision_m: 0.0713 - val_recall_m: 0.6667\n",
      "Epoch 23/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4928 - f1_m: 0.3243 - precision_m: 0.1935 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 375us/sample - loss: 0.5744 - f1_m: 0.2466 - precision_m: 0.1489 - recall_m: 0.9583 - val_loss: 0.8414 - val_f1_m: 0.2100 - val_precision_m: 0.1201 - val_recall_m: 1.0000\n",
      "Epoch 24/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5482 - f1_m: 0.2424 - precision_m: 0.1379 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 222us/sample - loss: 0.5424 - f1_m: 0.1822 - precision_m: 0.1037 - recall_m: 0.7778 - val_loss: 0.8736 - val_f1_m: 0.2628 - val_precision_m: 0.1718 - val_recall_m: 1.0000\n",
      "Epoch 25/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4231 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 259us/sample - loss: 0.5636 - f1_m: 0.1827 - precision_m: 0.1055 - recall_m: 0.8000 - val_loss: 0.8216 - val_f1_m: 0.1254 - val_precision_m: 0.0694 - val_recall_m: 0.6667\n",
      "Epoch 26/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5069 - f1_m: 0.3226 - precision_m: 0.1923 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 381us/sample - loss: 0.5417 - f1_m: 0.1987 - precision_m: 0.1153 - recall_m: 0.7778 - val_loss: 0.7595 - val_f1_m: 0.1159 - val_precision_m: 0.0659 - val_recall_m: 0.5000\n",
      "Epoch 27/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4549 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 351us/sample - loss: 0.5455 - f1_m: 0.2194 - precision_m: 0.1298 - recall_m: 0.7778 - val_loss: 0.6966 - val_f1_m: 0.1135 - val_precision_m: 0.0653 - val_recall_m: 0.4444\n",
      "Epoch 28/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4941 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 384us/sample - loss: 0.5453 - f1_m: 0.3072 - precision_m: 0.1848 - recall_m: 0.9444 - val_loss: 0.6734 - val_f1_m: 0.0952 - val_precision_m: 0.0625 - val_recall_m: 0.2000\n",
      "Epoch 29/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5777 - f1_m: 0.2609 - precision_m: 0.1579 - recall_m: 0.7500Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 374us/sample - loss: 0.5412 - f1_m: 0.3308 - precision_m: 0.2054 - recall_m: 0.9583 - val_loss: 0.7119 - val_f1_m: 0.0909 - val_precision_m: 0.0556 - val_recall_m: 0.2500\n",
      "Epoch 30/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4512 - f1_m: 0.3478 - precision_m: 0.2105 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 398us/sample - loss: 0.5319 - f1_m: 0.2253 - precision_m: 0.1330 - recall_m: 0.7917 - val_loss: 0.7708 - val_f1_m: 0.2540 - val_precision_m: 0.1595 - val_recall_m: 0.8889\n",
      "Epoch 31/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5592 - f1_m: 0.2069 - precision_m: 0.1154 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 419us/sample - loss: 0.5078 - f1_m: 0.3062 - precision_m: 0.1949 - recall_m: 0.9583 - val_loss: 0.7933 - val_f1_m: 0.1133 - val_precision_m: 0.0640 - val_recall_m: 0.5000\n",
      "Epoch 32/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4778 - f1_m: 0.2963 - precision_m: 0.1739 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 318us/sample - loss: 0.5064 - f1_m: 0.2022 - precision_m: 0.1167 - recall_m: 0.8333 - val_loss: 0.8343 - val_f1_m: 0.1323 - val_precision_m: 0.0768 - val_recall_m: 0.6667\n",
      "Epoch 33/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4404 - f1_m: 0.3030 - precision_m: 0.1786 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 319us/sample - loss: 0.5247 - f1_m: 0.2001 - precision_m: 0.1152 - recall_m: 0.8333 - val_loss: 0.8783 - val_f1_m: 0.1202 - val_precision_m: 0.0662 - val_recall_m: 0.6667\n",
      "Epoch 34/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6042 - f1_m: 0.1250 - precision_m: 0.0667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166/166 [==============================] - 0s 356us/sample - loss: 0.5411 - f1_m: 0.1990 - precision_m: 0.1184 - recall_m: 0.8333 - val_loss: 0.7972 - val_f1_m: 0.1994 - val_precision_m: 0.1310 - val_recall_m: 0.8333\n",
      "Epoch 35/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4795 - f1_m: 0.2308 - precision_m: 0.1304 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 377us/sample - loss: 0.5079 - f1_m: 0.2169 - precision_m: 0.1281 - recall_m: 0.8000 - val_loss: 0.7094 - val_f1_m: 0.1072 - val_precision_m: 0.0613 - val_recall_m: 0.5556\n",
      "Epoch 36/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5361 - f1_m: 0.2759 - precision_m: 0.1600 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 357us/sample - loss: 0.5056 - f1_m: 0.3005 - precision_m: 0.1811 - recall_m: 0.9444 - val_loss: 0.6561 - val_f1_m: 0.1111 - val_precision_m: 0.0667 - val_recall_m: 0.3333\n",
      "Epoch 37/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4538 - f1_m: 0.3478 - precision_m: 0.2105 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 402us/sample - loss: 0.5283 - f1_m: 0.2607 - precision_m: 0.1620 - recall_m: 0.8000 - val_loss: 0.6610 - val_f1_m: 0.2384 - val_precision_m: 0.1444 - val_recall_m: 0.7778\n",
      "Epoch 38/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3928 - f1_m: 0.3478 - precision_m: 0.2105 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 417us/sample - loss: 0.5080 - f1_m: 0.2671 - precision_m: 0.1617 - recall_m: 0.7778 - val_loss: 0.6605 - val_f1_m: 0.1397 - val_precision_m: 0.0862 - val_recall_m: 0.3889\n",
      "Epoch 39/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5549 - f1_m: 0.2609 - precision_m: 0.1500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 312us/sample - loss: 0.5082 - f1_m: 0.3181 - precision_m: 0.1940 - recall_m: 0.9722 - val_loss: 0.6609 - val_f1_m: 0.2656 - val_precision_m: 0.1692 - val_recall_m: 0.6667\n",
      "Epoch 40/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5331 - f1_m: 0.3571 - precision_m: 0.2174 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 323us/sample - loss: 0.4932 - f1_m: 0.3270 - precision_m: 0.2047 - recall_m: 0.9444 - val_loss: 0.7051 - val_f1_m: 0.1143 - val_precision_m: 0.0673 - val_recall_m: 0.5000\n",
      "Epoch 41/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4098 - f1_m: 0.4167 - precision_m: 0.2632 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 407us/sample - loss: 0.4812 - f1_m: 0.3023 - precision_m: 0.1814 - recall_m: 1.0000 - val_loss: 0.7711 - val_f1_m: 0.1058 - val_precision_m: 0.0602 - val_recall_m: 0.4444\n",
      "Epoch 42/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4440 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 404us/sample - loss: 0.5368 - f1_m: 0.2760 - precision_m: 0.1662 - recall_m: 0.8333 - val_loss: 0.8133 - val_f1_m: 0.0884 - val_precision_m: 0.0478 - val_recall_m: 0.6667\n",
      "Epoch 43/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4237 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 370us/sample - loss: 0.4912 - f1_m: 0.2146 - precision_m: 0.1262 - recall_m: 0.8333 - val_loss: 0.8101 - val_f1_m: 0.0857 - val_precision_m: 0.0476 - val_recall_m: 0.4444\n",
      "Epoch 44/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6498 - f1_m: 0.0833 - precision_m: 0.0435 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 366us/sample - loss: 0.4975 - f1_m: 0.2824 - precision_m: 0.1731 - recall_m: 1.0000 - val_loss: 0.7244 - val_f1_m: 0.1836 - val_precision_m: 0.1067 - val_recall_m: 0.6667\n",
      "Epoch 45/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4749 - f1_m: 0.2000 - precision_m: 0.1111 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 423us/sample - loss: 0.4960 - f1_m: 0.2336 - precision_m: 0.1397 - recall_m: 0.8333 - val_loss: 0.6957 - val_f1_m: 0.1053 - val_precision_m: 0.0714 - val_recall_m: 0.2000\n",
      "Epoch 46/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4867 - f1_m: 0.1905 - precision_m: 0.1053 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 347us/sample - loss: 0.4732 - f1_m: 0.2551 - precision_m: 0.1540 - recall_m: 0.8333 - val_loss: 0.6551 - val_f1_m: 0.1444 - val_precision_m: 0.0846 - val_recall_m: 0.5000\n",
      "Epoch 47/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4284 - f1_m: 0.4444 - precision_m: 0.2857 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 403us/sample - loss: 0.5081 - f1_m: 0.2376 - precision_m: 0.1535 - recall_m: 0.6333 - val_loss: 0.6057 - val_f1_m: 0.1538 - val_precision_m: 0.1250 - val_recall_m: 0.2000\n",
      "Epoch 48/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5232 - f1_m: 0.2222 - precision_m: 0.1333 - recall_m: 0.6667Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 299us/sample - loss: 0.4858 - f1_m: 0.3716 - precision_m: 0.2514 - recall_m: 0.8611 - val_loss: 0.5875 - val_f1_m: 0.1778 - val_precision_m: 0.1111 - val_recall_m: 0.5556\n",
      "Epoch 49/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4855 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 390us/sample - loss: 0.4835 - f1_m: 0.2569 - precision_m: 0.1592 - recall_m: 0.6806 - val_loss: 0.6371 - val_f1_m: 0.1587 - val_precision_m: 0.1019 - val_recall_m: 0.3889\n",
      "Epoch 50/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4445 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 365us/sample - loss: 0.4750 - f1_m: 0.2718 - precision_m: 0.1673 - recall_m: 0.8000 - val_loss: 0.6522 - val_f1_m: 0.1390 - val_precision_m: 0.0861 - val_recall_m: 0.4444\n",
      "Epoch 51/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5370 - f1_m: 0.3636 - precision_m: 0.2222 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
      "166/166 [==============================] - 0s 377us/sample - loss: 0.4675 - f1_m: 0.2794 - precision_m: 0.1685 - recall_m: 0.8333 - val_loss: 0.6599 - val_f1_m: 0.3056 - val_precision_m: 0.2143 - val_recall_m: 0.6667\n",
      "Epoch 52/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5258 - f1_m: 0.1111 - precision_m: 0.0588 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 346us/sample - loss: 0.4768 - f1_m: 0.3074 - precision_m: 0.1899 - recall_m: 1.0000 - val_loss: 0.6592 - val_f1_m: 0.2222 - val_precision_m: 0.1389 - val_recall_m: 0.5556\n",
      "Epoch 53/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4717 - f1_m: 0.2105 - precision_m: 0.1176 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 230us/sample - loss: 0.4805 - f1_m: 0.3104 - precision_m: 0.1896 - recall_m: 1.0000 - val_loss: 0.6592 - val_f1_m: 0.1261 - val_precision_m: 0.0754 - val_recall_m: 0.3889\n",
      "Epoch 54/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3763 - f1_m: 0.4615 - precision_m: 0.3000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 337us/sample - loss: 0.4722 - f1_m: 0.2721 - precision_m: 0.1650 - recall_m: 0.8333 - val_loss: 0.6599 - val_f1_m: 0.1111 - val_precision_m: 0.0714 - val_recall_m: 0.2500\n",
      "Epoch 55/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4806 - f1_m: 0.5333 - precision_m: 0.3636 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 292us/sample - loss: 0.4569 - f1_m: 0.3134 - precision_m: 0.1937 - recall_m: 1.0000 - val_loss: 0.6602 - val_f1_m: 0.1310 - val_precision_m: 0.0812 - val_recall_m: 0.5000\n",
      "Epoch 56/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3807 - f1_m: 0.3810 - precision_m: 0.2353 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 335us/sample - loss: 0.4681 - f1_m: 0.2704 - precision_m: 0.1646 - recall_m: 0.8333 - val_loss: 0.6605 - val_f1_m: 0.1278 - val_precision_m: 0.0769 - val_recall_m: 0.3889\n",
      "Epoch 57/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5142 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 386us/sample - loss: 0.4649 - f1_m: 0.3425 - precision_m: 0.2096 - recall_m: 1.0000 - val_loss: 0.6608 - val_f1_m: 0.1229 - val_precision_m: 0.0733 - val_recall_m: 0.3889\n",
      "Epoch 58/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4275 - f1_m: 0.3000 - precision_m: 0.1765 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 384us/sample - loss: 0.4523 - f1_m: 0.3371 - precision_m: 0.2073 - recall_m: 1.0000 - val_loss: 0.6616 - val_f1_m: 0.3179 - val_precision_m: 0.2525 - val_recall_m: 0.5000\n",
      "Epoch 59/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4288 - f1_m: 0.4545 - precision_m: 0.2941 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 486us/sample - loss: 0.4666 - f1_m: 0.2789 - precision_m: 0.1714 - recall_m: 0.8333 - val_loss: 0.6624 - val_f1_m: 0.1402 - val_precision_m: 0.0859 - val_recall_m: 0.3889\n",
      "Epoch 60/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5169 - f1_m: 0.2400 - precision_m: 0.1364 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 390us/sample - loss: 0.4630 - f1_m: 0.3535 - precision_m: 0.2194 - recall_m: 1.0000 - val_loss: 0.6625 - val_f1_m: 0.2291 - val_precision_m: 0.1652 - val_recall_m: 0.6667\n",
      "Epoch 61/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3603 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 420us/sample - loss: 0.4836 - f1_m: 0.3200 - precision_m: 0.2008 - recall_m: 0.8333 - val_loss: 0.6623 - val_f1_m: 0.1389 - val_precision_m: 0.0846 - val_recall_m: 0.3889\n",
      "Epoch 62/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5058 - f1_m: 0.3200 - precision_m: 0.1905 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 425us/sample - loss: 0.4627 - f1_m: 0.2694 - precision_m: 0.1633 - recall_m: 0.8333 - val_loss: 0.6628 - val_f1_m: 0.1201 - val_precision_m: 0.0735 - val_recall_m: 0.5000\n",
      "Epoch 63/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5195 - f1_m: 0.3478 - precision_m: 0.2105 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 331us/sample - loss: 0.4859 - f1_m: 0.2615 - precision_m: 0.1595 - recall_m: 0.8333 - val_loss: 0.6626 - val_f1_m: 0.1273 - val_precision_m: 0.0725 - val_recall_m: 0.5556\n",
      "Epoch 64/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5310 - f1_m: 0.2727 - precision_m: 0.1579 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 278us/sample - loss: 0.4575 - f1_m: 0.3055 - precision_m: 0.1836 - recall_m: 1.0000 - val_loss: 0.6621 - val_f1_m: 0.2286 - val_precision_m: 0.1667 - val_recall_m: 0.5000\n",
      "Epoch 65/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4980 - f1_m: 0.3077 - precision_m: 0.1818 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 378us/sample - loss: 0.4647 - f1_m: 0.2739 - precision_m: 0.1657 - recall_m: 0.8333 - val_loss: 0.6628 - val_f1_m: 0.1397 - val_precision_m: 0.0905 - val_recall_m: 0.5000\n",
      "Epoch 66/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5721 - f1_m: 0.2105 - precision_m: 0.1176 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 431us/sample - loss: 0.5179 - f1_m: 0.3351 - precision_m: 0.2135 - recall_m: 0.8333 - val_loss: 0.6622 - val_f1_m: 0.3179 - val_precision_m: 0.2500 - val_recall_m: 0.6667\n",
      "Epoch 67/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5467 - f1_m: 0.2105 - precision_m: 0.1176 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 393us/sample - loss: 0.4641 - f1_m: 0.3264 - precision_m: 0.1968 - recall_m: 1.0000 - val_loss: 0.6629 - val_f1_m: 0.2924 - val_precision_m: 0.2083 - val_recall_m: 0.5556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5249 - f1_m: 0.2609 - precision_m: 0.1500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 383us/sample - loss: 0.4720 - f1_m: 0.2677 - precision_m: 0.1609 - recall_m: 0.8333 - val_loss: 0.6640 - val_f1_m: 0.1176 - val_precision_m: 0.0769 - val_recall_m: 0.2500\n",
      "Epoch 69/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5824 - f1_m: 0.1818 - precision_m: 0.1000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 361us/sample - loss: 0.4679 - f1_m: 0.2632 - precision_m: 0.1607 - recall_m: 0.8333 - val_loss: 0.6642 - val_f1_m: 0.3007 - val_precision_m: 0.2111 - val_recall_m: 0.6667\n",
      "Epoch 70/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4663 - f1_m: 0.2727 - precision_m: 0.1579 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 400us/sample - loss: 0.4799 - f1_m: 0.2532 - precision_m: 0.1558 - recall_m: 0.8333 - val_loss: 0.6639 - val_f1_m: 0.2698 - val_precision_m: 0.2479 - val_recall_m: 0.5556\n",
      "Epoch 71/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4537 - f1_m: 0.1818 - precision_m: 0.1000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 392us/sample - loss: 0.4910 - f1_m: 0.3165 - precision_m: 0.1971 - recall_m: 1.0000 - val_loss: 0.6632 - val_f1_m: 0.1217 - val_precision_m: 0.0722 - val_recall_m: 0.3889\n",
      "Epoch 72/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4149 - f1_m: 0.3810 - precision_m: 0.2353 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 378us/sample - loss: 0.4610 - f1_m: 0.3213 - precision_m: 0.1920 - recall_m: 1.0000 - val_loss: 0.6641 - val_f1_m: 0.2667 - val_precision_m: 0.1923 - val_recall_m: 0.5000\n",
      "Epoch 73/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5949 - f1_m: 0.1667 - precision_m: 0.0909 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 429us/sample - loss: 0.4681 - f1_m: 0.3694 - precision_m: 0.2393 - recall_m: 1.0000 - val_loss: 0.6652 - val_f1_m: 0.1369 - val_precision_m: 0.0844 - val_recall_m: 0.3889\n",
      "Epoch 74/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4414 - f1_m: 0.3200 - precision_m: 0.1905 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 429us/sample - loss: 0.4784 - f1_m: 0.2658 - precision_m: 0.1617 - recall_m: 0.8333 - val_loss: 0.6666 - val_f1_m: 0.1176 - val_precision_m: 0.0769 - val_recall_m: 0.2500\n",
      "Epoch 75/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5750 - f1_m: 0.0952 - precision_m: 0.0500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 363us/sample - loss: 0.4676 - f1_m: 0.3642 - precision_m: 0.2407 - recall_m: 1.0000 - val_loss: 0.6667 - val_f1_m: 0.1503 - val_precision_m: 0.0889 - val_recall_m: 0.5000\n",
      "Epoch 76/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4178 - f1_m: 0.3846 - precision_m: 0.2381 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 390us/sample - loss: 0.4604 - f1_m: 0.2724 - precision_m: 0.1642 - recall_m: 0.8333 - val_loss: 0.6674 - val_f1_m: 0.2692 - val_precision_m: 0.1717 - val_recall_m: 0.6667\n",
      "Epoch 77/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5127 - f1_m: 0.2609 - precision_m: 0.1500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 404us/sample - loss: 0.4598 - f1_m: 0.2595 - precision_m: 0.1547 - recall_m: 0.8333 - val_loss: 0.6676 - val_f1_m: 0.1306 - val_precision_m: 0.0794 - val_recall_m: 0.3889\n",
      "Epoch 78/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5069 - f1_m: 0.1000 - precision_m: 0.0526 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 339us/sample - loss: 0.4593 - f1_m: 0.2613 - precision_m: 0.1596 - recall_m: 0.8333 - val_loss: 0.6672 - val_f1_m: 0.2035 - val_precision_m: 0.1250 - val_recall_m: 0.5556\n",
      "Epoch 79/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4306 - f1_m: 0.3571 - precision_m: 0.2174 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 229us/sample - loss: 0.4984 - f1_m: 0.3179 - precision_m: 0.1971 - recall_m: 0.8333 - val_loss: 0.6663 - val_f1_m: 0.0952 - val_precision_m: 0.0625 - val_recall_m: 0.2000\n",
      "Epoch 80/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4373 - f1_m: 0.3846 - precision_m: 0.2381 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 298us/sample - loss: 0.4794 - f1_m: 0.2632 - precision_m: 0.1606 - recall_m: 0.8333 - val_loss: 0.6660 - val_f1_m: 0.1451 - val_precision_m: 0.0893 - val_recall_m: 0.3889\n",
      "Epoch 81/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3534 - f1_m: 0.4348 - precision_m: 0.2778 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 509us/sample - loss: 0.4762 - f1_m: 0.3652 - precision_m: 0.2368 - recall_m: 1.0000 - val_loss: 0.6657 - val_f1_m: 0.1369 - val_precision_m: 0.0828 - val_recall_m: 0.5556\n",
      "Epoch 82/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4411 - f1_m: 0.4286 - precision_m: 0.2727 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 368us/sample - loss: 0.4774 - f1_m: 0.2674 - precision_m: 0.1631 - recall_m: 0.8333 - val_loss: 0.6661 - val_f1_m: 0.2451 - val_precision_m: 0.1587 - val_recall_m: 0.5556\n",
      "Epoch 83/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4303 - f1_m: 0.2000 - precision_m: 0.1111 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 391us/sample - loss: 0.4662 - f1_m: 0.3292 - precision_m: 0.2022 - recall_m: 1.0000 - val_loss: 0.6658 - val_f1_m: 0.1296 - val_precision_m: 0.0778 - val_recall_m: 0.3889\n",
      "Epoch 84/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3764 - f1_m: 0.3571 - precision_m: 0.2174 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166/166 [==============================] - 0s 376us/sample - loss: 0.4960 - f1_m: 0.3760 - precision_m: 0.2649 - recall_m: 1.0000 - val_loss: 0.6665 - val_f1_m: 0.1261 - val_precision_m: 0.0754 - val_recall_m: 0.3889\n",
      "Epoch 85/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4268 - f1_m: 0.5185 - precision_m: 0.3500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 399us/sample - loss: 0.4996 - f1_m: 0.2445 - precision_m: 0.1549 - recall_m: 0.6667 - val_loss: 0.6670 - val_f1_m: 0.1296 - val_precision_m: 0.0787 - val_recall_m: 0.4444\n",
      "Epoch 86/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5398 - f1_m: 0.2500 - precision_m: 0.1429 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 400us/sample - loss: 0.4683 - f1_m: 0.3176 - precision_m: 0.1914 - recall_m: 1.0000 - val_loss: 0.6665 - val_f1_m: 0.2556 - val_precision_m: 0.2179 - val_recall_m: 0.5000\n",
      "Epoch 87/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4722 - f1_m: 0.5600 - precision_m: 0.3889 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 388us/sample - loss: 0.4600 - f1_m: 0.2665 - precision_m: 0.1651 - recall_m: 0.8333 - val_loss: 0.6663 - val_f1_m: 0.1297 - val_precision_m: 0.0778 - val_recall_m: 0.4444\n",
      "Epoch 88/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3999 - f1_m: 0.4800 - precision_m: 0.3158 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 349us/sample - loss: 0.4783 - f1_m: 0.2662 - precision_m: 0.1665 - recall_m: 0.8333 - val_loss: 0.6660 - val_f1_m: 0.1296 - val_precision_m: 0.0750 - val_recall_m: 0.5000\n",
      "Epoch 89/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5878 - f1_m: 0.0870 - precision_m: 0.0455 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 233us/sample - loss: 0.4714 - f1_m: 0.3259 - precision_m: 0.2023 - recall_m: 1.0000 - val_loss: 0.6649 - val_f1_m: 0.1439 - val_precision_m: 0.0847 - val_recall_m: 0.5000\n",
      "Epoch 90/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3639 - f1_m: 0.3478 - precision_m: 0.2105 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 284us/sample - loss: 0.4536 - f1_m: 0.3681 - precision_m: 0.2380 - recall_m: 1.0000 - val_loss: 0.6652 - val_f1_m: 0.1217 - val_precision_m: 0.0722 - val_recall_m: 0.3889\n",
      "Epoch 91/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6060 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 219us/sample - loss: 0.4701 - f1_m: 0.2649 - precision_m: 0.1609 - recall_m: 0.8333 - val_loss: 0.6660 - val_f1_m: 0.1308 - val_precision_m: 0.0750 - val_recall_m: 0.5556\n",
      "Epoch 92/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4829 - f1_m: 0.3158 - precision_m: 0.1875 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 299us/sample - loss: 0.4943 - f1_m: 0.3130 - precision_m: 0.1931 - recall_m: 0.8333 - val_loss: 0.6658 - val_f1_m: 0.1176 - val_precision_m: 0.0714 - val_recall_m: 0.3333\n",
      "Epoch 93/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4438 - f1_m: 0.3200 - precision_m: 0.1905 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 351us/sample - loss: 0.4565 - f1_m: 0.3252 - precision_m: 0.2025 - recall_m: 1.0000 - val_loss: 0.6658 - val_f1_m: 0.2291 - val_precision_m: 0.1393 - val_recall_m: 0.6667\n",
      "Epoch 94/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4463 - f1_m: 0.2105 - precision_m: 0.1176 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 530us/sample - loss: 0.4644 - f1_m: 0.2664 - precision_m: 0.1630 - recall_m: 0.8333 - val_loss: 0.6660 - val_f1_m: 0.2167 - val_precision_m: 0.1389 - val_recall_m: 0.5000\n",
      "Epoch 95/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4311 - f1_m: 0.2353 - precision_m: 0.1333 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 416us/sample - loss: 0.4644 - f1_m: 0.2702 - precision_m: 0.1650 - recall_m: 0.8333 - val_loss: 0.6653 - val_f1_m: 0.1053 - val_precision_m: 0.0625 - val_recall_m: 0.3333\n",
      "Epoch 96/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4373 - f1_m: 0.3846 - precision_m: 0.2381 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 336us/sample - loss: 0.4734 - f1_m: 0.3215 - precision_m: 0.1957 - recall_m: 1.0000 - val_loss: 0.6646 - val_f1_m: 0.2924 - val_precision_m: 0.2111 - val_recall_m: 0.5000\n",
      "Epoch 97/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4312 - f1_m: 0.5385 - precision_m: 0.3684 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 394us/sample - loss: 0.4856 - f1_m: 0.2681 - precision_m: 0.1682 - recall_m: 0.8333 - val_loss: 0.6647 - val_f1_m: 0.1201 - val_precision_m: 0.0714 - val_recall_m: 0.3889\n",
      "Epoch 98/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4294 - f1_m: 0.5217 - precision_m: 0.3529 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 324us/sample - loss: 0.4635 - f1_m: 0.3484 - precision_m: 0.2191 - recall_m: 1.0000 - val_loss: 0.6644 - val_f1_m: 0.2587 - val_precision_m: 0.2179 - val_recall_m: 0.6667\n",
      "Epoch 99/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4793 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 265us/sample - loss: 0.4828 - f1_m: 0.2601 - precision_m: 0.1571 - recall_m: 0.8333 - val_loss: 0.6645 - val_f1_m: 0.1111 - val_precision_m: 0.0714 - val_recall_m: 0.2500\n",
      "Epoch 100/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4474 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 222us/sample - loss: 0.4788 - f1_m: 0.3282 - precision_m: 0.2036 - recall_m: 1.0000 - val_loss: 0.6640 - val_f1_m: 0.1257 - val_precision_m: 0.0747 - val_recall_m: 0.5000\n",
      "Epoch 101/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6015 - f1_m: 0.2609 - precision_m: 0.1500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "\n",
      "Epoch 00101: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "166/166 [==============================] - 0s 332us/sample - loss: 0.4704 - f1_m: 0.2724 - precision_m: 0.1652 - recall_m: 0.8333 - val_loss: 0.6640 - val_f1_m: 0.2587 - val_precision_m: 0.2183 - val_recall_m: 0.6667\n",
      "Epoch 102/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4079 - f1_m: 0.2000 - precision_m: 0.1111 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 430us/sample - loss: 0.4537 - f1_m: 0.3256 - precision_m: 0.1966 - recall_m: 1.0000 - val_loss: 0.6640 - val_f1_m: 0.1310 - val_precision_m: 0.0791 - val_recall_m: 0.3889\n",
      "Epoch 103/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4750 - f1_m: 0.2727 - precision_m: 0.1579 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 511us/sample - loss: 0.4647 - f1_m: 0.3613 - precision_m: 0.2396 - recall_m: 1.0000 - val_loss: 0.6640 - val_f1_m: 0.1402 - val_precision_m: 0.0859 - val_recall_m: 0.3889\n",
      "Epoch 104/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4747 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 458us/sample - loss: 0.4697 - f1_m: 0.3569 - precision_m: 0.2317 - recall_m: 1.0000 - val_loss: 0.6641 - val_f1_m: 0.1429 - val_precision_m: 0.0884 - val_recall_m: 0.3889\n",
      "Epoch 105/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3991 - f1_m: 0.4167 - precision_m: 0.2632 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 488us/sample - loss: 0.5088 - f1_m: 0.2625 - precision_m: 0.1642 - recall_m: 0.6667 - val_loss: 0.6641 - val_f1_m: 0.3175 - val_precision_m: 0.2222 - val_recall_m: 0.6667\n",
      "Epoch 106/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4969 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 497us/sample - loss: 0.4662 - f1_m: 0.2703 - precision_m: 0.1624 - recall_m: 0.8333 - val_loss: 0.6641 - val_f1_m: 0.1841 - val_precision_m: 0.1111 - val_recall_m: 0.5556\n",
      "Epoch 107/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5227 - f1_m: 0.2308 - precision_m: 0.1304 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 483us/sample - loss: 0.4682 - f1_m: 0.3693 - precision_m: 0.2542 - recall_m: 1.0000 - val_loss: 0.6641 - val_f1_m: 0.1429 - val_precision_m: 0.0862 - val_recall_m: 0.5556\n",
      "Epoch 108/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4070 - f1_m: 0.3000 - precision_m: 0.1765 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 460us/sample - loss: 0.4665 - f1_m: 0.3299 - precision_m: 0.2037 - recall_m: 1.0000 - val_loss: 0.6641 - val_f1_m: 0.1053 - val_precision_m: 0.0714 - val_recall_m: 0.2000\n",
      "Epoch 109/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4844 - f1_m: 0.2609 - precision_m: 0.1500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 347us/sample - loss: 0.4628 - f1_m: 0.3307 - precision_m: 0.2006 - recall_m: 1.0000 - val_loss: 0.6642 - val_f1_m: 0.1396 - val_precision_m: 0.0937 - val_recall_m: 0.5000\n",
      "Epoch 110/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4398 - f1_m: 0.4138 - precision_m: 0.2609 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 452us/sample - loss: 0.4742 - f1_m: 0.3228 - precision_m: 0.1961 - recall_m: 1.0000 - val_loss: 0.6642 - val_f1_m: 0.2556 - val_precision_m: 0.1624 - val_recall_m: 0.6667\n",
      "Epoch 111/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4734 - f1_m: 0.3636 - precision_m: 0.2222 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 382us/sample - loss: 0.4733 - f1_m: 0.2763 - precision_m: 0.1679 - recall_m: 0.8333 - val_loss: 0.6643 - val_f1_m: 0.1053 - val_precision_m: 0.0625 - val_recall_m: 0.3333\n",
      "Epoch 112/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3646 - f1_m: 0.3846 - precision_m: 0.2381 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 507us/sample - loss: 0.4660 - f1_m: 0.2645 - precision_m: 0.1597 - recall_m: 0.8333 - val_loss: 0.6643 - val_f1_m: 0.1310 - val_precision_m: 0.0769 - val_recall_m: 0.5556\n",
      "Epoch 113/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3941 - f1_m: 0.4211 - precision_m: 0.2667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 377us/sample - loss: 0.4653 - f1_m: 0.2780 - precision_m: 0.1677 - recall_m: 0.8333 - val_loss: 0.6643 - val_f1_m: 0.1296 - val_precision_m: 0.0787 - val_recall_m: 0.4444\n",
      "Epoch 114/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5182 - f1_m: 0.3571 - precision_m: 0.2174 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 502us/sample - loss: 0.4607 - f1_m: 0.2735 - precision_m: 0.1644 - recall_m: 0.8333 - val_loss: 0.6643 - val_f1_m: 0.1604 - val_precision_m: 0.1056 - val_recall_m: 0.3889\n",
      "Epoch 115/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3557 - f1_m: 0.4615 - precision_m: 0.3000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 418us/sample - loss: 0.4864 - f1_m: 0.2719 - precision_m: 0.1670 - recall_m: 0.8333 - val_loss: 0.6643 - val_f1_m: 0.1185 - val_precision_m: 0.0701 - val_recall_m: 0.3889\n",
      "Epoch 116/1000\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 0.4573 - f1_m: 0.3295 - precision_m: 0.1993 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 523us/sample - loss: 0.4663 - f1_m: 0.2746 - precision_m: 0.1661 - recall_m: 0.8333 - val_loss: 0.6642 - val_f1_m: 0.1310 - val_precision_m: 0.0791 - val_recall_m: 0.3889\n",
      "Epoch 117/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5047 - f1_m: 0.3200 - precision_m: 0.1905 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 430us/sample - loss: 0.4641 - f1_m: 0.3713 - precision_m: 0.2426 - recall_m: 1.0000 - val_loss: 0.6642 - val_f1_m: 0.2596 - val_precision_m: 0.1652 - val_recall_m: 0.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4727 - f1_m: 0.4615 - precision_m: 0.3000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 429us/sample - loss: 0.4566 - f1_m: 0.3267 - precision_m: 0.1991 - recall_m: 1.0000 - val_loss: 0.6642 - val_f1_m: 0.1397 - val_precision_m: 0.0862 - val_recall_m: 0.3889\n",
      "Epoch 119/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5227 - f1_m: 0.1818 - precision_m: 0.1000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 544us/sample - loss: 0.4736 - f1_m: 0.2611 - precision_m: 0.1604 - recall_m: 0.8333 - val_loss: 0.6642 - val_f1_m: 0.1273 - val_precision_m: 0.0725 - val_recall_m: 0.5556\n",
      "Epoch 120/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5166 - f1_m: 0.2963 - precision_m: 0.1739 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 485us/sample - loss: 0.4772 - f1_m: 0.2741 - precision_m: 0.1735 - recall_m: 0.8333 - val_loss: 0.6643 - val_f1_m: 0.1278 - val_precision_m: 0.0754 - val_recall_m: 0.4444\n",
      "Epoch 121/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4231 - f1_m: 0.4138 - precision_m: 0.2609 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 415us/sample - loss: 0.4714 - f1_m: 0.3377 - precision_m: 0.2099 - recall_m: 1.0000 - val_loss: 0.6642 - val_f1_m: 0.1250 - val_precision_m: 0.0751 - val_recall_m: 0.3889\n",
      "Epoch 122/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4488 - f1_m: 0.1818 - precision_m: 0.1000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 442us/sample - loss: 0.4791 - f1_m: 0.3204 - precision_m: 0.1973 - recall_m: 1.0000 - val_loss: 0.6642 - val_f1_m: 0.1369 - val_precision_m: 0.0844 - val_recall_m: 0.3889\n",
      "Epoch 123/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3930 - f1_m: 0.2353 - precision_m: 0.1333 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 473us/sample - loss: 0.4594 - f1_m: 0.2688 - precision_m: 0.1644 - recall_m: 0.8333 - val_loss: 0.6643 - val_f1_m: 0.1345 - val_precision_m: 0.0794 - val_recall_m: 0.4444\n",
      "Epoch 124/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4645 - f1_m: 0.1739 - precision_m: 0.0952 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 431us/sample - loss: 0.4531 - f1_m: 0.3288 - precision_m: 0.1992 - recall_m: 1.0000 - val_loss: 0.6642 - val_f1_m: 0.2587 - val_precision_m: 0.1652 - val_recall_m: 0.7778\n",
      "Epoch 125/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5030 - f1_m: 0.3200 - precision_m: 0.1905 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 431us/sample - loss: 0.4717 - f1_m: 0.2642 - precision_m: 0.1617 - recall_m: 0.8333 - val_loss: 0.6643 - val_f1_m: 0.1418 - val_precision_m: 0.0828 - val_recall_m: 0.5000\n",
      "Epoch 126/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4854 - f1_m: 0.1905 - precision_m: 0.1053 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 361us/sample - loss: 0.4736 - f1_m: 0.2617 - precision_m: 0.1641 - recall_m: 0.8333 - val_loss: 0.6642 - val_f1_m: 0.1261 - val_precision_m: 0.0754 - val_recall_m: 0.3889\n",
      "Epoch 127/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5590 - f1_m: 0.1000 - precision_m: 0.0526 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 392us/sample - loss: 0.4742 - f1_m: 0.3495 - precision_m: 0.2277 - recall_m: 1.0000 - val_loss: 0.6642 - val_f1_m: 0.1176 - val_precision_m: 0.0769 - val_recall_m: 0.2500\n",
      "Epoch 128/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3870 - f1_m: 0.6667 - precision_m: 0.5000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 271us/sample - loss: 0.5112 - f1_m: 0.2709 - precision_m: 0.1791 - recall_m: 0.6667 - val_loss: 0.6642 - val_f1_m: 0.2587 - val_precision_m: 0.1652 - val_recall_m: 0.7778\n",
      "Epoch 129/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4026 - f1_m: 0.2400 - precision_m: 0.1364 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 417us/sample - loss: 0.4548 - f1_m: 0.3452 - precision_m: 0.2124 - recall_m: 1.0000 - val_loss: 0.6642 - val_f1_m: 0.1296 - val_precision_m: 0.0750 - val_recall_m: 0.5000\n",
      "Epoch 130/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4406 - f1_m: 0.3636 - precision_m: 0.2222 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 351us/sample - loss: 0.4573 - f1_m: 0.3684 - precision_m: 0.2371 - recall_m: 1.0000 - val_loss: 0.6642 - val_f1_m: 0.1310 - val_precision_m: 0.0812 - val_recall_m: 0.5000\n",
      "Epoch 131/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6681 - f1_m: 0.2000 - precision_m: 0.1111 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 318us/sample - loss: 0.4689 - f1_m: 0.3239 - precision_m: 0.1950 - recall_m: 1.0000 - val_loss: 0.6642 - val_f1_m: 0.1296 - val_precision_m: 0.0750 - val_recall_m: 0.5000\n",
      "Epoch 132/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4378 - f1_m: 0.5385 - precision_m: 0.3684 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 540us/sample - loss: 0.4700 - f1_m: 0.2599 - precision_m: 0.1604 - recall_m: 0.8333 - val_loss: 0.6642 - val_f1_m: 0.1340 - val_precision_m: 0.0810 - val_recall_m: 0.3889\n",
      "Epoch 133/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4486 - f1_m: 0.2400 - precision_m: 0.1364 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 372us/sample - loss: 0.4592 - f1_m: 0.2662 - precision_m: 0.1608 - recall_m: 0.8333 - val_loss: 0.6642 - val_f1_m: 0.3007 - val_precision_m: 0.2143 - val_recall_m: 0.5556\n",
      "Epoch 134/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4822 - f1_m: 0.3810 - precision_m: 0.2353 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 493us/sample - loss: 0.4630 - f1_m: 0.3281 - precision_m: 0.1989 - recall_m: 1.0000 - val_loss: 0.6642 - val_f1_m: 0.1176 - val_precision_m: 0.0714 - val_recall_m: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5470 - f1_m: 0.2105 - precision_m: 0.1176 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 399us/sample - loss: 0.4719 - f1_m: 0.2653 - precision_m: 0.1616 - recall_m: 0.8333 - val_loss: 0.6642 - val_f1_m: 0.1470 - val_precision_m: 0.0905 - val_recall_m: 0.5556\n",
      "Epoch 136/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3639 - f1_m: 0.3846 - precision_m: 0.2381 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 475us/sample - loss: 0.4715 - f1_m: 0.2762 - precision_m: 0.1676 - recall_m: 0.8333 - val_loss: 0.6642 - val_f1_m: 0.2596 - val_precision_m: 0.1652 - val_recall_m: 0.6667\n",
      "Epoch 137/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4599 - f1_m: 0.2727 - precision_m: 0.1579 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 572us/sample - loss: 0.4765 - f1_m: 0.2724 - precision_m: 0.1664 - recall_m: 0.8333 - val_loss: 0.6642 - val_f1_m: 0.2118 - val_precision_m: 0.1556 - val_recall_m: 0.5000\n",
      "Epoch 138/1000\n",
      "128/166 [======================>.......] - ETA: 0s - loss: 0.4732 - f1_m: 0.2537 - precision_m: 0.1471 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 529us/sample - loss: 0.4628 - f1_m: 0.3192 - precision_m: 0.1953 - recall_m: 1.0000 - val_loss: 0.6642 - val_f1_m: 0.1346 - val_precision_m: 0.0810 - val_recall_m: 0.4444\n",
      "Epoch 139/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5137 - f1_m: 0.3077 - precision_m: 0.1818 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 428us/sample - loss: 0.4547 - f1_m: 0.3283 - precision_m: 0.1989 - recall_m: 1.0000 - val_loss: 0.6642 - val_f1_m: 0.1297 - val_precision_m: 0.0747 - val_recall_m: 0.5000\n",
      "Epoch 140/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6205 - f1_m: 0.1818 - precision_m: 0.1000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 355us/sample - loss: 0.4734 - f1_m: 0.3392 - precision_m: 0.2093 - recall_m: 1.0000 - val_loss: 0.6642 - val_f1_m: 0.1278 - val_precision_m: 0.0754 - val_recall_m: 0.4444\n",
      "Epoch 141/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4655 - f1_m: 0.2609 - precision_m: 0.1500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 254us/sample - loss: 0.4747 - f1_m: 0.3591 - precision_m: 0.2352 - recall_m: 1.0000 - val_loss: 0.6643 - val_f1_m: 0.2137 - val_precision_m: 0.1333 - val_recall_m: 0.5556\n",
      "Epoch 142/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4832 - f1_m: 0.2000 - precision_m: 0.1111 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 536us/sample - loss: 0.4747 - f1_m: 0.2702 - precision_m: 0.1667 - recall_m: 0.8333 - val_loss: 0.6643 - val_f1_m: 0.4273 - val_precision_m: 0.3863 - val_recall_m: 0.7778\n",
      "Epoch 143/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3680 - f1_m: 0.4286 - precision_m: 0.2727 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 399us/sample - loss: 0.4698 - f1_m: 0.3186 - precision_m: 0.1929 - recall_m: 1.0000 - val_loss: 0.6643 - val_f1_m: 0.1229 - val_precision_m: 0.0733 - val_recall_m: 0.3889\n",
      "Epoch 144/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4340 - f1_m: 0.1739 - precision_m: 0.0952 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 441us/sample - loss: 0.4771 - f1_m: 0.2695 - precision_m: 0.1710 - recall_m: 0.8333 - val_loss: 0.6644 - val_f1_m: 0.1250 - val_precision_m: 0.0833 - val_recall_m: 0.2500\n",
      "Epoch 145/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6124 - f1_m: 0.2105 - precision_m: 0.1176 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 309us/sample - loss: 0.4833 - f1_m: 0.2701 - precision_m: 0.1652 - recall_m: 0.8333 - val_loss: 0.6644 - val_f1_m: 0.2545 - val_precision_m: 0.1574 - val_recall_m: 0.6667\n",
      "Epoch 146/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5723 - f1_m: 0.0833 - precision_m: 0.0435 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 396us/sample - loss: 0.4771 - f1_m: 0.3196 - precision_m: 0.1970 - recall_m: 1.0000 - val_loss: 0.6643 - val_f1_m: 0.1442 - val_precision_m: 0.0905 - val_recall_m: 0.3889\n",
      "Epoch 147/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4500 - f1_m: 0.4828 - precision_m: 0.3182 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 370us/sample - loss: 0.4768 - f1_m: 0.3219 - precision_m: 0.2003 - recall_m: 1.0000 - val_loss: 0.6643 - val_f1_m: 0.1297 - val_precision_m: 0.0779 - val_recall_m: 0.3889\n",
      "Epoch 148/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4408 - f1_m: 0.4444 - precision_m: 0.2857 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 489us/sample - loss: 0.4656 - f1_m: 0.3212 - precision_m: 0.1977 - recall_m: 1.0000 - val_loss: 0.6643 - val_f1_m: 0.1481 - val_precision_m: 0.0889 - val_recall_m: 0.4444\n",
      "Epoch 149/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4888 - f1_m: 0.2609 - precision_m: 0.1500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 426us/sample - loss: 0.4688 - f1_m: 0.3352 - precision_m: 0.2092 - recall_m: 1.0000 - val_loss: 0.6644 - val_f1_m: 0.2286 - val_precision_m: 0.1500 - val_recall_m: 0.5000\n",
      "Epoch 150/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4457 - f1_m: 0.2308 - precision_m: 0.1304 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 482us/sample - loss: 0.4604 - f1_m: 0.2634 - precision_m: 0.1594 - recall_m: 0.8333 - val_loss: 0.6644 - val_f1_m: 0.3115 - val_precision_m: 0.2192 - val_recall_m: 0.7778\n",
      "Epoch 151/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4409 - f1_m: 0.3529 - precision_m: 0.2143 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 379us/sample - loss: 0.4674 - f1_m: 0.2798 - precision_m: 0.1684 - recall_m: 0.8333 - val_loss: 0.6644 - val_f1_m: 0.1000 - val_precision_m: 0.0667 - val_recall_m: 0.2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 152/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5060 - f1_m: 0.1111 - precision_m: 0.0588 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 289us/sample - loss: 0.4706 - f1_m: 0.3528 - precision_m: 0.2336 - recall_m: 1.0000 - val_loss: 0.6644 - val_f1_m: 0.1442 - val_precision_m: 0.0862 - val_recall_m: 0.4444\n",
      "Epoch 153/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4435 - f1_m: 0.3478 - precision_m: 0.2105 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 320us/sample - loss: 0.4635 - f1_m: 0.2771 - precision_m: 0.1666 - recall_m: 0.8333 - val_loss: 0.6644 - val_f1_m: 0.0909 - val_precision_m: 0.0556 - val_recall_m: 0.2500\n",
      "Epoch 154/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4609 - f1_m: 0.2727 - precision_m: 0.1579 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 480us/sample - loss: 0.4657 - f1_m: 0.2676 - precision_m: 0.1619 - recall_m: 0.8333 - val_loss: 0.6644 - val_f1_m: 0.2619 - val_precision_m: 0.1717 - val_recall_m: 0.5556\n",
      "Epoch 155/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5622 - f1_m: 0.2609 - precision_m: 0.1500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 454us/sample - loss: 0.4743 - f1_m: 0.3036 - precision_m: 0.1879 - recall_m: 1.0000 - val_loss: 0.6644 - val_f1_m: 0.2137 - val_precision_m: 0.1273 - val_recall_m: 0.6667\n",
      "Epoch 156/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4476 - f1_m: 0.4167 - precision_m: 0.2632 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 456us/sample - loss: 0.4631 - f1_m: 0.3405 - precision_m: 0.2102 - recall_m: 1.0000 - val_loss: 0.6644 - val_f1_m: 0.1254 - val_precision_m: 0.0747 - val_recall_m: 0.3889\n",
      "Epoch 157/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3586 - f1_m: 0.4348 - precision_m: 0.2778 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 400us/sample - loss: 0.4763 - f1_m: 0.3366 - precision_m: 0.2093 - recall_m: 1.0000 - val_loss: 0.6645 - val_f1_m: 0.1418 - val_precision_m: 0.0844 - val_recall_m: 0.4444\n",
      "Epoch 158/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4209 - f1_m: 0.4167 - precision_m: 0.2632 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 467us/sample - loss: 0.4830 - f1_m: 0.3480 - precision_m: 0.2206 - recall_m: 1.0000 - val_loss: 0.6645 - val_f1_m: 0.1111 - val_precision_m: 0.0714 - val_recall_m: 0.2500\n",
      "Epoch 159/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4389 - f1_m: 0.1053 - precision_m: 0.0556 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 442us/sample - loss: 0.4628 - f1_m: 0.2650 - precision_m: 0.1627 - recall_m: 0.8333 - val_loss: 0.6645 - val_f1_m: 0.2134 - val_precision_m: 0.1275 - val_recall_m: 0.6667\n",
      "Epoch 160/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4432 - f1_m: 0.3571 - precision_m: 0.2174 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 381us/sample - loss: 0.4795 - f1_m: 0.2754 - precision_m: 0.1674 - recall_m: 0.8333 - val_loss: 0.6646 - val_f1_m: 0.1053 - val_precision_m: 0.0667 - val_recall_m: 0.2500\n",
      "Epoch 161/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4388 - f1_m: 0.1429 - precision_m: 0.0769 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 433us/sample - loss: 0.4588 - f1_m: 0.3316 - precision_m: 0.2063 - recall_m: 1.0000 - val_loss: 0.6645 - val_f1_m: 0.1111 - val_precision_m: 0.0714 - val_recall_m: 0.2500\n",
      "Epoch 162/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3527 - f1_m: 0.4545 - precision_m: 0.2941 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 527us/sample - loss: 0.4761 - f1_m: 0.3345 - precision_m: 0.2083 - recall_m: 1.0000 - val_loss: 0.6646 - val_f1_m: 0.2222 - val_precision_m: 0.1439 - val_recall_m: 0.5000\n",
      "Epoch 163/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5960 - f1_m: 0.1053 - precision_m: 0.0556 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 386us/sample - loss: 0.4741 - f1_m: 0.3230 - precision_m: 0.1977 - recall_m: 1.0000 - val_loss: 0.6646 - val_f1_m: 0.1397 - val_precision_m: 0.0812 - val_recall_m: 0.5000\n",
      "Epoch 164/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3990 - f1_m: 0.3704 - precision_m: 0.2273 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 273us/sample - loss: 0.4745 - f1_m: 0.2673 - precision_m: 0.1608 - recall_m: 0.8333 - val_loss: 0.6647 - val_f1_m: 0.1345 - val_precision_m: 0.0875 - val_recall_m: 0.5000\n",
      "Epoch 165/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4621 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 491us/sample - loss: 0.4593 - f1_m: 0.3302 - precision_m: 0.2003 - recall_m: 1.0000 - val_loss: 0.6647 - val_f1_m: 0.1397 - val_precision_m: 0.0862 - val_recall_m: 0.3889\n",
      "Epoch 166/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3321 - f1_m: 0.4167 - precision_m: 0.2632 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 231us/sample - loss: 0.4754 - f1_m: 0.2756 - precision_m: 0.1684 - recall_m: 0.8333 - val_loss: 0.6648 - val_f1_m: 0.1229 - val_precision_m: 0.0733 - val_recall_m: 0.3889\n",
      "Epoch 167/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5070 - f1_m: 0.3810 - precision_m: 0.2353 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 250us/sample - loss: 0.4509 - f1_m: 0.3226 - precision_m: 0.1930 - recall_m: 1.0000 - val_loss: 0.6648 - val_f1_m: 0.1111 - val_precision_m: 0.0667 - val_recall_m: 0.3333\n",
      "Epoch 168/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4357 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 313us/sample - loss: 0.4786 - f1_m: 0.2713 - precision_m: 0.1712 - recall_m: 0.8333 - val_loss: 0.6649 - val_f1_m: 0.1429 - val_precision_m: 0.0859 - val_recall_m: 0.4444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4782 - f1_m: 0.3158 - precision_m: 0.1875 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 428us/sample - loss: 0.4934 - f1_m: 0.2649 - precision_m: 0.1622 - recall_m: 0.8333 - val_loss: 0.6649 - val_f1_m: 0.1470 - val_precision_m: 0.0923 - val_recall_m: 0.3889\n",
      "Epoch 170/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3779 - f1_m: 0.4167 - precision_m: 0.2632 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 314us/sample - loss: 0.5042 - f1_m: 0.2547 - precision_m: 0.1606 - recall_m: 0.8333 - val_loss: 0.6648 - val_f1_m: 0.1389 - val_precision_m: 0.0847 - val_recall_m: 0.4444\n",
      "Epoch 171/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4515 - f1_m: 0.2500 - precision_m: 0.1429 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 411us/sample - loss: 0.4706 - f1_m: 0.2755 - precision_m: 0.1681 - recall_m: 0.8333 - val_loss: 0.6648 - val_f1_m: 0.1215 - val_precision_m: 0.0720 - val_recall_m: 0.3889\n",
      "Epoch 172/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5006 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 392us/sample - loss: 0.4618 - f1_m: 0.2760 - precision_m: 0.1688 - recall_m: 0.8333 - val_loss: 0.6648 - val_f1_m: 0.1254 - val_precision_m: 0.0754 - val_recall_m: 0.5000\n",
      "Epoch 173/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4260 - f1_m: 0.4667 - precision_m: 0.3043 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 448us/sample - loss: 0.4746 - f1_m: 0.3148 - precision_m: 0.1948 - recall_m: 1.0000 - val_loss: 0.6647 - val_f1_m: 0.1176 - val_precision_m: 0.0769 - val_recall_m: 0.2500\n",
      "Epoch 174/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3952 - f1_m: 0.4444 - precision_m: 0.2857 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 442us/sample - loss: 0.4671 - f1_m: 0.2684 - precision_m: 0.1614 - recall_m: 0.8333 - val_loss: 0.6647 - val_f1_m: 0.2111 - val_precision_m: 0.1405 - val_recall_m: 0.6667\n",
      "Epoch 175/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4347 - f1_m: 0.5217 - precision_m: 0.3529 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 306us/sample - loss: 0.4640 - f1_m: 0.3625 - precision_m: 0.2388 - recall_m: 1.0000 - val_loss: 0.6647 - val_f1_m: 0.1217 - val_precision_m: 0.0722 - val_recall_m: 0.3889\n",
      "Epoch 176/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3421 - f1_m: 0.5517 - precision_m: 0.3810 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 465us/sample - loss: 0.5166 - f1_m: 0.2535 - precision_m: 0.1598 - recall_m: 0.6667 - val_loss: 0.6647 - val_f1_m: 0.1111 - val_precision_m: 0.0714 - val_recall_m: 0.2500\n",
      "Epoch 177/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4484 - f1_m: 0.4348 - precision_m: 0.2778 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 429us/sample - loss: 0.4669 - f1_m: 0.3660 - precision_m: 0.2386 - recall_m: 1.0000 - val_loss: 0.6647 - val_f1_m: 0.1250 - val_precision_m: 0.0769 - val_recall_m: 0.3333\n",
      "Epoch 178/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4084 - f1_m: 0.4800 - precision_m: 0.3158 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 492us/sample - loss: 0.5101 - f1_m: 0.3620 - precision_m: 0.2420 - recall_m: 0.8333 - val_loss: 0.6646 - val_f1_m: 0.1944 - val_precision_m: 0.1346 - val_recall_m: 0.3889\n",
      "Epoch 179/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5782 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 501us/sample - loss: 0.4804 - f1_m: 0.3474 - precision_m: 0.2190 - recall_m: 1.0000 - val_loss: 0.6647 - val_f1_m: 0.2111 - val_precision_m: 0.1247 - val_recall_m: 0.7778\n",
      "Epoch 180/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4347 - f1_m: 0.3810 - precision_m: 0.2353 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 486us/sample - loss: 0.4769 - f1_m: 0.2734 - precision_m: 0.1664 - recall_m: 0.8333 - val_loss: 0.6647 - val_f1_m: 0.1297 - val_precision_m: 0.0779 - val_recall_m: 0.3889\n",
      "Epoch 181/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5273 - f1_m: 0.2500 - precision_m: 0.1429 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 415us/sample - loss: 0.4994 - f1_m: 0.3180 - precision_m: 0.1982 - recall_m: 1.0000 - val_loss: 0.6647 - val_f1_m: 0.1229 - val_precision_m: 0.0751 - val_recall_m: 0.5000\n",
      "Epoch 182/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3756 - f1_m: 0.3478 - precision_m: 0.2105 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 420us/sample - loss: 0.4713 - f1_m: 0.2816 - precision_m: 0.1726 - recall_m: 0.8333 - val_loss: 0.6647 - val_f1_m: 0.1442 - val_precision_m: 0.0893 - val_recall_m: 0.3889\n",
      "Epoch 183/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5150 - f1_m: 0.2727 - precision_m: 0.1579 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 431us/sample - loss: 0.4782 - f1_m: 0.2466 - precision_m: 0.1512 - recall_m: 0.8333 - val_loss: 0.6647 - val_f1_m: 0.1297 - val_precision_m: 0.0779 - val_recall_m: 0.3889\n",
      "Epoch 184/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4533 - f1_m: 0.2308 - precision_m: 0.1304 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 400us/sample - loss: 0.4768 - f1_m: 0.3563 - precision_m: 0.2300 - recall_m: 1.0000 - val_loss: 0.6647 - val_f1_m: 0.2407 - val_precision_m: 0.1587 - val_recall_m: 0.5000\n",
      "Epoch 185/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3679 - f1_m: 0.4444 - precision_m: 0.2857 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 484us/sample - loss: 0.4737 - f1_m: 0.2629 - precision_m: 0.1612 - recall_m: 0.8333 - val_loss: 0.6647 - val_f1_m: 0.1297 - val_precision_m: 0.0779 - val_recall_m: 0.3889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 186/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4382 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 443us/sample - loss: 0.4717 - f1_m: 0.2632 - precision_m: 0.1614 - recall_m: 0.8333 - val_loss: 0.6647 - val_f1_m: 0.2587 - val_precision_m: 0.1645 - val_recall_m: 0.7778\n",
      "Epoch 187/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5257 - f1_m: 0.2500 - precision_m: 0.1429 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 468us/sample - loss: 0.4674 - f1_m: 0.2697 - precision_m: 0.1636 - recall_m: 0.8333 - val_loss: 0.6646 - val_f1_m: 0.1053 - val_precision_m: 0.0667 - val_recall_m: 0.2500\n",
      "Epoch 188/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4446 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 528us/sample - loss: 0.5008 - f1_m: 0.3492 - precision_m: 0.2295 - recall_m: 0.8333 - val_loss: 0.6646 - val_f1_m: 0.1345 - val_precision_m: 0.0828 - val_recall_m: 0.3889\n",
      "Epoch 189/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4343 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 432us/sample - loss: 0.4619 - f1_m: 0.3624 - precision_m: 0.2316 - recall_m: 1.0000 - val_loss: 0.6646 - val_f1_m: 0.1389 - val_precision_m: 0.0846 - val_recall_m: 0.3889\n",
      "Epoch 190/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4055 - f1_m: 0.4444 - precision_m: 0.2857 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 445us/sample - loss: 0.4715 - f1_m: 0.2688 - precision_m: 0.1626 - recall_m: 0.8333 - val_loss: 0.6646 - val_f1_m: 0.1369 - val_precision_m: 0.0844 - val_recall_m: 0.3889\n",
      "Epoch 191/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5136 - f1_m: 0.2727 - precision_m: 0.1579 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 396us/sample - loss: 0.4736 - f1_m: 0.2476 - precision_m: 0.1516 - recall_m: 0.8333 - val_loss: 0.6646 - val_f1_m: 0.3056 - val_precision_m: 0.2222 - val_recall_m: 0.5000\n",
      "Epoch 192/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4321 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 489us/sample - loss: 0.4743 - f1_m: 0.2688 - precision_m: 0.1624 - recall_m: 0.8333 - val_loss: 0.6646 - val_f1_m: 0.2332 - val_precision_m: 0.1426 - val_recall_m: 0.6667\n",
      "Epoch 193/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3682 - f1_m: 0.3636 - precision_m: 0.2222 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 445us/sample - loss: 0.5262 - f1_m: 0.3024 - precision_m: 0.2000 - recall_m: 0.8333 - val_loss: 0.6645 - val_f1_m: 0.1442 - val_precision_m: 0.0844 - val_recall_m: 0.5000\n",
      "Epoch 194/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5508 - f1_m: 0.1905 - precision_m: 0.1053 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 421us/sample - loss: 0.4638 - f1_m: 0.3290 - precision_m: 0.2007 - recall_m: 1.0000 - val_loss: 0.6645 - val_f1_m: 0.1053 - val_precision_m: 0.0667 - val_recall_m: 0.2500\n",
      "Epoch 195/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5766 - f1_m: 0.2727 - precision_m: 0.1579 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 312us/sample - loss: 0.4699 - f1_m: 0.3837 - precision_m: 0.2633 - recall_m: 1.0000 - val_loss: 0.6645 - val_f1_m: 0.1538 - val_precision_m: 0.1111 - val_recall_m: 0.2500\n",
      "Epoch 196/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3735 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 234us/sample - loss: 0.4569 - f1_m: 0.2727 - precision_m: 0.1654 - recall_m: 0.8333 - val_loss: 0.6646 - val_f1_m: 0.1310 - val_precision_m: 0.0791 - val_recall_m: 0.3889\n",
      "Epoch 197/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4194 - f1_m: 0.2500 - precision_m: 0.1429 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 291us/sample - loss: 0.4555 - f1_m: 0.2689 - precision_m: 0.1616 - recall_m: 0.8333 - val_loss: 0.6646 - val_f1_m: 0.1215 - val_precision_m: 0.0720 - val_recall_m: 0.3889\n",
      "Epoch 198/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4302 - f1_m: 0.2727 - precision_m: 0.1579 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 432us/sample - loss: 0.4658 - f1_m: 0.3805 - precision_m: 0.2644 - recall_m: 1.0000 - val_loss: 0.6646 - val_f1_m: 0.1297 - val_precision_m: 0.0722 - val_recall_m: 0.6667\n",
      "Epoch 199/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4872 - f1_m: 0.3200 - precision_m: 0.1905 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 459us/sample - loss: 0.4696 - f1_m: 0.2645 - precision_m: 0.1594 - recall_m: 0.8333 - val_loss: 0.6646 - val_f1_m: 0.1296 - val_precision_m: 0.0779 - val_recall_m: 0.5000\n",
      "Epoch 200/1000\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 0.4649 - f1_m: 0.3361 - precision_m: 0.2097 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 537us/sample - loss: 0.4888 - f1_m: 0.2801 - precision_m: 0.1747 - recall_m: 0.8333 - val_loss: 0.6646 - val_f1_m: 0.1310 - val_precision_m: 0.0791 - val_recall_m: 0.3889\n",
      "Epoch 201/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4555 - f1_m: 0.3448 - precision_m: 0.2083 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 425us/sample - loss: 0.4650 - f1_m: 0.2659 - precision_m: 0.1624 - recall_m: 0.8333 - val_loss: 0.6645 - val_f1_m: 0.1000 - val_precision_m: 0.0625 - val_recall_m: 0.2500\n",
      "Epoch 202/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4336 - f1_m: 0.4167 - precision_m: 0.2632 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 407us/sample - loss: 0.4594 - f1_m: 0.2717 - precision_m: 0.1643 - recall_m: 0.8333 - val_loss: 0.6645 - val_f1_m: 0.1333 - val_precision_m: 0.0812 - val_recall_m: 0.3889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5082 - f1_m: 0.4167 - precision_m: 0.2632 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 323us/sample - loss: 0.4596 - f1_m: 0.2648 - precision_m: 0.1597 - recall_m: 0.8333 - val_loss: 0.6645 - val_f1_m: 0.1563 - val_precision_m: 0.1019 - val_recall_m: 0.5556\n",
      "Epoch 204/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4924 - f1_m: 0.1818 - precision_m: 0.1000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 432us/sample - loss: 0.4685 - f1_m: 0.2716 - precision_m: 0.1655 - recall_m: 0.8333 - val_loss: 0.6644 - val_f1_m: 0.1111 - val_precision_m: 0.0769 - val_recall_m: 0.2000\n",
      "Epoch 205/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5601 - f1_m: 0.2963 - precision_m: 0.1739 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 346us/sample - loss: 0.4577 - f1_m: 0.3301 - precision_m: 0.1985 - recall_m: 1.0000 - val_loss: 0.6644 - val_f1_m: 0.1250 - val_precision_m: 0.0833 - val_recall_m: 0.2500\n",
      "Epoch 206/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4719 - f1_m: 0.4444 - precision_m: 0.2857 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 499us/sample - loss: 0.4574 - f1_m: 0.2675 - precision_m: 0.1629 - recall_m: 0.8333 - val_loss: 0.6644 - val_f1_m: 0.1111 - val_precision_m: 0.0714 - val_recall_m: 0.2500\n",
      "Epoch 207/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4008 - f1_m: 0.4375 - precision_m: 0.2800 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 663us/sample - loss: 0.5084 - f1_m: 0.2660 - precision_m: 0.1668 - recall_m: 0.6667 - val_loss: 0.6644 - val_f1_m: 0.1111 - val_precision_m: 0.0714 - val_recall_m: 0.2500\n",
      "Epoch 208/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3875 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 401us/sample - loss: 0.4567 - f1_m: 0.3362 - precision_m: 0.2083 - recall_m: 1.0000 - val_loss: 0.6643 - val_f1_m: 0.2222 - val_precision_m: 0.1346 - val_recall_m: 0.6667\n",
      "Epoch 209/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4451 - f1_m: 0.3704 - precision_m: 0.2273 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 204us/sample - loss: 0.4599 - f1_m: 0.3307 - precision_m: 0.2031 - recall_m: 1.0000 - val_loss: 0.6643 - val_f1_m: 0.1278 - val_precision_m: 0.0769 - val_recall_m: 0.3889\n",
      "Epoch 210/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6238 - f1_m: 0.2105 - precision_m: 0.1176 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 219us/sample - loss: 0.4950 - f1_m: 0.2701 - precision_m: 0.1678 - recall_m: 0.8333 - val_loss: 0.6644 - val_f1_m: 0.3222 - val_precision_m: 0.2233 - val_recall_m: 0.7778\n",
      "Epoch 211/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4798 - f1_m: 0.3478 - precision_m: 0.2105 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 386us/sample - loss: 0.4674 - f1_m: 0.3805 - precision_m: 0.2586 - recall_m: 1.0000 - val_loss: 0.6644 - val_f1_m: 0.1397 - val_precision_m: 0.0862 - val_recall_m: 0.3889\n",
      "Epoch 212/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6366 - f1_m: 0.1111 - precision_m: 0.0588 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 309us/sample - loss: 0.4884 - f1_m: 0.2700 - precision_m: 0.1654 - recall_m: 0.8333 - val_loss: 0.6644 - val_f1_m: 0.1259 - val_precision_m: 0.0764 - val_recall_m: 0.3889\n",
      "Epoch 213/1000\n",
      " 96/166 [================>.............] - ETA: 0s - loss: 0.4612 - f1_m: 0.3469 - precision_m: 0.2196 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 796us/sample - loss: 0.4774 - f1_m: 0.2669 - precision_m: 0.1642 - recall_m: 0.8333 - val_loss: 0.6644 - val_f1_m: 0.1369 - val_precision_m: 0.0812 - val_recall_m: 0.4444\n",
      "Epoch 214/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4672 - f1_m: 0.2400 - precision_m: 0.1364 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 335us/sample - loss: 0.4534 - f1_m: 0.3228 - precision_m: 0.1931 - recall_m: 1.0000 - val_loss: 0.6644 - val_f1_m: 0.1053 - val_precision_m: 0.0667 - val_recall_m: 0.2500\n",
      "Epoch 215/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4912 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 381us/sample - loss: 0.5069 - f1_m: 0.3142 - precision_m: 0.1985 - recall_m: 0.8333 - val_loss: 0.6644 - val_f1_m: 0.1333 - val_precision_m: 0.0812 - val_recall_m: 0.3889\n",
      "Epoch 216/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3844 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 356us/sample - loss: 0.4697 - f1_m: 0.2783 - precision_m: 0.1683 - recall_m: 0.8333 - val_loss: 0.6645 - val_f1_m: 0.1306 - val_precision_m: 0.0794 - val_recall_m: 0.3889\n",
      "Epoch 217/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5251 - f1_m: 0.3571 - precision_m: 0.2174 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 218us/sample - loss: 0.4665 - f1_m: 0.2591 - precision_m: 0.1575 - recall_m: 0.8333 - val_loss: 0.6644 - val_f1_m: 0.1000 - val_precision_m: 0.0588 - val_recall_m: 0.3333\n",
      "Epoch 218/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5946 - f1_m: 0.1600 - precision_m: 0.0870 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 359us/sample - loss: 0.4739 - f1_m: 0.2803 - precision_m: 0.1756 - recall_m: 0.8333 - val_loss: 0.6644 - val_f1_m: 0.0952 - val_precision_m: 0.0625 - val_recall_m: 0.2000\n",
      "Epoch 219/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4227 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 388us/sample - loss: 0.4705 - f1_m: 0.3237 - precision_m: 0.2033 - recall_m: 1.0000 - val_loss: 0.6644 - val_f1_m: 0.1229 - val_precision_m: 0.0701 - val_recall_m: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3892 - f1_m: 0.3158 - precision_m: 0.1875 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 232us/sample - loss: 0.4608 - f1_m: 0.2774 - precision_m: 0.1673 - recall_m: 0.8333 - val_loss: 0.6644 - val_f1_m: 0.2332 - val_precision_m: 0.1405 - val_recall_m: 0.7778\n",
      "Epoch 221/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4461 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 494us/sample - loss: 0.4659 - f1_m: 0.2768 - precision_m: 0.1702 - recall_m: 0.8333 - val_loss: 0.6644 - val_f1_m: 0.1418 - val_precision_m: 0.0889 - val_recall_m: 0.3889\n",
      "Epoch 222/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4588 - f1_m: 0.3636 - precision_m: 0.2222 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 484us/sample - loss: 0.4662 - f1_m: 0.3694 - precision_m: 0.2440 - recall_m: 1.0000 - val_loss: 0.6644 - val_f1_m: 0.2111 - val_precision_m: 0.1275 - val_recall_m: 0.7778\n",
      "Epoch 223/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4968 - f1_m: 0.3478 - precision_m: 0.2105 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 347us/sample - loss: 0.5027 - f1_m: 0.3415 - precision_m: 0.2213 - recall_m: 1.0000 - val_loss: 0.6644 - val_f1_m: 0.4217 - val_precision_m: 0.3863 - val_recall_m: 0.7778\n",
      "Epoch 224/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5230 - f1_m: 0.3077 - precision_m: 0.1818 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 505us/sample - loss: 0.5022 - f1_m: 0.3413 - precision_m: 0.2183 - recall_m: 0.8333 - val_loss: 0.6644 - val_f1_m: 0.1297 - val_precision_m: 0.0779 - val_recall_m: 0.3889\n",
      "Epoch 225/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4581 - f1_m: 0.3636 - precision_m: 0.2222 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 500us/sample - loss: 0.4626 - f1_m: 0.3235 - precision_m: 0.1960 - recall_m: 1.0000 - val_loss: 0.6644 - val_f1_m: 0.2291 - val_precision_m: 0.1389 - val_recall_m: 0.7778\n",
      "Epoch 226/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4919 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 407us/sample - loss: 0.4710 - f1_m: 0.2493 - precision_m: 0.1510 - recall_m: 0.8333 - val_loss: 0.6645 - val_f1_m: 0.1365 - val_precision_m: 0.0833 - val_recall_m: 0.3889\n",
      "Epoch 227/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3824 - f1_m: 0.4348 - precision_m: 0.2778 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 499us/sample - loss: 0.4724 - f1_m: 0.3562 - precision_m: 0.2327 - recall_m: 1.0000 - val_loss: 0.6645 - val_f1_m: 0.1365 - val_precision_m: 0.0833 - val_recall_m: 0.3889\n",
      "Epoch 228/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5432 - f1_m: 0.4167 - precision_m: 0.2632 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 404us/sample - loss: 0.4530 - f1_m: 0.2672 - precision_m: 0.1618 - recall_m: 0.8333 - val_loss: 0.6645 - val_f1_m: 0.2556 - val_precision_m: 0.1667 - val_recall_m: 0.5556\n",
      "Epoch 229/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3710 - f1_m: 0.4800 - precision_m: 0.3158 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 513us/sample - loss: 0.4948 - f1_m: 0.2717 - precision_m: 0.1698 - recall_m: 0.8333 - val_loss: 0.6645 - val_f1_m: 0.1333 - val_precision_m: 0.0909 - val_recall_m: 0.2500\n",
      "Epoch 230/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4463 - f1_m: 0.2727 - precision_m: 0.1579 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 497us/sample - loss: 0.4595 - f1_m: 0.3288 - precision_m: 0.1976 - recall_m: 1.0000 - val_loss: 0.6645 - val_f1_m: 0.1053 - val_precision_m: 0.0667 - val_recall_m: 0.2500\n",
      "Epoch 231/1000\n",
      "128/166 [======================>.......] - ETA: 0s - loss: 0.4565 - f1_m: 0.3230 - precision_m: 0.2047 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 676us/sample - loss: 0.4791 - f1_m: 0.2608 - precision_m: 0.1628 - recall_m: 0.8333 - val_loss: 0.6645 - val_f1_m: 0.2291 - val_precision_m: 0.1389 - val_recall_m: 0.7778\n",
      "Epoch 232/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5957 - f1_m: 0.2105 - precision_m: 0.1176 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 392us/sample - loss: 0.4783 - f1_m: 0.2688 - precision_m: 0.1628 - recall_m: 0.8333 - val_loss: 0.6645 - val_f1_m: 0.1905 - val_precision_m: 0.1111 - val_recall_m: 0.6667\n",
      "Epoch 233/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5196 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 527us/sample - loss: 0.4773 - f1_m: 0.3224 - precision_m: 0.1969 - recall_m: 1.0000 - val_loss: 0.6645 - val_f1_m: 0.2639 - val_precision_m: 0.1905 - val_recall_m: 0.5000\n",
      "Epoch 234/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5146 - f1_m: 0.3478 - precision_m: 0.2105 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 432us/sample - loss: 0.4694 - f1_m: 0.3296 - precision_m: 0.2078 - recall_m: 1.0000 - val_loss: 0.6645 - val_f1_m: 0.2500 - val_precision_m: 0.1624 - val_recall_m: 0.5556\n",
      "Epoch 235/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4016 - f1_m: 0.2727 - precision_m: 0.1579 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 430us/sample - loss: 0.4711 - f1_m: 0.2662 - precision_m: 0.1594 - recall_m: 0.8333 - val_loss: 0.6646 - val_f1_m: 0.1212 - val_precision_m: 0.0721 - val_recall_m: 0.3889\n",
      "Epoch 236/1000\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 0.4858 - f1_m: 0.3191 - precision_m: 0.1955 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 550us/sample - loss: 0.4773 - f1_m: 0.3326 - precision_m: 0.2046 - recall_m: 1.0000 - val_loss: 0.6646 - val_f1_m: 0.2667 - val_precision_m: 0.1923 - val_recall_m: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 237/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5499 - f1_m: 0.3636 - precision_m: 0.2222 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 528us/sample - loss: 0.4816 - f1_m: 0.3659 - precision_m: 0.2384 - recall_m: 1.0000 - val_loss: 0.6646 - val_f1_m: 0.2306 - val_precision_m: 0.1405 - val_recall_m: 0.6667\n",
      "Epoch 238/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3443 - f1_m: 0.4615 - precision_m: 0.3000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 424us/sample - loss: 0.4801 - f1_m: 0.3289 - precision_m: 0.2047 - recall_m: 1.0000 - val_loss: 0.6647 - val_f1_m: 0.1250 - val_precision_m: 0.0769 - val_recall_m: 0.3333\n",
      "Epoch 239/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4394 - f1_m: 0.2308 - precision_m: 0.1304 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 424us/sample - loss: 0.4523 - f1_m: 0.3708 - precision_m: 0.2380 - recall_m: 1.0000 - val_loss: 0.6647 - val_f1_m: 0.1333 - val_precision_m: 0.0812 - val_recall_m: 0.3889\n",
      "Epoch 240/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3438 - f1_m: 0.2963 - precision_m: 0.1739 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 374us/sample - loss: 0.4612 - f1_m: 0.2768 - precision_m: 0.1670 - recall_m: 0.8333 - val_loss: 0.6648 - val_f1_m: 0.2306 - val_precision_m: 0.1405 - val_recall_m: 0.6667\n",
      "Epoch 241/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4635 - f1_m: 0.3636 - precision_m: 0.2222 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 524us/sample - loss: 0.4725 - f1_m: 0.2806 - precision_m: 0.1713 - recall_m: 0.8333 - val_loss: 0.6648 - val_f1_m: 0.1111 - val_precision_m: 0.0667 - val_recall_m: 0.3333\n",
      "Epoch 242/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4793 - f1_m: 0.3000 - precision_m: 0.1765 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 281us/sample - loss: 0.4628 - f1_m: 0.2741 - precision_m: 0.1650 - recall_m: 0.8333 - val_loss: 0.6648 - val_f1_m: 0.1111 - val_precision_m: 0.0769 - val_recall_m: 0.2000\n",
      "Epoch 243/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5800 - f1_m: 0.2400 - precision_m: 0.1364 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 247us/sample - loss: 0.4667 - f1_m: 0.3590 - precision_m: 0.2302 - recall_m: 1.0000 - val_loss: 0.6648 - val_f1_m: 0.1176 - val_precision_m: 0.0769 - val_recall_m: 0.2500\n",
      "Epoch 244/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4628 - f1_m: 0.2105 - precision_m: 0.1176 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 286us/sample - loss: 0.4654 - f1_m: 0.2688 - precision_m: 0.1617 - recall_m: 0.8333 - val_loss: 0.6648 - val_f1_m: 0.1296 - val_precision_m: 0.0787 - val_recall_m: 0.4444\n",
      "Epoch 245/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4764 - f1_m: 0.1739 - precision_m: 0.0952 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 513us/sample - loss: 0.4619 - f1_m: 0.3317 - precision_m: 0.2034 - recall_m: 1.0000 - val_loss: 0.6648 - val_f1_m: 0.1323 - val_precision_m: 0.0863 - val_recall_m: 0.5000\n",
      "Epoch 246/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4230 - f1_m: 0.1538 - precision_m: 0.0833 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 325us/sample - loss: 0.4752 - f1_m: 0.3495 - precision_m: 0.2280 - recall_m: 1.0000 - val_loss: 0.6648 - val_f1_m: 0.2619 - val_precision_m: 0.1717 - val_recall_m: 0.5556\n",
      "Epoch 247/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5904 - f1_m: 0.0952 - precision_m: 0.0500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 229us/sample - loss: 0.5164 - f1_m: 0.2541 - precision_m: 0.1614 - recall_m: 0.8333 - val_loss: 0.6649 - val_f1_m: 0.1345 - val_precision_m: 0.0828 - val_recall_m: 0.3889\n",
      "Epoch 248/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6851 - f1_m: 0.1000 - precision_m: 0.0526 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 318us/sample - loss: 0.4880 - f1_m: 0.3505 - precision_m: 0.2278 - recall_m: 1.0000 - val_loss: 0.6648 - val_f1_m: 0.2222 - val_precision_m: 0.1439 - val_recall_m: 0.5000\n",
      "Epoch 249/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4738 - f1_m: 0.4615 - precision_m: 0.3000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 389us/sample - loss: 0.4550 - f1_m: 0.3148 - precision_m: 0.1914 - recall_m: 1.0000 - val_loss: 0.6649 - val_f1_m: 0.1261 - val_precision_m: 0.0769 - val_recall_m: 0.5000\n",
      "Epoch 250/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5594 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 238us/sample - loss: 0.4589 - f1_m: 0.2618 - precision_m: 0.1575 - recall_m: 0.8333 - val_loss: 0.6649 - val_f1_m: 0.1053 - val_precision_m: 0.0667 - val_recall_m: 0.2500\n",
      "Epoch 251/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4832 - f1_m: 0.2727 - precision_m: 0.1579 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 248us/sample - loss: 0.4685 - f1_m: 0.2655 - precision_m: 0.1585 - recall_m: 0.8333 - val_loss: 0.6649 - val_f1_m: 0.3750 - val_precision_m: 0.3590 - val_recall_m: 0.4444\n",
      "Epoch 252/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5502 - f1_m: 0.1600 - precision_m: 0.0870 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 534us/sample - loss: 0.4820 - f1_m: 0.2838 - precision_m: 0.1817 - recall_m: 0.8333 - val_loss: 0.6649 - val_f1_m: 0.1254 - val_precision_m: 0.0747 - val_recall_m: 0.3889\n",
      "Epoch 253/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4227 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166/166 [==============================] - 0s 432us/sample - loss: 0.4650 - f1_m: 0.3686 - precision_m: 0.2400 - recall_m: 1.0000 - val_loss: 0.6649 - val_f1_m: 0.3115 - val_precision_m: 0.2179 - val_recall_m: 0.7778\n",
      "Epoch 254/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5618 - f1_m: 0.1111 - precision_m: 0.0588 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 440us/sample - loss: 0.4728 - f1_m: 0.3289 - precision_m: 0.2039 - recall_m: 1.0000 - val_loss: 0.6649 - val_f1_m: 0.1201 - val_precision_m: 0.0701 - val_recall_m: 0.4444\n",
      "Epoch 255/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5593 - f1_m: 0.1000 - precision_m: 0.0526 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 505us/sample - loss: 0.4619 - f1_m: 0.3125 - precision_m: 0.1917 - recall_m: 1.0000 - val_loss: 0.6649 - val_f1_m: 0.3115 - val_precision_m: 0.2179 - val_recall_m: 0.7778\n",
      "Epoch 256/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3833 - f1_m: 0.4167 - precision_m: 0.2632 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 451us/sample - loss: 0.4896 - f1_m: 0.3248 - precision_m: 0.2018 - recall_m: 1.0000 - val_loss: 0.6650 - val_f1_m: 0.3056 - val_precision_m: 0.2143 - val_recall_m: 0.6667\n",
      "Epoch 257/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6983 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 476us/sample - loss: 0.4904 - f1_m: 0.3164 - precision_m: 0.1955 - recall_m: 0.8333 - val_loss: 0.6650 - val_f1_m: 0.1278 - val_precision_m: 0.0769 - val_recall_m: 0.3889\n",
      "Epoch 258/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4150 - f1_m: 0.5385 - precision_m: 0.3684 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 491us/sample - loss: 0.4728 - f1_m: 0.2709 - precision_m: 0.1669 - recall_m: 0.8333 - val_loss: 0.6650 - val_f1_m: 0.1278 - val_precision_m: 0.0794 - val_recall_m: 0.5000\n",
      "Epoch 259/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5055 - f1_m: 0.2400 - precision_m: 0.1364 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 440us/sample - loss: 0.4642 - f1_m: 0.2783 - precision_m: 0.1694 - recall_m: 0.8333 - val_loss: 0.6650 - val_f1_m: 0.2074 - val_precision_m: 0.1278 - val_recall_m: 0.5556\n",
      "Epoch 260/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5021 - f1_m: 0.2222 - precision_m: 0.1250 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 475us/sample - loss: 0.4609 - f1_m: 0.3374 - precision_m: 0.2082 - recall_m: 1.0000 - val_loss: 0.6650 - val_f1_m: 0.1278 - val_precision_m: 0.0754 - val_recall_m: 0.4444\n",
      "Epoch 261/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3417 - f1_m: 0.4828 - precision_m: 0.3182 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 467us/sample - loss: 0.4923 - f1_m: 0.2588 - precision_m: 0.1580 - recall_m: 0.8333 - val_loss: 0.6650 - val_f1_m: 0.2359 - val_precision_m: 0.1439 - val_recall_m: 0.6667\n",
      "Epoch 262/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4751 - f1_m: 0.4762 - precision_m: 0.3125 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 432us/sample - loss: 0.4663 - f1_m: 0.3178 - precision_m: 0.1948 - recall_m: 1.0000 - val_loss: 0.6650 - val_f1_m: 0.1229 - val_precision_m: 0.0701 - val_recall_m: 0.5000\n",
      "Epoch 263/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4967 - f1_m: 0.4138 - precision_m: 0.2609 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 392us/sample - loss: 0.5127 - f1_m: 0.2685 - precision_m: 0.1694 - recall_m: 0.6667 - val_loss: 0.6650 - val_f1_m: 0.2692 - val_precision_m: 0.2273 - val_recall_m: 0.5000\n",
      "Epoch 264/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4572 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 293us/sample - loss: 0.4842 - f1_m: 0.3213 - precision_m: 0.2015 - recall_m: 1.0000 - val_loss: 0.6650 - val_f1_m: 0.1111 - val_precision_m: 0.0714 - val_recall_m: 0.2500\n",
      "Epoch 265/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5107 - f1_m: 0.1739 - precision_m: 0.0952 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 387us/sample - loss: 0.4647 - f1_m: 0.2716 - precision_m: 0.1662 - recall_m: 0.8333 - val_loss: 0.6650 - val_f1_m: 0.1297 - val_precision_m: 0.0754 - val_recall_m: 0.5556\n",
      "Epoch 266/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5253 - f1_m: 0.1176 - precision_m: 0.0625 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 366us/sample - loss: 0.4628 - f1_m: 0.3312 - precision_m: 0.2079 - recall_m: 1.0000 - val_loss: 0.6650 - val_f1_m: 0.1111 - val_precision_m: 0.0714 - val_recall_m: 0.2500\n",
      "Epoch 267/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5282 - f1_m: 0.3000 - precision_m: 0.1765 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 255us/sample - loss: 0.4511 - f1_m: 0.3160 - precision_m: 0.1889 - recall_m: 1.0000 - val_loss: 0.6650 - val_f1_m: 0.1310 - val_precision_m: 0.0791 - val_recall_m: 0.3889\n",
      "Epoch 268/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3492 - f1_m: 0.5217 - precision_m: 0.3529 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 436us/sample - loss: 0.4850 - f1_m: 0.2599 - precision_m: 0.1650 - recall_m: 0.8333 - val_loss: 0.6650 - val_f1_m: 0.1297 - val_precision_m: 0.0778 - val_recall_m: 0.4444\n",
      "Epoch 269/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5420 - f1_m: 0.1818 - precision_m: 0.1000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 480us/sample - loss: 0.4691 - f1_m: 0.3444 - precision_m: 0.2162 - recall_m: 1.0000 - val_loss: 0.6650 - val_f1_m: 0.1278 - val_precision_m: 0.0794 - val_recall_m: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 270/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5780 - f1_m: 0.2727 - precision_m: 0.1579 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 384us/sample - loss: 0.4711 - f1_m: 0.3214 - precision_m: 0.1962 - recall_m: 1.0000 - val_loss: 0.6651 - val_f1_m: 0.2100 - val_precision_m: 0.1247 - val_recall_m: 0.6667\n",
      "Epoch 271/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4385 - f1_m: 0.3077 - precision_m: 0.1818 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 569us/sample - loss: 0.4659 - f1_m: 0.3278 - precision_m: 0.1988 - recall_m: 1.0000 - val_loss: 0.6651 - val_f1_m: 0.1310 - val_precision_m: 0.0791 - val_recall_m: 0.3889\n",
      "Epoch 272/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5328 - f1_m: 0.1176 - precision_m: 0.0625 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 324us/sample - loss: 0.4782 - f1_m: 0.2526 - precision_m: 0.1551 - recall_m: 0.8333 - val_loss: 0.6652 - val_f1_m: 0.1347 - val_precision_m: 0.0815 - val_recall_m: 0.3889\n",
      "Epoch 273/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4449 - f1_m: 0.2353 - precision_m: 0.1333 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 243us/sample - loss: 0.4614 - f1_m: 0.2700 - precision_m: 0.1630 - recall_m: 0.8333 - val_loss: 0.6652 - val_f1_m: 0.2587 - val_precision_m: 0.1645 - val_recall_m: 0.7778\n",
      "Epoch 274/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4976 - f1_m: 0.3448 - precision_m: 0.2083 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 499us/sample - loss: 0.4745 - f1_m: 0.3610 - precision_m: 0.2321 - recall_m: 1.0000 - val_loss: 0.6652 - val_f1_m: 0.1111 - val_precision_m: 0.0667 - val_recall_m: 0.3333\n",
      "Epoch 275/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4504 - f1_m: 0.2500 - precision_m: 0.1429 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 505us/sample - loss: 0.4526 - f1_m: 0.3815 - precision_m: 0.2591 - recall_m: 1.0000 - val_loss: 0.6652 - val_f1_m: 0.1215 - val_precision_m: 0.0722 - val_recall_m: 0.5000\n",
      "Epoch 276/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.7034 - f1_m: 0.1053 - precision_m: 0.0556 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 399us/sample - loss: 0.4853 - f1_m: 0.3117 - precision_m: 0.1883 - recall_m: 1.0000 - val_loss: 0.6653 - val_f1_m: 0.2134 - val_precision_m: 0.1405 - val_recall_m: 0.6667\n",
      "Epoch 277/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3634 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 538us/sample - loss: 0.4774 - f1_m: 0.2757 - precision_m: 0.1702 - recall_m: 0.8333 - val_loss: 0.6653 - val_f1_m: 0.1439 - val_precision_m: 0.0883 - val_recall_m: 0.3889\n",
      "Epoch 278/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5061 - f1_m: 0.3478 - precision_m: 0.2105 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 493us/sample - loss: 0.4772 - f1_m: 0.2644 - precision_m: 0.1610 - recall_m: 0.8333 - val_loss: 0.6653 - val_f1_m: 0.2118 - val_precision_m: 0.1346 - val_recall_m: 0.5000\n",
      "Epoch 279/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4626 - f1_m: 0.2500 - precision_m: 0.1429 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 460us/sample - loss: 0.4837 - f1_m: 0.2691 - precision_m: 0.1661 - recall_m: 0.8333 - val_loss: 0.6653 - val_f1_m: 0.1333 - val_precision_m: 0.0844 - val_recall_m: 0.5000\n",
      "Epoch 280/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4405 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 346us/sample - loss: 0.4645 - f1_m: 0.2731 - precision_m: 0.1637 - recall_m: 0.8333 - val_loss: 0.6653 - val_f1_m: 0.1000 - val_precision_m: 0.0625 - val_recall_m: 0.2500\n",
      "Epoch 281/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4367 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 247us/sample - loss: 0.4704 - f1_m: 0.3384 - precision_m: 0.2134 - recall_m: 1.0000 - val_loss: 0.6653 - val_f1_m: 0.1895 - val_precision_m: 0.1278 - val_recall_m: 0.5000\n",
      "Epoch 282/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4106 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 322us/sample - loss: 0.4533 - f1_m: 0.3203 - precision_m: 0.1917 - recall_m: 1.0000 - val_loss: 0.6653 - val_f1_m: 0.2587 - val_precision_m: 0.1645 - val_recall_m: 0.6667\n",
      "Epoch 283/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4183 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 351us/sample - loss: 0.4681 - f1_m: 0.2757 - precision_m: 0.1668 - recall_m: 0.8333 - val_loss: 0.6653 - val_f1_m: 0.1346 - val_precision_m: 0.0779 - val_recall_m: 0.5000\n",
      "Epoch 284/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5382 - f1_m: 0.3200 - precision_m: 0.1905 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 268us/sample - loss: 0.4671 - f1_m: 0.3644 - precision_m: 0.2368 - recall_m: 1.0000 - val_loss: 0.6653 - val_f1_m: 0.2619 - val_precision_m: 0.2222 - val_recall_m: 0.5000\n",
      "Epoch 285/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5431 - f1_m: 0.1818 - precision_m: 0.1000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 318us/sample - loss: 0.4799 - f1_m: 0.3311 - precision_m: 0.2074 - recall_m: 1.0000 - val_loss: 0.6654 - val_f1_m: 0.2035 - val_precision_m: 0.1250 - val_recall_m: 0.5556\n",
      "Epoch 286/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4483 - f1_m: 0.2500 - precision_m: 0.1429 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 331us/sample - loss: 0.4573 - f1_m: 0.3463 - precision_m: 0.2137 - recall_m: 1.0000 - val_loss: 0.6654 - val_f1_m: 0.1229 - val_precision_m: 0.0733 - val_recall_m: 0.3889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 287/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4213 - f1_m: 0.3000 - precision_m: 0.1765 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 249us/sample - loss: 0.4556 - f1_m: 0.3411 - precision_m: 0.2100 - recall_m: 1.0000 - val_loss: 0.6655 - val_f1_m: 0.1111 - val_precision_m: 0.0714 - val_recall_m: 0.2500\n",
      "Epoch 288/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3784 - f1_m: 0.2609 - precision_m: 0.1500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 249us/sample - loss: 0.4792 - f1_m: 0.2674 - precision_m: 0.1662 - recall_m: 0.8333 - val_loss: 0.6655 - val_f1_m: 0.1261 - val_precision_m: 0.0754 - val_recall_m: 0.3889\n",
      "Epoch 289/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4755 - f1_m: 0.4286 - precision_m: 0.2727 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 282us/sample - loss: 0.4740 - f1_m: 0.2658 - precision_m: 0.1661 - recall_m: 0.8333 - val_loss: 0.6656 - val_f1_m: 0.1278 - val_precision_m: 0.0794 - val_recall_m: 0.5000\n",
      "Epoch 290/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5413 - f1_m: 0.2105 - precision_m: 0.1176 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 378us/sample - loss: 0.4619 - f1_m: 0.3108 - precision_m: 0.1865 - recall_m: 1.0000 - val_loss: 0.6655 - val_f1_m: 0.1111 - val_precision_m: 0.0667 - val_recall_m: 0.3333\n",
      "Epoch 291/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3953 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 301us/sample - loss: 0.4603 - f1_m: 0.3194 - precision_m: 0.1923 - recall_m: 1.0000 - val_loss: 0.6656 - val_f1_m: 0.2100 - val_precision_m: 0.1247 - val_recall_m: 0.6667\n",
      "Epoch 292/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6200 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 321us/sample - loss: 0.5268 - f1_m: 0.2790 - precision_m: 0.1769 - recall_m: 0.8333 - val_loss: 0.6656 - val_f1_m: 0.1053 - val_precision_m: 0.0714 - val_recall_m: 0.2000\n",
      "Epoch 293/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4177 - f1_m: 0.3571 - precision_m: 0.2174 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 512us/sample - loss: 0.4638 - f1_m: 0.2790 - precision_m: 0.1684 - recall_m: 0.8333 - val_loss: 0.6656 - val_f1_m: 0.2222 - val_precision_m: 0.1389 - val_recall_m: 0.5556\n",
      "Epoch 294/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4805 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 431us/sample - loss: 0.4693 - f1_m: 0.2801 - precision_m: 0.1728 - recall_m: 0.8333 - val_loss: 0.6656 - val_f1_m: 0.0952 - val_precision_m: 0.0625 - val_recall_m: 0.2000\n",
      "Epoch 295/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3673 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 424us/sample - loss: 0.4726 - f1_m: 0.3274 - precision_m: 0.2029 - recall_m: 1.0000 - val_loss: 0.6656 - val_f1_m: 0.1111 - val_precision_m: 0.0714 - val_recall_m: 0.2500\n",
      "Epoch 296/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3952 - f1_m: 0.5600 - precision_m: 0.3889 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 517us/sample - loss: 0.4632 - f1_m: 0.3127 - precision_m: 0.1921 - recall_m: 1.0000 - val_loss: 0.6656 - val_f1_m: 0.1333 - val_precision_m: 0.0833 - val_recall_m: 0.3333\n",
      "Epoch 297/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3614 - f1_m: 0.4615 - precision_m: 0.3000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 476us/sample - loss: 0.4793 - f1_m: 0.2680 - precision_m: 0.1641 - recall_m: 0.8333 - val_loss: 0.6656 - val_f1_m: 0.1176 - val_precision_m: 0.0769 - val_recall_m: 0.2500\n",
      "Epoch 298/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4203 - f1_m: 0.4138 - precision_m: 0.2609 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 421us/sample - loss: 0.4560 - f1_m: 0.2596 - precision_m: 0.1560 - recall_m: 0.8333 - val_loss: 0.6656 - val_f1_m: 0.1254 - val_precision_m: 0.0747 - val_recall_m: 0.3889\n",
      "Epoch 299/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3524 - f1_m: 0.3810 - precision_m: 0.2353 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 519us/sample - loss: 0.4822 - f1_m: 0.2665 - precision_m: 0.1630 - recall_m: 0.8333 - val_loss: 0.6656 - val_f1_m: 0.1369 - val_precision_m: 0.0844 - val_recall_m: 0.3889\n",
      "Epoch 300/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4455 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 463us/sample - loss: 0.4840 - f1_m: 0.2702 - precision_m: 0.1710 - recall_m: 0.8333 - val_loss: 0.6655 - val_f1_m: 0.1297 - val_precision_m: 0.0779 - val_recall_m: 0.3889\n",
      "Epoch 301/1000\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 0.4717 - f1_m: 0.3076 - precision_m: 0.1896 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 563us/sample - loss: 0.4689 - f1_m: 0.3397 - precision_m: 0.2135 - recall_m: 1.0000 - val_loss: 0.6655 - val_f1_m: 0.1340 - val_precision_m: 0.0815 - val_recall_m: 0.4444\n",
      "Epoch 302/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4279 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 480us/sample - loss: 0.4619 - f1_m: 0.2704 - precision_m: 0.1643 - recall_m: 0.8333 - val_loss: 0.6655 - val_f1_m: 0.1306 - val_precision_m: 0.0794 - val_recall_m: 0.3889\n",
      "Epoch 303/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4114 - f1_m: 0.2609 - precision_m: 0.1500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 424us/sample - loss: 0.4781 - f1_m: 0.2731 - precision_m: 0.1676 - recall_m: 0.8333 - val_loss: 0.6655 - val_f1_m: 0.2000 - val_precision_m: 0.1503 - val_recall_m: 0.3889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 304/1000\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 0.4754 - f1_m: 0.3074 - precision_m: 0.1843 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 545us/sample - loss: 0.4691 - f1_m: 0.3514 - precision_m: 0.2203 - recall_m: 1.0000 - val_loss: 0.6654 - val_f1_m: 0.2778 - val_precision_m: 0.1778 - val_recall_m: 0.6667\n",
      "Epoch 305/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3549 - f1_m: 0.3704 - precision_m: 0.2273 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 535us/sample - loss: 0.4655 - f1_m: 0.3294 - precision_m: 0.2055 - recall_m: 1.0000 - val_loss: 0.6655 - val_f1_m: 0.2118 - val_precision_m: 0.1278 - val_recall_m: 0.6667\n",
      "Epoch 306/1000\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 0.4472 - f1_m: 0.3252 - precision_m: 0.1966 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 550us/sample - loss: 0.4587 - f1_m: 0.2710 - precision_m: 0.1638 - recall_m: 0.8333 - val_loss: 0.6655 - val_f1_m: 0.1442 - val_precision_m: 0.0862 - val_recall_m: 0.4444\n",
      "Epoch 307/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4587 - f1_m: 0.4167 - precision_m: 0.2632 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 396us/sample - loss: 0.4905 - f1_m: 0.3300 - precision_m: 0.2053 - recall_m: 1.0000 - val_loss: 0.6655 - val_f1_m: 0.1340 - val_precision_m: 0.0810 - val_recall_m: 0.3889\n",
      "Epoch 308/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4489 - f1_m: 0.3077 - precision_m: 0.1818 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 361us/sample - loss: 0.4755 - f1_m: 0.2701 - precision_m: 0.1671 - recall_m: 0.8333 - val_loss: 0.6655 - val_f1_m: 0.1333 - val_precision_m: 0.0844 - val_recall_m: 0.5000\n",
      "Epoch 309/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5002 - f1_m: 0.3704 - precision_m: 0.2273 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 443us/sample - loss: 0.4634 - f1_m: 0.3367 - precision_m: 0.2077 - recall_m: 1.0000 - val_loss: 0.6655 - val_f1_m: 0.1310 - val_precision_m: 0.0812 - val_recall_m: 0.5000\n",
      "Epoch 310/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4254 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 324us/sample - loss: 0.4756 - f1_m: 0.2799 - precision_m: 0.1706 - recall_m: 0.8333 - val_loss: 0.6655 - val_f1_m: 0.1111 - val_precision_m: 0.0667 - val_recall_m: 0.3333\n",
      "Epoch 311/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4694 - f1_m: 0.1905 - precision_m: 0.1053 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 262us/sample - loss: 0.4746 - f1_m: 0.3360 - precision_m: 0.2078 - recall_m: 1.0000 - val_loss: 0.6655 - val_f1_m: 0.1346 - val_precision_m: 0.0833 - val_recall_m: 0.5000\n",
      "Epoch 312/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3876 - f1_m: 0.5185 - precision_m: 0.3500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 549us/sample - loss: 0.5039 - f1_m: 0.2631 - precision_m: 0.1666 - recall_m: 0.8333 - val_loss: 0.6656 - val_f1_m: 0.1429 - val_precision_m: 0.1000 - val_recall_m: 0.2500\n",
      "Epoch 313/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4081 - f1_m: 0.3571 - precision_m: 0.2174 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 299us/sample - loss: 0.4656 - f1_m: 0.3241 - precision_m: 0.1988 - recall_m: 1.0000 - val_loss: 0.6656 - val_f1_m: 0.1053 - val_precision_m: 0.0714 - val_recall_m: 0.2000\n",
      "Epoch 314/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5794 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 298us/sample - loss: 0.4681 - f1_m: 0.3517 - precision_m: 0.2206 - recall_m: 1.0000 - val_loss: 0.6656 - val_f1_m: 0.2596 - val_precision_m: 0.1652 - val_recall_m: 0.6667\n",
      "Epoch 315/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4991 - f1_m: 0.2353 - precision_m: 0.1333 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 424us/sample - loss: 0.4697 - f1_m: 0.3154 - precision_m: 0.1929 - recall_m: 1.0000 - val_loss: 0.6656 - val_f1_m: 0.1176 - val_precision_m: 0.0769 - val_recall_m: 0.2500\n",
      "Epoch 316/1000\n",
      "128/166 [======================>.......] - ETA: 0s - loss: 0.4837 - f1_m: 0.3034 - precision_m: 0.1789 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 575us/sample - loss: 0.4561 - f1_m: 0.3295 - precision_m: 0.1979 - recall_m: 1.0000 - val_loss: 0.6657 - val_f1_m: 0.1257 - val_precision_m: 0.0747 - val_recall_m: 0.5000\n",
      "Epoch 317/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5436 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 428us/sample - loss: 0.4566 - f1_m: 0.2681 - precision_m: 0.1618 - recall_m: 0.8333 - val_loss: 0.6658 - val_f1_m: 0.1604 - val_precision_m: 0.1161 - val_recall_m: 0.5000\n",
      "Epoch 318/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4647 - f1_m: 0.3077 - precision_m: 0.1818 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 379us/sample - loss: 0.4560 - f1_m: 0.2745 - precision_m: 0.1650 - recall_m: 0.8333 - val_loss: 0.6658 - val_f1_m: 0.1345 - val_precision_m: 0.0814 - val_recall_m: 0.5556\n",
      "Epoch 319/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4346 - f1_m: 0.2000 - precision_m: 0.1111 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 416us/sample - loss: 0.4599 - f1_m: 0.2691 - precision_m: 0.1637 - recall_m: 0.8333 - val_loss: 0.6658 - val_f1_m: 0.1563 - val_precision_m: 0.1029 - val_recall_m: 0.3889\n",
      "Epoch 320/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3806 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166/166 [==============================] - 0s 342us/sample - loss: 0.4777 - f1_m: 0.2611 - precision_m: 0.1596 - recall_m: 0.8333 - val_loss: 0.6658 - val_f1_m: 0.1296 - val_precision_m: 0.0779 - val_recall_m: 0.5000\n",
      "Epoch 321/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4387 - f1_m: 0.2000 - precision_m: 0.1111 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 460us/sample - loss: 0.4616 - f1_m: 0.3137 - precision_m: 0.1890 - recall_m: 1.0000 - val_loss: 0.6657 - val_f1_m: 0.2000 - val_precision_m: 0.1346 - val_recall_m: 0.5000\n",
      "Epoch 322/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4380 - f1_m: 0.4138 - precision_m: 0.2609 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 414us/sample - loss: 0.4652 - f1_m: 0.2654 - precision_m: 0.1596 - recall_m: 0.8333 - val_loss: 0.6657 - val_f1_m: 0.1254 - val_precision_m: 0.0747 - val_recall_m: 0.3889\n",
      "Epoch 323/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3818 - f1_m: 0.3200 - precision_m: 0.1905 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 204us/sample - loss: 0.4657 - f1_m: 0.3617 - precision_m: 0.2323 - recall_m: 1.0000 - val_loss: 0.6658 - val_f1_m: 0.1296 - val_precision_m: 0.0778 - val_recall_m: 0.3889\n",
      "Epoch 324/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3806 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 219us/sample - loss: 0.4510 - f1_m: 0.3388 - precision_m: 0.2069 - recall_m: 1.0000 - val_loss: 0.6658 - val_f1_m: 0.1346 - val_precision_m: 0.0816 - val_recall_m: 0.3889\n",
      "Epoch 325/1000\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 0.4901 - f1_m: 0.2870 - precision_m: 0.1768 - recall_m: 0.8000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 656us/sample - loss: 0.4906 - f1_m: 0.3344 - precision_m: 0.2140 - recall_m: 0.8333 - val_loss: 0.6659 - val_f1_m: 0.2692 - val_precision_m: 0.1778 - val_recall_m: 0.5556\n",
      "Epoch 326/1000\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 0.4648 - f1_m: 0.3261 - precision_m: 0.1967 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 562us/sample - loss: 0.4772 - f1_m: 0.2717 - precision_m: 0.1639 - recall_m: 0.8333 - val_loss: 0.6659 - val_f1_m: 0.1310 - val_precision_m: 0.0812 - val_recall_m: 0.5000\n",
      "Epoch 327/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4906 - f1_m: 0.3200 - precision_m: 0.1905 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 460us/sample - loss: 0.4829 - f1_m: 0.2714 - precision_m: 0.1665 - recall_m: 0.8333 - val_loss: 0.6659 - val_f1_m: 0.1365 - val_precision_m: 0.0791 - val_recall_m: 0.5000\n",
      "Epoch 328/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5293 - f1_m: 0.2400 - precision_m: 0.1364 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 399us/sample - loss: 0.4716 - f1_m: 0.3578 - precision_m: 0.2291 - recall_m: 1.0000 - val_loss: 0.6659 - val_f1_m: 0.1346 - val_precision_m: 0.0833 - val_recall_m: 0.5000\n",
      "Epoch 329/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3909 - f1_m: 0.4286 - precision_m: 0.2727 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 337us/sample - loss: 0.4944 - f1_m: 0.2579 - precision_m: 0.1592 - recall_m: 0.8333 - val_loss: 0.6660 - val_f1_m: 0.1389 - val_precision_m: 0.0816 - val_recall_m: 0.5556\n",
      "Epoch 330/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4794 - f1_m: 0.3158 - precision_m: 0.1875 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 393us/sample - loss: 0.4627 - f1_m: 0.3447 - precision_m: 0.2145 - recall_m: 1.0000 - val_loss: 0.6659 - val_f1_m: 0.2291 - val_precision_m: 0.1389 - val_recall_m: 0.7778\n",
      "Epoch 331/1000\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 0.4611 - f1_m: 0.3292 - precision_m: 0.2012 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 503us/sample - loss: 0.4699 - f1_m: 0.2744 - precision_m: 0.1677 - recall_m: 0.8333 - val_loss: 0.6660 - val_f1_m: 0.1278 - val_precision_m: 0.0769 - val_recall_m: 0.3889\n",
      "Epoch 332/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4883 - f1_m: 0.2963 - precision_m: 0.1739 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 306us/sample - loss: 0.4700 - f1_m: 0.2761 - precision_m: 0.1678 - recall_m: 0.8333 - val_loss: 0.6660 - val_f1_m: 0.1333 - val_precision_m: 0.0833 - val_recall_m: 0.3333\n",
      "Epoch 333/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5534 - f1_m: 0.3200 - precision_m: 0.1905 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 209us/sample - loss: 0.4645 - f1_m: 0.2744 - precision_m: 0.1678 - recall_m: 0.8333 - val_loss: 0.6659 - val_f1_m: 0.2587 - val_precision_m: 0.1652 - val_recall_m: 0.7778\n",
      "Epoch 334/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5411 - f1_m: 0.2222 - precision_m: 0.1250 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 454us/sample - loss: 0.4614 - f1_m: 0.3589 - precision_m: 0.2296 - recall_m: 1.0000 - val_loss: 0.6659 - val_f1_m: 0.1323 - val_precision_m: 0.0814 - val_recall_m: 0.3889\n",
      "Epoch 335/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4066 - f1_m: 0.3200 - precision_m: 0.1905 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 301us/sample - loss: 0.4615 - f1_m: 0.3180 - precision_m: 0.1911 - recall_m: 1.0000 - val_loss: 0.6659 - val_f1_m: 0.2306 - val_precision_m: 0.1405 - val_recall_m: 0.6667\n",
      "Epoch 336/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5295 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 263us/sample - loss: 0.4809 - f1_m: 0.3048 - precision_m: 0.1872 - recall_m: 1.0000 - val_loss: 0.6660 - val_f1_m: 0.1297 - val_precision_m: 0.0779 - val_recall_m: 0.3889\n",
      "Epoch 337/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5368 - f1_m: 0.3077 - precision_m: 0.1818 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 357us/sample - loss: 0.4623 - f1_m: 0.3190 - precision_m: 0.1922 - recall_m: 1.0000 - val_loss: 0.6660 - val_f1_m: 0.1346 - val_precision_m: 0.0779 - val_recall_m: 0.5000\n",
      "Epoch 338/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5822 - f1_m: 0.2105 - precision_m: 0.1176 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 441us/sample - loss: 0.4808 - f1_m: 0.2663 - precision_m: 0.1613 - recall_m: 0.8333 - val_loss: 0.6660 - val_f1_m: 0.1185 - val_precision_m: 0.0701 - val_recall_m: 0.3889\n",
      "Epoch 339/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4013 - f1_m: 0.3636 - precision_m: 0.2222 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 319us/sample - loss: 0.4997 - f1_m: 0.3189 - precision_m: 0.1976 - recall_m: 0.8333 - val_loss: 0.6660 - val_f1_m: 0.2556 - val_precision_m: 0.1717 - val_recall_m: 0.5000\n",
      "Epoch 340/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4699 - f1_m: 0.2105 - precision_m: 0.1176 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 264us/sample - loss: 0.4560 - f1_m: 0.3373 - precision_m: 0.2101 - recall_m: 1.0000 - val_loss: 0.6660 - val_f1_m: 0.1347 - val_precision_m: 0.0778 - val_recall_m: 0.5556\n",
      "Epoch 341/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5396 - f1_m: 0.1429 - precision_m: 0.0769 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 295us/sample - loss: 0.4632 - f1_m: 0.4077 - precision_m: 0.3100 - recall_m: 1.0000 - val_loss: 0.6660 - val_f1_m: 0.3179 - val_precision_m: 0.2233 - val_recall_m: 0.6667\n",
      "Epoch 342/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6566 - f1_m: 0.2105 - precision_m: 0.1176 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 342us/sample - loss: 0.4688 - f1_m: 0.3253 - precision_m: 0.1961 - recall_m: 1.0000 - val_loss: 0.6661 - val_f1_m: 0.1053 - val_precision_m: 0.0667 - val_recall_m: 0.2500\n",
      "Epoch 343/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5377 - f1_m: 0.2222 - precision_m: 0.1250 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 206us/sample - loss: 0.4782 - f1_m: 0.2678 - precision_m: 0.1622 - recall_m: 0.8333 - val_loss: 0.6661 - val_f1_m: 0.2306 - val_precision_m: 0.1405 - val_recall_m: 0.6667\n",
      "Epoch 344/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4826 - f1_m: 0.3200 - precision_m: 0.1905 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 276us/sample - loss: 0.4667 - f1_m: 0.2709 - precision_m: 0.1641 - recall_m: 0.8333 - val_loss: 0.6662 - val_f1_m: 0.1333 - val_precision_m: 0.0769 - val_recall_m: 0.5000\n",
      "Epoch 345/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5211 - f1_m: 0.3158 - precision_m: 0.1875 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 232us/sample - loss: 0.4652 - f1_m: 0.2757 - precision_m: 0.1666 - recall_m: 0.8333 - val_loss: 0.6661 - val_f1_m: 0.1306 - val_precision_m: 0.0794 - val_recall_m: 0.3889\n",
      "Epoch 346/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4099 - f1_m: 0.2727 - precision_m: 0.1579 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 252us/sample - loss: 0.4623 - f1_m: 0.2729 - precision_m: 0.1670 - recall_m: 0.8333 - val_loss: 0.6661 - val_f1_m: 0.2407 - val_precision_m: 0.1587 - val_recall_m: 0.5000\n",
      "Epoch 347/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3989 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 568us/sample - loss: 0.4672 - f1_m: 0.2760 - precision_m: 0.1667 - recall_m: 0.8333 - val_loss: 0.6661 - val_f1_m: 0.2222 - val_precision_m: 0.1389 - val_recall_m: 0.5556\n",
      "Epoch 348/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4893 - f1_m: 0.2400 - precision_m: 0.1364 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 277us/sample - loss: 0.4535 - f1_m: 0.3265 - precision_m: 0.1973 - recall_m: 1.0000 - val_loss: 0.6660 - val_f1_m: 0.1225 - val_precision_m: 0.0764 - val_recall_m: 0.5000\n",
      "Epoch 349/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5162 - f1_m: 0.1176 - precision_m: 0.0625 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 236us/sample - loss: 0.4714 - f1_m: 0.2582 - precision_m: 0.1594 - recall_m: 0.8333 - val_loss: 0.6660 - val_f1_m: 0.1333 - val_precision_m: 0.0791 - val_recall_m: 0.4444\n",
      "Epoch 350/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3853 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 250us/sample - loss: 0.4770 - f1_m: 0.2648 - precision_m: 0.1628 - recall_m: 0.8333 - val_loss: 0.6660 - val_f1_m: 0.1310 - val_precision_m: 0.0791 - val_recall_m: 0.3889\n",
      "Epoch 351/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3740 - f1_m: 0.4167 - precision_m: 0.2632 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 267us/sample - loss: 0.4676 - f1_m: 0.3280 - precision_m: 0.1997 - recall_m: 1.0000 - val_loss: 0.6660 - val_f1_m: 0.1261 - val_precision_m: 0.0747 - val_recall_m: 0.4444\n",
      "Epoch 352/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5703 - f1_m: 0.2143 - precision_m: 0.1200 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 316us/sample - loss: 0.5037 - f1_m: 0.3540 - precision_m: 0.2416 - recall_m: 0.8333 - val_loss: 0.6660 - val_f1_m: 0.1442 - val_precision_m: 0.0905 - val_recall_m: 0.3889\n",
      "Epoch 353/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4661 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 323us/sample - loss: 0.4771 - f1_m: 0.3191 - precision_m: 0.1938 - recall_m: 1.0000 - val_loss: 0.6660 - val_f1_m: 0.1176 - val_precision_m: 0.0769 - val_recall_m: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 354/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5102 - f1_m: 0.3200 - precision_m: 0.1905 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 356us/sample - loss: 0.4682 - f1_m: 0.3253 - precision_m: 0.1969 - recall_m: 1.0000 - val_loss: 0.6660 - val_f1_m: 0.1429 - val_precision_m: 0.0923 - val_recall_m: 0.5000\n",
      "Epoch 355/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6221 - f1_m: 0.2105 - precision_m: 0.1176 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 359us/sample - loss: 0.4714 - f1_m: 0.3675 - precision_m: 0.2381 - recall_m: 1.0000 - val_loss: 0.6661 - val_f1_m: 0.1310 - val_precision_m: 0.0791 - val_recall_m: 0.3889\n",
      "Epoch 356/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4026 - f1_m: 0.3000 - precision_m: 0.1765 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 260us/sample - loss: 0.4638 - f1_m: 0.2687 - precision_m: 0.1614 - recall_m: 0.8333 - val_loss: 0.6662 - val_f1_m: 0.3333 - val_precision_m: 0.2407 - val_recall_m: 0.5556\n",
      "Epoch 357/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4985 - f1_m: 0.2500 - precision_m: 0.1429 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 284us/sample - loss: 0.4699 - f1_m: 0.2745 - precision_m: 0.1661 - recall_m: 0.8333 - val_loss: 0.6662 - val_f1_m: 0.2596 - val_precision_m: 0.2192 - val_recall_m: 0.6667\n",
      "Epoch 358/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4189 - f1_m: 0.2727 - precision_m: 0.1579 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 301us/sample - loss: 0.4664 - f1_m: 0.3392 - precision_m: 0.2095 - recall_m: 1.0000 - val_loss: 0.6662 - val_f1_m: 0.1345 - val_precision_m: 0.0794 - val_recall_m: 0.4444\n",
      "Epoch 359/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3674 - f1_m: 0.3200 - precision_m: 0.1905 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 294us/sample - loss: 0.5035 - f1_m: 0.3481 - precision_m: 0.2307 - recall_m: 0.8333 - val_loss: 0.6662 - val_f1_m: 0.1053 - val_precision_m: 0.0667 - val_recall_m: 0.2500\n",
      "Epoch 360/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4872 - f1_m: 0.3636 - precision_m: 0.2222 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 412us/sample - loss: 0.4690 - f1_m: 0.2865 - precision_m: 0.1800 - recall_m: 0.8333 - val_loss: 0.6662 - val_f1_m: 0.1347 - val_precision_m: 0.0810 - val_recall_m: 0.5000\n",
      "Epoch 361/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4922 - f1_m: 0.2609 - precision_m: 0.1500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 427us/sample - loss: 0.4763 - f1_m: 0.2703 - precision_m: 0.1665 - recall_m: 0.8333 - val_loss: 0.6662 - val_f1_m: 0.1297 - val_precision_m: 0.0778 - val_recall_m: 0.4444\n",
      "Epoch 362/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4752 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 375us/sample - loss: 0.4983 - f1_m: 0.2582 - precision_m: 0.1624 - recall_m: 0.6667 - val_loss: 0.6661 - val_f1_m: 0.1310 - val_precision_m: 0.0791 - val_recall_m: 0.3889\n",
      "Epoch 363/1000\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 0.4709 - f1_m: 0.3217 - precision_m: 0.1938 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 516us/sample - loss: 0.4669 - f1_m: 0.3237 - precision_m: 0.1948 - recall_m: 1.0000 - val_loss: 0.6661 - val_f1_m: 0.1310 - val_precision_m: 0.0779 - val_recall_m: 0.4444\n",
      "Epoch 364/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5651 - f1_m: 0.1739 - precision_m: 0.0952 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 423us/sample - loss: 0.4713 - f1_m: 0.3376 - precision_m: 0.2099 - recall_m: 1.0000 - val_loss: 0.6661 - val_f1_m: 0.2167 - val_precision_m: 0.1587 - val_recall_m: 0.5000\n",
      "Epoch 365/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4457 - f1_m: 0.3704 - precision_m: 0.2273 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 341us/sample - loss: 0.4756 - f1_m: 0.3247 - precision_m: 0.2000 - recall_m: 1.0000 - val_loss: 0.6662 - val_f1_m: 0.1278 - val_precision_m: 0.0769 - val_recall_m: 0.3889\n",
      "Epoch 366/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4448 - f1_m: 0.4800 - precision_m: 0.3158 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 220us/sample - loss: 0.4603 - f1_m: 0.3192 - precision_m: 0.1969 - recall_m: 1.0000 - val_loss: 0.6662 - val_f1_m: 0.1053 - val_precision_m: 0.0714 - val_recall_m: 0.2000\n",
      "Epoch 367/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4768 - f1_m: 0.2500 - precision_m: 0.1429 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 281us/sample - loss: 0.4680 - f1_m: 0.3135 - precision_m: 0.1888 - recall_m: 1.0000 - val_loss: 0.6663 - val_f1_m: 0.1365 - val_precision_m: 0.0862 - val_recall_m: 0.5000\n",
      "Epoch 368/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4294 - f1_m: 0.3200 - precision_m: 0.1905 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 225us/sample - loss: 0.4705 - f1_m: 0.2673 - precision_m: 0.1630 - recall_m: 0.8333 - val_loss: 0.6663 - val_f1_m: 0.1257 - val_precision_m: 0.0750 - val_recall_m: 0.3889\n",
      "Epoch 369/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3807 - f1_m: 0.4545 - precision_m: 0.2941 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 246us/sample - loss: 0.4660 - f1_m: 0.3089 - precision_m: 0.1857 - recall_m: 1.0000 - val_loss: 0.6663 - val_f1_m: 0.1333 - val_precision_m: 0.0833 - val_recall_m: 0.3333\n",
      "Epoch 370/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4699 - f1_m: 0.2609 - precision_m: 0.1500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 541us/sample - loss: 0.4697 - f1_m: 0.2752 - precision_m: 0.1674 - recall_m: 0.8333 - val_loss: 0.6664 - val_f1_m: 0.2778 - val_precision_m: 0.2333 - val_recall_m: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 371/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4591 - f1_m: 0.3000 - precision_m: 0.1765 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 530us/sample - loss: 0.4574 - f1_m: 0.3378 - precision_m: 0.2083 - recall_m: 1.0000 - val_loss: 0.6664 - val_f1_m: 0.1376 - val_precision_m: 0.0809 - val_recall_m: 0.5000\n",
      "Epoch 372/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4881 - f1_m: 0.3077 - precision_m: 0.1818 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 311us/sample - loss: 0.4665 - f1_m: 0.3461 - precision_m: 0.2154 - recall_m: 1.0000 - val_loss: 0.6664 - val_f1_m: 0.1297 - val_precision_m: 0.0779 - val_recall_m: 0.3889\n",
      "Epoch 373/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4757 - f1_m: 0.2609 - precision_m: 0.1500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 222us/sample - loss: 0.4645 - f1_m: 0.2671 - precision_m: 0.1639 - recall_m: 0.8333 - val_loss: 0.6665 - val_f1_m: 0.2167 - val_precision_m: 0.1310 - val_recall_m: 0.6667\n",
      "Epoch 374/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4282 - f1_m: 0.2609 - precision_m: 0.1500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 595us/sample - loss: 0.4723 - f1_m: 0.3649 - precision_m: 0.2499 - recall_m: 1.0000 - val_loss: 0.6665 - val_f1_m: 0.1250 - val_precision_m: 0.0909 - val_recall_m: 0.2000\n",
      "Epoch 375/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4398 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 355us/sample - loss: 0.4659 - f1_m: 0.3271 - precision_m: 0.1999 - recall_m: 1.0000 - val_loss: 0.6665 - val_f1_m: 0.2619 - val_precision_m: 0.1717 - val_recall_m: 0.5556\n",
      "Epoch 376/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4005 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 250us/sample - loss: 0.4607 - f1_m: 0.4247 - precision_m: 0.3205 - recall_m: 1.0000 - val_loss: 0.6666 - val_f1_m: 0.2286 - val_precision_m: 0.1439 - val_recall_m: 0.5556\n",
      "Epoch 377/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4471 - f1_m: 0.2308 - precision_m: 0.1304 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 318us/sample - loss: 0.4644 - f1_m: 0.2718 - precision_m: 0.1649 - recall_m: 0.8333 - val_loss: 0.6667 - val_f1_m: 0.0952 - val_precision_m: 0.0588 - val_recall_m: 0.2500\n",
      "Epoch 378/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5827 - f1_m: 0.0952 - precision_m: 0.0500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 407us/sample - loss: 0.4758 - f1_m: 0.3748 - precision_m: 0.2557 - recall_m: 1.0000 - val_loss: 0.6667 - val_f1_m: 0.1396 - val_precision_m: 0.0828 - val_recall_m: 0.4444\n",
      "Epoch 379/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4236 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 251us/sample - loss: 0.4682 - f1_m: 0.3113 - precision_m: 0.1884 - recall_m: 1.0000 - val_loss: 0.6667 - val_f1_m: 0.1418 - val_precision_m: 0.0889 - val_recall_m: 0.3889\n",
      "Epoch 380/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5025 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 325us/sample - loss: 0.4973 - f1_m: 0.2571 - precision_m: 0.1610 - recall_m: 0.8333 - val_loss: 0.6668 - val_f1_m: 0.2222 - val_precision_m: 0.1407 - val_recall_m: 0.5556\n",
      "Epoch 381/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5368 - f1_m: 0.2727 - precision_m: 0.1579 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 217us/sample - loss: 0.4573 - f1_m: 0.3273 - precision_m: 0.1972 - recall_m: 1.0000 - val_loss: 0.6668 - val_f1_m: 0.1306 - val_precision_m: 0.0828 - val_recall_m: 0.5000\n",
      "Epoch 382/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4466 - f1_m: 0.2727 - precision_m: 0.1579 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 215us/sample - loss: 0.4611 - f1_m: 0.2637 - precision_m: 0.1590 - recall_m: 0.8333 - val_loss: 0.6668 - val_f1_m: 0.1346 - val_precision_m: 0.0810 - val_recall_m: 0.4444\n",
      "Epoch 383/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5728 - f1_m: 0.1053 - precision_m: 0.0556 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 308us/sample - loss: 0.5176 - f1_m: 0.2411 - precision_m: 0.1543 - recall_m: 0.6667 - val_loss: 0.6668 - val_f1_m: 0.1296 - val_precision_m: 0.0747 - val_recall_m: 0.5556\n",
      "Epoch 384/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4382 - f1_m: 0.5217 - precision_m: 0.3529 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 313us/sample - loss: 0.4734 - f1_m: 0.2780 - precision_m: 0.1718 - recall_m: 0.8333 - val_loss: 0.6667 - val_f1_m: 0.1397 - val_precision_m: 0.0812 - val_recall_m: 0.5000\n",
      "Epoch 385/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4303 - f1_m: 0.3000 - precision_m: 0.1765 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 309us/sample - loss: 0.4568 - f1_m: 0.3212 - precision_m: 0.1940 - recall_m: 1.0000 - val_loss: 0.6666 - val_f1_m: 0.1000 - val_precision_m: 0.0625 - val_recall_m: 0.2500\n",
      "Epoch 386/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.7134 - f1_m: 0.1053 - precision_m: 0.0556 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 285us/sample - loss: 0.4956 - f1_m: 0.3353 - precision_m: 0.2200 - recall_m: 1.0000 - val_loss: 0.6667 - val_f1_m: 0.1944 - val_precision_m: 0.1346 - val_recall_m: 0.3889\n",
      "Epoch 387/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4569 - f1_m: 0.2308 - precision_m: 0.1304 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 243us/sample - loss: 0.4672 - f1_m: 0.2750 - precision_m: 0.1698 - recall_m: 0.8333 - val_loss: 0.6667 - val_f1_m: 0.2167 - val_precision_m: 0.1346 - val_recall_m: 0.5556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 388/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4214 - f1_m: 0.2500 - precision_m: 0.1429 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 419us/sample - loss: 0.4628 - f1_m: 0.2673 - precision_m: 0.1652 - recall_m: 0.8333 - val_loss: 0.6667 - val_f1_m: 0.2500 - val_precision_m: 0.2000 - val_recall_m: 0.3333\n",
      "Epoch 389/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4078 - f1_m: 0.3810 - precision_m: 0.2353 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 479us/sample - loss: 0.4667 - f1_m: 0.3828 - precision_m: 0.2613 - recall_m: 1.0000 - val_loss: 0.6667 - val_f1_m: 0.1310 - val_precision_m: 0.0754 - val_recall_m: 0.5000\n",
      "Epoch 390/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3931 - f1_m: 0.3200 - precision_m: 0.1905 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 479us/sample - loss: 0.4581 - f1_m: 0.3196 - precision_m: 0.1950 - recall_m: 1.0000 - val_loss: 0.6667 - val_f1_m: 0.1465 - val_precision_m: 0.0909 - val_recall_m: 0.3889\n",
      "Epoch 391/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3804 - f1_m: 0.3871 - precision_m: 0.2400 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 553us/sample - loss: 0.4902 - f1_m: 0.2590 - precision_m: 0.1567 - recall_m: 0.8333 - val_loss: 0.6667 - val_f1_m: 0.2587 - val_precision_m: 0.2179 - val_recall_m: 0.6667\n",
      "Epoch 392/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3288 - f1_m: 0.4348 - precision_m: 0.2778 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 435us/sample - loss: 0.4637 - f1_m: 0.3270 - precision_m: 0.1979 - recall_m: 1.0000 - val_loss: 0.6667 - val_f1_m: 0.1297 - val_precision_m: 0.0778 - val_recall_m: 0.4444\n",
      "Epoch 393/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4673 - f1_m: 0.2308 - precision_m: 0.1304 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 307us/sample - loss: 0.4654 - f1_m: 0.2748 - precision_m: 0.1660 - recall_m: 0.8333 - val_loss: 0.6668 - val_f1_m: 0.1111 - val_precision_m: 0.0714 - val_recall_m: 0.2500\n",
      "Epoch 394/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3487 - f1_m: 0.4444 - precision_m: 0.2857 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 370us/sample - loss: 0.5193 - f1_m: 0.2527 - precision_m: 0.1665 - recall_m: 0.6667 - val_loss: 0.6667 - val_f1_m: 0.1390 - val_precision_m: 0.0861 - val_recall_m: 0.4444\n",
      "Epoch 395/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4561 - f1_m: 0.3200 - precision_m: 0.1905 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 311us/sample - loss: 0.4638 - f1_m: 0.2692 - precision_m: 0.1651 - recall_m: 0.8333 - val_loss: 0.6667 - val_f1_m: 0.1340 - val_precision_m: 0.0810 - val_recall_m: 0.3889\n",
      "Epoch 396/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4388 - f1_m: 0.3704 - precision_m: 0.2273 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 219us/sample - loss: 0.4651 - f1_m: 0.2672 - precision_m: 0.1609 - recall_m: 0.8333 - val_loss: 0.6666 - val_f1_m: 0.1278 - val_precision_m: 0.0754 - val_recall_m: 0.4444\n",
      "Epoch 397/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3601 - f1_m: 0.4286 - precision_m: 0.2727 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 267us/sample - loss: 0.4734 - f1_m: 0.3121 - precision_m: 0.1883 - recall_m: 1.0000 - val_loss: 0.6666 - val_f1_m: 0.1217 - val_precision_m: 0.0722 - val_recall_m: 0.3889\n",
      "Epoch 398/1000\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 0.4696 - f1_m: 0.2775 - precision_m: 0.1662 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 592us/sample - loss: 0.4706 - f1_m: 0.3646 - precision_m: 0.2496 - recall_m: 1.0000 - val_loss: 0.6666 - val_f1_m: 0.1465 - val_precision_m: 0.0909 - val_recall_m: 0.3889\n",
      "Epoch 399/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5496 - f1_m: 0.2105 - precision_m: 0.1176 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 603us/sample - loss: 0.4666 - f1_m: 0.3253 - precision_m: 0.1992 - recall_m: 1.0000 - val_loss: 0.6666 - val_f1_m: 0.2100 - val_precision_m: 0.1389 - val_recall_m: 0.6667\n",
      "Epoch 400/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4836 - f1_m: 0.3810 - precision_m: 0.2353 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 486us/sample - loss: 0.4591 - f1_m: 0.3671 - precision_m: 0.2367 - recall_m: 1.0000 - val_loss: 0.6667 - val_f1_m: 0.1225 - val_precision_m: 0.0714 - val_recall_m: 0.4444\n",
      "Epoch 401/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3966 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 388us/sample - loss: 0.4794 - f1_m: 0.3410 - precision_m: 0.2142 - recall_m: 1.0000 - val_loss: 0.6668 - val_f1_m: 0.2556 - val_precision_m: 0.1624 - val_recall_m: 0.6667\n",
      "Epoch 402/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4391 - f1_m: 0.4848 - precision_m: 0.3200 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 379us/sample - loss: 0.4745 - f1_m: 0.2525 - precision_m: 0.1550 - recall_m: 0.8333 - val_loss: 0.6668 - val_f1_m: 0.1402 - val_precision_m: 0.0859 - val_recall_m: 0.3889\n",
      "Epoch 403/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4156 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 283us/sample - loss: 0.4661 - f1_m: 0.3384 - precision_m: 0.2075 - recall_m: 1.0000 - val_loss: 0.6668 - val_f1_m: 0.1111 - val_precision_m: 0.0667 - val_recall_m: 0.3333\n",
      "Epoch 404/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.7872 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 231us/sample - loss: 0.5095 - f1_m: 0.2667 - precision_m: 0.1685 - recall_m: 0.6667 - val_loss: 0.6668 - val_f1_m: 0.1053 - val_precision_m: 0.0625 - val_recall_m: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 405/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3961 - f1_m: 0.4516 - precision_m: 0.2917 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 282us/sample - loss: 0.4753 - f1_m: 0.3119 - precision_m: 0.1938 - recall_m: 1.0000 - val_loss: 0.6668 - val_f1_m: 0.1176 - val_precision_m: 0.0714 - val_recall_m: 0.3333\n",
      "Epoch 406/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4321 - f1_m: 0.5185 - precision_m: 0.3500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 313us/sample - loss: 0.4784 - f1_m: 0.3335 - precision_m: 0.2102 - recall_m: 1.0000 - val_loss: 0.6668 - val_f1_m: 0.1340 - val_precision_m: 0.0810 - val_recall_m: 0.3889\n",
      "Epoch 407/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4186 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 328us/sample - loss: 0.4858 - f1_m: 0.3210 - precision_m: 0.2025 - recall_m: 1.0000 - val_loss: 0.6668 - val_f1_m: 0.2100 - val_precision_m: 0.1645 - val_recall_m: 0.7778\n",
      "Epoch 408/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4816 - f1_m: 0.3478 - precision_m: 0.2105 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 533us/sample - loss: 0.4605 - f1_m: 0.3308 - precision_m: 0.2050 - recall_m: 1.0000 - val_loss: 0.6669 - val_f1_m: 0.1176 - val_precision_m: 0.0714 - val_recall_m: 0.3333\n",
      "Epoch 409/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4381 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 436us/sample - loss: 0.4600 - f1_m: 0.3196 - precision_m: 0.1925 - recall_m: 1.0000 - val_loss: 0.6669 - val_f1_m: 0.1254 - val_precision_m: 0.0747 - val_recall_m: 0.3889\n",
      "Epoch 410/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4146 - f1_m: 0.2500 - precision_m: 0.1429 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 303us/sample - loss: 0.4522 - f1_m: 0.3273 - precision_m: 0.1977 - recall_m: 1.0000 - val_loss: 0.6670 - val_f1_m: 0.1229 - val_precision_m: 0.0733 - val_recall_m: 0.3889\n",
      "Epoch 411/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4030 - f1_m: 0.3704 - precision_m: 0.2273 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 463us/sample - loss: 0.4698 - f1_m: 0.2697 - precision_m: 0.1616 - recall_m: 0.8333 - val_loss: 0.6671 - val_f1_m: 0.1369 - val_precision_m: 0.0812 - val_recall_m: 0.4444\n",
      "Epoch 412/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5243 - f1_m: 0.2963 - precision_m: 0.1739 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 429us/sample - loss: 0.4500 - f1_m: 0.3229 - precision_m: 0.1945 - recall_m: 1.0000 - val_loss: 0.6671 - val_f1_m: 0.1470 - val_precision_m: 0.0979 - val_recall_m: 0.5000\n",
      "Epoch 413/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4893 - f1_m: 0.3200 - precision_m: 0.1905 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 332us/sample - loss: 0.4647 - f1_m: 0.2613 - precision_m: 0.1565 - recall_m: 0.8333 - val_loss: 0.6671 - val_f1_m: 0.1278 - val_precision_m: 0.0794 - val_recall_m: 0.5000\n",
      "Epoch 414/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4569 - f1_m: 0.2667 - precision_m: 0.1538 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 425us/sample - loss: 0.4909 - f1_m: 0.3116 - precision_m: 0.1932 - recall_m: 0.8333 - val_loss: 0.6671 - val_f1_m: 0.1365 - val_precision_m: 0.0816 - val_recall_m: 0.4444\n",
      "Epoch 415/1000\n",
      "128/166 [======================>.......] - ETA: 0s - loss: 0.4930 - f1_m: 0.2676 - precision_m: 0.1676 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 597us/sample - loss: 0.5008 - f1_m: 0.2513 - precision_m: 0.1584 - recall_m: 0.8333 - val_loss: 0.6671 - val_f1_m: 0.1254 - val_precision_m: 0.0754 - val_recall_m: 0.5000\n",
      "Epoch 416/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4478 - f1_m: 0.3478 - precision_m: 0.2105 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 615us/sample - loss: 0.4825 - f1_m: 0.3284 - precision_m: 0.2051 - recall_m: 1.0000 - val_loss: 0.6670 - val_f1_m: 0.1111 - val_precision_m: 0.0769 - val_recall_m: 0.2000\n",
      "Epoch 417/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4155 - f1_m: 0.3704 - precision_m: 0.2273 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 406us/sample - loss: 0.5049 - f1_m: 0.2116 - precision_m: 0.1325 - recall_m: 0.6667 - val_loss: 0.6670 - val_f1_m: 0.1346 - val_precision_m: 0.0816 - val_recall_m: 0.3889\n",
      "Epoch 418/1000\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 0.4499 - f1_m: 0.3244 - precision_m: 0.1985 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 534us/sample - loss: 0.4582 - f1_m: 0.3259 - precision_m: 0.1987 - recall_m: 1.0000 - val_loss: 0.6670 - val_f1_m: 0.2111 - val_precision_m: 0.1256 - val_recall_m: 0.6667\n",
      "Epoch 419/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4656 - f1_m: 0.3200 - precision_m: 0.1905 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 512us/sample - loss: 0.4602 - f1_m: 0.4106 - precision_m: 0.3097 - recall_m: 1.0000 - val_loss: 0.6670 - val_f1_m: 0.1333 - val_precision_m: 0.0812 - val_recall_m: 0.3889\n",
      "Epoch 420/1000\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 0.4674 - f1_m: 0.3230 - precision_m: 0.1973 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 612us/sample - loss: 0.4786 - f1_m: 0.2692 - precision_m: 0.1644 - recall_m: 0.8333 - val_loss: 0.6670 - val_f1_m: 0.1217 - val_precision_m: 0.0722 - val_recall_m: 0.3889\n",
      "Epoch 421/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5479 - f1_m: 0.0952 - precision_m: 0.0500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 317us/sample - loss: 0.4766 - f1_m: 0.2643 - precision_m: 0.1621 - recall_m: 0.8333 - val_loss: 0.6670 - val_f1_m: 0.2619 - val_precision_m: 0.1717 - val_recall_m: 0.5556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 422/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4390 - f1_m: 0.3571 - precision_m: 0.2174 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 322us/sample - loss: 0.4736 - f1_m: 0.2761 - precision_m: 0.1708 - recall_m: 0.8333 - val_loss: 0.6670 - val_f1_m: 0.1250 - val_precision_m: 0.0833 - val_recall_m: 0.2500\n",
      "Epoch 423/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5081 - f1_m: 0.1818 - precision_m: 0.1000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 427us/sample - loss: 0.4543 - f1_m: 0.3435 - precision_m: 0.2149 - recall_m: 1.0000 - val_loss: 0.6670 - val_f1_m: 0.1528 - val_precision_m: 0.0979 - val_recall_m: 0.3889\n",
      "Epoch 424/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4784 - f1_m: 0.1739 - precision_m: 0.0952 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 467us/sample - loss: 0.4696 - f1_m: 0.2582 - precision_m: 0.1566 - recall_m: 0.8333 - val_loss: 0.6670 - val_f1_m: 0.1222 - val_precision_m: 0.0720 - val_recall_m: 0.5000\n",
      "Epoch 425/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5404 - f1_m: 0.1538 - precision_m: 0.0833 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 333us/sample - loss: 0.4581 - f1_m: 0.3299 - precision_m: 0.2002 - recall_m: 1.0000 - val_loss: 0.6670 - val_f1_m: 0.2444 - val_precision_m: 0.1778 - val_recall_m: 0.5000\n",
      "Epoch 426/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4846 - f1_m: 0.3529 - precision_m: 0.2143 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 416us/sample - loss: 0.4596 - f1_m: 0.3227 - precision_m: 0.1955 - recall_m: 1.0000 - val_loss: 0.6671 - val_f1_m: 0.1053 - val_precision_m: 0.0667 - val_recall_m: 0.2500\n",
      "Epoch 427/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5445 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 423us/sample - loss: 0.4805 - f1_m: 0.2632 - precision_m: 0.1631 - recall_m: 0.8333 - val_loss: 0.6671 - val_f1_m: 0.1345 - val_precision_m: 0.0828 - val_recall_m: 0.3889\n",
      "Epoch 428/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4077 - f1_m: 0.4444 - precision_m: 0.2857 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 331us/sample - loss: 0.4754 - f1_m: 0.2605 - precision_m: 0.1593 - recall_m: 0.8333 - val_loss: 0.6671 - val_f1_m: 0.1306 - val_precision_m: 0.0769 - val_recall_m: 0.4444\n",
      "Epoch 429/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5463 - f1_m: 0.2105 - precision_m: 0.1176 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 427us/sample - loss: 0.4725 - f1_m: 0.3588 - precision_m: 0.2301 - recall_m: 1.0000 - val_loss: 0.6671 - val_f1_m: 0.2500 - val_precision_m: 0.1624 - val_recall_m: 0.5556\n",
      "Epoch 430/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4394 - f1_m: 0.2727 - precision_m: 0.1579 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 379us/sample - loss: 0.4712 - f1_m: 0.2590 - precision_m: 0.1563 - recall_m: 0.8333 - val_loss: 0.6671 - val_f1_m: 0.2222 - val_precision_m: 0.1333 - val_recall_m: 0.6667\n",
      "Epoch 431/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5599 - f1_m: 0.1905 - precision_m: 0.1053 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 376us/sample - loss: 0.4716 - f1_m: 0.3166 - precision_m: 0.1978 - recall_m: 1.0000 - val_loss: 0.6670 - val_f1_m: 0.1176 - val_precision_m: 0.0769 - val_recall_m: 0.2500\n",
      "Epoch 432/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4758 - f1_m: 0.1481 - precision_m: 0.0800 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 261us/sample - loss: 0.4739 - f1_m: 0.2732 - precision_m: 0.1714 - recall_m: 0.8333 - val_loss: 0.6670 - val_f1_m: 0.1111 - val_precision_m: 0.0714 - val_recall_m: 0.2500\n",
      "Epoch 433/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4446 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 240us/sample - loss: 0.4715 - f1_m: 0.3620 - precision_m: 0.2345 - recall_m: 1.0000 - val_loss: 0.6670 - val_f1_m: 0.1215 - val_precision_m: 0.0720 - val_recall_m: 0.3889\n",
      "Epoch 434/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4534 - f1_m: 0.2609 - precision_m: 0.1500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 268us/sample - loss: 0.4681 - f1_m: 0.3414 - precision_m: 0.2132 - recall_m: 1.0000 - val_loss: 0.6671 - val_f1_m: 0.1333 - val_precision_m: 0.0909 - val_recall_m: 0.2500\n",
      "Epoch 435/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4667 - f1_m: 0.4444 - precision_m: 0.2857 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 279us/sample - loss: 0.4778 - f1_m: 0.3237 - precision_m: 0.2002 - recall_m: 1.0000 - val_loss: 0.6671 - val_f1_m: 0.2698 - val_precision_m: 0.1923 - val_recall_m: 0.6667\n",
      "Epoch 436/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.7181 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 249us/sample - loss: 0.4963 - f1_m: 0.3351 - precision_m: 0.2210 - recall_m: 0.8333 - val_loss: 0.6671 - val_f1_m: 0.1308 - val_precision_m: 0.0778 - val_recall_m: 0.5000\n",
      "Epoch 437/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4230 - f1_m: 0.2222 - precision_m: 0.1250 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 297us/sample - loss: 0.4481 - f1_m: 0.3059 - precision_m: 0.1815 - recall_m: 1.0000 - val_loss: 0.6672 - val_f1_m: 0.1310 - val_precision_m: 0.0791 - val_recall_m: 0.3889\n",
      "Epoch 438/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3733 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166/166 [==============================] - 0s 282us/sample - loss: 0.4539 - f1_m: 0.3748 - precision_m: 0.2539 - recall_m: 1.0000 - val_loss: 0.6673 - val_f1_m: 0.1176 - val_precision_m: 0.0833 - val_recall_m: 0.2000\n",
      "Epoch 439/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3738 - f1_m: 0.3846 - precision_m: 0.2381 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 290us/sample - loss: 0.4686 - f1_m: 0.3605 - precision_m: 0.2318 - recall_m: 1.0000 - val_loss: 0.6674 - val_f1_m: 0.2451 - val_precision_m: 0.1587 - val_recall_m: 0.5556\n",
      "Epoch 440/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5185 - f1_m: 0.4167 - precision_m: 0.2632 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 278us/sample - loss: 0.4508 - f1_m: 0.3232 - precision_m: 0.1953 - recall_m: 1.0000 - val_loss: 0.6675 - val_f1_m: 0.1333 - val_precision_m: 0.0812 - val_recall_m: 0.3889\n",
      "Epoch 441/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3337 - f1_m: 0.5217 - precision_m: 0.3529 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 251us/sample - loss: 0.5123 - f1_m: 0.2673 - precision_m: 0.1699 - recall_m: 0.6667 - val_loss: 0.6675 - val_f1_m: 0.1053 - val_precision_m: 0.0667 - val_recall_m: 0.2500\n",
      "Epoch 442/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4750 - f1_m: 0.3810 - precision_m: 0.2353 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 300us/sample - loss: 0.4743 - f1_m: 0.2653 - precision_m: 0.1638 - recall_m: 0.8333 - val_loss: 0.6675 - val_f1_m: 0.1278 - val_precision_m: 0.0794 - val_recall_m: 0.5000\n",
      "Epoch 443/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4299 - f1_m: 0.3810 - precision_m: 0.2353 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 528us/sample - loss: 0.4837 - f1_m: 0.2713 - precision_m: 0.1662 - recall_m: 0.8333 - val_loss: 0.6675 - val_f1_m: 0.1215 - val_precision_m: 0.0725 - val_recall_m: 0.4444\n",
      "Epoch 444/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4512 - f1_m: 0.2727 - precision_m: 0.1579 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 363us/sample - loss: 0.4760 - f1_m: 0.3525 - precision_m: 0.2277 - recall_m: 1.0000 - val_loss: 0.6674 - val_f1_m: 0.1176 - val_precision_m: 0.0769 - val_recall_m: 0.2500\n",
      "Epoch 445/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4473 - f1_m: 0.4800 - precision_m: 0.3158 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 216us/sample - loss: 0.4639 - f1_m: 0.3152 - precision_m: 0.1922 - recall_m: 1.0000 - val_loss: 0.6674 - val_f1_m: 0.1308 - val_precision_m: 0.0778 - val_recall_m: 0.5000\n",
      "Epoch 446/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4451 - f1_m: 0.2000 - precision_m: 0.1111 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 307us/sample - loss: 0.4705 - f1_m: 0.3467 - precision_m: 0.2246 - recall_m: 1.0000 - val_loss: 0.6674 - val_f1_m: 0.1261 - val_precision_m: 0.0747 - val_recall_m: 0.4444\n",
      "Epoch 447/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4088 - f1_m: 0.4828 - precision_m: 0.3182 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 553us/sample - loss: 0.5181 - f1_m: 0.2927 - precision_m: 0.1888 - recall_m: 0.6667 - val_loss: 0.6675 - val_f1_m: 0.1528 - val_precision_m: 0.0963 - val_recall_m: 0.5556\n",
      "Epoch 448/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4267 - f1_m: 0.3571 - precision_m: 0.2174 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 525us/sample - loss: 0.4592 - f1_m: 0.2656 - precision_m: 0.1604 - recall_m: 0.8333 - val_loss: 0.6675 - val_f1_m: 0.1254 - val_precision_m: 0.0747 - val_recall_m: 0.3889\n",
      "Epoch 449/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4443 - f1_m: 0.4615 - precision_m: 0.3000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 526us/sample - loss: 0.4814 - f1_m: 0.2630 - precision_m: 0.1619 - recall_m: 0.8333 - val_loss: 0.6674 - val_f1_m: 0.1278 - val_precision_m: 0.0754 - val_recall_m: 0.4444\n",
      "Epoch 450/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4356 - f1_m: 0.3077 - precision_m: 0.1818 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 504us/sample - loss: 0.4572 - f1_m: 0.3267 - precision_m: 0.1983 - recall_m: 1.0000 - val_loss: 0.6674 - val_f1_m: 0.1345 - val_precision_m: 0.0828 - val_recall_m: 0.3889\n",
      "Epoch 451/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4369 - f1_m: 0.4444 - precision_m: 0.2857 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 360us/sample - loss: 0.4588 - f1_m: 0.3280 - precision_m: 0.1995 - recall_m: 1.0000 - val_loss: 0.6674 - val_f1_m: 0.1984 - val_precision_m: 0.1247 - val_recall_m: 0.6667\n",
      "Epoch 452/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4469 - f1_m: 0.3077 - precision_m: 0.1818 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 408us/sample - loss: 0.4526 - f1_m: 0.3426 - precision_m: 0.2151 - recall_m: 1.0000 - val_loss: 0.6675 - val_f1_m: 0.1250 - val_precision_m: 0.0751 - val_recall_m: 0.3889\n",
      "Epoch 453/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4803 - f1_m: 0.2400 - precision_m: 0.1364 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 408us/sample - loss: 0.4671 - f1_m: 0.2723 - precision_m: 0.1676 - recall_m: 0.8333 - val_loss: 0.6675 - val_f1_m: 0.1365 - val_precision_m: 0.0812 - val_recall_m: 0.5556\n",
      "Epoch 454/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4599 - f1_m: 0.1667 - precision_m: 0.0909 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 415us/sample - loss: 0.4602 - f1_m: 0.2679 - precision_m: 0.1617 - recall_m: 0.8333 - val_loss: 0.6675 - val_f1_m: 0.2451 - val_precision_m: 0.1624 - val_recall_m: 0.5000\n",
      "Epoch 455/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5248 - f1_m: 0.1739 - precision_m: 0.0952 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 489us/sample - loss: 0.4669 - f1_m: 0.3124 - precision_m: 0.1887 - recall_m: 1.0000 - val_loss: 0.6675 - val_f1_m: 0.1365 - val_precision_m: 0.0833 - val_recall_m: 0.3889\n",
      "Epoch 456/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6033 - f1_m: 0.1667 - precision_m: 0.0909 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 374us/sample - loss: 0.4675 - f1_m: 0.3185 - precision_m: 0.1926 - recall_m: 1.0000 - val_loss: 0.6676 - val_f1_m: 0.2170 - val_precision_m: 0.1351 - val_recall_m: 0.7778\n",
      "Epoch 457/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3882 - f1_m: 0.3200 - precision_m: 0.1905 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 223us/sample - loss: 0.4581 - f1_m: 0.3119 - precision_m: 0.1861 - recall_m: 1.0000 - val_loss: 0.6676 - val_f1_m: 0.1377 - val_precision_m: 0.0863 - val_recall_m: 0.3889\n",
      "Epoch 458/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4051 - f1_m: 0.4348 - precision_m: 0.2778 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 371us/sample - loss: 0.4787 - f1_m: 0.2648 - precision_m: 0.1605 - recall_m: 0.8333 - val_loss: 0.6677 - val_f1_m: 0.1000 - val_precision_m: 0.0588 - val_recall_m: 0.3333\n",
      "Epoch 459/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4852 - f1_m: 0.4167 - precision_m: 0.2632 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 255us/sample - loss: 0.4731 - f1_m: 0.2729 - precision_m: 0.1664 - recall_m: 0.8333 - val_loss: 0.6678 - val_f1_m: 0.1346 - val_precision_m: 0.0779 - val_recall_m: 0.5000\n",
      "Epoch 460/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3778 - f1_m: 0.3636 - precision_m: 0.2222 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 477us/sample - loss: 0.4583 - f1_m: 0.2645 - precision_m: 0.1583 - recall_m: 0.8333 - val_loss: 0.6678 - val_f1_m: 0.1176 - val_precision_m: 0.0769 - val_recall_m: 0.2500\n",
      "Epoch 461/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4111 - f1_m: 0.5600 - precision_m: 0.3889 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 244us/sample - loss: 0.4624 - f1_m: 0.3680 - precision_m: 0.2436 - recall_m: 1.0000 - val_loss: 0.6678 - val_f1_m: 0.2118 - val_precision_m: 0.1310 - val_recall_m: 0.5556\n",
      "Epoch 462/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4722 - f1_m: 0.4348 - precision_m: 0.2778 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 243us/sample - loss: 0.4509 - f1_m: 0.3260 - precision_m: 0.1971 - recall_m: 1.0000 - val_loss: 0.6678 - val_f1_m: 0.3115 - val_precision_m: 0.2179 - val_recall_m: 0.7778\n",
      "Epoch 463/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3952 - f1_m: 0.4348 - precision_m: 0.2778 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 296us/sample - loss: 0.4600 - f1_m: 0.2669 - precision_m: 0.1614 - recall_m: 0.8333 - val_loss: 0.6678 - val_f1_m: 0.1402 - val_precision_m: 0.0833 - val_recall_m: 0.5556\n",
      "Epoch 464/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4606 - f1_m: 0.1905 - precision_m: 0.1053 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 317us/sample - loss: 0.4590 - f1_m: 0.3509 - precision_m: 0.2250 - recall_m: 1.0000 - val_loss: 0.6678 - val_f1_m: 0.1365 - val_precision_m: 0.0862 - val_recall_m: 0.5000\n",
      "Epoch 465/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4491 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 327us/sample - loss: 0.4537 - f1_m: 0.3411 - precision_m: 0.2093 - recall_m: 1.0000 - val_loss: 0.6679 - val_f1_m: 0.1185 - val_precision_m: 0.0701 - val_recall_m: 0.3889\n",
      "Epoch 466/1000\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 0.4535 - f1_m: 0.3118 - precision_m: 0.1866 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 647us/sample - loss: 0.4523 - f1_m: 0.3154 - precision_m: 0.1888 - recall_m: 1.0000 - val_loss: 0.6680 - val_f1_m: 0.2222 - val_precision_m: 0.1346 - val_recall_m: 0.6667\n",
      "Epoch 467/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4778 - f1_m: 0.3448 - precision_m: 0.2083 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 429us/sample - loss: 0.4744 - f1_m: 0.2639 - precision_m: 0.1611 - recall_m: 0.8333 - val_loss: 0.6680 - val_f1_m: 0.1261 - val_precision_m: 0.0754 - val_recall_m: 0.3889\n",
      "Epoch 468/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3709 - f1_m: 0.4444 - precision_m: 0.2857 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 430us/sample - loss: 0.4987 - f1_m: 0.2478 - precision_m: 0.1542 - recall_m: 0.8333 - val_loss: 0.6680 - val_f1_m: 0.2451 - val_precision_m: 0.1587 - val_recall_m: 0.5556\n",
      "Epoch 469/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4366 - f1_m: 0.2000 - precision_m: 0.1111 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 335us/sample - loss: 0.4638 - f1_m: 0.2617 - precision_m: 0.1589 - recall_m: 0.8333 - val_loss: 0.6679 - val_f1_m: 0.1296 - val_precision_m: 0.0787 - val_recall_m: 0.4444\n",
      "Epoch 470/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4586 - f1_m: 0.2759 - precision_m: 0.1600 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 230us/sample - loss: 0.4518 - f1_m: 0.3192 - precision_m: 0.1908 - recall_m: 1.0000 - val_loss: 0.6679 - val_f1_m: 0.1215 - val_precision_m: 0.0720 - val_recall_m: 0.3889\n",
      "Epoch 471/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4565 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 324us/sample - loss: 0.4634 - f1_m: 0.2689 - precision_m: 0.1611 - recall_m: 0.8333 - val_loss: 0.6679 - val_f1_m: 0.1000 - val_precision_m: 0.0667 - val_recall_m: 0.2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 472/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4908 - f1_m: 0.3636 - precision_m: 0.2222 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 391us/sample - loss: 0.4573 - f1_m: 0.3425 - precision_m: 0.2118 - recall_m: 1.0000 - val_loss: 0.6679 - val_f1_m: 0.1278 - val_precision_m: 0.0769 - val_recall_m: 0.3889\n",
      "Epoch 473/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4057 - f1_m: 0.4615 - precision_m: 0.3000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 232us/sample - loss: 0.4696 - f1_m: 0.2687 - precision_m: 0.1636 - recall_m: 0.8333 - val_loss: 0.6679 - val_f1_m: 0.0952 - val_precision_m: 0.0625 - val_recall_m: 0.2000\n",
      "Epoch 474/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4192 - f1_m: 0.4545 - precision_m: 0.2941 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 304us/sample - loss: 0.4836 - f1_m: 0.2676 - precision_m: 0.1645 - recall_m: 0.8333 - val_loss: 0.6679 - val_f1_m: 0.1000 - val_precision_m: 0.0588 - val_recall_m: 0.3333\n",
      "Epoch 475/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6231 - f1_m: 0.2222 - precision_m: 0.1250 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 239us/sample - loss: 0.4728 - f1_m: 0.2616 - precision_m: 0.1575 - recall_m: 0.8333 - val_loss: 0.6679 - val_f1_m: 0.1377 - val_precision_m: 0.0863 - val_recall_m: 0.3889\n",
      "Epoch 476/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3773 - f1_m: 0.2963 - precision_m: 0.1739 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 500us/sample - loss: 0.4776 - f1_m: 0.2615 - precision_m: 0.1604 - recall_m: 0.8333 - val_loss: 0.6678 - val_f1_m: 0.2444 - val_precision_m: 0.1786 - val_recall_m: 0.3889\n",
      "Epoch 477/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4688 - f1_m: 0.2222 - precision_m: 0.1250 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 285us/sample - loss: 0.4938 - f1_m: 0.3670 - precision_m: 0.2544 - recall_m: 1.0000 - val_loss: 0.6678 - val_f1_m: 0.1273 - val_precision_m: 0.0763 - val_recall_m: 0.3889\n",
      "Epoch 478/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5300 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 233us/sample - loss: 0.4640 - f1_m: 0.3404 - precision_m: 0.2099 - recall_m: 1.0000 - val_loss: 0.6678 - val_f1_m: 0.1369 - val_precision_m: 0.0844 - val_recall_m: 0.3889\n",
      "Epoch 479/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5620 - f1_m: 0.2400 - precision_m: 0.1364 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 230us/sample - loss: 0.4624 - f1_m: 0.3533 - precision_m: 0.2280 - recall_m: 1.0000 - val_loss: 0.6679 - val_f1_m: 0.3280 - val_precision_m: 0.2318 - val_recall_m: 0.6667\n",
      "Epoch 480/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4572 - f1_m: 0.2308 - precision_m: 0.1304 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 279us/sample - loss: 0.4627 - f1_m: 0.2703 - precision_m: 0.1711 - recall_m: 0.8333 - val_loss: 0.6679 - val_f1_m: 0.1254 - val_precision_m: 0.0747 - val_recall_m: 0.3889\n",
      "Epoch 481/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5474 - f1_m: 0.3846 - precision_m: 0.2381 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 529us/sample - loss: 0.4587 - f1_m: 0.2666 - precision_m: 0.1609 - recall_m: 0.8333 - val_loss: 0.6679 - val_f1_m: 0.2614 - val_precision_m: 0.1690 - val_recall_m: 0.7778\n",
      "Epoch 482/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.7947 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 376us/sample - loss: 0.5128 - f1_m: 0.3272 - precision_m: 0.2145 - recall_m: 0.8333 - val_loss: 0.6678 - val_f1_m: 0.0952 - val_precision_m: 0.0625 - val_recall_m: 0.2000\n",
      "Epoch 483/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3796 - f1_m: 0.4167 - precision_m: 0.2632 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 230us/sample - loss: 0.4764 - f1_m: 0.3412 - precision_m: 0.2139 - recall_m: 1.0000 - val_loss: 0.6678 - val_f1_m: 0.1323 - val_precision_m: 0.0863 - val_recall_m: 0.5000\n",
      "Epoch 484/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5773 - f1_m: 0.1739 - precision_m: 0.0952 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 221us/sample - loss: 0.4928 - f1_m: 0.2310 - precision_m: 0.1435 - recall_m: 0.8333 - val_loss: 0.6678 - val_f1_m: 0.1278 - val_precision_m: 0.0733 - val_recall_m: 0.5000\n",
      "Epoch 485/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4270 - f1_m: 0.4348 - precision_m: 0.2778 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 283us/sample - loss: 0.4729 - f1_m: 0.2661 - precision_m: 0.1614 - recall_m: 0.8333 - val_loss: 0.6678 - val_f1_m: 0.1273 - val_precision_m: 0.0763 - val_recall_m: 0.3889\n",
      "Epoch 486/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4287 - f1_m: 0.3000 - precision_m: 0.1765 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 268us/sample - loss: 0.4489 - f1_m: 0.3256 - precision_m: 0.1952 - recall_m: 1.0000 - val_loss: 0.6678 - val_f1_m: 0.1296 - val_precision_m: 0.0779 - val_recall_m: 0.5000\n",
      "Epoch 487/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4862 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 333us/sample - loss: 0.4669 - f1_m: 0.2669 - precision_m: 0.1609 - recall_m: 0.8333 - val_loss: 0.6678 - val_f1_m: 0.2286 - val_precision_m: 0.1389 - val_recall_m: 0.6667\n",
      "Epoch 488/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3470 - f1_m: 0.3636 - precision_m: 0.2222 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 355us/sample - loss: 0.4654 - f1_m: 0.3195 - precision_m: 0.1941 - recall_m: 1.0000 - val_loss: 0.6678 - val_f1_m: 0.2684 - val_precision_m: 0.1690 - val_recall_m: 0.7778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 489/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4342 - f1_m: 0.2500 - precision_m: 0.1429 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 341us/sample - loss: 0.5074 - f1_m: 0.2531 - precision_m: 0.1602 - recall_m: 0.6667 - val_loss: 0.6678 - val_f1_m: 0.1053 - val_precision_m: 0.0714 - val_recall_m: 0.2000\n",
      "Epoch 490/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3870 - f1_m: 0.4615 - precision_m: 0.3000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 213us/sample - loss: 0.4788 - f1_m: 0.2657 - precision_m: 0.1641 - recall_m: 0.8333 - val_loss: 0.6677 - val_f1_m: 0.1215 - val_precision_m: 0.0725 - val_recall_m: 0.4444\n",
      "Epoch 491/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5080 - f1_m: 0.3077 - precision_m: 0.1818 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 313us/sample - loss: 0.4714 - f1_m: 0.2691 - precision_m: 0.1634 - recall_m: 0.8333 - val_loss: 0.6677 - val_f1_m: 0.1429 - val_precision_m: 0.0884 - val_recall_m: 0.3889\n",
      "Epoch 492/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5115 - f1_m: 0.2105 - precision_m: 0.1176 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 343us/sample - loss: 0.4712 - f1_m: 0.2577 - precision_m: 0.1547 - recall_m: 0.8333 - val_loss: 0.6676 - val_f1_m: 0.1254 - val_precision_m: 0.0720 - val_recall_m: 0.5000\n",
      "Epoch 493/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4355 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 293us/sample - loss: 0.4719 - f1_m: 0.2591 - precision_m: 0.1575 - recall_m: 0.8333 - val_loss: 0.6676 - val_f1_m: 0.2451 - val_precision_m: 0.1587 - val_recall_m: 0.5556\n",
      "Epoch 494/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4665 - f1_m: 0.5161 - precision_m: 0.3478 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 221us/sample - loss: 0.4591 - f1_m: 0.3138 - precision_m: 0.1944 - recall_m: 1.0000 - val_loss: 0.6675 - val_f1_m: 0.2593 - val_precision_m: 0.1889 - val_recall_m: 0.4444\n",
      "Epoch 495/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4573 - f1_m: 0.2500 - precision_m: 0.1429 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 282us/sample - loss: 0.4691 - f1_m: 0.2767 - precision_m: 0.1684 - recall_m: 0.8333 - val_loss: 0.6675 - val_f1_m: 0.2889 - val_precision_m: 0.2083 - val_recall_m: 0.5000\n",
      "Epoch 496/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3546 - f1_m: 0.4800 - precision_m: 0.3158 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 275us/sample - loss: 0.4765 - f1_m: 0.2707 - precision_m: 0.1656 - recall_m: 0.8333 - val_loss: 0.6675 - val_f1_m: 0.1297 - val_precision_m: 0.0778 - val_recall_m: 0.4444\n",
      "Epoch 497/1000\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 0.4518 - f1_m: 0.3044 - precision_m: 0.1824 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 652us/sample - loss: 0.4510 - f1_m: 0.3370 - precision_m: 0.2076 - recall_m: 1.0000 - val_loss: 0.6674 - val_f1_m: 0.1297 - val_precision_m: 0.0779 - val_recall_m: 0.3889\n",
      "Epoch 498/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4230 - f1_m: 0.3704 - precision_m: 0.2273 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 288us/sample - loss: 0.4674 - f1_m: 0.3035 - precision_m: 0.1838 - recall_m: 1.0000 - val_loss: 0.6675 - val_f1_m: 0.1333 - val_precision_m: 0.1000 - val_recall_m: 0.2000\n",
      "Epoch 499/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4887 - f1_m: 0.1538 - precision_m: 0.0833 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 220us/sample - loss: 0.4592 - f1_m: 0.3154 - precision_m: 0.1943 - recall_m: 1.0000 - val_loss: 0.6675 - val_f1_m: 0.2137 - val_precision_m: 0.1333 - val_recall_m: 0.5556\n",
      "Epoch 500/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5104 - f1_m: 0.2222 - precision_m: 0.1250 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 336us/sample - loss: 0.4673 - f1_m: 0.3269 - precision_m: 0.1996 - recall_m: 1.0000 - val_loss: 0.6675 - val_f1_m: 0.1250 - val_precision_m: 0.0751 - val_recall_m: 0.3889\n",
      "Epoch 501/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3985 - f1_m: 0.3077 - precision_m: 0.1818 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 314us/sample - loss: 0.4630 - f1_m: 0.2724 - precision_m: 0.1639 - recall_m: 0.8333 - val_loss: 0.6676 - val_f1_m: 0.1944 - val_precision_m: 0.1179 - val_recall_m: 0.5556\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unable to create link (name already exists)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-103-b29c6b424842>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, x_train, y_train, x_test, y_test, batch_size, epochs, class_weight, start_epoch, workers, callback_list, model_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcallback_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             ])\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, save_weights_only=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_path'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   1332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m     \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/saving.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    150\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             param_dset = optimizer_weights_group.create_dataset(\n\u001b[0;32m--> 152\u001b[0;31m                 name, val.shape, dtype=val.dtype)\n\u001b[0m\u001b[1;32m    153\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m               \u001b[0;31m# scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, name, obj)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHLObject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                 \u001b[0mh5o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSoftLink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.link\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to create link (name already exists)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "freeze_layer = FreezeLayer(hyperparams['freeze_patience'], set_to=not hyperparams['trainable_embeddings'])\n",
    "weights_history = WeightsHistory()\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=hyperparams['reduce_lr_factor'],\n",
    "                          patience=hyperparams['reduce_lr_patience'], min_lr=0.000001, verbose=1)\n",
    "\n",
    "history = train_model(model, x_train, y_train, x_valid, y_valid,\n",
    "           epochs=1000, batch_size=hyperparams['batch_size'],\n",
    "                      class_weight={0:0.5, 1:10}, start_epoch=0,\n",
    "                      callback_list = [freeze_layer, weights_history, reduce_lr],\n",
    "                      model_path='models/lstm_plus_ablated_user3', workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.44971931e+00],\n",
       "        [ 5.99275768e-01],\n",
       "        [ 4.79707479e-01],\n",
       "        [-1.02073348e+00],\n",
       "        [ 7.06191480e-01],\n",
       "        [-5.70192754e-01],\n",
       "        [-6.15594923e-01],\n",
       "        [-8.78930449e-01],\n",
       "        [-3.44653964e+00],\n",
       "        [-4.17300969e-01],\n",
       "        [-3.47588360e-01],\n",
       "        [ 4.52555120e-01],\n",
       "        [ 3.45672297e+00],\n",
       "        [-4.62037057e-01],\n",
       "        [-2.84551525e+00],\n",
       "        [-3.03839874e+00],\n",
       "        [ 1.36626410e+00],\n",
       "        [-1.01885557e+00],\n",
       "        [-2.80554235e-01],\n",
       "        [-1.33873188e+00],\n",
       "        [ 6.81673288e-01],\n",
       "        [-1.62495267e+00],\n",
       "        [-5.50505295e+01],\n",
       "        [-1.37413883e+00],\n",
       "        [-1.92312765e+00],\n",
       "        [ 6.35300815e-01],\n",
       "        [ 1.14118254e+00],\n",
       "        [-2.15535760e+00],\n",
       "        [-2.97780067e-01],\n",
       "        [-3.38145828e+00],\n",
       "        [ 1.78429782e-01],\n",
       "        [ 1.10952210e+00],\n",
       "        [-1.82261813e+00],\n",
       "        [ 1.32531798e+00],\n",
       "        [-1.74499857e+00],\n",
       "        [ 1.44931817e+00],\n",
       "        [-7.66425788e-01],\n",
       "        [-4.54369038e-01],\n",
       "        [-1.16801679e+00],\n",
       "        [-3.87844086e-01],\n",
       "        [ 2.28751719e-01],\n",
       "        [ 1.56348526e+00],\n",
       "        [-1.63736224e+00],\n",
       "        [ 3.82462770e-01],\n",
       "        [-1.03432298e+00],\n",
       "        [-7.87146270e-01],\n",
       "        [-2.62502146e+00],\n",
       "        [-2.20739746e+00],\n",
       "        [ 5.63182294e-01],\n",
       "        [ 8.16648304e-01],\n",
       "        [-3.21735024e+00],\n",
       "        [ 5.76238155e-01],\n",
       "        [-1.11849129e+00],\n",
       "        [ 2.86429214e+00],\n",
       "        [ 3.59776092e+00],\n",
       "        [-2.37842631e+00],\n",
       "        [-1.04187226e+00],\n",
       "        [-1.62664819e+00],\n",
       "        [ 1.39853597e+00],\n",
       "        [-1.47555101e+00],\n",
       "        [-1.28982377e+00],\n",
       "        [-6.34847343e-01],\n",
       "        [ 3.06845516e-01],\n",
       "        [ 5.43143213e-01],\n",
       "        [-6.39110357e-02],\n",
       "        [-1.80369878e+00],\n",
       "        [ 9.95237753e-03],\n",
       "        [ 2.02436543e+00],\n",
       "        [ 1.20272577e+00],\n",
       "        [-1.74875998e+00],\n",
       "        [-1.44120014e+00],\n",
       "        [ 1.42027450e+00],\n",
       "        [-2.24666548e+00],\n",
       "        [-1.64019156e+00],\n",
       "        [-8.06684971e-01],\n",
       "        [ 3.39318424e-01],\n",
       "        [ 5.30882537e-01],\n",
       "        [ 3.53614151e-01],\n",
       "        [ 8.44027996e-01],\n",
       "        [-1.70989111e-01],\n",
       "        [-4.57769662e-01],\n",
       "        [-1.28381786e+01],\n",
       "        [-1.62698019e+00],\n",
       "        [-1.24002673e-01],\n",
       "        [ 7.90004879e-02],\n",
       "        [-9.80859548e-02],\n",
       "        [ 1.06734931e-02],\n",
       "        [ 2.51013041e-02],\n",
       "        [ 8.20404068e-02],\n",
       "        [-1.49290776e+00],\n",
       "        [ 4.26178515e-01],\n",
       "        [-1.25629730e+01],\n",
       "        [-2.04875559e-01],\n",
       "        [ 3.00567418e-01],\n",
       "        [ 1.92661826e-02],\n",
       "        [-2.04317880e+00],\n",
       "        [ 5.75706512e-02],\n",
       "        [-8.21892470e-02],\n",
       "        [ 2.18561254e-02],\n",
       "        [-1.32251549e+01],\n",
       "        [-1.49451628e+01],\n",
       "        [ 1.48946047e-01],\n",
       "        [ 1.05145842e-01],\n",
       "        [-1.57194976e-02],\n",
       "        [-5.76147199e-01],\n",
       "        [-2.20573284e-02],\n",
       "        [ 2.15103514e-02],\n",
       "        [-2.55923092e-01],\n",
       "        [-1.34906569e+01],\n",
       "        [-1.16044474e+00],\n",
       "        [-3.41843143e-02],\n",
       "        [-5.29423356e-01],\n",
       "        [-3.16753566e-01],\n",
       "        [-1.27255523e+00],\n",
       "        [-2.60655850e-01],\n",
       "        [-1.36507243e-01],\n",
       "        [ 9.97354984e-02],\n",
       "        [-5.90253389e-03],\n",
       "        [-1.29536971e-01],\n",
       "        [ 6.17593706e-01],\n",
       "        [ 3.97334769e-02],\n",
       "        [-2.15299815e-01],\n",
       "        [-3.79910017e-03],\n",
       "        [ 5.38597628e-02],\n",
       "        [-5.39592616e-02],\n",
       "        [ 1.89929649e-01],\n",
       "        [ 2.10691109e-01],\n",
       "        [ 1.98218390e-01],\n",
       "        [-1.59804478e-01],\n",
       "        [ 1.97658598e-01],\n",
       "        [ 3.67126703e-01],\n",
       "        [ 3.40350628e-01],\n",
       "        [ 1.99894771e-01],\n",
       "        [ 1.08632468e-01],\n",
       "        [ 6.80857003e-02],\n",
       "        [-6.66646147e-03],\n",
       "        [-1.93662360e-01],\n",
       "        [ 8.96340832e-02],\n",
       "        [ 2.06507415e-01],\n",
       "        [ 1.14517771e-01],\n",
       "        [ 6.46155775e-02],\n",
       "        [ 3.34020197e-01],\n",
       "        [ 1.48022309e-01],\n",
       "        [-5.26544213e-01],\n",
       "        [ 1.24531411e-01],\n",
       "        [ 2.31826201e-01],\n",
       "        [-2.64728051e-02],\n",
       "        [-1.15541674e-01],\n",
       "        [-3.53352100e-01],\n",
       "        [ 9.27269757e-02],\n",
       "        [ 4.04484347e-02],\n",
       "        [ 7.84482136e-02],\n",
       "        [-9.58437979e-01],\n",
       "        [-1.13082059e-01],\n",
       "        [ 1.05072230e-01],\n",
       "        [ 9.86793712e-02],\n",
       "        [-4.13428456e-01],\n",
       "        [ 2.00053096e-01],\n",
       "        [ 1.62010953e-01],\n",
       "        [-3.74420464e-01],\n",
       "        [-1.11065435e+00],\n",
       "        [-1.62434071e-01],\n",
       "        [-4.86821681e-01],\n",
       "        [ 4.73745689e-02],\n",
       "        [ 1.42917767e-01],\n",
       "        [-1.22786678e-01],\n",
       "        [ 2.19687462e-01],\n",
       "        [-3.08683276e-01],\n",
       "        [-5.51062971e-02],\n",
       "        [-2.35218778e-01],\n",
       "        [-4.18748823e-04],\n",
       "        [ 9.82765406e-02],\n",
       "        [ 5.87903103e-03],\n",
       "        [ 3.29366252e-02],\n",
       "        [-4.08651441e-01],\n",
       "        [-3.66301030e-01],\n",
       "        [-9.19910297e-02],\n",
       "        [ 2.35519499e-01],\n",
       "        [-5.75208664e-01],\n",
       "        [-9.35269403e-04],\n",
       "        [-1.35722116e-01],\n",
       "        [-9.92173776e-02],\n",
       "        [ 3.49313989e-02],\n",
       "        [-6.61788834e-03],\n",
       "        [-8.25603828e-02],\n",
       "        [ 1.03399612e-01],\n",
       "        [-1.35449484e-01],\n",
       "        [-5.48128271e-04],\n",
       "        [ 3.13403785e-01],\n",
       "        [ 1.12053223e-01],\n",
       "        [ 3.47759694e-01],\n",
       "        [ 4.18254435e-01],\n",
       "        [-1.00189358e-01],\n",
       "        [-2.95435280e-01],\n",
       "        [ 1.95707887e-01],\n",
       "        [-1.11491054e-01],\n",
       "        [ 2.44915307e-01],\n",
       "        [ 1.23568386e-01],\n",
       "        [ 2.61244118e-01],\n",
       "        [ 1.29018068e-01],\n",
       "        [ 1.69486225e-01],\n",
       "        [-1.74785897e-01],\n",
       "        [-1.74565185e-02],\n",
       "        [ 1.11848325e-01],\n",
       "        [-2.41175935e-01],\n",
       "        [ 2.72021383e-01],\n",
       "        [ 4.54719841e-01],\n",
       "        [ 9.27790403e-02],\n",
       "        [ 2.20335931e-01],\n",
       "        [-7.48416185e-02],\n",
       "        [ 1.30733252e-01],\n",
       "        [ 1.73391536e-01],\n",
       "        [ 1.07428297e-01],\n",
       "        [ 1.45580709e-01],\n",
       "        [ 1.55159131e-01],\n",
       "        [-2.72214651e-01],\n",
       "        [ 5.57822101e-02],\n",
       "        [-1.17972374e+00],\n",
       "        [ 7.60140896e-01],\n",
       "        [ 2.39980593e-01],\n",
       "        [-1.28228098e-01],\n",
       "        [-3.51427436e-01],\n",
       "        [ 1.51922911e-01],\n",
       "        [-1.28607094e-01],\n",
       "        [-2.24804729e-02],\n",
       "        [-3.93109739e-01],\n",
       "        [ 1.36323422e-01],\n",
       "        [ 4.40599136e-02],\n",
       "        [ 1.96252763e-02],\n",
       "        [-8.49482596e-01],\n",
       "        [-1.38452023e-01],\n",
       "        [ 4.48073834e-01],\n",
       "        [ 3.29953879e-02],\n",
       "        [ 2.69325614e-01],\n",
       "        [ 1.42175704e-01],\n",
       "        [-1.47752924e+01],\n",
       "        [ 1.22107178e-01],\n",
       "        [ 8.40429217e-02],\n",
       "        [ 5.07928878e-02],\n",
       "        [-2.02921927e-02],\n",
       "        [-9.49106312e+00],\n",
       "        [ 1.46319211e-01],\n",
       "        [-9.42043686e+00],\n",
       "        [-1.01587743e-01],\n",
       "        [-1.07210374e+00],\n",
       "        [-2.76902318e-03],\n",
       "        [ 4.68347594e-02],\n",
       "        [ 4.88146842e-02],\n",
       "        [-4.99129117e-01],\n",
       "        [ 1.51265264e-01],\n",
       "        [ 4.41928774e-01],\n",
       "        [ 2.87823230e-02],\n",
       "        [-3.41137230e-01],\n",
       "        [ 8.50599110e-02]], dtype=float32), array([-1.2966485], dtype=float32)]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='models/lstm_plus4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels Tensor(\"dense_1_target_1:0\", shape=(?, ?), dtype=float32)\n",
      "Predictions Tensor(\"Sigmoid_2:0\", shape=(?, 1), dtype=float32)\n",
      "True positives Tensor(\"metrics_1/f1_m/Sum_2:0\", shape=(), dtype=float32)\n",
      "Labels Tensor(\"dense_1_target_1:0\", shape=(?, ?), dtype=float32)\n",
      "Predictions Tensor(\"Sigmoid_2:0\", shape=(?, 1), dtype=float32)\n",
      "True positives Tensor(\"metrics_1/f1_m/Sum_7:0\", shape=(), dtype=float32)\n",
      "Labels Tensor(\"dense_1_target_1:0\", shape=(?, ?), dtype=float32)\n",
      "Predictions Tensor(\"Sigmoid_2:0\", shape=(?, 1), dtype=float32)\n",
      "True positives Tensor(\"metrics_1/recall_m/Sum:0\", shape=(), dtype=float32)\n",
      "Labels Tensor(\"dense_1_target_1:0\", shape=(?, ?), dtype=float32)\n",
      "Predictions Tensor(\"Sigmoid_2:0\", shape=(?, 1), dtype=float32)\n",
      "True positives Tensor(\"metrics_1/recall_m/Sum_3:0\", shape=(), dtype=float32)\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "dependencies = {\n",
    "    'f1_m': f1_m,\n",
    "    'precision_m': precision_m,\n",
    "    'recall_m': recall_m,\n",
    "    'binary_crossentropy_custom': binary_crossentropy_custom\n",
    "}\n",
    "model = load_model('models/lstm_plus8_remote', custom_objects=dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 63us/sample - loss: 0.4804 - f1_m: 0.4186 - precision_m: 0.4687 - recall_m: 0.4464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4804278371982204, 0.4185897, 0.46874997, 0.44642857]"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254 254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('herself', 'stopword'), -1.3356316089630127),\n",
       " (('ma', 'stopword'), -1.2046297788619995),\n",
       " (('theirs', 'stopword'), -1.053725242614746),\n",
       " (('work', 'liwc'), -1.0329701900482178),\n",
       " ('pers_pronouns', 0.9913920164108276),\n",
       " (('hers', 'stopword'), -0.9762506484985352),\n",
       " (('ours', 'stopword'), -0.8895556330680847),\n",
       " (('i', 'liwc'), 0.8726097941398621),\n",
       " (('himself', 'stopword'), -0.8083417415618896),\n",
       " (('yourselves', 'stopword'), -0.7712955474853516),\n",
       " (('tentat', 'liwc'), 0.7579227089881897),\n",
       " (('until', 'stopword'), 0.7409663200378418),\n",
       " (('conj', 'liwc'), 0.7305939793586731),\n",
       " (('myself', 'stopword'), 0.7131538391113281),\n",
       " (('feel', 'liwc'), 0.7049807906150818),\n",
       " (('o', 'stopword'), -0.678808867931366),\n",
       " (('few', 'stopword'), 0.6215648055076599),\n",
       " (('we', 'liwc'), -0.6206851601600647),\n",
       " (('his', 'stopword'), -0.6190535426139832),\n",
       " (('excl', 'liwc'), 0.6161367297172546),\n",
       " (('anger', 'liwc'), -0.5914202332496643),\n",
       " (('below', 'stopword'), -0.5909287929534912),\n",
       " (('ppron', 'liwc'), 0.5817850828170776),\n",
       " (('swear', 'liwc'), -0.581218421459198),\n",
       " (('yours', 'stopword'), -0.5725081562995911),\n",
       " (('article', 'liwc'), -0.5439066290855408),\n",
       " (('hasn', 'stopword'), -0.5247345566749573),\n",
       " (('very', 'stopword'), 0.5000014305114746),\n",
       " (('relig', 'liwc'), -0.4997096061706543),\n",
       " (('anx', 'liwc'), 0.4947203993797302),\n",
       " (('ingest', 'liwc'), 0.4922645092010498),\n",
       " (('hadn', 'stopword'), 0.4752688705921173),\n",
       " (('present', 'liwc'), 0.4713248610496521),\n",
       " (('verb', 'liwc'), 0.4595204293727875),\n",
       " (('by', 'stopword'), -0.45368775725364685),\n",
       " (('shan', 'stopword'), -0.44859275221824646),\n",
       " (('he', 'stopword'), -0.4307499825954437),\n",
       " (('time', 'liwc'), -0.418591171503067),\n",
       " (('aren', 'stopword'), 0.4138379693031311),\n",
       " (('insight', 'liwc'), 0.40919163823127747),\n",
       " (('future', 'liwc'), 0.4080585241317749),\n",
       " (('leisure', 'liwc'), -0.4071531891822815),\n",
       " (('under', 'stopword'), 0.4022611975669861),\n",
       " (('discrep', 'liwc'), 0.39239001274108887),\n",
       " (('him', 'stopword'), -0.38899335265159607),\n",
       " (('needn', 'stopword'), -0.38787218928337097),\n",
       " (('achieve', 'liwc'), -0.3876109719276428),\n",
       " (('bio', 'liwc'), 0.3789690434932709),\n",
       " (('cogmech', 'liwc'), 0.3768191337585449),\n",
       " (('motion', 'liwc'), -0.3728558421134949),\n",
       " (('weren', 'stopword'), 0.365419864654541),\n",
       " (('body', 'liwc'), -0.3645724058151245),\n",
       " (('family', 'liwc'), -0.36287128925323486),\n",
       " (('adverb', 'liwc'), 0.36157554388046265),\n",
       " (('home', 'liwc'), -0.35912832617759705),\n",
       " (('pronoun', 'liwc'), 0.3581582307815552),\n",
       " (('out', 'stopword'), 0.3579675257205963),\n",
       " (('shehe', 'liwc'), -0.3519456386566162),\n",
       " (('after', 'stopword'), -0.3496268391609192),\n",
       " (('sadness', 'nrc'), 0.34892940521240234),\n",
       " (('auxverb', 'liwc'), 0.33512982726097107),\n",
       " (('themselves', 'stopword'), -0.33341631293296814),\n",
       " (('both', 'stopword'), -0.3298112154006958),\n",
       " (('health', 'liwc'), 0.32835784554481506),\n",
       " (('money', 'liwc'), -0.31899940967559814),\n",
       " (('they', 'liwc'), 0.3179054856300354),\n",
       " (('positive', 'nrc'), -0.31289586424827576),\n",
       " (('funct', 'liwc'), 0.3119775652885437),\n",
       " (('trust', 'nrc'), -0.3093487322330475),\n",
       " (('shouldn', 'stopword'), -0.30607879161834717),\n",
       " (('disgust', 'nrc'), 0.3033227324485779),\n",
       " (('ll', 'stopword'), 0.293954074382782),\n",
       " (('space', 'liwc'), -0.29085439443588257),\n",
       " (('further', 'stopword'), -0.2798490524291992),\n",
       " (('percept', 'liwc'), 0.27881792187690735),\n",
       " (('been', 'stopword'), 0.2608383595943451),\n",
       " (('yourself', 'stopword'), 0.2543991208076477),\n",
       " (('them', 'stopword'), 0.25240030884742737),\n",
       " (('surprise', 'nrc'), -0.2503567934036255),\n",
       " (('ve', 'stopword'), 0.2496464103460312),\n",
       " (('as', 'stopword'), -0.24870102107524872),\n",
       " (('each', 'stopword'), -0.245611310005188),\n",
       " (('number', 'liwc'), -0.24530421197414398),\n",
       " (('haven', 'stopword'), 0.24318109452724457),\n",
       " (('and', 'stopword'), -0.24257555603981018),\n",
       " (('itself', 'stopword'), -0.23581476509571075),\n",
       " (('same', 'stopword'), -0.2342565357685089),\n",
       " (('incl', 'liwc'), 0.23325571417808533),\n",
       " (('relativ', 'liwc'), -0.23283173143863678),\n",
       " (('negative', 'nrc'), 0.22808456420898438),\n",
       " (('quant', 'liwc'), 0.22544842958450317),\n",
       " (('at', 'stopword'), 0.22377219796180725),\n",
       " (('the', 'stopword'), -0.22198012471199036),\n",
       " (('from', 'stopword'), -0.21687014400959015),\n",
       " (('we', 'stopword'), -0.21655981242656708),\n",
       " (('such', 'stopword'), -0.21212950348854065),\n",
       " (('up', 'stopword'), -0.2112637609243393),\n",
       " (('above', 'stopword'), -0.20800115168094635),\n",
       " (('re', 'stopword'), 0.20762237906455994),\n",
       " (('was', 'stopword'), 0.20656119287014008),\n",
       " (('of', 'stopword'), -0.20248201489448547),\n",
       " (('ipron', 'liwc'), 0.19887207448482513),\n",
       " (('couldn', 'stopword'), 0.1955803781747818),\n",
       " (('who', 'stopword'), -0.19025352597236633),\n",
       " (('once', 'stopword'), 0.18295541405677795),\n",
       " (('isn', 'stopword'), 0.18227005004882812),\n",
       " (('y', 'stopword'), -0.18139927089214325),\n",
       " (('affect', 'liwc'), -0.1781630963087082),\n",
       " (('be', 'stopword'), -0.17665843665599823),\n",
       " (('over', 'stopword'), 0.17379377782344818),\n",
       " (('past', 'liwc'), -0.1721743494272232),\n",
       " (('again', 'stopword'), -0.17194461822509766),\n",
       " (('have', 'stopword'), -0.16945166885852814),\n",
       " (('whom', 'stopword'), 0.16306211054325104),\n",
       " (('ain', 'stopword'), 0.16090939939022064),\n",
       " (('having', 'stopword'), 0.15988227725028992),\n",
       " (('nor', 'stopword'), 0.15981262922286987),\n",
       " (('being', 'stopword'), 0.15936267375946045),\n",
       " (('death', 'liwc'), -0.15907643735408783),\n",
       " (('they', 'stopword'), -0.1587887406349182),\n",
       " (('you', 'liwc'), 0.15767675638198853),\n",
       " (('or', 'stopword'), 0.1569456160068512),\n",
       " (('now', 'stopword'), -0.15690122544765472),\n",
       " (('our', 'stopword'), 0.15653271973133087),\n",
       " (('did', 'stopword'), -0.15523745119571686),\n",
       " (('against', 'stopword'), 0.1523963212966919),\n",
       " (('than', 'stopword'), 0.15210117399692535),\n",
       " (('certain', 'liwc'), -0.1505623608827591),\n",
       " (('any', 'stopword'), 0.1498701274394989),\n",
       " (('no', 'stopword'), -0.14858011901378632),\n",
       " (('when', 'stopword'), 0.14639966189861298),\n",
       " (('through', 'stopword'), -0.1461431086063385),\n",
       " (('see', 'liwc'), -0.14061015844345093),\n",
       " (('cause', 'liwc'), 0.1402399241924286),\n",
       " (('negemo', 'liwc'), -0.1371796876192093),\n",
       " (('so', 'stopword'), -0.13152635097503662),\n",
       " (('during', 'stopword'), -0.13076840341091156),\n",
       " (('negate', 'liwc'), 0.12986278533935547),\n",
       " (('down', 'stopword'), -0.12894216179847717),\n",
       " (('to', 'stopword'), -0.1277695596218109),\n",
       " (('her', 'stopword'), -0.12695199251174927),\n",
       " (('preps', 'liwc'), -0.12617912888526917),\n",
       " (('do', 'stopword'), 0.12519802153110504),\n",
       " (('just', 'stopword'), 0.12452486157417297),\n",
       " (('your', 'stopword'), 0.12339791655540466),\n",
       " (('between', 'stopword'), -0.12084139138460159),\n",
       " (('does', 'stopword'), 0.12059500813484192),\n",
       " (('where', 'stopword'), 0.11789218336343765),\n",
       " (('had', 'stopword'), -0.11755301058292389),\n",
       " (('why', 'stopword'), 0.11627567559480667),\n",
       " (('that', 'stopword'), -0.11624356359243393),\n",
       " (('most', 'stopword'), 0.11621569097042084),\n",
       " (('d', 'stopword'), -0.11580691486597061),\n",
       " (('it', 'stopword'), 0.11469321697950363),\n",
       " (('here', 'stopword'), -0.11285648494958878),\n",
       " (('this', 'stopword'), -0.11177773028612137),\n",
       " (('if', 'stopword'), 0.1091422438621521),\n",
       " (('into', 'stopword'), -0.10838837921619415),\n",
       " (('while', 'stopword'), 0.10702334344387054),\n",
       " (('hear', 'liwc'), 0.1011635959148407),\n",
       " (('own', 'stopword'), 0.10094280540943146),\n",
       " (('its', 'stopword'), -0.10046254843473434),\n",
       " (('my', 'stopword'), -0.09742218255996704),\n",
       " (('off', 'stopword'), 0.09663952142000198),\n",
       " (('some', 'stopword'), 0.09639038890600204),\n",
       " (('which', 'stopword'), 0.09573265165090561),\n",
       " (('ourselves', 'stopword'), 0.09565218538045883),\n",
       " (('sexual', 'liwc'), 0.09513385593891144),\n",
       " (('these', 'stopword'), 0.09491167962551117),\n",
       " (('should', 'stopword'), 0.09465561807155609),\n",
       " (('doing', 'stopword'), 0.09269241988658905),\n",
       " (('those', 'stopword'), -0.09187839925289154),\n",
       " (('sad', 'liwc'), 0.09161501377820969),\n",
       " (('wouldn', 'stopword'), -0.08971942216157913),\n",
       " (('anger', 'nrc'), -0.08304160833358765),\n",
       " (('other', 'stopword'), -0.08298277109861374),\n",
       " (('their', 'stopword'), 0.0821615532040596),\n",
       " (('has', 'stopword'), -0.08057548850774765),\n",
       " (('for', 'stopword'), -0.07765636593103409),\n",
       " (('in', 'stopword'), -0.0775950700044632),\n",
       " (('m', 'stopword'), 0.07438976317644119),\n",
       " (('more', 'stopword'), -0.07392022013664246),\n",
       " (('will', 'stopword'), -0.06547122448682785),\n",
       " (('social', 'liwc'), 0.06494510173797607),\n",
       " (('too', 'stopword'), 0.06468281894922256),\n",
       " (('what', 'stopword'), -0.06412648409605026),\n",
       " (('posemo', 'liwc'), 0.06336244195699692),\n",
       " (('can', 'stopword'), -0.06320313364267349),\n",
       " (('doesn', 'stopword'), 0.061901796609163284),\n",
       " (('with', 'stopword'), -0.06097278743982315),\n",
       " (('how', 'stopword'), -0.06076586991548538),\n",
       " (('an', 'stopword'), 0.06059075519442558),\n",
       " (('were', 'stopword'), -0.05412108823657036),\n",
       " (('there', 'stopword'), -0.04987156763672829),\n",
       " (('am', 'stopword'), 0.04812067747116089),\n",
       " (('wasn', 'stopword'), -0.04781721904873848),\n",
       " (('assent', 'liwc'), -0.04703705012798309),\n",
       " (('won', 'stopword'), 0.044944435358047485),\n",
       " (('friend', 'liwc'), -0.044939104467630386),\n",
       " (('only', 'stopword'), 0.044179730117321014),\n",
       " (('t', 'stopword'), 0.044149454683065414),\n",
       " (('a', 'stopword'), 0.04115057736635208),\n",
       " (('but', 'stopword'), 0.03832564502954483),\n",
       " (('joy', 'nrc'), -0.03815344721078873),\n",
       " (('i', 'stopword'), -0.03788602352142334),\n",
       " (('s', 'stopword'), -0.03562389686703682),\n",
       " (('didn', 'stopword'), 0.033468108624219894),\n",
       " (('me', 'stopword'), 0.0321679450571537),\n",
       " (('fear', 'nrc'), 0.03204983472824097),\n",
       " (('she', 'stopword'), 0.03191811591386795),\n",
       " (('humans', 'liwc'), -0.03117934986948967),\n",
       " (('you', 'stopword'), 0.028208190575242043),\n",
       " (('on', 'stopword'), -0.028110895305871964),\n",
       " (('because', 'stopword'), 0.02758379653096199),\n",
       " (('about', 'stopword'), 0.02348855882883072),\n",
       " (('is', 'stopword'), -0.02236570231616497),\n",
       " (('anticipation', 'nrc'), 0.015547719784080982),\n",
       " (('before', 'stopword'), 0.015047989785671234),\n",
       " (('don', 'stopword'), -0.01425930019468069),\n",
       " (('nonfl', 'liwc'), 0.012189127504825592),\n",
       " (('all', 'stopword'), -0.010963043197989464),\n",
       " (('inhib', 'liwc'), 0.010879608802497387),\n",
       " (('then', 'stopword'), 0.010646040551364422),\n",
       " (('not', 'stopword'), -0.008417755365371704),\n",
       " (('are', 'stopword'), 0.005582477897405624),\n",
       " (('filler', 'liwc'), -0.003179051447659731),\n",
       " ((\"wouldn't\", 'stopword'), -5.453806545406998e-32),\n",
       " ((\"mightn't\", 'stopword'), -5.257430810147328e-32),\n",
       " ((\"she's\", 'stopword'), -5.103391091679699e-32),\n",
       " ((\"you'd\", 'stopword'), 4.962030255280038e-32),\n",
       " ((\"hasn't\", 'stopword'), 4.9146654736551806e-32),\n",
       " ((\"doesn't\", 'stopword'), -4.845891707292384e-32),\n",
       " ((\"mustn't\", 'stopword'), -4.448507907410042e-32),\n",
       " ((\"shouldn't\", 'stopword'), 3.925458471700218e-32),\n",
       " ((\"you're\", 'stopword'), 3.396222115348429e-32),\n",
       " ((\"hadn't\", 'stopword'), 3.229437981002772e-32),\n",
       " ((\"shan't\", 'stopword'), 3.145352519099752e-32),\n",
       " ((\"don't\", 'stopword'), 3.0496899072090714e-32),\n",
       " ((\"isn't\", 'stopword'), -2.9659917706946474e-32),\n",
       " (('mustn', 'stopword'), -2.8515056738906616e-32),\n",
       " (('mightn', 'stopword'), -2.440059582026226e-32),\n",
       " ((\"weren't\", 'stopword'), -2.0919948231269836e-32),\n",
       " ((\"should've\", 'stopword'), 1.7811875046138474e-32),\n",
       " ((\"couldn't\", 'stopword'), 1.446776994220169e-32),\n",
       " ((\"you'll\", 'stopword'), 1.3835706629764546e-32),\n",
       " ((\"haven't\", 'stopword'), -1.221902280045576e-32),\n",
       " ((\"won't\", 'stopword'), -1.134160883774705e-32),\n",
       " ((\"it's\", 'stopword'), -1.1217034354550719e-32),\n",
       " ((\"needn't\", 'stopword'), -1.0666406352002666e-32),\n",
       " ((\"aren't\", 'stopword'), 9.390903145864326e-33),\n",
       " ((\"you've\", 'stopword'), 7.764519284109349e-33),\n",
       " ((\"that'll\", 'stopword'), 5.1391261199446964e-33),\n",
       " ((\"wasn't\", 'stopword'), -4.209051479687086e-33),\n",
       " ((\"didn't\", 'stopword'), -3.883667296539784e-33)]"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.get_weights()[0].tolist()\n",
    "features = [(e, 'nrc') for e in emotions] + ['pers_pronouns'] + [\n",
    "    (c, 'liwc') for c in list(categories)] + [(st, 'stopword') for st in stopword_list]\n",
    "print(len(weights), len(features))\n",
    "feature_importance = {}\n",
    "for (i, f) in enumerate(features):\n",
    "    feature_importance[f] = weights[i][0]\n",
    "\n",
    "sorted(feature_importance.items(), key=lambda t: abs(t[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "save() got an unexpected keyword argument 'save_weights_only'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-324-694b85eaee79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/lstm_plus_ablated_user'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_weights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: save() got an unexpected keyword argument 'save_weights_only'"
     ]
    }
   ],
   "source": [
    "model.save('models/lstm_plus_ablated_user', save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5631"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5644"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "voc2=pickle.load(open('vocabulary20K_selfharm.pkl', 'rb'))\n",
    "voc2['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(predictions.flatten()).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([84])"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(predictions>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19])"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(predictions<0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_point(subject, voc, hyperparams_features=hyperparams_features, nrc_lexicon=nrc_lexicon,\n",
    "                      emotions=emotions):\n",
    "    eval_writings_df = writings_df[writings_df['subject']==subject]\n",
    "    correct_label = eval_writings_df.label.values[0]\n",
    "    (x_train, y_train), (x_valid, y_valid), (x_test, y_test), voc = load_erisk_data(eval_writings_df,\n",
    "                        seq_len=hyperparams_features['maxlen'],\n",
    "                        voc_size=hyperparams_features['max_features'],\n",
    "                        emotion_lexicon=nrc_lexicon,\n",
    "                        emotions=emotions, user_level=False,\n",
    "                        train_prop=0.0, vocabulary=voc)\n",
    "    return x_test, y_test, correct_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_per_user(writings_df, majority_prop=0.2, train_prop=0.7, majority_nr=0, validate=False, voc=None):\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    thresh=0.5\n",
    "    majority_proportion=majority_prop\n",
    "    valid_prop = 0.3\n",
    "    \n",
    "    if 'subset' in writings_df.columns:\n",
    "        training_subjects = list(set(writings_df[writings_df['subset']=='train'].subject))\n",
    "        test_subjects = list(set(writings_df[writings_df['subset']=='test'].subject))\n",
    "    else:\n",
    "        all_subjects = sorted(list(set(writings_df.subject)))\n",
    "        training_subjects_size = int(len(all_subjects) * train_prop)\n",
    "        training_subjects = all_subjects[:training_subjects_size]\n",
    "        test_subjects = all_subjects[training_subjects_size:]\n",
    "    training_subjects = sorted(training_subjects) # ensuring reproducibility\n",
    "    valid_subjects_size = int(len(training_subjects) * valid_prop)\n",
    "    valid_subjects = training_subjects[:valid_subjects_size]\n",
    "    training_subjects = training_subjects[valid_subjects_size:]\n",
    "    \n",
    "    if validate:\n",
    "        subjects = valid_subjects\n",
    "    else:\n",
    "        subjects = test_subjects\n",
    "    for subject in subjects:\n",
    "        x_test_user, y_test_user, label = get_data_for_point(subject, voc=voc)\n",
    "        outputs = model.predict(x_test_user)\n",
    "        positive_pred = sum(outputs>=thresh)\n",
    "        negative_pred = sum(outputs<thresh)\n",
    "        majority_pred = 0\n",
    "        if majority_proportion and positive_pred >= majority_proportion*negative_pred:\n",
    "            majority_pred = 1\n",
    "        if majority_nr and positive_pred>=majority_nr:\n",
    "            majority_pred = 1\n",
    "        if label == 1:\n",
    "            if majority_pred == 1:\n",
    "                tp+=1\n",
    "            else:\n",
    "                fn+=1\n",
    "        else:\n",
    "            if majority_pred == 0:\n",
    "                tn+=1\n",
    "            else:\n",
    "                fp+=1\n",
    "        print(negative_pred, positive_pred, majority_pred)\n",
    "        all_predictions.append(majority_pred)\n",
    "        all_labels.append(label)\n",
    "    def prec_recall_f1(tp, fp, tn, fn):\n",
    "        recall = tp/(tp+fn+0.0000001)\n",
    "        precision = tp/(tp+fp+0.0000001)\n",
    "        f1 = 2*precision*recall/(precision+recall+0.0000001)\n",
    "        print(\"Recall\", recall, \"Precision\", precision, \"F1\", f1)\n",
    "    if majority_prop:\n",
    "        print(\"Vote proportion\", majority_prop)\n",
    "    if majority_nr:\n",
    "        print(\"Vote points\", majority_nr)\n",
    "    prec_recall_f1(tp, fp, tn, fn)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[339] [100] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[52] [33] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[451] [52] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1046] [301] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[147] [39] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[104] [8] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1076] [182] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1813] [63] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[227] [65] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[106] [8] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[437] [43] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[7] [2] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[145] [13] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[178] [21] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[822] [200] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[786] [98] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[89] [18] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[61] [9] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[35] [8] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[543] [182] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[886] [93] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[141] [31] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[265] [63] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[93] [8] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[291] [109] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[22] [10] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1013] [102] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[332] [34] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[63] [33] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[17] [2] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[155] [5] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[52] [10] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1109] [77] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[21] [6] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[122] [44] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[480] [105] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[997] [241] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[260] [26] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[14] [2] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[836] [134] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[818] [130] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[364] [30] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[22] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[698] [192] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1193] [187] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1104] [68] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[16] [13] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[18] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[37] [9] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[12] [4] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[38] [18] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1018] [135] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[214] [95] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[102] [17] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[177] [101] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[232] [36] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[30] [25] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[21] [7] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1031] [117] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[465] [117] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[339] [132] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[46] [3] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[46] [6] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[95] [2] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[988] [103] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[201] [43] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[31] [3] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[301] [15] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[873] [52] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1202] [204] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[290] [41] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1455] [144] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[19] [10] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[31] [74] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[365] [55] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[39] [13] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[417] [72] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[35] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[110] [25] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[120] [24] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[28] [7] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[31] [8] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[372] [158] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1404] [158] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[28] [5] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[15] [7] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[82] [13] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[21] [25] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[125] [16] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[10] [2] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[46] [9] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[31] [4] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[103] [24] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[713] [105] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[89] [9] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[217] [33] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[112] [10] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[77] [17] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[31] [117] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[241] [48] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[903] [160] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[527] [64] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[82] [18] 1\n",
      "Vote proportion 0.2\n",
      "Recall 0.9411764650519031 Precision 0.3478260862003781 F1 0.5079364669186223\n"
     ]
    }
   ],
   "source": [
    "predict_per_user(writings_df=writings_df, voc=voc, majority_prop=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tokens(row):\n",
    "    tokens = []\n",
    "    if row.tokenized_text:\n",
    "        tokens += row.tokenized_text\n",
    "    if row.tokenized_title:\n",
    "        tokens += row.tokenized_title\n",
    "    return tokens\n",
    "writings_df['all_tokens'] = writings_df.apply (lambda row: merge_tokens(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: include the title\n",
    "def extract_emotions(tokens, emotion, relative=True):\n",
    "    if not tokens:\n",
    "        return None\n",
    "    emotion_words = [t for t in tokens \n",
    "                     if t in nrc_lexicon[emotion]]\n",
    "    if relative:\n",
    "        return len(emotion_words) / len(tokens)\n",
    "    else:\n",
    "        return len(emotion_words)\n",
    "    \n",
    "    return encoded_emotions\n",
    "\n",
    "from functools import partial\n",
    "for emotion in emotions:\n",
    "    writings_df[emotion] = writings_df['all_tokens'].apply(partial(extract_emotions, emotion=emotion, \n",
    "                                                                   relative=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['pronouns'] = writings_df['all_tokens'].apply(partial(encode_pronouns, relative=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>text_len</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.104269</td>\n",
       "      <td>0.011986</td>\n",
       "      <td>0.020197</td>\n",
       "      <td>0.031982</td>\n",
       "      <td>0.031271</td>\n",
       "      <td>0.019335</td>\n",
       "      <td>0.040782</td>\n",
       "      <td>0.023853</td>\n",
       "      <td>0.023621</td>\n",
       "      <td>0.032969</td>\n",
       "      <td>0.020421</td>\n",
       "      <td>0.023590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pronouns</th>\n",
       "      <td>0.104269</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.636745</td>\n",
       "      <td>0.449384</td>\n",
       "      <td>0.567496</td>\n",
       "      <td>0.452098</td>\n",
       "      <td>0.464899</td>\n",
       "      <td>0.548570</td>\n",
       "      <td>0.513029</td>\n",
       "      <td>0.571303</td>\n",
       "      <td>0.524614</td>\n",
       "      <td>0.461328</td>\n",
       "      <td>0.538335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_len</th>\n",
       "      <td>0.011986</td>\n",
       "      <td>0.636745</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.708853</td>\n",
       "      <td>0.791715</td>\n",
       "      <td>0.642980</td>\n",
       "      <td>0.738146</td>\n",
       "      <td>0.728836</td>\n",
       "      <td>0.823974</td>\n",
       "      <td>0.867609</td>\n",
       "      <td>0.723653</td>\n",
       "      <td>0.650420</td>\n",
       "      <td>0.834939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.020197</td>\n",
       "      <td>0.449384</td>\n",
       "      <td>0.708853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.643459</td>\n",
       "      <td>0.762591</td>\n",
       "      <td>0.858442</td>\n",
       "      <td>0.564162</td>\n",
       "      <td>0.835345</td>\n",
       "      <td>0.681573</td>\n",
       "      <td>0.774846</td>\n",
       "      <td>0.583704</td>\n",
       "      <td>0.671042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anticipation</th>\n",
       "      <td>0.031982</td>\n",
       "      <td>0.567496</td>\n",
       "      <td>0.791715</td>\n",
       "      <td>0.643459</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.573916</td>\n",
       "      <td>0.668326</td>\n",
       "      <td>0.834784</td>\n",
       "      <td>0.684882</td>\n",
       "      <td>0.849864</td>\n",
       "      <td>0.668269</td>\n",
       "      <td>0.727331</td>\n",
       "      <td>0.818885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.031271</td>\n",
       "      <td>0.452098</td>\n",
       "      <td>0.642980</td>\n",
       "      <td>0.762591</td>\n",
       "      <td>0.573916</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.729799</td>\n",
       "      <td>0.526733</td>\n",
       "      <td>0.765865</td>\n",
       "      <td>0.603013</td>\n",
       "      <td>0.737717</td>\n",
       "      <td>0.540439</td>\n",
       "      <td>0.589641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.019335</td>\n",
       "      <td>0.464899</td>\n",
       "      <td>0.738146</td>\n",
       "      <td>0.858442</td>\n",
       "      <td>0.668326</td>\n",
       "      <td>0.729799</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.570632</td>\n",
       "      <td>0.862778</td>\n",
       "      <td>0.706676</td>\n",
       "      <td>0.824782</td>\n",
       "      <td>0.569688</td>\n",
       "      <td>0.687232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.040782</td>\n",
       "      <td>0.548570</td>\n",
       "      <td>0.728836</td>\n",
       "      <td>0.564162</td>\n",
       "      <td>0.834784</td>\n",
       "      <td>0.526733</td>\n",
       "      <td>0.570632</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.604964</td>\n",
       "      <td>0.850961</td>\n",
       "      <td>0.603296</td>\n",
       "      <td>0.722710</td>\n",
       "      <td>0.811529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0.023853</td>\n",
       "      <td>0.513029</td>\n",
       "      <td>0.823974</td>\n",
       "      <td>0.835345</td>\n",
       "      <td>0.684882</td>\n",
       "      <td>0.765865</td>\n",
       "      <td>0.862778</td>\n",
       "      <td>0.604964</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.735431</td>\n",
       "      <td>0.840379</td>\n",
       "      <td>0.597634</td>\n",
       "      <td>0.706808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>0.023621</td>\n",
       "      <td>0.571303</td>\n",
       "      <td>0.867609</td>\n",
       "      <td>0.681573</td>\n",
       "      <td>0.849864</td>\n",
       "      <td>0.603013</td>\n",
       "      <td>0.706676</td>\n",
       "      <td>0.850961</td>\n",
       "      <td>0.735431</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.702751</td>\n",
       "      <td>0.678778</td>\n",
       "      <td>0.916526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.032969</td>\n",
       "      <td>0.524614</td>\n",
       "      <td>0.723653</td>\n",
       "      <td>0.774846</td>\n",
       "      <td>0.668269</td>\n",
       "      <td>0.737717</td>\n",
       "      <td>0.824782</td>\n",
       "      <td>0.603296</td>\n",
       "      <td>0.840379</td>\n",
       "      <td>0.702751</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.584816</td>\n",
       "      <td>0.665750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.020421</td>\n",
       "      <td>0.461328</td>\n",
       "      <td>0.650420</td>\n",
       "      <td>0.583704</td>\n",
       "      <td>0.727331</td>\n",
       "      <td>0.540439</td>\n",
       "      <td>0.569688</td>\n",
       "      <td>0.722710</td>\n",
       "      <td>0.597634</td>\n",
       "      <td>0.678778</td>\n",
       "      <td>0.584816</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.660681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust</th>\n",
       "      <td>0.023590</td>\n",
       "      <td>0.538335</td>\n",
       "      <td>0.834939</td>\n",
       "      <td>0.671042</td>\n",
       "      <td>0.818885</td>\n",
       "      <td>0.589641</td>\n",
       "      <td>0.687232</td>\n",
       "      <td>0.811529</td>\n",
       "      <td>0.706808</td>\n",
       "      <td>0.916526</td>\n",
       "      <td>0.665750</td>\n",
       "      <td>0.660681</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 label  pronouns  text_len     anger  anticipation   disgust  \\\n",
       "label         1.000000  0.104269  0.011986  0.020197      0.031982  0.031271   \n",
       "pronouns      0.104269  1.000000  0.636745  0.449384      0.567496  0.452098   \n",
       "text_len      0.011986  0.636745  1.000000  0.708853      0.791715  0.642980   \n",
       "anger         0.020197  0.449384  0.708853  1.000000      0.643459  0.762591   \n",
       "anticipation  0.031982  0.567496  0.791715  0.643459      1.000000  0.573916   \n",
       "disgust       0.031271  0.452098  0.642980  0.762591      0.573916  1.000000   \n",
       "fear          0.019335  0.464899  0.738146  0.858442      0.668326  0.729799   \n",
       "joy           0.040782  0.548570  0.728836  0.564162      0.834784  0.526733   \n",
       "negative      0.023853  0.513029  0.823974  0.835345      0.684882  0.765865   \n",
       "positive      0.023621  0.571303  0.867609  0.681573      0.849864  0.603013   \n",
       "sadness       0.032969  0.524614  0.723653  0.774846      0.668269  0.737717   \n",
       "surprise      0.020421  0.461328  0.650420  0.583704      0.727331  0.540439   \n",
       "trust         0.023590  0.538335  0.834939  0.671042      0.818885  0.589641   \n",
       "\n",
       "                  fear       joy  negative  positive   sadness  surprise  \\\n",
       "label         0.019335  0.040782  0.023853  0.023621  0.032969  0.020421   \n",
       "pronouns      0.464899  0.548570  0.513029  0.571303  0.524614  0.461328   \n",
       "text_len      0.738146  0.728836  0.823974  0.867609  0.723653  0.650420   \n",
       "anger         0.858442  0.564162  0.835345  0.681573  0.774846  0.583704   \n",
       "anticipation  0.668326  0.834784  0.684882  0.849864  0.668269  0.727331   \n",
       "disgust       0.729799  0.526733  0.765865  0.603013  0.737717  0.540439   \n",
       "fear          1.000000  0.570632  0.862778  0.706676  0.824782  0.569688   \n",
       "joy           0.570632  1.000000  0.604964  0.850961  0.603296  0.722710   \n",
       "negative      0.862778  0.604964  1.000000  0.735431  0.840379  0.597634   \n",
       "positive      0.706676  0.850961  0.735431  1.000000  0.702751  0.678778   \n",
       "sadness       0.824782  0.603296  0.840379  0.702751  1.000000  0.584816   \n",
       "surprise      0.569688  0.722710  0.597634  0.678778  0.584816  1.000000   \n",
       "trust         0.687232  0.811529  0.706808  0.916526  0.665750  0.660681   \n",
       "\n",
       "                 trust  \n",
       "label         0.023590  \n",
       "pronouns      0.538335  \n",
       "text_len      0.834939  \n",
       "anger         0.671042  \n",
       "anticipation  0.818885  \n",
       "disgust       0.589641  \n",
       "fear          0.687232  \n",
       "joy           0.811529  \n",
       "negative      0.706808  \n",
       "positive      0.916526  \n",
       "sadness       0.665750  \n",
       "surprise      0.660681  \n",
       "trust         1.000000  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len'] + emotions].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pronouns</th>\n",
       "      <th>text_len</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.868213</td>\n",
       "      <td>32.031615</td>\n",
       "      <td>0.386069</td>\n",
       "      <td>0.58984</td>\n",
       "      <td>0.263683</td>\n",
       "      <td>0.478014</td>\n",
       "      <td>0.479908</td>\n",
       "      <td>0.818800</td>\n",
       "      <td>1.280788</td>\n",
       "      <td>0.385315</td>\n",
       "      <td>0.284790</td>\n",
       "      <td>0.830560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.484271</td>\n",
       "      <td>36.398389</td>\n",
       "      <td>0.529232</td>\n",
       "      <td>0.86985</td>\n",
       "      <td>0.416203</td>\n",
       "      <td>0.654371</td>\n",
       "      <td>0.769766</td>\n",
       "      <td>1.152422</td>\n",
       "      <td>1.717428</td>\n",
       "      <td>0.627088</td>\n",
       "      <td>0.375418</td>\n",
       "      <td>1.128341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pronouns   text_len     anger  anticipation   disgust      fear  \\\n",
       "label                                                                    \n",
       "0      0.868213  32.031615  0.386069       0.58984  0.263683  0.478014   \n",
       "1      2.484271  36.398389  0.529232       0.86985  0.416203  0.654371   \n",
       "\n",
       "            joy  negative  positive   sadness  surprise     trust  \n",
       "label                                                              \n",
       "0      0.479908  0.818800  1.280788  0.385315  0.284790  0.830560  \n",
       "1      0.769766  1.152422  1.717428  0.627088  0.375418  1.128341  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len'] + emotions].groupby('label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentAnalyzer, SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.66, 'pos': 0.34, 'compound': 0.5574}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid.polarity_scores(\"We are here today happiness is all around\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['neg_vader'] = writings_df.text.apply(lambda t: sid.polarity_scores(t)['neg']\n",
    "                                                 if type(t)==str else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>title_len</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>text_len</th>\n",
       "      <th>emotions</th>\n",
       "      <th>...</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>all_tokens</th>\n",
       "      <th>neg_vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>If anyone could help with which sub to put thi...</td>\n",
       "      <td>2016-08-02 09:22:12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[if, anyone, could, help, with, which, sub, to...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[if, anyone, could, help, with, which, sub, to...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>I'm literally never gonna stop waiting...</td>\n",
       "      <td>2016-08-05 09:35:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, m, literally, never, gonna, stop, waiting]</td>\n",
       "      <td>7.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[i, m, literally, never, gonna, stop, waiting]</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>This is a really interesting study! Makes sens...</td>\n",
       "      <td>2016-08-05 21:36:24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[this, is, a, really, interesting, study, make...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[this, is, a, really, interesting, study, make...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>The only thing Frank is building ...</td>\n",
       "      <td>2016-08-07 23:35:23</td>\n",
       "      <td>... Is hype. Think about it, every time he wor...</td>\n",
       "      <td>0</td>\n",
       "      <td>[the, only, thing, frank, is, building]</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[is, hype, think, about, it, every, time, he, ...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[is, hype, think, about, it, every, time, he, ...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>Mostly always me during this whole charade</td>\n",
       "      <td>2016-08-09 08:39:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[mostly, always, me, during, this, whole, char...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[mostly, always, me, during, this, whole, char...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170693</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 11:29:21</td>\n",
       "      <td>this is my personal experience ,it may not ref...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[this, is, my, personal, experience, it, may, ...</td>\n",
       "      <td>153.0</td>\n",
       "      <td>0.026144</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[this, is, my, personal, experience, it, may, ...</td>\n",
       "      <td>0.089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170694</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 16:17:34</td>\n",
       "      <td>stop looking at 20 million saudis as one entit...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[stop, looking, at, 20, million, saudis, as, o...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[stop, looking, at, 20, million, saudis, as, o...</td>\n",
       "      <td>0.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170695</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 20:00:31</td>\n",
       "      <td>i am aware of stats now and then. i was just s...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[i, am, aware, of, stats, now, and, then, i, w...</td>\n",
       "      <td>198.0</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>[i, am, aware, of, stats, now, and, then, i, w...</td>\n",
       "      <td>0.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170696</th>\n",
       "      <td>subject217</td>\n",
       "      <td>WHAT DID YOU SAY TO ME?</td>\n",
       "      <td>2018-08-20 10:54:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[what, did, you, say, to, me]</td>\n",
       "      <td>6.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[what, did, you, say, to, me]</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170697</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-20 12:07:44</td>\n",
       "      <td>me smellz fish,me find no fish!...what the fuc...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[me, smellz, fish, me, find, no, fish, what, t...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[me, smellz, fish, me, find, no, fish, what, t...</td>\n",
       "      <td>0.484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170698 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            subject                                              title  \\\n",
       "0       subject8292  If anyone could help with which sub to put thi...   \n",
       "1       subject8292          I'm literally never gonna stop waiting...   \n",
       "2       subject8292  This is a really interesting study! Makes sens...   \n",
       "3       subject8292               The only thing Frank is building ...   \n",
       "4       subject8292         Mostly always me during this whole charade   \n",
       "...             ...                                                ...   \n",
       "170693   subject217                                                NaN   \n",
       "170694   subject217                                                NaN   \n",
       "170695   subject217                                                NaN   \n",
       "170696   subject217                            WHAT DID YOU SAY TO ME?   \n",
       "170697   subject217                                                NaN   \n",
       "\n",
       "                       date  \\\n",
       "0       2016-08-02 09:22:12   \n",
       "1       2016-08-05 09:35:55   \n",
       "2       2016-08-05 21:36:24   \n",
       "3       2016-08-07 23:35:23   \n",
       "4       2016-08-09 08:39:41   \n",
       "...                     ...   \n",
       "170693  2018-08-19 11:29:21   \n",
       "170694  2018-08-19 16:17:34   \n",
       "170695  2018-08-19 20:00:31   \n",
       "170696  2018-08-20 10:54:11   \n",
       "170697  2018-08-20 12:07:44   \n",
       "\n",
       "                                                     text  label  \\\n",
       "0                                                     NaN      0   \n",
       "1                                                     NaN      0   \n",
       "2                                                     NaN      0   \n",
       "3       ... Is hype. Think about it, every time he wor...      0   \n",
       "4                                                     NaN      0   \n",
       "...                                                   ...    ...   \n",
       "170693  this is my personal experience ,it may not ref...      0   \n",
       "170694  stop looking at 20 million saudis as one entit...      0   \n",
       "170695  i am aware of stats now and then. i was just s...      0   \n",
       "170696                                                NaN      0   \n",
       "170697  me smellz fish,me find no fish!...what the fuc...      0   \n",
       "\n",
       "                                          tokenized_title  title_len  \\\n",
       "0       [if, anyone, could, help, with, which, sub, to...       11.0   \n",
       "1          [i, m, literally, never, gonna, stop, waiting]        7.0   \n",
       "2       [this, is, a, really, interesting, study, make...        9.0   \n",
       "3                 [the, only, thing, frank, is, building]        6.0   \n",
       "4       [mostly, always, me, during, this, whole, char...        7.0   \n",
       "...                                                   ...        ...   \n",
       "170693                                               None        NaN   \n",
       "170694                                               None        NaN   \n",
       "170695                                               None        NaN   \n",
       "170696                      [what, did, you, say, to, me]        6.0   \n",
       "170697                                               None        NaN   \n",
       "\n",
       "                                           tokenized_text  text_len  emotions  \\\n",
       "0                                                    None       NaN       NaN   \n",
       "1                                                    None       NaN       NaN   \n",
       "2                                                    None       NaN       NaN   \n",
       "3       [is, hype, think, about, it, every, time, he, ...      26.0  0.000000   \n",
       "4                                                    None       NaN       NaN   \n",
       "...                                                   ...       ...       ...   \n",
       "170693  [this, is, my, personal, experience, it, may, ...     153.0  0.026144   \n",
       "170694  [stop, looking, at, 20, million, saudis, as, o...      15.0  0.000000   \n",
       "170695  [i, am, aware, of, stats, now, and, then, i, w...     198.0  0.030303   \n",
       "170696                                               None       NaN       NaN   \n",
       "170697  [me, smellz, fish, me, find, no, fish, what, t...      11.0  0.000000   \n",
       "\n",
       "        ...  fear  joy  negative  positive  sadness  surprise  trust  \\\n",
       "0       ...   0.0  0.0       0.0       0.0      0.0       0.0    0.0   \n",
       "1       ...   0.0  0.0       0.0       0.0      0.0       0.0    0.0   \n",
       "2       ...   0.0  0.0       0.0       3.0      0.0       0.0    0.0   \n",
       "3       ...   0.0  0.0       3.0       3.0      0.0       0.0    1.0   \n",
       "4       ...   0.0  0.0       1.0       0.0      0.0       0.0    0.0   \n",
       "...     ...   ...  ...       ...       ...      ...       ...    ...   \n",
       "170693  ...   1.0  1.0       1.0       7.0      0.0       1.0    4.0   \n",
       "170694  ...   1.0  0.0       1.0       0.0      0.0       0.0    0.0   \n",
       "170695  ...   2.0  3.0       4.0      11.0      3.0       0.0    6.0   \n",
       "170696  ...   0.0  0.0       0.0       0.0      0.0       0.0    0.0   \n",
       "170697  ...   0.0  0.0       0.0       0.0      0.0       0.0    0.0   \n",
       "\n",
       "        pronouns                                         all_tokens  neg_vader  \n",
       "0            0.0  [if, anyone, could, help, with, which, sub, to...      0.000  \n",
       "1            1.0     [i, m, literally, never, gonna, stop, waiting]      0.000  \n",
       "2            0.0  [this, is, a, really, interesting, study, make...      0.000  \n",
       "3            0.0  [is, hype, think, about, it, every, time, he, ...      0.000  \n",
       "4            1.0  [mostly, always, me, during, this, whole, char...      0.000  \n",
       "...          ...                                                ...        ...  \n",
       "170693       4.0  [this, is, my, personal, experience, it, may, ...      0.089  \n",
       "170694       0.0  [stop, looking, at, 20, million, saudis, as, o...      0.145  \n",
       "170695      16.0  [i, am, aware, of, stats, now, and, then, i, w...      0.070  \n",
       "170696       1.0                      [what, did, you, say, to, me]      0.000  \n",
       "170697       2.0  [me, smellz, fish, me, find, no, fish, what, t...      0.484  \n",
       "\n",
       "[170698 rows x 23 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['pos_vader'] = writings_df.text.apply(lambda t: sid.polarity_scores(t)['pos']\n",
    "                                                 if type(t)==str else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pronouns</th>\n",
       "      <th>text_len</th>\n",
       "      <th>neg_vader</th>\n",
       "      <th>pos_vader</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.868213</td>\n",
       "      <td>32.031615</td>\n",
       "      <td>0.054259</td>\n",
       "      <td>0.109981</td>\n",
       "      <td>0.386069</td>\n",
       "      <td>0.58984</td>\n",
       "      <td>0.263683</td>\n",
       "      <td>0.478014</td>\n",
       "      <td>0.479908</td>\n",
       "      <td>0.818800</td>\n",
       "      <td>1.280788</td>\n",
       "      <td>0.385315</td>\n",
       "      <td>0.284790</td>\n",
       "      <td>0.830560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.484271</td>\n",
       "      <td>36.398389</td>\n",
       "      <td>0.079191</td>\n",
       "      <td>0.148154</td>\n",
       "      <td>0.529232</td>\n",
       "      <td>0.86985</td>\n",
       "      <td>0.416203</td>\n",
       "      <td>0.654371</td>\n",
       "      <td>0.769766</td>\n",
       "      <td>1.152422</td>\n",
       "      <td>1.717428</td>\n",
       "      <td>0.627088</td>\n",
       "      <td>0.375418</td>\n",
       "      <td>1.128341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pronouns   text_len  neg_vader  pos_vader     anger  anticipation  \\\n",
       "label                                                                      \n",
       "0      0.868213  32.031615   0.054259   0.109981  0.386069       0.58984   \n",
       "1      2.484271  36.398389   0.079191   0.148154  0.529232       0.86985   \n",
       "\n",
       "        disgust      fear       joy  negative  positive   sadness  surprise  \\\n",
       "label                                                                         \n",
       "0      0.263683  0.478014  0.479908  0.818800  1.280788  0.385315  0.284790   \n",
       "1      0.416203  0.654371  0.769766  1.152422  1.717428  0.627088  0.375418   \n",
       "\n",
       "          trust  \n",
       "label            \n",
       "0      0.830560  \n",
       "1      1.128341  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len', 'neg_vader', 'pos_vader'] + emotions].groupby('label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>text_len</th>\n",
       "      <th>neg_vader</th>\n",
       "      <th>pos_vader</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.097800</td>\n",
       "      <td>0.033477</td>\n",
       "      <td>0.067170</td>\n",
       "      <td>0.065211</td>\n",
       "      <td>0.022057</td>\n",
       "      <td>0.025666</td>\n",
       "      <td>0.030664</td>\n",
       "      <td>0.019114</td>\n",
       "      <td>0.033977</td>\n",
       "      <td>0.022934</td>\n",
       "      <td>0.019590</td>\n",
       "      <td>0.032641</td>\n",
       "      <td>0.018109</td>\n",
       "      <td>0.024014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pronouns</th>\n",
       "      <td>0.097800</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.332071</td>\n",
       "      <td>0.193938</td>\n",
       "      <td>0.221419</td>\n",
       "      <td>0.076345</td>\n",
       "      <td>0.128030</td>\n",
       "      <td>0.094069</td>\n",
       "      <td>0.063176</td>\n",
       "      <td>0.144011</td>\n",
       "      <td>0.076670</td>\n",
       "      <td>0.106055</td>\n",
       "      <td>0.100827</td>\n",
       "      <td>0.106790</td>\n",
       "      <td>0.122914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_len</th>\n",
       "      <td>0.033477</td>\n",
       "      <td>0.332071</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.343154</td>\n",
       "      <td>0.159673</td>\n",
       "      <td>0.360460</td>\n",
       "      <td>0.386351</td>\n",
       "      <td>0.312393</td>\n",
       "      <td>0.381410</td>\n",
       "      <td>0.339398</td>\n",
       "      <td>0.370250</td>\n",
       "      <td>0.330075</td>\n",
       "      <td>0.384031</td>\n",
       "      <td>0.349498</td>\n",
       "      <td>0.389620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_vader</th>\n",
       "      <td>0.067170</td>\n",
       "      <td>0.193938</td>\n",
       "      <td>0.343154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.169624</td>\n",
       "      <td>0.384510</td>\n",
       "      <td>0.141868</td>\n",
       "      <td>0.362582</td>\n",
       "      <td>0.339245</td>\n",
       "      <td>0.126042</td>\n",
       "      <td>0.431111</td>\n",
       "      <td>0.099767</td>\n",
       "      <td>0.374256</td>\n",
       "      <td>0.159302</td>\n",
       "      <td>0.143060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_vader</th>\n",
       "      <td>0.065211</td>\n",
       "      <td>0.221419</td>\n",
       "      <td>0.159673</td>\n",
       "      <td>0.169624</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.079693</td>\n",
       "      <td>0.225925</td>\n",
       "      <td>0.087309</td>\n",
       "      <td>0.071450</td>\n",
       "      <td>0.323148</td>\n",
       "      <td>0.058266</td>\n",
       "      <td>0.270687</td>\n",
       "      <td>0.095040</td>\n",
       "      <td>0.186243</td>\n",
       "      <td>0.231954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.022057</td>\n",
       "      <td>0.076345</td>\n",
       "      <td>0.360460</td>\n",
       "      <td>0.384510</td>\n",
       "      <td>0.079693</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.196795</td>\n",
       "      <td>0.583864</td>\n",
       "      <td>0.587460</td>\n",
       "      <td>0.157202</td>\n",
       "      <td>0.631708</td>\n",
       "      <td>0.128169</td>\n",
       "      <td>0.528980</td>\n",
       "      <td>0.273195</td>\n",
       "      <td>0.169261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anticipation</th>\n",
       "      <td>0.025666</td>\n",
       "      <td>0.128030</td>\n",
       "      <td>0.386351</td>\n",
       "      <td>0.141868</td>\n",
       "      <td>0.225925</td>\n",
       "      <td>0.196795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.164649</td>\n",
       "      <td>0.241958</td>\n",
       "      <td>0.583107</td>\n",
       "      <td>0.178827</td>\n",
       "      <td>0.452457</td>\n",
       "      <td>0.198972</td>\n",
       "      <td>0.460851</td>\n",
       "      <td>0.469028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.030664</td>\n",
       "      <td>0.094069</td>\n",
       "      <td>0.312393</td>\n",
       "      <td>0.362582</td>\n",
       "      <td>0.087309</td>\n",
       "      <td>0.583864</td>\n",
       "      <td>0.164649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.440376</td>\n",
       "      <td>0.152731</td>\n",
       "      <td>0.552021</td>\n",
       "      <td>0.116588</td>\n",
       "      <td>0.490181</td>\n",
       "      <td>0.232166</td>\n",
       "      <td>0.153723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.019114</td>\n",
       "      <td>0.063176</td>\n",
       "      <td>0.381410</td>\n",
       "      <td>0.339245</td>\n",
       "      <td>0.071450</td>\n",
       "      <td>0.587460</td>\n",
       "      <td>0.241958</td>\n",
       "      <td>0.440376</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.159907</td>\n",
       "      <td>0.576962</td>\n",
       "      <td>0.141985</td>\n",
       "      <td>0.583703</td>\n",
       "      <td>0.248160</td>\n",
       "      <td>0.184240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.033977</td>\n",
       "      <td>0.144011</td>\n",
       "      <td>0.339398</td>\n",
       "      <td>0.126042</td>\n",
       "      <td>0.323148</td>\n",
       "      <td>0.157202</td>\n",
       "      <td>0.583107</td>\n",
       "      <td>0.152731</td>\n",
       "      <td>0.159907</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.113400</td>\n",
       "      <td>0.645827</td>\n",
       "      <td>0.176440</td>\n",
       "      <td>0.477317</td>\n",
       "      <td>0.582920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0.022934</td>\n",
       "      <td>0.076670</td>\n",
       "      <td>0.370250</td>\n",
       "      <td>0.431111</td>\n",
       "      <td>0.058266</td>\n",
       "      <td>0.631708</td>\n",
       "      <td>0.178827</td>\n",
       "      <td>0.552021</td>\n",
       "      <td>0.576962</td>\n",
       "      <td>0.113400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.105821</td>\n",
       "      <td>0.612781</td>\n",
       "      <td>0.226230</td>\n",
       "      <td>0.145220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>0.019590</td>\n",
       "      <td>0.106055</td>\n",
       "      <td>0.330075</td>\n",
       "      <td>0.099767</td>\n",
       "      <td>0.270687</td>\n",
       "      <td>0.128169</td>\n",
       "      <td>0.452457</td>\n",
       "      <td>0.116588</td>\n",
       "      <td>0.141985</td>\n",
       "      <td>0.645827</td>\n",
       "      <td>0.105821</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.139827</td>\n",
       "      <td>0.333998</td>\n",
       "      <td>0.648163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.032641</td>\n",
       "      <td>0.100827</td>\n",
       "      <td>0.384031</td>\n",
       "      <td>0.374256</td>\n",
       "      <td>0.095040</td>\n",
       "      <td>0.528980</td>\n",
       "      <td>0.198972</td>\n",
       "      <td>0.490181</td>\n",
       "      <td>0.583703</td>\n",
       "      <td>0.176440</td>\n",
       "      <td>0.612781</td>\n",
       "      <td>0.139827</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.265026</td>\n",
       "      <td>0.171245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.018109</td>\n",
       "      <td>0.106790</td>\n",
       "      <td>0.349498</td>\n",
       "      <td>0.159302</td>\n",
       "      <td>0.186243</td>\n",
       "      <td>0.273195</td>\n",
       "      <td>0.460851</td>\n",
       "      <td>0.232166</td>\n",
       "      <td>0.248160</td>\n",
       "      <td>0.477317</td>\n",
       "      <td>0.226230</td>\n",
       "      <td>0.333998</td>\n",
       "      <td>0.265026</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.354746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust</th>\n",
       "      <td>0.024014</td>\n",
       "      <td>0.122914</td>\n",
       "      <td>0.389620</td>\n",
       "      <td>0.143060</td>\n",
       "      <td>0.231954</td>\n",
       "      <td>0.169261</td>\n",
       "      <td>0.469028</td>\n",
       "      <td>0.153723</td>\n",
       "      <td>0.184240</td>\n",
       "      <td>0.582920</td>\n",
       "      <td>0.145220</td>\n",
       "      <td>0.648163</td>\n",
       "      <td>0.171245</td>\n",
       "      <td>0.354746</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 label  pronouns  text_len  neg_vader  pos_vader     anger  \\\n",
       "label         1.000000  0.097800  0.033477   0.067170   0.065211  0.022057   \n",
       "pronouns      0.097800  1.000000  0.332071   0.193938   0.221419  0.076345   \n",
       "text_len      0.033477  0.332071  1.000000   0.343154   0.159673  0.360460   \n",
       "neg_vader     0.067170  0.193938  0.343154   1.000000   0.169624  0.384510   \n",
       "pos_vader     0.065211  0.221419  0.159673   0.169624   1.000000  0.079693   \n",
       "anger         0.022057  0.076345  0.360460   0.384510   0.079693  1.000000   \n",
       "anticipation  0.025666  0.128030  0.386351   0.141868   0.225925  0.196795   \n",
       "disgust       0.030664  0.094069  0.312393   0.362582   0.087309  0.583864   \n",
       "fear          0.019114  0.063176  0.381410   0.339245   0.071450  0.587460   \n",
       "joy           0.033977  0.144011  0.339398   0.126042   0.323148  0.157202   \n",
       "negative      0.022934  0.076670  0.370250   0.431111   0.058266  0.631708   \n",
       "positive      0.019590  0.106055  0.330075   0.099767   0.270687  0.128169   \n",
       "sadness       0.032641  0.100827  0.384031   0.374256   0.095040  0.528980   \n",
       "surprise      0.018109  0.106790  0.349498   0.159302   0.186243  0.273195   \n",
       "trust         0.024014  0.122914  0.389620   0.143060   0.231954  0.169261   \n",
       "\n",
       "              anticipation   disgust      fear       joy  negative  positive  \\\n",
       "label             0.025666  0.030664  0.019114  0.033977  0.022934  0.019590   \n",
       "pronouns          0.128030  0.094069  0.063176  0.144011  0.076670  0.106055   \n",
       "text_len          0.386351  0.312393  0.381410  0.339398  0.370250  0.330075   \n",
       "neg_vader         0.141868  0.362582  0.339245  0.126042  0.431111  0.099767   \n",
       "pos_vader         0.225925  0.087309  0.071450  0.323148  0.058266  0.270687   \n",
       "anger             0.196795  0.583864  0.587460  0.157202  0.631708  0.128169   \n",
       "anticipation      1.000000  0.164649  0.241958  0.583107  0.178827  0.452457   \n",
       "disgust           0.164649  1.000000  0.440376  0.152731  0.552021  0.116588   \n",
       "fear              0.241958  0.440376  1.000000  0.159907  0.576962  0.141985   \n",
       "joy               0.583107  0.152731  0.159907  1.000000  0.113400  0.645827   \n",
       "negative          0.178827  0.552021  0.576962  0.113400  1.000000  0.105821   \n",
       "positive          0.452457  0.116588  0.141985  0.645827  0.105821  1.000000   \n",
       "sadness           0.198972  0.490181  0.583703  0.176440  0.612781  0.139827   \n",
       "surprise          0.460851  0.232166  0.248160  0.477317  0.226230  0.333998   \n",
       "trust             0.469028  0.153723  0.184240  0.582920  0.145220  0.648163   \n",
       "\n",
       "               sadness  surprise     trust  \n",
       "label         0.032641  0.018109  0.024014  \n",
       "pronouns      0.100827  0.106790  0.122914  \n",
       "text_len      0.384031  0.349498  0.389620  \n",
       "neg_vader     0.374256  0.159302  0.143060  \n",
       "pos_vader     0.095040  0.186243  0.231954  \n",
       "anger         0.528980  0.273195  0.169261  \n",
       "anticipation  0.198972  0.460851  0.469028  \n",
       "disgust       0.490181  0.232166  0.153723  \n",
       "fear          0.583703  0.248160  0.184240  \n",
       "joy           0.176440  0.477317  0.582920  \n",
       "negative      0.612781  0.226230  0.145220  \n",
       "positive      0.139827  0.333998  0.648163  \n",
       "sadness       1.000000  0.265026  0.171245  \n",
       "surprise      0.265026  1.000000  0.354746  \n",
       "trust         0.171245  0.354746  1.000000  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len', 'neg_vader', 'pos_vader'] + emotions].corr('spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from liwc_readDict import readDict\n",
    "\n",
    "liwc = readDict('/home/ana/resources/FakeOrFact/features/LIWC/LIWC/liwc.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'achieve',\n",
       " 'adverb',\n",
       " 'affect',\n",
       " 'anger',\n",
       " 'anx',\n",
       " 'article',\n",
       " 'assent',\n",
       " 'auxverb',\n",
       " 'bio',\n",
       " 'body',\n",
       " 'cause',\n",
       " 'certain',\n",
       " 'cogmech',\n",
       " 'conj',\n",
       " 'death',\n",
       " 'discrep',\n",
       " 'excl',\n",
       " 'family',\n",
       " 'feel',\n",
       " 'filler',\n",
       " 'friend',\n",
       " 'funct',\n",
       " 'future',\n",
       " 'health',\n",
       " 'hear',\n",
       " 'home',\n",
       " 'humans',\n",
       " 'i',\n",
       " 'incl',\n",
       " 'ingest',\n",
       " 'inhib',\n",
       " 'insight',\n",
       " 'ipron',\n",
       " 'leisure',\n",
       " 'money',\n",
       " 'motion',\n",
       " 'negate',\n",
       " 'negemo',\n",
       " 'nonfl',\n",
       " 'number',\n",
       " 'past',\n",
       " 'percept',\n",
       " 'posemo',\n",
       " 'ppron',\n",
       " 'preps',\n",
       " 'present',\n",
       " 'pronoun',\n",
       " 'quant',\n",
       " 'relativ',\n",
       " 'relig',\n",
       " 'sad',\n",
       " 'see',\n",
       " 'sexual',\n",
       " 'shehe',\n",
       " 'social',\n",
       " 'space',\n",
       " 'swear',\n",
       " 'tentat',\n",
       " 'they',\n",
       " 'time',\n",
       " 'verb',\n",
       " 'we',\n",
       " 'work',\n",
       " 'you'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = [c for (w,c) in liwc]\n",
    "set(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 'funct'],\n",
       " ['a', 'article'],\n",
       " ['abandon*', 'affect'],\n",
       " ['abandon*', 'negemo'],\n",
       " ['abandon*', 'sad'],\n",
       " ['abandon*', 'cogmech'],\n",
       " ['abandon*', 'inhib'],\n",
       " ['abdomen*', 'bio'],\n",
       " ['abdomen*', 'body'],\n",
       " ['abilit*', 'achieve'],\n",
       " ['able*', 'achieve'],\n",
       " ['abortion*', 'bio'],\n",
       " ['abortion*', 'health'],\n",
       " ['abortion*', 'sexual'],\n",
       " ['about', 'funct'],\n",
       " ['about', 'adverb'],\n",
       " ['about', 'preps'],\n",
       " ['above', 'funct'],\n",
       " ['above', 'preps'],\n",
       " ['above', 'space'],\n",
       " ['above', 'relativ'],\n",
       " ['abrupt*', 'time'],\n",
       " ['abrupt*', 'relativ'],\n",
       " ['abs', 'bio'],\n",
       " ['abs', 'body'],\n",
       " ['absent*', 'work'],\n",
       " ['absolute', 'cogmech'],\n",
       " ['absolute', 'certain'],\n",
       " ['absolutely', 'funct'],\n",
       " ['absolutely', 'adverb'],\n",
       " ['absolutely', 'cogmech'],\n",
       " ['absolutely', 'certain'],\n",
       " ['absolutely', 'assent'],\n",
       " ['abstain*', 'cogmech'],\n",
       " ['abstain*', 'inhib'],\n",
       " ['abuse*', 'affect'],\n",
       " ['abuse*', 'negemo'],\n",
       " ['abuse*', 'anger'],\n",
       " ['abusi*', 'affect'],\n",
       " ['abusi*', 'negemo'],\n",
       " ['abusi*', 'anger'],\n",
       " ['academ*', 'work'],\n",
       " ['accept', 'affect'],\n",
       " ['accept', 'posemo'],\n",
       " ['accept', 'cogmech'],\n",
       " ['accept', 'insight'],\n",
       " ['accepta*', 'affect'],\n",
       " ['accepta*', 'posemo'],\n",
       " ['accepta*', 'cogmech'],\n",
       " ['accepta*', 'insight'],\n",
       " ['accepted', 'verb'],\n",
       " ['accepted', 'past'],\n",
       " ['accepted', 'affect'],\n",
       " ['accepted', 'posemo'],\n",
       " ['accepted', 'cogmech'],\n",
       " ['accepted', 'insight'],\n",
       " ['accepting', 'affect'],\n",
       " ['accepting', 'posemo'],\n",
       " ['accepting', 'cogmech'],\n",
       " ['accepting', 'insight'],\n",
       " ['accepts', 'affect'],\n",
       " ['accepts', 'posemo'],\n",
       " ['accepts', 'cogmech'],\n",
       " ['accepts', 'insight'],\n",
       " ['accomplish*', 'work'],\n",
       " ['accomplish*', 'achieve'],\n",
       " ['account*', 'money'],\n",
       " ['accura*', 'cogmech'],\n",
       " ['accura*', 'certain'],\n",
       " ['ace', 'achieve'],\n",
       " ['ache*', 'affect'],\n",
       " ['ache*', 'negemo'],\n",
       " ['ache*', 'sad'],\n",
       " ['ache*', 'bio'],\n",
       " ['ache*', 'health'],\n",
       " ['achiev*', 'work'],\n",
       " ['achiev*', 'achieve'],\n",
       " ['aching', 'affect'],\n",
       " ['aching', 'negemo'],\n",
       " ['aching', 'sad'],\n",
       " ['aching', 'bio'],\n",
       " ['aching', 'health'],\n",
       " ['acid*', 'percept'],\n",
       " ['acknowledg*', 'cogmech'],\n",
       " ['acknowledg*', 'insight'],\n",
       " ['acne', 'bio'],\n",
       " ['acne', 'health'],\n",
       " ['acquainta*', 'social'],\n",
       " ['acquainta*', 'friend'],\n",
       " ['acquir*', 'achieve'],\n",
       " ['acquisition*', 'achieve'],\n",
       " ['acrid*', 'percept'],\n",
       " ['across', 'funct'],\n",
       " ['across', 'preps'],\n",
       " ['across', 'space'],\n",
       " ['across', 'relativ'],\n",
       " ['act', 'relativ'],\n",
       " ['act', 'motion'],\n",
       " ['action*', 'motion'],\n",
       " ['action*', 'relativ'],\n",
       " ['activat*', 'cogmech'],\n",
       " ['activat*', 'cause'],\n",
       " ['active*', 'affect'],\n",
       " ['active*', 'posemo'],\n",
       " ['actor*', 'leisure'],\n",
       " ['actress*', 'leisure'],\n",
       " ['actually', 'funct'],\n",
       " ['actually', 'adverb'],\n",
       " ['add', 'cogmech'],\n",
       " ['add', 'incl'],\n",
       " ['addict*', 'bio'],\n",
       " ['addict*', 'health'],\n",
       " ['addit*', 'cogmech'],\n",
       " ['addit*', 'incl'],\n",
       " ['address', 'home'],\n",
       " ['adequa*', 'achieve'],\n",
       " ['adjust*', 'cogmech'],\n",
       " ['adjust*', 'insight'],\n",
       " ['administrat*', 'work'],\n",
       " ['admir*', 'affect'],\n",
       " ['admir*', 'posemo'],\n",
       " ['admit', 'verb'],\n",
       " ['admit', 'present'],\n",
       " ['admit', 'social'],\n",
       " ['admit', 'cogmech'],\n",
       " ['admit', 'insight'],\n",
       " ['admits', 'verb'],\n",
       " ['admits', 'present'],\n",
       " ['admits', 'social'],\n",
       " ['admits', 'cogmech'],\n",
       " ['admits', 'insight'],\n",
       " ['admitted', 'verb'],\n",
       " ['admitted', 'past'],\n",
       " ['admitted', 'social'],\n",
       " ['admitted', 'cogmech'],\n",
       " ['admitted', 'insight'],\n",
       " ['admitting', 'social'],\n",
       " ['admitting', 'cogmech'],\n",
       " ['admitting', 'insight'],\n",
       " ['ador*', 'affect'],\n",
       " ['ador*', 'posemo'],\n",
       " ['adult', 'social'],\n",
       " ['adult', 'humans'],\n",
       " ['adults', 'social'],\n",
       " ['adults', 'humans'],\n",
       " ['advanc*', 'motion'],\n",
       " ['advanc*', 'relativ'],\n",
       " ['advanc*', 'achieve'],\n",
       " ['advantag*', 'affect'],\n",
       " ['advantag*', 'posemo'],\n",
       " ['advantag*', 'achieve'],\n",
       " ['adventur*', 'affect'],\n",
       " ['adventur*', 'posemo'],\n",
       " ['advers*', 'affect'],\n",
       " ['advers*', 'negemo'],\n",
       " ['advertising', 'work'],\n",
       " ['advice', 'social'],\n",
       " ['advil', 'bio'],\n",
       " ['advil', 'health'],\n",
       " ['advis*', 'social'],\n",
       " ['advis*', 'work'],\n",
       " ['aerobic*', 'leisure'],\n",
       " ['affair*', 'social'],\n",
       " ['affect', 'cogmech'],\n",
       " ['affect', 'cause'],\n",
       " ['affected', 'verb'],\n",
       " ['affected', 'past'],\n",
       " ['affected', 'cogmech'],\n",
       " ['affected', 'cause'],\n",
       " ['affecting', 'cogmech'],\n",
       " ['affecting', 'cause'],\n",
       " ['affection*', 'affect'],\n",
       " ['affection*', 'posemo'],\n",
       " ['affects', 'cogmech'],\n",
       " ['affects', 'cause'],\n",
       " ['afraid', 'affect'],\n",
       " ['afraid', 'negemo'],\n",
       " ['afraid', 'anx'],\n",
       " ['after', 'funct'],\n",
       " ['after', 'preps'],\n",
       " ['after', 'time'],\n",
       " ['after', 'relativ'],\n",
       " ['afterlife*', 'time'],\n",
       " ['afterlife*', 'relativ'],\n",
       " ['afterlife*', 'relig'],\n",
       " ['aftermath*', 'time'],\n",
       " ['aftermath*', 'relativ'],\n",
       " ['afternoon*', 'time'],\n",
       " ['afternoon*', 'relativ'],\n",
       " ['afterthought*', 'cogmech'],\n",
       " ['afterthought*', 'insight'],\n",
       " ['afterthought*', 'time'],\n",
       " ['afterthought*', 'relativ'],\n",
       " ['afterward*', 'time'],\n",
       " ['afterward*', 'relativ'],\n",
       " ['again', 'funct'],\n",
       " ['again', 'adverb'],\n",
       " ['again', 'time'],\n",
       " ['again', 'relativ'],\n",
       " ['against', 'funct'],\n",
       " ['against', 'preps'],\n",
       " ['age', 'time'],\n",
       " ['age', 'relativ'],\n",
       " ['aged', 'time'],\n",
       " ['aged', 'relativ'],\n",
       " ['agent', 'work'],\n",
       " ['agents', 'work'],\n",
       " ['ages', 'time'],\n",
       " ['ages', 'relativ'],\n",
       " ['aggravat*', 'affect'],\n",
       " ['aggravat*', 'negemo'],\n",
       " ['aggravat*', 'anger'],\n",
       " ['aggravat*', 'cogmech'],\n",
       " ['aggravat*', 'cause'],\n",
       " ['aggress*', 'affect'],\n",
       " ['aggress*', 'negemo'],\n",
       " ['aggress*', 'anger'],\n",
       " ['aging', 'time'],\n",
       " ['aging', 'relativ'],\n",
       " ['agitat*', 'affect'],\n",
       " ['agitat*', 'negemo'],\n",
       " ['agitat*', 'anger'],\n",
       " ['agnost*', 'relig'],\n",
       " ['ago', 'time'],\n",
       " ['ago', 'relativ'],\n",
       " ['agoniz*', 'affect'],\n",
       " ['agoniz*', 'negemo'],\n",
       " ['agoniz*', 'sad'],\n",
       " ['agony', 'affect'],\n",
       " ['agony', 'negemo'],\n",
       " ['agony', 'sad'],\n",
       " ['agree', 'affect'],\n",
       " ['agree', 'posemo'],\n",
       " ['agree', 'assent'],\n",
       " ['agreeab*', 'affect'],\n",
       " ['agreeab*', 'posemo'],\n",
       " ['agreed', 'affect'],\n",
       " ['agreed', 'posemo'],\n",
       " ['agreeing', 'affect'],\n",
       " ['agreeing', 'posemo'],\n",
       " ['agreement*', 'affect'],\n",
       " ['agreement*', 'posemo'],\n",
       " ['agrees', 'affect'],\n",
       " ['agrees', 'posemo'],\n",
       " ['ah', 'assent'],\n",
       " ['ahead', 'funct'],\n",
       " ['ahead', 'preps'],\n",
       " ['ahead', 'time'],\n",
       " ['ahead', 'relativ'],\n",
       " ['ahead', 'achieve'],\n",
       " ['aids', 'bio'],\n",
       " ['aids', 'health'],\n",
       " ['aids', 'sexual'],\n",
       " [\"ain't\", 'verb'],\n",
       " [\"ain't\", 'funct'],\n",
       " [\"ain't\", 'auxverb'],\n",
       " [\"ain't\", 'present'],\n",
       " [\"ain't\", 'negate'],\n",
       " ['aint', 'verb'],\n",
       " ['aint', 'funct'],\n",
       " ['aint', 'auxverb'],\n",
       " ['aint', 'present'],\n",
       " ['aint', 'negate'],\n",
       " ['air', 'relativ'],\n",
       " ['air', 'space'],\n",
       " ['alarm*', 'affect'],\n",
       " ['alarm*', 'negemo'],\n",
       " ['alarm*', 'anx'],\n",
       " ['alcohol*', 'bio'],\n",
       " ['alcohol*', 'health'],\n",
       " ['alcohol*', 'ingest'],\n",
       " ['alive', 'bio'],\n",
       " ['alive', 'health'],\n",
       " ['alive', 'death'],\n",
       " ['all', 'funct'],\n",
       " ['all', 'quant'],\n",
       " ['all', 'cogmech'],\n",
       " ['all', 'certain'],\n",
       " ['alla', 'relig'],\n",
       " ['allah*', 'relig'],\n",
       " ['allerg*', 'bio'],\n",
       " ['allerg*', 'health'],\n",
       " ['allot', 'funct'],\n",
       " ['allot', 'quant'],\n",
       " ['allot', 'cogmech'],\n",
       " ['allot', 'tentat'],\n",
       " ['allow*', 'cogmech'],\n",
       " ['allow*', 'cause'],\n",
       " ['almost', 'cogmech'],\n",
       " ['almost', 'tentat'],\n",
       " ['alone', 'affect'],\n",
       " ['alone', 'negemo'],\n",
       " ['alone', 'sad'],\n",
       " ['along', 'funct'],\n",
       " ['along', 'preps'],\n",
       " ['along', 'cogmech'],\n",
       " ['along', 'incl'],\n",
       " ['alot', 'funct'],\n",
       " ['alot', 'article'],\n",
       " ['alot', 'quant'],\n",
       " ['alot', 'cogmech'],\n",
       " ['alot', 'tentat'],\n",
       " ['already', 'time'],\n",
       " ['already', 'relativ'],\n",
       " ['alright*', 'affect'],\n",
       " ['alright*', 'posemo'],\n",
       " ['alright*', 'assent'],\n",
       " ['also', 'funct'],\n",
       " ['also', 'adverb'],\n",
       " ['also', 'conj'],\n",
       " ['altar*', 'relig'],\n",
       " ['although', 'funct'],\n",
       " ['although', 'conj'],\n",
       " ['altogether', 'cogmech'],\n",
       " ['altogether', 'certain'],\n",
       " ['always', 'cogmech'],\n",
       " ['always', 'certain'],\n",
       " ['always', 'time'],\n",
       " ['always', 'relativ'],\n",
       " ['am', 'verb'],\n",
       " ['am', 'funct'],\n",
       " ['am', 'auxverb'],\n",
       " ['am', 'present'],\n",
       " ['amaz*', 'affect'],\n",
       " ['amaz*', 'posemo'],\n",
       " ['ambigu*', 'cogmech'],\n",
       " ['ambigu*', 'tentat'],\n",
       " ['ambiti*', 'work'],\n",
       " ['ambiti*', 'achieve'],\n",
       " ['amen', 'relig'],\n",
       " ['amigo*', 'social'],\n",
       " ['amigo*', 'friend'],\n",
       " ['amish', 'relig'],\n",
       " ['among*', 'funct'],\n",
       " ['among*', 'preps'],\n",
       " ['among*', 'space'],\n",
       " ['among*', 'relativ'],\n",
       " ['amor*', 'affect'],\n",
       " ['amor*', 'posemo'],\n",
       " ['amount*', 'quant'],\n",
       " ['amput*', 'bio'],\n",
       " ['amput*', 'health'],\n",
       " ['amus*', 'affect'],\n",
       " ['amus*', 'posemo'],\n",
       " ['amus*', 'leisure'],\n",
       " ['an', 'funct'],\n",
       " ['an', 'article'],\n",
       " ['anal', 'cogmech'],\n",
       " ['anal', 'inhib'],\n",
       " ['anal', 'bio'],\n",
       " ['anal', 'body'],\n",
       " ['analy*', 'cogmech'],\n",
       " ['analy*', 'insight'],\n",
       " ['ancient*', 'time'],\n",
       " ['ancient*', 'relativ'],\n",
       " ['and', 'funct'],\n",
       " ['and', 'conj'],\n",
       " ['and', 'cogmech'],\n",
       " ['and', 'incl'],\n",
       " ['angel', 'relig'],\n",
       " ['angelic*', 'relig'],\n",
       " ['angels', 'relig'],\n",
       " ['anger*', 'affect'],\n",
       " ['anger*', 'negemo'],\n",
       " ['anger*', 'anger'],\n",
       " ['angr*', 'affect'],\n",
       " ['angr*', 'negemo'],\n",
       " ['angr*', 'anger'],\n",
       " ['anguish*', 'affect'],\n",
       " ['anguish*', 'negemo'],\n",
       " ['anguish*', 'anx'],\n",
       " ['ankle*', 'bio'],\n",
       " ['ankle*', 'body'],\n",
       " ['annoy*', 'affect'],\n",
       " ['annoy*', 'negemo'],\n",
       " ['annoy*', 'anger'],\n",
       " ['annual*', 'time'],\n",
       " ['annual*', 'relativ'],\n",
       " ['anorexi*', 'bio'],\n",
       " ['anorexi*', 'health'],\n",
       " ['anorexi*', 'ingest'],\n",
       " ['another', 'funct'],\n",
       " ['another', 'quant'],\n",
       " ['answer*', 'cogmech'],\n",
       " ['answer*', 'insight'],\n",
       " ['antacid*', 'bio'],\n",
       " ['antacid*', 'health'],\n",
       " ['antagoni*', 'affect'],\n",
       " ['antagoni*', 'negemo'],\n",
       " ['antagoni*', 'anger'],\n",
       " ['antidepressant*', 'bio'],\n",
       " ['antidepressant*', 'health'],\n",
       " ['anus*', 'bio'],\n",
       " ['anus*', 'body'],\n",
       " ['anxi*', 'affect'],\n",
       " ['anxi*', 'negemo'],\n",
       " ['anxi*', 'anx'],\n",
       " ['any', 'funct'],\n",
       " ['any', 'quant'],\n",
       " ['any', 'cogmech'],\n",
       " ['any', 'tentat'],\n",
       " ['anybod*', 'funct'],\n",
       " ['anybod*', 'pronoun'],\n",
       " ['anybod*', 'ipron'],\n",
       " ['anybod*', 'social'],\n",
       " ['anybod*', 'cogmech'],\n",
       " ['anybod*', 'tentat'],\n",
       " ['anyhow', 'cogmech'],\n",
       " ['anyhow', 'tentat'],\n",
       " ['anymore', 'funct'],\n",
       " ['anymore', 'quant'],\n",
       " ['anymore', 'relativ'],\n",
       " ['anymore', 'time'],\n",
       " ['anyone*', 'funct'],\n",
       " ['anyone*', 'pronoun'],\n",
       " ['anyone*', 'ipron'],\n",
       " ['anyone*', 'social'],\n",
       " ['anyone*', 'cogmech'],\n",
       " ['anyone*', 'tentat'],\n",
       " ['anything', 'funct'],\n",
       " ['anything', 'pronoun'],\n",
       " ['anything', 'ipron'],\n",
       " ['anything', 'cogmech'],\n",
       " ['anything', 'tentat'],\n",
       " ['anytime', 'cogmech'],\n",
       " ['anytime', 'tentat'],\n",
       " ['anytime', 'time'],\n",
       " ['anytime', 'relativ'],\n",
       " ['anyway*', 'funct'],\n",
       " ['anyway*', 'adverb'],\n",
       " ['anywhere', 'funct'],\n",
       " ['anywhere', 'adverb'],\n",
       " ['anywhere', 'cogmech'],\n",
       " ['anywhere', 'tentat'],\n",
       " ['anywhere', 'space'],\n",
       " ['anywhere', 'relativ'],\n",
       " ['aok', 'affect'],\n",
       " ['aok', 'posemo'],\n",
       " ['aok', 'assent'],\n",
       " ['apart', 'space'],\n",
       " ['apart', 'relativ'],\n",
       " ['apartment*', 'leisure'],\n",
       " ['apartment*', 'home'],\n",
       " ['apath*', 'affect'],\n",
       " ['apath*', 'negemo'],\n",
       " ['apolog*', 'social'],\n",
       " ['appall*', 'affect'],\n",
       " ['appall*', 'negemo'],\n",
       " ['apparent', 'cogmech'],\n",
       " ['apparent', 'certain'],\n",
       " ['apparently', 'funct'],\n",
       " ['apparently', 'adverb'],\n",
       " ['apparently', 'cogmech'],\n",
       " ['apparently', 'tentat'],\n",
       " ['appear', 'verb'],\n",
       " ['appear', 'present'],\n",
       " ['appear', 'cogmech'],\n",
       " ['appear', 'tentat'],\n",
       " ['appear', 'motion'],\n",
       " ['appear', 'relativ'],\n",
       " ['appeared', 'verb'],\n",
       " ['appeared', 'past'],\n",
       " ['appeared', 'cogmech'],\n",
       " ['appeared', 'tentat'],\n",
       " ['appeared', 'motion'],\n",
       " ['appeared', 'relativ'],\n",
       " ['appearing', 'cogmech'],\n",
       " ['appearing', 'tentat'],\n",
       " ['appearing', 'motion'],\n",
       " ['appearing', 'relativ'],\n",
       " ['appears', 'verb'],\n",
       " ['appears', 'present'],\n",
       " ['appears', 'cogmech'],\n",
       " ['appears', 'tentat'],\n",
       " ['appears', 'motion'],\n",
       " ['appears', 'relativ'],\n",
       " ['appendic*', 'bio'],\n",
       " ['appendic*', 'health'],\n",
       " ['appendix', 'bio'],\n",
       " ['appendix', 'body'],\n",
       " ['appeti*', 'bio'],\n",
       " ['appeti*', 'ingest'],\n",
       " ['applicant*', 'work'],\n",
       " ['applicat*', 'work'],\n",
       " ['appreciat*', 'affect'],\n",
       " ['appreciat*', 'posemo'],\n",
       " ['appreciat*', 'cogmech'],\n",
       " ['appreciat*', 'insight'],\n",
       " ['apprehens*', 'affect'],\n",
       " ['apprehens*', 'negemo'],\n",
       " ['apprehens*', 'anx'],\n",
       " ['apprentic*', 'work'],\n",
       " ['approach*', 'motion'],\n",
       " ['approach*', 'relativ'],\n",
       " ['approv*', 'achieve'],\n",
       " ['approximat*', 'cogmech'],\n",
       " ['approximat*', 'tentat'],\n",
       " ['april', 'time'],\n",
       " ['april', 'relativ'],\n",
       " ['arbitrar*', 'cogmech'],\n",
       " ['arbitrar*', 'tentat'],\n",
       " ['arch', 'bio'],\n",
       " ['arch', 'body'],\n",
       " ['are', 'verb'],\n",
       " ['are', 'funct'],\n",
       " ['are', 'auxverb'],\n",
       " ['are', 'present'],\n",
       " ['area*', 'space'],\n",
       " ['area*', 'relativ'],\n",
       " [\"aren't\", 'verb'],\n",
       " [\"aren't\", 'funct'],\n",
       " [\"aren't\", 'auxverb'],\n",
       " [\"aren't\", 'present'],\n",
       " [\"aren't\", 'negate'],\n",
       " ['arent', 'verb'],\n",
       " ['arent', 'funct'],\n",
       " ['arent', 'auxverb'],\n",
       " ['arent', 'present'],\n",
       " ['arent', 'negate'],\n",
       " ['argh*', 'affect'],\n",
       " ['argh*', 'negemo'],\n",
       " ['argh*', 'anger'],\n",
       " ['argu*', 'social'],\n",
       " ['argu*', 'affect'],\n",
       " ['argu*', 'negemo'],\n",
       " ['argu*', 'anger'],\n",
       " ['arm', 'bio'],\n",
       " ['arm', 'body'],\n",
       " ['armies', 'social'],\n",
       " ['armpit*', 'bio'],\n",
       " ['armpit*', 'body'],\n",
       " ['arms*', 'bio'],\n",
       " ['arms*', 'body'],\n",
       " ['army', 'social'],\n",
       " ['aroma*', 'percept'],\n",
       " ['around', 'funct'],\n",
       " ['around', 'adverb'],\n",
       " ['around', 'preps'],\n",
       " ['around', 'cogmech'],\n",
       " ['around', 'incl'],\n",
       " ['around', 'space'],\n",
       " ['around', 'relativ'],\n",
       " ['arous*', 'bio'],\n",
       " ['arous*', 'body'],\n",
       " ['arous*', 'sexual'],\n",
       " ['arrival*', 'motion'],\n",
       " ['arrival*', 'relativ'],\n",
       " ['arrive', 'verb'],\n",
       " ['arrive', 'present'],\n",
       " ['arrive', 'motion'],\n",
       " ['arrive', 'relativ'],\n",
       " ['arrived', 'verb'],\n",
       " ['arrived', 'past'],\n",
       " ['arrived', 'motion'],\n",
       " ['arrived', 'relativ'],\n",
       " ['arrives', 'verb'],\n",
       " ['arrives', 'present'],\n",
       " ['arrives', 'motion'],\n",
       " ['arrives', 'relativ'],\n",
       " ['arriving', 'motion'],\n",
       " ['arriving', 'relativ'],\n",
       " ['arrogan*', 'affect'],\n",
       " ['arrogan*', 'negemo'],\n",
       " ['arrogan*', 'anger'],\n",
       " ['arse', 'bio'],\n",
       " ['arse', 'body'],\n",
       " ['arse', 'swear'],\n",
       " ['arsehole*', 'swear'],\n",
       " ['arses', 'bio'],\n",
       " ['arses', 'body'],\n",
       " ['arses', 'swear'],\n",
       " ['art', 'leisure'],\n",
       " ['arter*', 'bio'],\n",
       " ['arter*', 'body'],\n",
       " ['arthr*', 'bio'],\n",
       " ['arthr*', 'health'],\n",
       " ['artist*', 'leisure'],\n",
       " ['arts', 'leisure'],\n",
       " ['as', 'funct'],\n",
       " ['as', 'preps'],\n",
       " ['as', 'conj'],\n",
       " ['asham*', 'affect'],\n",
       " ['asham*', 'negemo'],\n",
       " ['asham*', 'anx'],\n",
       " ['ask', 'verb'],\n",
       " ['ask', 'present'],\n",
       " ['ask', 'social'],\n",
       " ['asked', 'verb'],\n",
       " ['asked', 'past'],\n",
       " ['asked', 'social'],\n",
       " ['asking', 'social'],\n",
       " ['asks', 'verb'],\n",
       " ['asks', 'present'],\n",
       " ['asks', 'social'],\n",
       " ['asleep', 'bio'],\n",
       " ['asleep', 'body'],\n",
       " ['aspirin*', 'bio'],\n",
       " ['aspirin*', 'health'],\n",
       " ['ass', 'bio'],\n",
       " ['ass', 'body'],\n",
       " ['ass', 'sexual'],\n",
       " ['ass', 'swear'],\n",
       " ['assault*', 'affect'],\n",
       " ['assault*', 'negemo'],\n",
       " ['assault*', 'anger'],\n",
       " ['assembl*', 'social'],\n",
       " ['asses', 'bio'],\n",
       " ['asses', 'body'],\n",
       " ['asses', 'sexual'],\n",
       " ['asses', 'swear'],\n",
       " ['asshole*', 'affect'],\n",
       " ['asshole*', 'negemo'],\n",
       " ['asshole*', 'anger'],\n",
       " ['asshole*', 'swear'],\n",
       " ['assign*', 'work'],\n",
       " ['assistan*', 'work'],\n",
       " ['associat*', 'work'],\n",
       " ['assum*', 'cogmech'],\n",
       " ['assum*', 'insight'],\n",
       " ['assum*', 'tentat'],\n",
       " ['assur*', 'affect'],\n",
       " ['assur*', 'posemo'],\n",
       " ['assur*', 'cogmech'],\n",
       " ['assur*', 'certain'],\n",
       " ['asthma*', 'bio'],\n",
       " ['asthma*', 'health'],\n",
       " ['at', 'funct'],\n",
       " ['at', 'preps'],\n",
       " ['at', 'space'],\n",
       " ['at', 'relativ'],\n",
       " ['ate', 'verb'],\n",
       " ['ate', 'past'],\n",
       " ['ate', 'bio'],\n",
       " ['ate', 'ingest'],\n",
       " ['athletic*', 'leisure'],\n",
       " ['atho', 'funct'],\n",
       " ['atho', 'conj'],\n",
       " ['atm', 'money'],\n",
       " ['atms', 'money'],\n",
       " ['atop', 'funct'],\n",
       " ['atop', 'preps'],\n",
       " ['atop', 'space'],\n",
       " ['atop', 'relativ'],\n",
       " ['attachment*', 'affect'],\n",
       " ['attachment*', 'posemo'],\n",
       " ['attack*', 'affect'],\n",
       " ['attack*', 'negemo'],\n",
       " ['attack*', 'anger'],\n",
       " ['attain*', 'achieve'],\n",
       " ['attempt*', 'achieve'],\n",
       " ['attend', 'motion'],\n",
       " ['attend', 'relativ'],\n",
       " ['attended', 'motion'],\n",
       " ['attended', 'relativ'],\n",
       " ['attending', 'motion'],\n",
       " ['attending', 'relativ'],\n",
       " ['attends', 'motion'],\n",
       " ['attends', 'relativ'],\n",
       " ['attent*', 'cogmech'],\n",
       " ['attent*', 'insight'],\n",
       " ['attract*', 'affect'],\n",
       " ['attract*', 'posemo'],\n",
       " ['attribut*', 'cogmech'],\n",
       " ['attribut*', 'cause'],\n",
       " ['auction*', 'money'],\n",
       " ['audibl*', 'percept'],\n",
       " ['audibl*', 'hear'],\n",
       " ['audio*', 'percept'],\n",
       " ['audio*', 'hear'],\n",
       " ['audit', 'money'],\n",
       " ['audited', 'money'],\n",
       " ['auditing', 'money'],\n",
       " ['auditor', 'money'],\n",
       " ['auditorium*', 'work'],\n",
       " ['auditors', 'money'],\n",
       " ['audits', 'money'],\n",
       " ['august', 'time'],\n",
       " ['august', 'relativ'],\n",
       " ['aunt*', 'social'],\n",
       " ['aunt*', 'family'],\n",
       " ['authorit*', 'achieve'],\n",
       " ['autops*', 'death'],\n",
       " ['autumn', 'time'],\n",
       " ['autumn', 'relativ'],\n",
       " ['aversi*', 'affect'],\n",
       " ['aversi*', 'negemo'],\n",
       " ['aversi*', 'anx'],\n",
       " ['avert*', 'cogmech'],\n",
       " ['avert*', 'inhib'],\n",
       " ['avoid*', 'affect'],\n",
       " ['avoid*', 'negemo'],\n",
       " ['avoid*', 'anx'],\n",
       " ['avoid*', 'cogmech'],\n",
       " ['avoid*', 'inhib'],\n",
       " ['aw', 'assent'],\n",
       " ['award*', 'affect'],\n",
       " ['award*', 'posemo'],\n",
       " ['award*', 'work'],\n",
       " ['award*', 'achieve'],\n",
       " ['aware*', 'cogmech'],\n",
       " ['aware*', 'insight'],\n",
       " ['away', 'funct'],\n",
       " ['away', 'preps'],\n",
       " ['away', 'space'],\n",
       " ['away', 'relativ'],\n",
       " ['awesome', 'affect'],\n",
       " ['awesome', 'posemo'],\n",
       " ['awesome', 'assent'],\n",
       " ['awful', 'affect'],\n",
       " ['awful', 'negemo'],\n",
       " ['awhile', 'time'],\n",
       " ['awhile', 'relativ'],\n",
       " ['awkward*', 'affect'],\n",
       " ['awkward*', 'negemo'],\n",
       " ['awkward*', 'anx'],\n",
       " ['babe*', 'social'],\n",
       " ['babe*', 'humans'],\n",
       " ['babies', 'social'],\n",
       " ['babies', 'humans'],\n",
       " ['baby*', 'social'],\n",
       " ['baby*', 'humans'],\n",
       " ['back', 'funct'],\n",
       " ['back', 'adverb'],\n",
       " ['back', 'time'],\n",
       " ['back', 'relativ'],\n",
       " ['backward*', 'space'],\n",
       " ['backward*', 'relativ'],\n",
       " ['backyard', 'home'],\n",
       " ['bad', 'affect'],\n",
       " ['bad', 'negemo'],\n",
       " ['bake*', 'bio'],\n",
       " ['bake*', 'ingest'],\n",
       " ['bake*', 'home'],\n",
       " ['baking', 'bio'],\n",
       " ['baking', 'ingest'],\n",
       " ['baking', 'home'],\n",
       " ['balcon*', 'home'],\n",
       " ['bald', 'bio'],\n",
       " ['bald', 'body'],\n",
       " ['ball', 'leisure'],\n",
       " ['ballet*', 'leisure'],\n",
       " ['bambino*', 'social'],\n",
       " ['bambino*', 'humans'],\n",
       " ['ban', 'cogmech'],\n",
       " ['ban', 'inhib'],\n",
       " ['band', 'social'],\n",
       " ['band', 'leisure'],\n",
       " ['bandage*', 'bio'],\n",
       " ['bandage*', 'health'],\n",
       " ['bandaid', 'bio'],\n",
       " ['bandaid', 'health'],\n",
       " ['bands', 'social'],\n",
       " ['bands', 'leisure'],\n",
       " ['bank*', 'money'],\n",
       " ['banned', 'cogmech'],\n",
       " ['banned', 'inhib'],\n",
       " ['banning', 'cogmech'],\n",
       " ['banning', 'inhib'],\n",
       " ['bans', 'cogmech'],\n",
       " ['bans', 'inhib'],\n",
       " ['baptis*', 'relig'],\n",
       " ['baptiz*', 'relig'],\n",
       " ['bar', 'bio'],\n",
       " ['bar', 'ingest'],\n",
       " ['bar', 'leisure'],\n",
       " ['barely', 'cogmech'],\n",
       " ['barely', 'tentat'],\n",
       " ['bargain*', 'money'],\n",
       " ['barrier*', 'cogmech'],\n",
       " ['barrier*', 'inhib'],\n",
       " ['bars', 'bio'],\n",
       " ['bars', 'ingest'],\n",
       " ['bars', 'leisure'],\n",
       " ['baseball*', 'leisure'],\n",
       " ['based', 'cogmech'],\n",
       " ['based', 'cause'],\n",
       " ['bases', 'cogmech'],\n",
       " ['bases', 'cause'],\n",
       " ['bashful*', 'affect'],\n",
       " ['bashful*', 'negemo'],\n",
       " ['basically', 'funct'],\n",
       " ['basically', 'adverb'],\n",
       " ['basis', 'cogmech'],\n",
       " ['basis', 'cause'],\n",
       " ['basketball*', 'leisure'],\n",
       " ['bastard*', 'affect'],\n",
       " ['bastard*', 'negemo'],\n",
       " ['bastard*', 'anger'],\n",
       " ['bastard*', 'swear'],\n",
       " ['bath*', 'leisure'],\n",
       " ['bath*', 'home'],\n",
       " ['battl*', 'affect'],\n",
       " ['battl*', 'negemo'],\n",
       " ['battl*', 'anger'],\n",
       " ['be', 'verb'],\n",
       " ['be', 'funct'],\n",
       " ['be', 'auxverb'],\n",
       " ['beach*', 'leisure'],\n",
       " ['beat', 'achieve'],\n",
       " ['beaten', 'affect'],\n",
       " ['beaten', 'negemo'],\n",
       " ['beaten', 'anger'],\n",
       " ['beaten', 'work'],\n",
       " ['beaten', 'achieve'],\n",
       " ['beaut*', 'affect'],\n",
       " ['beaut*', 'posemo'],\n",
       " ['beaut*', 'percept'],\n",
       " ['beaut*', 'see'],\n",
       " ['became', 'verb'],\n",
       " ['became', 'funct'],\n",
       " ['became', 'auxverb'],\n",
       " ['became', 'past'],\n",
       " ['became', 'cogmech'],\n",
       " ['became', 'insight'],\n",
       " ['because', 'funct'],\n",
       " ['because', 'conj'],\n",
       " ['because', 'cogmech'],\n",
       " ['because', 'cause'],\n",
       " ['become', 'verb'],\n",
       " ['become', 'funct'],\n",
       " ['become', 'auxverb'],\n",
       " ['become', 'present'],\n",
       " ['become', 'cogmech'],\n",
       " ['become', 'insight'],\n",
       " ['becomes', 'verb'],\n",
       " ['becomes', 'funct'],\n",
       " ['becomes', 'auxverb'],\n",
       " ['becomes', 'present'],\n",
       " ['becomes', 'cogmech'],\n",
       " ['becomes', 'insight'],\n",
       " ['becoming', 'verb'],\n",
       " ['becoming', 'funct'],\n",
       " ['becoming', 'auxverb'],\n",
       " ['becoming', 'cogmech'],\n",
       " ['becoming', 'insight'],\n",
       " ['bed', 'home'],\n",
       " ['bedding', 'home'],\n",
       " ['bedroom*', 'home'],\n",
       " ['beds', 'home'],\n",
       " ['been', 'verb'],\n",
       " ['been', 'funct'],\n",
       " ['been', 'auxverb'],\n",
       " ['been', 'past'],\n",
       " ['beer*', 'bio'],\n",
       " ['beer*', 'ingest'],\n",
       " ['beer*', 'leisure'],\n",
       " ['before', 'funct'],\n",
       " ['before', 'preps'],\n",
       " ['before', 'time'],\n",
       " ['before', 'relativ'],\n",
       " ['began', 'verb'],\n",
       " ['began', 'past'],\n",
       " ['began', 'time'],\n",
       " ['began', 'relativ'],\n",
       " ['beggar*', 'money'],\n",
       " ['begging', 'money'],\n",
       " ['begin', 'verb'],\n",
       " ['begin', 'present'],\n",
       " ['begin', 'time'],\n",
       " ['begin', 'relativ'],\n",
       " ['beginn*', 'time'],\n",
       " ['beginn*', 'relativ'],\n",
       " ['begins', 'verb'],\n",
       " ['begins', 'present'],\n",
       " ['begins', 'time'],\n",
       " ['begins', 'relativ'],\n",
       " ['begun', 'time'],\n",
       " ['begun', 'relativ'],\n",
       " ['behavio*', 'relativ'],\n",
       " ['behavio*', 'motion'],\n",
       " ['behind', 'funct'],\n",
       " ['behind', 'preps'],\n",
       " ['being', 'verb'],\n",
       " ['being', 'funct'],\n",
       " ['being', 'auxverb'],\n",
       " ['belief*', 'cogmech'],\n",
       " ['belief*', 'insight'],\n",
       " ['belief*', 'relig'],\n",
       " ['believe', 'verb'],\n",
       " ['believe', 'present'],\n",
       " ['believe', 'cogmech'],\n",
       " ['believe', 'insight'],\n",
       " ['believed', 'verb'],\n",
       " ['believed', 'past'],\n",
       " ['believed', 'cogmech'],\n",
       " ['believed', 'insight'],\n",
       " ['believes', 'verb'],\n",
       " ['believes', 'present'],\n",
       " ['believes', 'cogmech'],\n",
       " ['believes', 'insight'],\n",
       " ['believing', 'cogmech'],\n",
       " ['believing', 'insight'],\n",
       " ['bellies', 'bio'],\n",
       " ['bellies', 'body'],\n",
       " ['belly', 'bio'],\n",
       " ['belly', 'body'],\n",
       " ['beloved', 'affect'],\n",
       " ['beloved', 'posemo'],\n",
       " ['below', 'funct'],\n",
       " ['below', 'preps'],\n",
       " ['below', 'space'],\n",
       " ['below', 'relativ'],\n",
       " ['bend', 'space'],\n",
       " ['bend', 'relativ'],\n",
       " ['bending', 'space'],\n",
       " ['bending', 'relativ'],\n",
       " ['bends', 'space'],\n",
       " ['bends', 'relativ'],\n",
       " ['beneath', 'funct'],\n",
       " ['beneath', 'preps'],\n",
       " ['beneath', 'space'],\n",
       " ['beneath', 'relativ'],\n",
       " ['benefic*', 'affect'],\n",
       " ['benefic*', 'posemo'],\n",
       " ['benefit', 'affect'],\n",
       " ['benefit', 'posemo'],\n",
       " ['benefits', 'affect'],\n",
       " ['benefits', 'posemo'],\n",
       " ['benefits', 'work'],\n",
       " ['benefitt*', 'affect'],\n",
       " ['benefitt*', 'posemo'],\n",
       " ['benevolen*', 'affect'],\n",
       " ['benevolen*', 'posemo'],\n",
       " ['benign*', 'affect'],\n",
       " ['benign*', 'posemo'],\n",
       " ['bent', 'space'],\n",
       " ['bent', 'relativ'],\n",
       " ['bereave*', 'death'],\n",
       " ['beside', 'funct'],\n",
       " ['beside', 'preps'],\n",
       " ['beside', 'space'],\n",
       " ['beside', 'relativ'],\n",
       " ['besides', 'funct'],\n",
       " ['besides', 'preps'],\n",
       " ['besides', 'quant'],\n",
       " ['besides', 'cogmech'],\n",
       " ['besides', 'discrep'],\n",
       " ['best', 'affect'],\n",
       " ['best', 'posemo'],\n",
       " ['best', 'achieve'],\n",
       " ['best', 'funct'],\n",
       " ['best', 'quant'],\n",
       " ['bet', 'cogmech'],\n",
       " ['bet', 'tentat'],\n",
       " ['bet', 'money'],\n",
       " ['bets', 'cogmech'],\n",
       " ['bets', 'tentat'],\n",
       " ['bets', 'money'],\n",
       " ['better', 'affect'],\n",
       " ['better', 'posemo'],\n",
       " ['better', 'achieve'],\n",
       " ['betting', 'cogmech'],\n",
       " ['betting', 'tentat'],\n",
       " ['betting', 'money'],\n",
       " ['between', 'funct'],\n",
       " ['between', 'preps'],\n",
       " ['beyond', 'funct'],\n",
       " ['beyond', 'adverb'],\n",
       " ['beyond', 'preps'],\n",
       " ['beyond', 'space'],\n",
       " ['beyond', 'relativ'],\n",
       " ['bf*', 'social'],\n",
       " ['bf*', 'friend'],\n",
       " ['bi', 'bio'],\n",
       " ['bi', 'sexual'],\n",
       " ['biannu*', 'time'],\n",
       " ['biannu*', 'relativ'],\n",
       " ['bible*', 'relig'],\n",
       " ['biblic*', 'relig'],\n",
       " ['bicep*', 'bio'],\n",
       " ['bicep*', 'body'],\n",
       " ['bicyc*', 'leisure'],\n",
       " ['big', 'space'],\n",
       " ['big', 'relativ'],\n",
       " ['bigger', 'space'],\n",
       " ['bigger', 'relativ'],\n",
       " ['biggest', 'space'],\n",
       " ['biggest', 'relativ'],\n",
       " ['bike*', 'leisure'],\n",
       " ['bill', 'money'],\n",
       " ['billed', 'money'],\n",
       " ['billing*', 'money'],\n",
       " ['billion*', 'funct'],\n",
       " ['billion*', 'number'],\n",
       " ['bills', 'money'],\n",
       " ['bimonth*', 'time'],\n",
       " ['bimonth*', 'relativ'],\n",
       " ['binding', 'cogmech'],\n",
       " ['binding', 'inhib'],\n",
       " ['binge*', 'bio'],\n",
       " ['binge*', 'health'],\n",
       " ['binge*', 'ingest'],\n",
       " ['binging', 'bio'],\n",
       " ['binging', 'health'],\n",
       " ['binging', 'ingest'],\n",
       " ['biolog*', 'work'],\n",
       " ['bipolar', 'bio'],\n",
       " ['bipolar', 'health'],\n",
       " ['birdie*', 'leisure'],\n",
       " ['birth*', 'time'],\n",
       " ...]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liwc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_dict = {}\n",
    "for (w, c) in liwc:\n",
    "    if c not in liwc_dict:\n",
    "        liwc_dict[c] = []\n",
    "    liwc_dict[c].append(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anybod*',\n",
       " 'anyone*',\n",
       " 'anything',\n",
       " 'everybod*',\n",
       " 'everyone*',\n",
       " 'everything*',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he's\",\n",
       " 'hed',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'hes',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'id',\n",
       " 'im',\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'itd',\n",
       " 'itll',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'ive',\n",
       " \"let's\",\n",
       " 'lets',\n",
       " 'me',\n",
       " 'mine',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'nobod*',\n",
       " 'oneself',\n",
       " 'other',\n",
       " 'others',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'shes',\n",
       " 'somebod*',\n",
       " 'someone*',\n",
       " 'something*',\n",
       " 'somewhere',\n",
       " 'stuff',\n",
       " 'that',\n",
       " \"that'd\",\n",
       " \"that'll\",\n",
       " \"that's\",\n",
       " 'thatd',\n",
       " 'thatll',\n",
       " 'thats',\n",
       " 'thee',\n",
       " 'their*',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they've\",\n",
       " 'theyd',\n",
       " 'theyll',\n",
       " 'theyve',\n",
       " 'thine',\n",
       " 'thing*',\n",
       " 'this',\n",
       " 'those',\n",
       " 'thou',\n",
       " 'thoust',\n",
       " 'thy',\n",
       " 'us',\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'weve',\n",
       " 'what',\n",
       " \"what's\",\n",
       " 'whatever',\n",
       " 'whats',\n",
       " 'which',\n",
       " 'whichever',\n",
       " 'who',\n",
       " \"who'd\",\n",
       " \"who'll\",\n",
       " 'whod',\n",
       " 'wholl',\n",
       " 'whom',\n",
       " 'whose',\n",
       " \"y'all\",\n",
       " 'ya',\n",
       " 'yall',\n",
       " 'ye',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'youd',\n",
       " 'youll',\n",
       " 'your',\n",
       " 'youre',\n",
       " 'yours',\n",
       " 'youve']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liwc_dict['pronoun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_liwc_categories(tokens, category_words, relative=True):\n",
    "    category_cnt = 0\n",
    "    if not tokens:\n",
    "        return None\n",
    "    text_len = len(tokens)\n",
    "    for t in tokens:\n",
    "        for word in category_words:\n",
    "            if t==word or (word[-1]=='*' and t.startswith(word[:-1])) \\\n",
    "            or (t==word.split(\"'\")[0]):\n",
    "                category_cnt += 1\n",
    "                break # one token cannot belong to more than one word in the category\n",
    "    if relative:\n",
    "        return category_cnt/text_len\n",
    "    else:\n",
    "        return category_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing for category pronoun...\n",
      "CPU times: user 1min 13s, sys: 37.9 ms, total: 1min 13s\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from functools import partial\n",
    "# for categ in ['negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']:#liwc_dict.keys():\n",
    "for categ in liwc_dict.keys():\n",
    "    if categ in writings_df.columns:\n",
    "        continue\n",
    "    print(\"Encoding for category %s...\" % categ)\n",
    "    writings_df[categ] = writings_df['all_tokens'].apply(partial(encode_liwc_categories, \n",
    "                                                                   category_words=liwc_dict[categ], \n",
    "                                                                   relative=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>negemo</th>\n",
       "      <th>posemo</th>\n",
       "      <th>affect</th>\n",
       "      <th>sad</th>\n",
       "      <th>anx</th>\n",
       "      <th>pronoun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007687</td>\n",
       "      <td>0.008943</td>\n",
       "      <td>0.012005</td>\n",
       "      <td>0.003770</td>\n",
       "      <td>0.009488</td>\n",
       "      <td>0.071618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negemo</th>\n",
       "      <td>0.007687</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.058048</td>\n",
       "      <td>0.456381</td>\n",
       "      <td>0.356025</td>\n",
       "      <td>0.303006</td>\n",
       "      <td>-0.011327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>posemo</th>\n",
       "      <td>0.008943</td>\n",
       "      <td>-0.058048</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.860121</td>\n",
       "      <td>-0.020983</td>\n",
       "      <td>-0.024652</td>\n",
       "      <td>-0.015620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>affect</th>\n",
       "      <td>0.012005</td>\n",
       "      <td>0.456381</td>\n",
       "      <td>0.860121</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.162322</td>\n",
       "      <td>0.131990</td>\n",
       "      <td>-0.019918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sad</th>\n",
       "      <td>0.003770</td>\n",
       "      <td>0.356025</td>\n",
       "      <td>-0.020983</td>\n",
       "      <td>0.162322</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004730</td>\n",
       "      <td>0.006895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anx</th>\n",
       "      <td>0.009488</td>\n",
       "      <td>0.303006</td>\n",
       "      <td>-0.024652</td>\n",
       "      <td>0.131990</td>\n",
       "      <td>0.004730</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.005346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pronoun</th>\n",
       "      <td>0.071618</td>\n",
       "      <td>-0.011327</td>\n",
       "      <td>-0.015620</td>\n",
       "      <td>-0.019918</td>\n",
       "      <td>0.006895</td>\n",
       "      <td>-0.005346</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            label    negemo    posemo    affect       sad       anx   pronoun\n",
       "label    1.000000  0.007687  0.008943  0.012005  0.003770  0.009488  0.071618\n",
       "negemo   0.007687  1.000000 -0.058048  0.456381  0.356025  0.303006 -0.011327\n",
       "posemo   0.008943 -0.058048  1.000000  0.860121 -0.020983 -0.024652 -0.015620\n",
       "affect   0.012005  0.456381  0.860121  1.000000  0.162322  0.131990 -0.019918\n",
       "sad      0.003770  0.356025 -0.020983  0.162322  1.000000  0.004730  0.006895\n",
       "anx      0.009488  0.303006 -0.024652  0.131990  0.004730  1.000000 -0.005346\n",
       "pronoun  0.071618 -0.011327 -0.015620 -0.019918  0.006895 -0.005346  1.000000"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['label', 'negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negemo</th>\n",
       "      <th>posemo</th>\n",
       "      <th>affect</th>\n",
       "      <th>sad</th>\n",
       "      <th>anx</th>\n",
       "      <th>pronoun</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.023493</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>0.074548</td>\n",
       "      <td>0.003242</td>\n",
       "      <td>0.002606</td>\n",
       "      <td>0.120154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.026116</td>\n",
       "      <td>0.056145</td>\n",
       "      <td>0.082611</td>\n",
       "      <td>0.003706</td>\n",
       "      <td>0.003591</td>\n",
       "      <td>0.162310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         negemo    posemo    affect       sad       anx   pronoun\n",
       "label                                                            \n",
       "0      0.023493  0.050800  0.074548  0.003242  0.002606  0.120154\n",
       "1      0.026116  0.056145  0.082611  0.003706  0.003591  0.162310"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['label', 'negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']].groupby('label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject', 'title', 'date', 'text', 'label', 'tokenized_title',\n",
       "       'title_len', 'tokenized_text', 'text_len', 'all_tokens', 'funct',\n",
       "       'article', 'negemo'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: COMET_OPTIMIZER_ID=786fc2b3654047e69f492db122f55b95\n",
      "COMET INFO: Using optimizer config: {'algorithm': 'random', 'configSpaceSize': 600000000000, 'endTime': None, 'id': '786fc2b3654047e69f492db122f55b95', 'lastUpdateTime': None, 'maxCombo': 0, 'name': '786fc2b3654047e69f492db122f55b95', 'parameters': {'batch_size': {'max': 512, 'min': 10, 'scalingType': 'loguniform', 'type': 'integer'}, 'decay': {'max': 0.5, 'min': 1e-08, 'scalingType': 'loguniform', 'type': 'float'}, 'dense_bow_units': {'max': 20, 'min': 1, 'scalingType': 'uniform', 'type': 'integer'}, 'dropout': {'max': 0.7, 'min': 0, 'scalingType': 'uniform', 'type': 'float'}, 'freeze_patience': {'max': 16, 'min': 2, 'scalingType': 'uniform', 'type': 'integer'}, 'l2_dense': {'max': 0.5, 'min': 1e-05, 'scalingType': 'loguniform', 'type': 'float'}, 'lr': {'max': 0.05, 'min': 1e-05, 'scalingType': 'loguniform', 'type': 'float'}, 'lr_reduce_factor': {'max': 0.8, 'min': 0.0001, 'scalingType': 'uniform', 'type': 'float'}, 'lr_reduce_patience': {'max': 16, 'min': 2, 'scalingType': 'uniform', 'type': 'integer'}, 'lstm_units': {'max': 100, 'min': 10, 'scalingType': 'uniform', 'type': 'integer'}, 'optimizer': {'type': 'categorical', 'values': ['adam', 'adagrad', '']}, 'positive_class_weight': {'max': 25, 'min': 1, 'scalingType': 'uniform', 'type': 'integer'}, 'trainable_embeddings': {'type': 'discrete', 'values': [True, False]}}, 'predictor': None, 'spec': {'gridSize': 10, 'maxCombo': 0, 'metric': 'loss', 'minSampleSize': 100, 'objective': 'minimize', 'retryAssignLimit': 0, 'retryLimit': 20, 'seed': 3968493229}, 'startTime': 15879681523, 'state': {'sequence_i': 0, 'sequence_pid': None, 'sequence_retry': 0}, 'status': 'running', 'suggestion_count': 0, 'trials': 1, 'version': '1.0.24'}\n",
      "COMET INFO: Optimizer metrics is 'loss' but no logged values found. Experiment ignored in sweep.\n",
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary:\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     url: https://www.comet.ml/ananana/mental/1ebbfc14aa9440e1948aabac5664d837\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     batch_binary_accuracy [29]: (0.5306122303009033, 0.9572663307189941)\n",
      "COMET INFO:     batch_f1_m [29]           : (0.0030426757875829935, 0.0336134247481823)\n",
      "COMET INFO:     batch_loss [29]           : (0.30054065585136414, 1.4031785726547241)\n",
      "COMET INFO:     batch_precision_m [29]    : (0.011335793882608414, 0.09260831028223038)\n",
      "COMET INFO:     batch_recall_m [29]       : (0.002346971072256565, 0.1666666716337204)\n",
      "COMET INFO:     sys.cpu.percent.01 [20]   : (4.8, 82.9)\n",
      "COMET INFO:     sys.cpu.percent.02 [20]   : (5.2, 73.9)\n",
      "COMET INFO:     sys.cpu.percent.03 [20]   : (4.6, 60.6)\n",
      "COMET INFO:     sys.cpu.percent.04 [20]   : (3.3, 57.8)\n",
      "COMET INFO:     sys.cpu.percent.avg [20]  : (4.925, 66.2)\n",
      "COMET INFO:     sys.gpu.0.total_memory    : (1073414144.0, 1073414144.0)\n",
      "COMET INFO:     sys.load.avg [20]         : (0.16, 4.02)\n",
      "COMET INFO:     sys.ram.total [20]        : (8277307392.0, 8277307392.0)\n",
      "COMET INFO:     sys.ram.used [20]         : (7586635776.0, 7714193408.0)\n",
      "COMET INFO:   Other [count]:\n",
      "COMET INFO:     optimizer_count       : 3\n",
      "COMET INFO:     optimizer_id          : 57eb178250e2401aa14cfc6860a4217a\n",
      "COMET INFO:     optimizer_metric      : loss\n",
      "COMET INFO:     optimizer_metric_value: None\n",
      "COMET INFO:     optimizer_parameters  : {'batch_size': 245, 'decay': 0.1402033634855056, 'dense_bow_units': 4, 'dropout': 0.46002110523805456, 'freeze_patience': 15, 'l2_dense': 0.0814165473266369, 'lr': 1.6244588122197995e-05, 'lr_reduce_factor': 0.6638163734300414, 'lr_reduce_patience': 10, 'lstm_units': 92, 'optimizer': 'adagrad', 'positive_class_weight': 5, 'set_trainable': True, 'trainable_embeddings': False}\n",
      "COMET INFO:     optimizer_pid         : 681f4971a3f98a865eb3100a742e6621e62a3420\n",
      "COMET INFO:     optimizer_process     : 2985\n",
      "COMET INFO:     optimizer_trial       : 1\n",
      "COMET INFO:     optimizer_version     : 1.0.24\n",
      "COMET INFO:     trainable_params      : 2071854\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     git-patch  : 1\n",
      "COMET INFO:     histogram3d: 1\n",
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/ananana/mental/6925a9644c034cbf9cd7f12c0a96e0f3\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 111375 samples, validate on 31863 samples\n",
      "Epoch 1/15\n",
      "111320/111375 [============================>.] - ETA: 0s - loss: 0.6696 - binary_accuracy: 0.8707 - f1_m: 0.1200 - precision_m: 0.0902 - recall_m: 0.2582\n",
      "Epoch 00001: val_loss improved from inf to 0.48204, saving model to models/experiment_best\n",
      "111375/111375 [==============================] - 317s 3ms/sample - loss: 0.6695 - binary_accuracy: 0.8707 - f1_m: 0.1201 - precision_m: 0.0902 - recall_m: 0.2584 - val_loss: 0.4820 - val_binary_accuracy: 0.7968 - val_f1_m: 0.2121 - val_precision_m: 0.1454 - val_recall_m: 0.4314\n",
      "Epoch 2/15\n",
      " 47190/111375 [===========>..................] - ETA: 2:52 - loss: 0.5882 - binary_accuracy: 0.8703 - f1_m: 0.1490 - precision_m: 0.1088 - recall_m: 0.2949"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-f94476829a7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m                           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                           \u001b[0mcallback_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfreeze_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_lr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                       model_path='models/experiment')\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-124-48d6d530785b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, x_train, y_train, x_test, y_test, batch_size, epochs, class_weight, start_epoch, workers, callback_list, model_path)\u001b[0m\n\u001b[1;32m     17\u001b[0m             callbacks = [\n\u001b[1;32m     18\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%s_best'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcallback_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             ])\n\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/comet_ml/monkey_patching.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m                     )\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Call after callbacks once we have the return value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Declare your hyperparameters search:\n",
    "tune_epochs=15\n",
    "config = {\n",
    "      \"algorithm\": \"random\",\n",
    "      \"parameters\": {\n",
    "          \"lstm_units\": {\"type\": \"integer\", \"min\": 10, \"max\": 100},\n",
    "          \"dense_bow_units\": {\"type\": \"integer\", \"min\": 1, \"max\": 20},\n",
    "          \"lr\": {\"type\": \"float\", \"min\": 0.00001, \"max\": 0.05, \"scalingType\": \"loguniform\"},\n",
    "          \"l2_dense\": {\"type\": \"float\", \"min\": 0.00001, \"max\": 0.5, \"scalingType\": \"loguniform\"},\n",
    "          \"dropout\": {\"type\": \"float\", \"min\": 0, \"max\": 0.7, \"scalingType\": \"uniform\"},\n",
    "          \"optimizer\": {\"type\": \"categorical\", \"values\": [\"adam\", \"adagrad\", \"\"]},\n",
    "          \"batch_size\": {\"type\": \"integer\", \"min\": 10, \"max\": 512, \"scalingType\": \"loguniform\"},\n",
    "          \"positive_class_weight\": {\"type\": \"integer\", \"min\": 1, \"max\": 25},\n",
    "          \"trainable_embeddings\": {\"type\": \"discrete\", \"values\": [True, False]},\n",
    "          \"freeze_patience\": {\"type\": \"integer\", \"min\": 2, \"max\": tune_epochs+1},\n",
    "          \"lr_reduce_factor\": {\"type\": \"float\", \"min\": 0.0001, \"max\": 0.8},\n",
    "          \"lr_reduce_patience\": {\"type\": \"integer\", \"min\": 2, \"max\": tune_epochs+1},\n",
    "          \"decay\": {\"type\": \"float\", \"min\": 0.00000001, \"max\": 0.5, \"scalingType\": \"loguniform\"},          \n",
    "      },\n",
    "      \"spec\": {\n",
    "          \"metric\": \"loss\",\n",
    "          \"objective\": \"minimize\",\n",
    "      },\n",
    "  }\n",
    "optimizer = Optimizer(config, api_key=\"eoBdVyznAhfg3bK9pZ58ZSXfv\")\n",
    "\n",
    "for experiment in optimizer.get_experiments(project_name=\"mental\"):\n",
    "    experiment.add_tag(\"tune\")\n",
    "    \n",
    "    # Test the model\n",
    "    hyperparams_config = {\n",
    "        param: experiment.get_parameter(param) for param in config['parameters'].keys()}\n",
    "    if not hyperparams_config['optimizer']:\n",
    "        hyperparams_config['optimizer'] = optimizers.Adam(lr=hyperparams_config['lr'], \n",
    "                                   decay=hyperparams_config['decay'])\n",
    "    model = build_model(hyperparams=hyperparams_config,\n",
    "                        hyperparams_features=hyperparams_features, \n",
    "                        embedding_matrix=embedding_matrix, emotions=emotions,\n",
    "                       stopwords_list=stopword_list)\n",
    "    freeze_layer = FreezeLayer(patience=experiment.get_parameter('freeze_patience'),\n",
    "                              set_to=not experiment.get_parameter('trainable_embeddings'))\n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            factor=experiment.get_parameter('lr_reduce_factor'),\n",
    "                                            patience=experiment.get_parameter('lr_reduce_patience'), \n",
    "                                            min_lr=0.000001, verbose=1)\n",
    "    history = train_model(model, \n",
    "            x_train, y_train, x_test, y_test,\n",
    "            epochs=tune_epochs, batch_size=experiment.get_parameter('batch_size'),\n",
    "                      class_weight={0:1, 1:experiment.get_parameter('positive_class_weight')}, \n",
    "                          workers=2,\n",
    "                          callback_list = [freeze_layer, reduce_lr],\n",
    "                      model_path='models/experiment')\n",
    "    loss = history.history['loss'][-1]\n",
    "    \n",
    "    # Report the loss, if not auto-logged:\n",
    "    experiment.log_metric(\"loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
