{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob, os\n",
    "import numpy as np\n",
    "from comet_ml import Experiment, Optimizer\n",
    "import pickle\n",
    "import logging\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_KERAS'] = '1'\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, CuDNNLSTM, Bidirectional, Input, concatenate\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import callbacks, optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# only reserve 1 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('training')\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_subject_writings(subject_file):\n",
    "    writings = []\n",
    "    with open(subject_file) as sf:\n",
    "        contents = sf.read()\n",
    "        root = ET.fromstring(contents)\n",
    "        try:\n",
    "            subject = root.findall('ID')[0].text.strip()\n",
    "        except Exception:\n",
    "            print('Cannot extract ID', contents[:500], '\\n-------\\n')        \n",
    "        for w in root.iter('WRITING'):\n",
    "            subject_writings = {'subject': subject}\n",
    "            for title in w.findall('TITLE'):\n",
    "                subject_writings['title'] = title.text\n",
    "            for text in w.findall('TEXT'):\n",
    "                subject_writings['text'] = text.text\n",
    "            for date in w.findall('DATE'):\n",
    "                subject_writings['date'] = date.text\n",
    "            writings.append(subject_writings)\n",
    "    return writings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dir = '/home/bighanem/ana_data/' \n",
    "root_dir = '/home/anasab/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eRisk 2020 T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir_T1 = root_dir + '/eRisk/data/eRisk2020_T1_train/eRISK2020_T1_training_data/eRISK2020_training_data/data/'\n",
    "labels_file_T1 = root_dir + '/eRisk/data//eRisk2020_T1_train/eRISK2020_T1_training_data/eRISK2020_training_data/golden_truth.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_2020(datadir_T1, labels_file_T1):\n",
    "    writings = []\n",
    "    for subject_file in os.listdir(datadir_T1):\n",
    "        print(subject_file)\n",
    "        writings.extend(read_subject_writings(os.path.join(datadir_T1, subject_file)))\n",
    "    writings_df = pd.DataFrame(writings)\n",
    "\n",
    "    labels_T1 = pd.read_csv(labels_file_T1, delimiter=' ', names=['subject', 'label'])\n",
    "    labels_T1 = labels_T1.set_index('subject')\n",
    "\n",
    "    writings_df['label'] = writings_df['subject'].apply(\n",
    "    lambda s: labels_T1.loc[s, 'label'])\n",
    "    \n",
    "    return writings_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eRisk 2019 T1 (Anorexia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadirs_T1_2019 = {\n",
    "    'train': ['2018 test/', '2018 train/positive_examples/', '2018 train/negative_examples/'],\n",
    "    'test': ['data/']\n",
    "}\n",
    "datadir_root_T1_2019 = {\n",
    "    'train': root_dir + '/eRisk/data/past/eRisk2019_T1/training data - t1/',\n",
    "    'test': root_dir + '/eRisk/data/past/eRisk2019_T1/test data - T1/'\n",
    "}\n",
    "    \n",
    "labels_files_T1_2019 = {\n",
    "    'train': ['2018 train/risk_golden_truth.txt', '2018 test/risk-golden-truth-test.txt'],\n",
    "    'test': ['T1_erisk_golden_truth.txt']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_2019(datadir_root_T1_2019,\n",
    "                   datadirs_T1_2019,\n",
    "                   labels_files_T1_2019,\n",
    "                   test_suffix='0000'):\n",
    "    writings = {'train': [], 'test': []}\n",
    "    writings_df = pd.DataFrame()\n",
    "    labels_df = pd.DataFrame()\n",
    "\n",
    "    for subset in ('train', 'test'):\n",
    "        for subdir in [os.path.join(datadir_root_T1_2019[subset], subp) for subp in datadirs_T1_2019[subset]]:\n",
    "            if subset=='train':\n",
    "                chunkdirs = [os.path.join(datadir_root_T1_2019[subset], subdir, chunkdir) \n",
    "                             for chunkdir in os.listdir(subdir)]\n",
    "            else:\n",
    "                chunkdirs = [os.path.join(datadir_root_T1_2019[subset], subdir)]\n",
    "                \n",
    "            for chunkdir in chunkdirs:\n",
    "                if not os.path.isdir(chunkdir):\n",
    "                    continue\n",
    "                for subject_file in os.listdir(chunkdir):\n",
    "                    writings[subset].extend(read_subject_writings(os.path.join(chunkdir, subject_file)))\n",
    "        writings_df_part = pd.DataFrame(writings[subset])\n",
    "        # add a suffix for users in the test -- the numbers are duplicated with the ones in train\n",
    "        if subset=='test':\n",
    "            writings_df_part['subject'] = writings_df_part['subject'].apply(lambda s: s+test_suffix)\n",
    "            print(subset, writings_df_part.subject)\n",
    "        writings_df_part['subset'] = subset\n",
    "        writings_df = pd.concat([writings_df, writings_df_part])\n",
    "        writings_df.reindex()\n",
    "\n",
    "        for label_file in labels_files_T1_2019[subset]:\n",
    "            labels = pd.read_csv(os.path.join(datadir_root_T1_2019[subset], label_file), \n",
    "                                 delimiter='\\s+', names=['subject', 'label'])\n",
    "            # add a suffix for users in the test -- the numbers are duplicated with the ones in train\n",
    "            if subset=='test':\n",
    "                labels['subject'] = labels['subject'].apply(lambda s: s+test_suffix)\n",
    "            labels_df = pd.concat([labels_df, labels])\n",
    "    labels_df = labels_df.drop_duplicates()\n",
    "    labels_df = labels_df.set_index('subject')\n",
    "\n",
    "    writings_df = writings_df.drop_duplicates()\n",
    "    \n",
    "    writings_df = writings_df.join(labels_df, on='subject')\n",
    "    \n",
    "    return writings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df = read_texts_2020(datadir_T1, labels_file_T1)\n",
    "# writings_df = read_texts_2019(datadir_root_T1_2019,\n",
    "#                    datadirs_T1_2019,\n",
    "#                    labels_files_T1_2019)\n",
    "writings_df = pickle.load(open('writings_df_anorexia_liwc', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# writings_df[writings_df['subset']=='test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ana/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>subject</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>subset</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>title_len</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>text_len</th>\n",
       "      <th>...</th>\n",
       "      <th>quant</th>\n",
       "      <th>sad</th>\n",
       "      <th>motion</th>\n",
       "      <th>verb</th>\n",
       "      <th>ingest</th>\n",
       "      <th>adverb</th>\n",
       "      <th>negemo</th>\n",
       "      <th>achieve</th>\n",
       "      <th>death</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date, subject, text, title, subset, label, tokenized_title, title_len, tokenized_text, text_len, all_tokens, present, discrep, cogmech, social, hear, family, preps, see, conj, time, cause, ipron, i, you, relativ, bio, ppron, body, humans, friend, space, negate, tentat, insight, inhib, auxverb, we, excl, they, shehe, work, number, health, assent, funct, swear, percept, past, relig, feel, pronoun, home, nonfl, anx, sexual, incl, affect, posemo, leisure, future, filler, anger, money, certain, quant, sad, motion, verb, ingest, adverb, negemo, achieve, death, article]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 75 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[writings_df['text'].isna()][~writings_df['title'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7faa4c630510>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "writings_df.label.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>subject</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>subset</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>title_len</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>text_len</th>\n",
       "      <th>...</th>\n",
       "      <th>quant</th>\n",
       "      <th>sad</th>\n",
       "      <th>motion</th>\n",
       "      <th>verb</th>\n",
       "      <th>ingest</th>\n",
       "      <th>adverb</th>\n",
       "      <th>negemo</th>\n",
       "      <th>achieve</th>\n",
       "      <th>death</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-12-01 07:47:48</td>\n",
       "      <td>subject5452</td>\n",
       "      <td></td>\n",
       "      <td>mew_irl</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>[mew_irl]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-11-23 03:42:13</td>\n",
       "      <td>subject5452</td>\n",
       "      <td></td>\n",
       "      <td>Trippy Kong.</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>[trippy, kong]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-11-22 22:12:02</td>\n",
       "      <td>subject5452</td>\n",
       "      <td></td>\n",
       "      <td>Dk rap but the acid is slowly kicking in.</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>[dk, rap, but, the, acid, is, slowly, kicking,...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-11-09 21:35:42</td>\n",
       "      <td>subject5452</td>\n",
       "      <td>America just did a heel turn.</td>\n",
       "      <td></td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[america, just, did, a, heel, turn]</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-11-05 08:05:15</td>\n",
       "      <td>subject5452</td>\n",
       "      <td></td>\n",
       "      <td>Pokemon theme but everytime they say Pokemon ...</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>[pokemon, theme, but, everytime, they, say, po...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570504</th>\n",
       "      <td>2013-06-28 23:42:37</td>\n",
       "      <td>subject73510000</td>\n",
       "      <td>I get terrified on high stairs with no backin...</td>\n",
       "      <td></td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[i, get, terrified, on, high, stairs, with, no...</td>\n",
       "      <td>37.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.081081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570505</th>\n",
       "      <td>2013-06-28 23:35:48</td>\n",
       "      <td>subject73510000</td>\n",
       "      <td>I listened to an episode of This American Lif...</td>\n",
       "      <td></td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[i, listened, to, an, episode, of, this, ameri...</td>\n",
       "      <td>46.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570506</th>\n",
       "      <td>2013-06-28 19:59:19</td>\n",
       "      <td>subject73510000</td>\n",
       "      <td>Building a good bonfire with a lighter, stick...</td>\n",
       "      <td></td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[building, a, good, bonfire, with, a, lighter,...</td>\n",
       "      <td>52.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570507</th>\n",
       "      <td>2013-06-28 19:33:04</td>\n",
       "      <td>subject73510000</td>\n",
       "      <td>I had the opportunity to get braces when I wa...</td>\n",
       "      <td></td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[i, had, the, opportunity, to, get, braces, wh...</td>\n",
       "      <td>84.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570508</th>\n",
       "      <td>2013-06-20 01:18:54</td>\n",
       "      <td>subject73510000</td>\n",
       "      <td>Screenshot: http://imgur.com/q7Ada4j</td>\n",
       "      <td></td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[screenshot, http, imgur, com, q7ada4j]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>823758 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         date          subject  \\\n",
       "0        2016-12-01 07:47:48       subject5452   \n",
       "1        2016-11-23 03:42:13       subject5452   \n",
       "2        2016-11-22 22:12:02       subject5452   \n",
       "3        2016-11-09 21:35:42       subject5452   \n",
       "4        2016-11-05 08:05:15       subject5452   \n",
       "...                       ...              ...   \n",
       "570504   2013-06-28 23:42:37   subject73510000   \n",
       "570505   2013-06-28 23:35:48   subject73510000   \n",
       "570506   2013-06-28 19:59:19   subject73510000   \n",
       "570507   2013-06-28 19:33:04   subject73510000   \n",
       "570508   2013-06-20 01:18:54   subject73510000   \n",
       "\n",
       "                                                     text  \\\n",
       "0                                                           \n",
       "1                                                           \n",
       "2                                                           \n",
       "3                          America just did a heel turn.    \n",
       "4                                                           \n",
       "...                                                   ...   \n",
       "570504   I get terrified on high stairs with no backin...   \n",
       "570505   I listened to an episode of This American Lif...   \n",
       "570506   Building a good bonfire with a lighter, stick...   \n",
       "570507   I had the opportunity to get braces when I wa...   \n",
       "570508              Screenshot: http://imgur.com/q7Ada4j    \n",
       "\n",
       "                                                    title subset  label  \\\n",
       "0                                                mew_irl   train      0   \n",
       "1                                           Trippy Kong.   train      0   \n",
       "2              Dk rap but the acid is slowly kicking in.   train      0   \n",
       "3                                                          train      0   \n",
       "4        Pokemon theme but everytime they say Pokemon ...  train      0   \n",
       "...                                                   ...    ...    ...   \n",
       "570504                                                      test      0   \n",
       "570505                                                      test      0   \n",
       "570506                                                      test      0   \n",
       "570507                                                      test      0   \n",
       "570508                                                      test      0   \n",
       "\n",
       "                                          tokenized_title  title_len  \\\n",
       "0                                               [mew_irl]        1.0   \n",
       "1                                          [trippy, kong]        2.0   \n",
       "2       [dk, rap, but, the, acid, is, slowly, kicking,...        9.0   \n",
       "3                                                      []        NaN   \n",
       "4       [pokemon, theme, but, everytime, they, say, po...       18.0   \n",
       "...                                                   ...        ...   \n",
       "570504                                                 []        NaN   \n",
       "570505                                                 []        NaN   \n",
       "570506                                                 []        NaN   \n",
       "570507                                                 []        NaN   \n",
       "570508                                                 []        NaN   \n",
       "\n",
       "                                           tokenized_text  text_len  ...  \\\n",
       "0                                                      []       NaN  ...   \n",
       "1                                                      []       NaN  ...   \n",
       "2                                                      []       NaN  ...   \n",
       "3                     [america, just, did, a, heel, turn]       6.0  ...   \n",
       "4                                                      []       NaN  ...   \n",
       "...                                                   ...       ...  ...   \n",
       "570504  [i, get, terrified, on, high, stairs, with, no...      37.0  ...   \n",
       "570505  [i, listened, to, an, episode, of, this, ameri...      46.0  ...   \n",
       "570506  [building, a, good, bonfire, with, a, lighter,...      52.0  ...   \n",
       "570507  [i, had, the, opportunity, to, get, braces, wh...      84.0  ...   \n",
       "570508            [screenshot, http, imgur, com, q7ada4j]       5.0  ...   \n",
       "\n",
       "           quant       sad    motion      verb    ingest    adverb    negemo  \\\n",
       "0       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2       0.000000  0.000000  0.000000  0.111111  0.000000  0.000000  0.000000   \n",
       "3       0.000000  0.000000  0.000000  0.333333  0.000000  0.166667  0.000000   \n",
       "4       0.000000  0.000000  0.000000  0.166667  0.000000  0.000000  0.000000   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "570504  0.027027  0.000000  0.108108  0.162162  0.000000  0.000000  0.054054   \n",
       "570505  0.000000  0.021739  0.000000  0.217391  0.000000  0.021739  0.086957   \n",
       "570506  0.019231  0.000000  0.000000  0.115385  0.038462  0.038462  0.000000   \n",
       "570507  0.000000  0.011905  0.011905  0.333333  0.000000  0.047619  0.035714   \n",
       "570508  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         achieve     death   article  \n",
       "0       0.000000  0.000000  0.000000  \n",
       "1       0.000000  0.000000  0.000000  \n",
       "2       0.000000  0.000000  0.111111  \n",
       "3       0.000000  0.000000  0.166667  \n",
       "4       0.000000  0.000000  0.111111  \n",
       "...          ...       ...       ...  \n",
       "570504  0.000000  0.027027  0.081081  \n",
       "570505  0.021739  0.000000  0.086957  \n",
       "570506  0.000000  0.000000  0.153846  \n",
       "570507  0.023810  0.000000  0.035714  \n",
       "570508  0.000000  0.000000  0.000000  \n",
       "\n",
       "[823758 rows x 75 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def tokenize(t):\n",
    "    return tokenizer.tokenize(t.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'wasn', 't', 'ready', 'to', 'leave', 'buh', 'buw', 'dd', 'sasa']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"I wasn't ready to leave! buh-buw(dd). Sasa .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['tokenized_title'] = writings_df['title'].apply(lambda t: tokenize(t) if type(t)==str and t else None)\n",
    "writings_df['title_len'] = writings_df['tokenized_title'].apply(lambda t: len(t) if type(t)==list and t else None)\n",
    "writings_df['tokenized_text'] = writings_df['text'].apply(lambda t: tokenize(t) if type(t)==str and t else None)\n",
    "writings_df['text_len'] = writings_df['tokenized_text'].apply(lambda t: len(t) if type(t)==list and t else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    127604.00000\n",
       "mean         32.35415\n",
       "std          82.68303\n",
       "min           1.00000\n",
       "25%           6.00000\n",
       "50%          13.00000\n",
       "75%          31.00000\n",
       "max        7201.00000\n",
       "Name: text_len, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.text_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    49752.000000\n",
       "mean        10.701922\n",
       "std          9.282147\n",
       "min          1.000000\n",
       "25%          4.000000\n",
       "50%          8.000000\n",
       "75%         14.000000\n",
       "max        149.000000\n",
       "Name: title_len, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.title_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title_len</th>\n",
       "      <th>text_len</th>\n",
       "      <th>funct</th>\n",
       "      <th>article</th>\n",
       "      <th>affect</th>\n",
       "      <th>negemo</th>\n",
       "      <th>sad</th>\n",
       "      <th>cogmech</th>\n",
       "      <th>inhib</th>\n",
       "      <th>...</th>\n",
       "      <th>feel</th>\n",
       "      <th>excl</th>\n",
       "      <th>future</th>\n",
       "      <th>nonfl</th>\n",
       "      <th>ppron</th>\n",
       "      <th>shehe</th>\n",
       "      <th>i</th>\n",
       "      <th>we</th>\n",
       "      <th>you</th>\n",
       "      <th>they</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subject0</th>\n",
       "      <td>0</td>\n",
       "      <td>20.285714</td>\n",
       "      <td>31.711712</td>\n",
       "      <td>0.544556</td>\n",
       "      <td>0.044899</td>\n",
       "      <td>0.100554</td>\n",
       "      <td>0.021298</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>0.156881</td>\n",
       "      <td>0.007290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009250</td>\n",
       "      <td>0.029561</td>\n",
       "      <td>0.154795</td>\n",
       "      <td>0.001963</td>\n",
       "      <td>0.135275</td>\n",
       "      <td>0.002919</td>\n",
       "      <td>0.084605</td>\n",
       "      <td>0.003715</td>\n",
       "      <td>0.038377</td>\n",
       "      <td>0.005659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1027</th>\n",
       "      <td>0</td>\n",
       "      <td>7.769231</td>\n",
       "      <td>1.190476</td>\n",
       "      <td>0.190928</td>\n",
       "      <td>0.005636</td>\n",
       "      <td>0.013322</td>\n",
       "      <td>0.004919</td>\n",
       "      <td>0.004202</td>\n",
       "      <td>0.051015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.004919</td>\n",
       "      <td>0.087587</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.116719</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.076786</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1055</th>\n",
       "      <td>0</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>79.983193</td>\n",
       "      <td>0.541480</td>\n",
       "      <td>0.048654</td>\n",
       "      <td>0.052665</td>\n",
       "      <td>0.025766</td>\n",
       "      <td>0.002609</td>\n",
       "      <td>0.161580</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>0.024850</td>\n",
       "      <td>0.125660</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.095239</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.038783</td>\n",
       "      <td>0.007599</td>\n",
       "      <td>0.021944</td>\n",
       "      <td>0.021913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1064</th>\n",
       "      <td>1</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>68.410256</td>\n",
       "      <td>0.544914</td>\n",
       "      <td>0.029865</td>\n",
       "      <td>0.140286</td>\n",
       "      <td>0.038420</td>\n",
       "      <td>0.012897</td>\n",
       "      <td>0.148135</td>\n",
       "      <td>0.005281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003128</td>\n",
       "      <td>0.041912</td>\n",
       "      <td>0.150172</td>\n",
       "      <td>0.001942</td>\n",
       "      <td>0.125145</td>\n",
       "      <td>0.002882</td>\n",
       "      <td>0.086882</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.030292</td>\n",
       "      <td>0.004286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1089</th>\n",
       "      <td>0</td>\n",
       "      <td>9.823529</td>\n",
       "      <td>13.254902</td>\n",
       "      <td>0.440338</td>\n",
       "      <td>0.048374</td>\n",
       "      <td>0.082136</td>\n",
       "      <td>0.027137</td>\n",
       "      <td>0.005128</td>\n",
       "      <td>0.125983</td>\n",
       "      <td>0.003108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008413</td>\n",
       "      <td>0.024749</td>\n",
       "      <td>0.104063</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>0.077539</td>\n",
       "      <td>0.006873</td>\n",
       "      <td>0.028960</td>\n",
       "      <td>0.006127</td>\n",
       "      <td>0.027354</td>\n",
       "      <td>0.008226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9917</th>\n",
       "      <td>1</td>\n",
       "      <td>8.983607</td>\n",
       "      <td>95.806897</td>\n",
       "      <td>0.646948</td>\n",
       "      <td>0.040454</td>\n",
       "      <td>0.061069</td>\n",
       "      <td>0.034207</td>\n",
       "      <td>0.007909</td>\n",
       "      <td>0.224470</td>\n",
       "      <td>0.004510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006677</td>\n",
       "      <td>0.048364</td>\n",
       "      <td>0.152385</td>\n",
       "      <td>0.001657</td>\n",
       "      <td>0.120856</td>\n",
       "      <td>0.001722</td>\n",
       "      <td>0.105751</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.004625</td>\n",
       "      <td>0.007889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9918</th>\n",
       "      <td>0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.900901</td>\n",
       "      <td>0.410173</td>\n",
       "      <td>0.045823</td>\n",
       "      <td>0.102763</td>\n",
       "      <td>0.044945</td>\n",
       "      <td>0.003296</td>\n",
       "      <td>0.118872</td>\n",
       "      <td>0.005408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004256</td>\n",
       "      <td>0.023128</td>\n",
       "      <td>0.090074</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.083653</td>\n",
       "      <td>0.013338</td>\n",
       "      <td>0.036926</td>\n",
       "      <td>0.007207</td>\n",
       "      <td>0.021180</td>\n",
       "      <td>0.005003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject992</th>\n",
       "      <td>0</td>\n",
       "      <td>5.872928</td>\n",
       "      <td>19.914122</td>\n",
       "      <td>0.394271</td>\n",
       "      <td>0.053367</td>\n",
       "      <td>0.086128</td>\n",
       "      <td>0.038611</td>\n",
       "      <td>0.003050</td>\n",
       "      <td>0.113686</td>\n",
       "      <td>0.005810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003799</td>\n",
       "      <td>0.024507</td>\n",
       "      <td>0.079539</td>\n",
       "      <td>0.004855</td>\n",
       "      <td>0.069299</td>\n",
       "      <td>0.006907</td>\n",
       "      <td>0.032040</td>\n",
       "      <td>0.003941</td>\n",
       "      <td>0.023434</td>\n",
       "      <td>0.002977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9949</th>\n",
       "      <td>0</td>\n",
       "      <td>10.609756</td>\n",
       "      <td>42.346979</td>\n",
       "      <td>0.435481</td>\n",
       "      <td>0.059932</td>\n",
       "      <td>0.073305</td>\n",
       "      <td>0.031003</td>\n",
       "      <td>0.003585</td>\n",
       "      <td>0.123296</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>0.024724</td>\n",
       "      <td>0.080135</td>\n",
       "      <td>0.001819</td>\n",
       "      <td>0.067461</td>\n",
       "      <td>0.012940</td>\n",
       "      <td>0.029135</td>\n",
       "      <td>0.006191</td>\n",
       "      <td>0.013108</td>\n",
       "      <td>0.006086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9961</th>\n",
       "      <td>0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>26.389313</td>\n",
       "      <td>0.380818</td>\n",
       "      <td>0.030147</td>\n",
       "      <td>0.118331</td>\n",
       "      <td>0.026220</td>\n",
       "      <td>0.001940</td>\n",
       "      <td>0.101743</td>\n",
       "      <td>0.007136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>0.018475</td>\n",
       "      <td>0.073481</td>\n",
       "      <td>0.001981</td>\n",
       "      <td>0.086225</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>0.053193</td>\n",
       "      <td>0.002178</td>\n",
       "      <td>0.027518</td>\n",
       "      <td>0.001803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>340 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             label  title_len   text_len     funct   article    affect  \\\n",
       "subject                                                                  \n",
       "subject0         0  20.285714  31.711712  0.544556  0.044899  0.100554   \n",
       "subject1027      0   7.769231   1.190476  0.190928  0.005636  0.013322   \n",
       "subject1055      0  16.666667  79.983193  0.541480  0.048654  0.052665   \n",
       "subject1064      1  13.000000  68.410256  0.544914  0.029865  0.140286   \n",
       "subject1089      0   9.823529  13.254902  0.440338  0.048374  0.082136   \n",
       "...            ...        ...        ...       ...       ...       ...   \n",
       "subject9917      1   8.983607  95.806897  0.646948  0.040454  0.061069   \n",
       "subject9918      0   5.000000  11.900901  0.410173  0.045823  0.102763   \n",
       "subject992       0   5.872928  19.914122  0.394271  0.053367  0.086128   \n",
       "subject9949      0  10.609756  42.346979  0.435481  0.059932  0.073305   \n",
       "subject9961      0   5.000000  26.389313  0.380818  0.030147  0.118331   \n",
       "\n",
       "               negemo       sad   cogmech     inhib  ...      feel      excl  \\\n",
       "subject                                              ...                       \n",
       "subject0     0.021298  0.001919  0.156881  0.007290  ...  0.009250  0.029561   \n",
       "subject1027  0.004919  0.004202  0.051015  0.000000  ...  0.011765  0.004919   \n",
       "subject1055  0.025766  0.002609  0.161580  0.005952  ...  0.002272  0.024850   \n",
       "subject1064  0.038420  0.012897  0.148135  0.005281  ...  0.003128  0.041912   \n",
       "subject1089  0.027137  0.005128  0.125983  0.003108  ...  0.008413  0.024749   \n",
       "...               ...       ...       ...       ...  ...       ...       ...   \n",
       "subject9917  0.034207  0.007909  0.224470  0.004510  ...  0.006677  0.048364   \n",
       "subject9918  0.044945  0.003296  0.118872  0.005408  ...  0.004256  0.023128   \n",
       "subject992   0.038611  0.003050  0.113686  0.005810  ...  0.003799  0.024507   \n",
       "subject9949  0.031003  0.003585  0.123296  0.004145  ...  0.002448  0.024724   \n",
       "subject9961  0.026220  0.001940  0.101743  0.007136  ...  0.002923  0.018475   \n",
       "\n",
       "               future     nonfl     ppron     shehe         i        we  \\\n",
       "subject                                                                   \n",
       "subject0     0.154795  0.001963  0.135275  0.002919  0.084605  0.003715   \n",
       "subject1027  0.087587  0.000717  0.116719  0.000717  0.076786  0.000000   \n",
       "subject1055  0.125660  0.001161  0.095239  0.005000  0.038783  0.007599   \n",
       "subject1064  0.150172  0.001942  0.125145  0.002882  0.086882  0.000803   \n",
       "subject1089  0.104063  0.001497  0.077539  0.006873  0.028960  0.006127   \n",
       "...               ...       ...       ...       ...       ...       ...   \n",
       "subject9917  0.152385  0.001657  0.120856  0.001722  0.105751  0.000779   \n",
       "subject9918  0.090074  0.001046  0.083653  0.013338  0.036926  0.007207   \n",
       "subject992   0.079539  0.004855  0.069299  0.006907  0.032040  0.003941   \n",
       "subject9949  0.080135  0.001819  0.067461  0.012940  0.029135  0.006191   \n",
       "subject9961  0.073481  0.001981  0.086225  0.001534  0.053193  0.002178   \n",
       "\n",
       "                  you      they  \n",
       "subject                          \n",
       "subject0     0.038377  0.005659  \n",
       "subject1027  0.039216  0.000000  \n",
       "subject1055  0.021944  0.021913  \n",
       "subject1064  0.030292  0.004286  \n",
       "subject1089  0.027354  0.008226  \n",
       "...               ...       ...  \n",
       "subject9917  0.004625  0.007889  \n",
       "subject9918  0.021180  0.005003  \n",
       "subject992   0.023434  0.002977  \n",
       "subject9949  0.013108  0.006086  \n",
       "subject9961  0.027518  0.001803  \n",
       "\n",
       "[340 rows x 67 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.groupby('subject').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title_len</th>\n",
       "      <th>text_len</th>\n",
       "      <th>all_tokens</th>\n",
       "      <th>funct</th>\n",
       "      <th>article</th>\n",
       "      <th>affect</th>\n",
       "      <th>negemo</th>\n",
       "      <th>sad</th>\n",
       "      <th>cogmech</th>\n",
       "      <th>...</th>\n",
       "      <th>feel</th>\n",
       "      <th>excl</th>\n",
       "      <th>future</th>\n",
       "      <th>nonfl</th>\n",
       "      <th>ppron</th>\n",
       "      <th>shehe</th>\n",
       "      <th>i</th>\n",
       "      <th>we</th>\n",
       "      <th>you</th>\n",
       "      <th>they</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>299</td>\n",
       "      <td>296</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>...</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>40</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       date  title_len  text_len  all_tokens  funct  article  affect  negemo  \\\n",
       "label                                                                          \n",
       "0       299        296       299         299    299      299     299     299   \n",
       "1        41         40        41          41     41       41      41      41   \n",
       "\n",
       "       sad  cogmech  ...  feel  excl  future  nonfl  ppron  shehe    i   we  \\\n",
       "label                ...                                                      \n",
       "0      299      299  ...   299   299     299    299    299    299  299  299   \n",
       "1       41       41  ...    41    41      41     41     41     41   41   41   \n",
       "\n",
       "       you  they  \n",
       "label             \n",
       "0      299   299  \n",
       "1       41    41  \n",
       "\n",
       "[2 rows x 68 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.groupby('subject').max().groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of posts per user 146.35882352941175\n",
      "Average number of comments per user 376.2970588235294\n"
     ]
    }
   ],
   "source": [
    "print(\"Average number of posts per user\", writings_df.groupby('subject').count().title.mean())\n",
    "print(\"Average number of comments per user\", writings_df.groupby('subject').count().text.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>title_len</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>text_len</th>\n",
       "      <th>all_tokens</th>\n",
       "      <th>...</th>\n",
       "      <th>feel</th>\n",
       "      <th>excl</th>\n",
       "      <th>future</th>\n",
       "      <th>nonfl</th>\n",
       "      <th>ppron</th>\n",
       "      <th>shehe</th>\n",
       "      <th>i</th>\n",
       "      <th>we</th>\n",
       "      <th>you</th>\n",
       "      <th>they</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>Okay friends so I messed up and posted to do a...</td>\n",
       "      <td>2017-04-25 22:37:57</td>\n",
       "      <td>Sorry for that, I truly didn't think it was go...</td>\n",
       "      <td>0</td>\n",
       "      <td>[okay, friends, so, i, messed, up, and, posted...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>[sorry, for, that, i, truly, didn, t, think, i...</td>\n",
       "      <td>120.0</td>\n",
       "      <td>[sorry, for, that, i, truly, didn, t, think, i...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103896</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-09-16 06:29:13</td>\n",
       "      <td>You've got plenty of time to fix that. You can...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you, ve, got, plenty, of, time, to, fix, that...</td>\n",
       "      <td>104.0</td>\n",
       "      <td>[you, ve, got, plenty, of, time, to, fix, that...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-11-24 01:33:22</td>\n",
       "      <td>LCD, Glass animals, Kendrick, The Weeknd, Jack...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[lcd, glass, animals, kendrick, the, weeknd, j...</td>\n",
       "      <td>127.0</td>\n",
       "      <td>[lcd, glass, animals, kendrick, the, weeknd, j...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015748</td>\n",
       "      <td>0.007874</td>\n",
       "      <td>0.047244</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055118</td>\n",
       "      <td>0.015748</td>\n",
       "      <td>0.039370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>Getting that coachella bod</td>\n",
       "      <td>2018-01-09 00:54:06</td>\n",
       "      <td>First I want to say whatever skin is your skin...</td>\n",
       "      <td>0</td>\n",
       "      <td>[getting, that, coachella, bod]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[first, i, want, to, say, whatever, skin, is, ...</td>\n",
       "      <td>149.0</td>\n",
       "      <td>[first, i, want, to, say, whatever, skin, is, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013072</td>\n",
       "      <td>0.052288</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045752</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026144</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-03-12 17:14:03</td>\n",
       "      <td>Not the same but me and my wife saw a man and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[not, the, same, but, me, and, my, wife, saw, ...</td>\n",
       "      <td>151.0</td>\n",
       "      <td>[not, the, same, but, me, and, my, wife, saw, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026490</td>\n",
       "      <td>0.066225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099338</td>\n",
       "      <td>0.046358</td>\n",
       "      <td>0.052980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170652</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-05-28 12:23:00</td>\n",
       "      <td>/r/keto /r/ketorecipes /r/ketodessert all are ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[r, keto, r, ketorecipes, r, ketodessert, all,...</td>\n",
       "      <td>197.0</td>\n",
       "      <td>[r, keto, r, ketorecipes, r, ketodessert, all,...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005076</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>0.076142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030457</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170653</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-05-28 12:32:36</td>\n",
       "      <td>its okay dont worry . as long as you don't exc...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[its, okay, dont, worry, as, long, as, you, do...</td>\n",
       "      <td>109.0</td>\n",
       "      <td>[its, okay, dont, worry, as, long, as, you, do...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027523</td>\n",
       "      <td>0.064220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036697</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170662</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-06-20 00:33:57</td>\n",
       "      <td>the national number is :1919 here are more com...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[the, national, number, is, 1919, here, are, m...</td>\n",
       "      <td>115.0</td>\n",
       "      <td>[the, national, number, is, 1919, here, are, m...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170693</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 11:29:21</td>\n",
       "      <td>this is my personal experience ,it may not ref...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[this, is, my, personal, experience, it, may, ...</td>\n",
       "      <td>153.0</td>\n",
       "      <td>[this, is, my, personal, experience, it, may, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006536</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.065359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.006536</td>\n",
       "      <td>0.026144</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170695</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 20:00:31</td>\n",
       "      <td>i am aware of stats now and then. i was just s...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[i, am, aware, of, stats, now, and, then, i, w...</td>\n",
       "      <td>198.0</td>\n",
       "      <td>[i, am, aware, of, stats, now, and, then, i, w...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.085859</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.116162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025253</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7655 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            subject                                              title  \\\n",
       "122     subject8292  Okay friends so I messed up and posted to do a...   \n",
       "390     subject8292                                                NaN   \n",
       "498     subject8292                                                NaN   \n",
       "752     subject8292                         Getting that coachella bod   \n",
       "904     subject8292                                                NaN   \n",
       "...             ...                                                ...   \n",
       "170652   subject217                                                NaN   \n",
       "170653   subject217                                                NaN   \n",
       "170662   subject217                                                NaN   \n",
       "170693   subject217                                                NaN   \n",
       "170695   subject217                                                NaN   \n",
       "\n",
       "                       date  \\\n",
       "122     2017-04-25 22:37:57   \n",
       "390     2017-09-16 06:29:13   \n",
       "498     2017-11-24 01:33:22   \n",
       "752     2018-01-09 00:54:06   \n",
       "904     2018-03-12 17:14:03   \n",
       "...                     ...   \n",
       "170652  2018-05-28 12:23:00   \n",
       "170653  2018-05-28 12:32:36   \n",
       "170662  2018-06-20 00:33:57   \n",
       "170693  2018-08-19 11:29:21   \n",
       "170695  2018-08-19 20:00:31   \n",
       "\n",
       "                                                     text  label  \\\n",
       "122     Sorry for that, I truly didn't think it was go...      0   \n",
       "390     You've got plenty of time to fix that. You can...      0   \n",
       "498     LCD, Glass animals, Kendrick, The Weeknd, Jack...      0   \n",
       "752     First I want to say whatever skin is your skin...      0   \n",
       "904     Not the same but me and my wife saw a man and ...      0   \n",
       "...                                                   ...    ...   \n",
       "170652  /r/keto /r/ketorecipes /r/ketodessert all are ...      0   \n",
       "170653  its okay dont worry . as long as you don't exc...      0   \n",
       "170662  the national number is :1919 here are more com...      0   \n",
       "170693  this is my personal experience ,it may not ref...      0   \n",
       "170695  i am aware of stats now and then. i was just s...      0   \n",
       "\n",
       "                                          tokenized_title  title_len  \\\n",
       "122     [okay, friends, so, i, messed, up, and, posted...       34.0   \n",
       "390                                                  None        NaN   \n",
       "498                                                  None        NaN   \n",
       "752                       [getting, that, coachella, bod]        4.0   \n",
       "904                                                  None        NaN   \n",
       "...                                                   ...        ...   \n",
       "170652                                               None        NaN   \n",
       "170653                                               None        NaN   \n",
       "170662                                               None        NaN   \n",
       "170693                                               None        NaN   \n",
       "170695                                               None        NaN   \n",
       "\n",
       "                                           tokenized_text  text_len  \\\n",
       "122     [sorry, for, that, i, truly, didn, t, think, i...     120.0   \n",
       "390     [you, ve, got, plenty, of, time, to, fix, that...     104.0   \n",
       "498     [lcd, glass, animals, kendrick, the, weeknd, j...     127.0   \n",
       "752     [first, i, want, to, say, whatever, skin, is, ...     149.0   \n",
       "904     [not, the, same, but, me, and, my, wife, saw, ...     151.0   \n",
       "...                                                   ...       ...   \n",
       "170652  [r, keto, r, ketorecipes, r, ketodessert, all,...     197.0   \n",
       "170653  [its, okay, dont, worry, as, long, as, you, do...     109.0   \n",
       "170662  [the, national, number, is, 1919, here, are, m...     115.0   \n",
       "170693  [this, is, my, personal, experience, it, may, ...     153.0   \n",
       "170695  [i, am, aware, of, stats, now, and, then, i, w...     198.0   \n",
       "\n",
       "                                               all_tokens  ...      feel  \\\n",
       "122     [sorry, for, that, i, truly, didn, t, think, i...  ...  0.006494   \n",
       "390     [you, ve, got, plenty, of, time, to, fix, that...  ...  0.000000   \n",
       "498     [lcd, glass, animals, kendrick, the, weeknd, j...  ...  0.015748   \n",
       "752     [first, i, want, to, say, whatever, skin, is, ...  ...  0.013072   \n",
       "904     [not, the, same, but, me, and, my, wife, saw, ...  ...  0.000000   \n",
       "...                                                   ...  ...       ...   \n",
       "170652  [r, keto, r, ketorecipes, r, ketodessert, all,...  ...  0.005076   \n",
       "170653  [its, okay, dont, worry, as, long, as, you, do...  ...  0.000000   \n",
       "170662  [the, national, number, is, 1919, here, are, m...  ...  0.000000   \n",
       "170693  [this, is, my, personal, experience, it, may, ...  ...  0.006536   \n",
       "170695  [i, am, aware, of, stats, now, and, then, i, w...  ...  0.005051   \n",
       "\n",
       "            excl    future     nonfl     ppron     shehe         i        we  \\\n",
       "122     0.012987  0.142857  0.000000  0.110390  0.000000  0.103896  0.006494   \n",
       "390     0.000000  0.163462  0.000000  0.163462  0.000000  0.009615  0.000000   \n",
       "498     0.007874  0.047244  0.000000  0.055118  0.015748  0.039370  0.000000   \n",
       "752     0.052288  0.098039  0.000000  0.071895  0.000000  0.045752  0.000000   \n",
       "904     0.026490  0.066225  0.000000  0.099338  0.046358  0.052980  0.000000   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "170652  0.020305  0.076142  0.000000  0.076142  0.000000  0.045685  0.000000   \n",
       "170653  0.027523  0.064220  0.000000  0.045872  0.000000  0.009174  0.000000   \n",
       "170662  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "170693  0.039216  0.065359  0.000000  0.058824  0.006536  0.026144  0.019608   \n",
       "170695  0.075758  0.085859  0.005051  0.116162  0.000000  0.090909  0.000000   \n",
       "\n",
       "             you      they  \n",
       "122     0.000000  0.000000  \n",
       "390     0.153846  0.000000  \n",
       "498     0.000000  0.000000  \n",
       "752     0.026144  0.000000  \n",
       "904     0.000000  0.000000  \n",
       "...          ...       ...  \n",
       "170652  0.030457  0.000000  \n",
       "170653  0.036697  0.000000  \n",
       "170662  0.000000  0.000000  \n",
       "170693  0.000000  0.006536  \n",
       "170695  0.025253  0.000000  \n",
       "\n",
       "[7655 rows x 74 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[(~writings_df['text_len'].isnull()) & (writings_df['text_len'] > 100)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features and encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_features = {\n",
    "    \"max_features\": 40000,\n",
    "    # cut texts after this number of words\n",
    "    # (among top max_features most common words)\n",
    "    \"maxlen\": 100,\n",
    "    \"embedding_dim\": 50,\n",
    "    \"user_level\": True,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_NRC(nrc_path):\n",
    "    word_emotions = {}\n",
    "    emotion_words = {}\n",
    "    with open(nrc_path) as in_f:\n",
    "        for line in in_f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            word, emotion, label = line.split()\n",
    "            if word not in word_emotions:\n",
    "                word_emotions[word] = set()\n",
    "            if emotion not in emotion_words:\n",
    "                emotion_words[emotion] = set()\n",
    "            label = int(label)\n",
    "            if label:\n",
    "                word_emotions[word].add(emotion)\n",
    "                emotion_words[emotion].add(word)\n",
    "    return emotion_words\n",
    "\n",
    "nrc_lexicon_path = root_dir + '/resources/NRC-Sentiment-Emotion-Lexicons/NRC-Sentiment-Emotion-Lexicons/NRC-Emotion-Lexicon-v0.92/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'\n",
    "nrc_lexicon = load_NRC(nrc_lexicon_path)\n",
    "emotions = list(nrc_lexicon.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_emotions(tokens, emotion_lexicon, emotions, relative=True):\n",
    "    text_len = len(tokens)\n",
    "    encoded_emotions = [0 for e in emotions]\n",
    "    for i, emotion in enumerate(emotions):\n",
    "        try:\n",
    "            emotion_words = [t for t in tokens if t in emotion_lexicon[emotion]]\n",
    "            if relative:\n",
    "                encoded_emotions[i] = len(emotion_words) / len(tokens)\n",
    "            else:\n",
    "                encoded_emotions[i] = len(emotion_words)\n",
    "        except ValueError:\n",
    "            print(\"Emotion not found.\")\n",
    "    return encoded_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from liwc_readDict import readDict\n",
    "\n",
    "liwc = readDict(root_dir + '/resources/liwc.dic')\n",
    "\n",
    "categories = set([c for (w,c) in liwc])\n",
    "len(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Char n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(tokens):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Personal pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_person_pronouns = {\"i\", \"me\", \"my\", \"mine\", \"myself\"}\n",
    "def encode_pronouns(tokens, pronouns={\"i\", \"me\", \"my\", \"mine\", \"myself\"}, relative=True):\n",
    "    if not tokens:\n",
    "        return np.nan\n",
    "    text_len = len(tokens)\n",
    "    nr_pronouns = len([t for t in tokens if t in pronouns])\n",
    "    if relative:\n",
    "        return nr_pronouns/text_len\n",
    "    else:\n",
    "        return nr_pronouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words(\"english\")\n",
    "def encode_stopwords(tokens, stopwords=stopword_list):\n",
    "    encoded_stopwords = [0 for s in stopword_list]\n",
    "    if not tokens:\n",
    "        return encoded_stopwords\n",
    "    for i, stopword in enumerate(stopwords):\n",
    "        if stopword in tokens:\n",
    "            encoded_stopwords[i] += 1\n",
    "    return encoded_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def load_erisk_data(writings_df, voc_size, emotion_lexicon, seq_len, emotions =  \n",
    "                    ['anger', 'anticipation', 'disgust', 'fear', 'joy', \n",
    "                     'negative', 'positive', 'sadness', 'surprise', 'trust'],\n",
    "                    liwc_categories = categories,\n",
    "                    pronouns = [\"i\", \"me\", \"my\", \"mine\", \"myself\"],\n",
    "                    train_prop=0.7, valid_prop=0.3, test_slice=2,\n",
    "                    nr_slices=5,\n",
    "                    min_post_len=3, min_word_len=1, \n",
    "                    user_level=True, vocabulary=None,\n",
    "                   logger=logger):\n",
    "    logger.debug(\"Loading data...\\n\")\n",
    "    if not vocabulary:\n",
    "        vocabulary = {}\n",
    "        word_freqs = Counter()\n",
    "        for words in writings_df.tokenized_text:\n",
    "            word_freqs.update(words)\n",
    "        for words in writings_df.tokenized_title:\n",
    "            word_freqs.update(words)\n",
    "        i = 1\n",
    "        for w, f in word_freqs.most_common(voc_size-2): # keeping voc_size-1 for unk\n",
    "            if len(w) < min_word_len:\n",
    "                continue\n",
    "            vocabulary[w] = i\n",
    "            i += 1\n",
    "    tokens_data_train = []\n",
    "    categ_data_train = []\n",
    "    sparse_data_train = []\n",
    "    tokens_data_valid = []\n",
    "    categ_data_valid = []\n",
    "    sparse_data_valid = []\n",
    "    tokens_data_test = []\n",
    "    categ_data_test = []\n",
    "    sparse_data_test = []\n",
    "    labels_train = []\n",
    "    users_train = []\n",
    "    labels_valid = []\n",
    "    users_valid = []\n",
    "    users_test = []\n",
    "    labels_test = []\n",
    "    if 'subset' in writings_df.columns:\n",
    "        training_subjects = list(set(writings_df[writings_df['subset']=='train'].subject))\n",
    "        test_subjects = list(set(writings_df[writings_df['subset']=='test'].subject))\n",
    "    else:\n",
    "        all_subjects = sorted(list(set(writings_df.subject)))\n",
    "        training_subjects_size = int(len(all_subjects) * train_prop)\n",
    "        test_subjects_size = len(all_subjects) - training_subjects_size\n",
    "        # Cross-validation, with fixed slice as input\n",
    "        test_prop = 1-train_prop\n",
    "        test_slice = min(test_slice, nr_slices)\n",
    "        logger.debug(\"start index: %f, from %f\\n\" % (\n",
    "            len(all_subjects)*(1/nr_slices)*test_slice, test_prop*test_slice))\n",
    "        start_slice = int(len(all_subjects)*(1/nr_slices)*test_slice)\n",
    "        test_subjects = all_subjects[start_slice: start_slice+test_subjects_size]\n",
    "        training_subjects = [s for s in all_subjects if s not in test_subjects]\n",
    "    training_subjects = sorted(training_subjects) # ensuring reproducibility\n",
    "    valid_subjects_size = int(len(training_subjects) * valid_prop)\n",
    "    valid_subjects = training_subjects[:valid_subjects_size]\n",
    "    training_subjects = training_subjects[valid_subjects_size:]\n",
    "    categories = [c for c in liwc_categories if c in writings_df.columns]\n",
    "    logger.debug(\"%d training users, %d validation users, %d test users.\" % (\n",
    "        len(training_subjects), \n",
    "          len(valid_subjects),\n",
    "          len(test_subjects)))\n",
    "#     training_rows = writings_df[writings_df['subject'].isin(training_subjects)].sample(frac=1) # shuffling\n",
    "#     test_rows = writings_df[~writings_df['subject'].isin(training_subjects)].sample(frac=1)\n",
    "#     positive_training_users = training_rows.groupby('subject').max().groupby('label').count().date[1]\n",
    "#     positive_test_users = test_rows.groupby('subject').max().groupby('label').count().date[1]\n",
    "#     print(\"Positive training users: \", positive_training_users, \", positive test users: \", positive_test_users)\n",
    "    def encode_text(tokens):\n",
    "        # Using voc_size-1 value for OOV token\n",
    "        encoded_tokens = [vocabulary.get(w, voc_size-1) for w in tokens]\n",
    "        encoded_emotions = encode_emotions(tokens, emotion_lexicon, emotions)\n",
    "        encoded_pronouns = encode_pronouns(tokens, pronouns)\n",
    "        encoded_stopwords = encode_stopwords(tokens)\n",
    "        return (encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords)\n",
    "    user_level_texts = {}\n",
    "    for row in writings_df.itertuples():\n",
    "        words = []\n",
    "        if row.tokenized_title:\n",
    "            words.extend(row.tokenized_title)\n",
    "        if row.tokenized_text:\n",
    "            words.extend(row.tokenized_text)\n",
    "        if not words or len(words)<min_post_len:\n",
    "            continue\n",
    "        label = row.label\n",
    "        liwc_categs = [getattr(row, categ) for categ in categories]\n",
    "        if row.subject not in user_level_texts.keys():\n",
    "            user_level_texts[row.subject] = {}\n",
    "            user_level_texts[row.subject]['texts'] = [words]\n",
    "            user_level_texts[row.subject]['label'] = label\n",
    "            user_level_texts[row.subject]['liwc'] = [liwc_categs]\n",
    "        else:\n",
    "            user_level_texts[row.subject]['texts'].append(words) # TODO: sort datapoints chronologically\n",
    "            user_level_texts[row.subject]['liwc'].append(liwc_categs)\n",
    "\n",
    "    for subject in user_level_texts.keys():\n",
    "        texts = user_level_texts[subject]['texts']\n",
    "        label = user_level_texts[subject]['label']\n",
    "        if user_level:\n",
    "            all_words = [sum(texts, [])] # merge all texts in one list\n",
    "            liwc_aggreg = [np.array(user_level_texts[subject]['liwc']).mean(axis=0).tolist()]\n",
    "        else:\n",
    "            all_words = texts\n",
    "            liwc_aggreg = user_level_texts[subject]['liwc']\n",
    "        for i, words in enumerate(all_words):\n",
    "            encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords = encode_text(words)\n",
    "            subject_id = int(subject.split('t')[1])\n",
    "            if subject in training_subjects:\n",
    "                tokens_data_train.append(encoded_tokens)\n",
    "                categ_data_train.append(encoded_emotions + [encoded_pronouns] + liwc_aggreg[i])\n",
    "                sparse_data_train.append(encoded_stopwords)\n",
    "                labels_train.append(label)\n",
    "                users_train.append(subject_id)\n",
    "            elif subject in valid_subjects:\n",
    "                tokens_data_valid.append(encoded_tokens)\n",
    "                categ_data_valid.append(encoded_emotions + [encoded_pronouns]  + liwc_aggreg[i])\n",
    "                sparse_data_valid.append(encoded_stopwords)\n",
    "                labels_valid.append(label)\n",
    "                users_valid.append(subject_id)\n",
    "            else:\n",
    "                tokens_data_test.append(encoded_tokens)\n",
    "                categ_data_test.append(encoded_emotions + [encoded_pronouns] + liwc_aggreg[i])\n",
    "                sparse_data_test.append(encoded_stopwords)\n",
    "                labels_test.append(label)\n",
    "                users_test.append(subject_id)\n",
    "        \n",
    "    # using zeros for padding\n",
    "    tokens_data_train_padded = sequence.pad_sequences(tokens_data_train, maxlen=seq_len)\n",
    "    tokens_data_valid_padded = sequence.pad_sequences(tokens_data_valid, maxlen=seq_len)\n",
    "    tokens_data_test_padded = sequence.pad_sequences(tokens_data_test, maxlen=seq_len)\n",
    "        \n",
    "    return ([np.array(tokens_data_train_padded), np.array(categ_data_train), np.array(sparse_data_train),\n",
    "            np.array(users_train)],\n",
    "            np.array(labels_train)), \\\n",
    "            ([np.array(tokens_data_valid_padded),\n",
    "              np.array(categ_data_valid), np.array(sparse_data_valid),\n",
    "            np.array(users_valid)],\n",
    "            np.array(labels_valid)), \\\n",
    "            ([np.array(tokens_data_test_padded), np.array(categ_data_test), np.array(sparse_data_test),\n",
    "             np.array(users_test)],\n",
    "             np.array(labels_test)), vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "331 training users, 141 validation users, 815 test users.\n"
     ]
    }
   ],
   "source": [
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "(x_train, y_train), (x_valid, y_valid), (x_test, y_test), voc = load_erisk_data(writings_df, \n",
    "                                                            seq_len=hyperparams_features['maxlen'],\n",
    "                                                            voc_size=hyperparams_features['max_features'],\n",
    "                                                           emotion_lexicon=nrc_lexicon,\n",
    "                                                           emotions=emotions,\n",
    "                                                           user_level=hyperparams_features['user_level'],\n",
    "                                                                                logger=logger\n",
    "#                                                            vocabulary=pickle.load(open('vocabulary20K_selfharm.pkl', 'rb'))\n",
    "                                                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330 train sequences\n",
      "141 train sequences\n",
      "815 test sequences\n"
     ]
    }
   ],
   "source": [
    "x_train_seq, x_train_categ, x_train_sparse, x_train_users = x_train\n",
    "x_valid_seq, x_valid_categ, x_valid_sparse, x_valid_users = x_valid\n",
    "x_test_seq, x_test_categ, x_test_sparse, x_test_users = x_test\n",
    "print(len(x_train_seq), 'train sequences')\n",
    "print(len(x_valid_seq), 'train sequences')\n",
    "print(len(x_test_seq), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 positive training examples\n",
      "24 positive validation examples\n",
      "73 positive test examples\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(y_train).sum(), \"positive training examples\")\n",
    "print(pd.Series(y_valid).sum(), \"positive validation examples\")\n",
    "\n",
    "print(pd.Series(y_test).sum(), \"positive test examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[   18,     7,    31, ...,     4,  8620,  1484],\n",
       "        [   41,   126,     3, ...,     2,    31,  4426],\n",
       "        [   58,    16,    54, ...,    78,    10,    33],\n",
       "        ...,\n",
       "        [   46,   634,    44, ...,    22,   213,   380],\n",
       "        [17754,  3596,   172, ...,  6724, 34222,  4128],\n",
       "        [   52,  5949,   389, ...,   299,    17,    36]], dtype=int32),\n",
       " array([[0.01253616, 0.01735776, 0.01253616, ..., 0.04086183, 0.00485555,\n",
       "         0.0107988 ],\n",
       "        [0.01246215, 0.02469136, 0.01094805, ..., 0.03599436, 0.0048635 ,\n",
       "         0.01677184],\n",
       "        [0.00677966, 0.01423729, 0.00338983, ..., 0.05674026, 0.00545086,\n",
       "         0.03650419],\n",
       "        ...,\n",
       "        [0.01602136, 0.03738318, 0.00934579, ..., 0.0257253 , 0.        ,\n",
       "         0.02617342],\n",
       "        [0.01342038, 0.02538992, 0.01242292, ..., 0.0477066 , 0.00534212,\n",
       "         0.02001849],\n",
       "        [0.01609575, 0.01981015, 0.01114321, ..., 0.04797595, 0.01213157,\n",
       "         0.00791681]]),\n",
       " array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 1, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 0, 1, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 1, 0]]),\n",
       " array([5452, 6310, 4155, 4916, 9903, 8750, 6414, 4592,  899, 5233, 9499,\n",
       "        8444, 6758, 4954, 5719, 4061, 7001, 8384, 6386, 9873, 7362, 7925,\n",
       "        9043, 4169, 6892, 4071, 4858, 5808, 7442, 8361, 8561, 6670,  898,\n",
       "        5177, 5236, 4569, 6214, 6910, 9654, 5969, 6947, 5512, 5311, 5095,\n",
       "        8587, 8740, 7632, 8240, 7249, 5838, 6877, 6994, 7778,  874, 4836,\n",
       "        9337,   96, 4470, 6790, 6695, 4443, 6837, 7831, 4551,  992, 9090,\n",
       "        5517, 9195, 5746, 6394, 4225, 5028, 5984, 9078, 6292, 6269, 9589,\n",
       "        5759, 5085,  559, 5220,  489, 9716, 8016,  811, 4113, 7616, 8401,\n",
       "        6219, 5532, 5854, 6216, 9744,  508, 8340, 7351, 7285, 9015, 6951,\n",
       "        4103, 5954, 8057, 8338, 4994, 4112, 9789, 6731, 6461, 9950, 4410,\n",
       "        8054, 7416, 5115, 4896, 5779, 7687, 6168, 8173, 7422,  536, 5297,\n",
       "        7617, 8512, 7483, 5663, 7165, 6680, 5067, 9098, 9419, 9773, 8377,\n",
       "         851, 7927, 6088, 6139, 4707, 6138, 8701, 8053, 4406, 6288,  467,\n",
       "        6525, 9003, 9230, 8597, 6334, 6792, 9218, 9597, 5802, 7433, 5241,\n",
       "        7529,  785, 6410, 7066, 5776, 5660, 4196, 5469, 9273, 9803, 7468,\n",
       "        7248, 9978, 8236, 6333, 5169, 5426, 4114, 6154, 6956, 4450, 4421,\n",
       "        5160, 8841, 4999, 7278, 7710, 7320, 6462, 4304, 6921, 5173, 7662,\n",
       "        8167, 7469, 3813, 8562, 4556, 7355, 9244, 6756, 5461, 5825, 4982,\n",
       "        7809,  531, 4961, 8247, 6755,  901,  959, 5577, 4073, 8900, 5155,\n",
       "        6978, 9334, 8296, 6800, 5103, 8754, 6358, 9225, 5118, 6293, 5078,\n",
       "         828, 5364, 9961, 6030, 6522, 4276, 5325, 6863, 5614, 8200, 6037,\n",
       "        7478, 9153, 5015, 6302, 7301, 4588, 8392, 7639, 7371,  896, 9371,\n",
       "        5196, 5830, 9653,  803, 4529, 4482, 8411, 7937, 6083, 6412, 6620,\n",
       "         973, 4838, 8932, 6029, 8133, 6644, 4244, 4283, 4371,  689, 6639,\n",
       "        8486, 7597, 6072, 6114, 8254, 4072, 9928,  626, 6075, 4351, 6407,\n",
       "        7388, 8255, 7779, 4222, 6714, 9982, 9436, 9981, 4153,   88, 8799,\n",
       "         545, 5221, 9166, 7251, 8032, 4110, 4160, 4187, 7940, 7087, 8371,\n",
       "        8127, 8607, 7077, 6807, 5562, 8986, 7221, 6446, 6210, 5127, 8720,\n",
       "        8657, 9229,  758, 3883,  845, 6681, 5711, 3944, 3750, 3859, 3788,\n",
       "        3901, 3787,  366, 3727,  375, 3772, 3835,  398, 3672, 3763,  381])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.56313993, 4.45945946])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 40000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "def load_embeddings(path, embedding_dim, voc):\n",
    "    # random matrix with mean value = 0\n",
    "    embedding_matrix = np.random.random((len(voc)+2, embedding_dim)) - 0.5 # voc + unk + pad value(0)\n",
    "#     embedding_matrix = np.zeros((len(voc)+1, embedding_dim))\n",
    "\n",
    "    f = open(path)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_i = voc.get(word)\n",
    "        if word_i is not None:\n",
    "            embedding_matrix[word_i] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print('Total %s word vectors.' % len(embedding_matrix))\n",
    "\n",
    " \n",
    "    return embedding_matrix\n",
    "\n",
    "# pretrained_embeddings_path = '/home/ana/resources/glove.6B/glove.6B.%dd.txt' % hyperparams_features['embedding_dim']\n",
    "pretrained_embeddings_path = root_dir + '/resources/glove.twitter.27B/glove.twitter.27B.%dd.txt' % hyperparams_features['embedding_dim']\n",
    "embedding_matrix = load_embeddings(pretrained_embeddings_path, hyperparams_features['embedding_dim'], voc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330, 179)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'lstm_units': 100,\n",
    "    'dense_bow_units': 5,\n",
    "    'dropout': 0.14,\n",
    "    'l2_dense': 0.00011,\n",
    "    'l2_embeddings': 0.00001,\n",
    "    'optimizer': 'adam',\n",
    "    'decay': 0.0001,\n",
    "    'lr': 0.001,\n",
    "    \"batch_size\": 32,\n",
    "    \"trainable_embeddings\": False,\n",
    "    \"reduce_lr_factor\": 0.002,\n",
    "    \"reduce_lr_patience\": 50,\n",
    "    \"freeze_patience\": 50,\n",
    "    'threshold': 0.5,\n",
    "    'ignore_layer': ['lstm_layers']\n",
    "\n",
    "}\n",
    "if not hyperparams['optimizer']:\n",
    "    hyperparams['optimizer'] = optimizers.Adam(lr=hyperparams['lr'], #beta_1=0.9, beta_2=0.999, epsilon=0.0001,\n",
    "                                   decay=hyperparams['decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics():\n",
    "    def __init__(self, threshold=0.5):\n",
    "        self.threshold=threshold\n",
    "        \n",
    "    def recall_m(self, y_true, y_pred):\n",
    "            y_labels = y_true#tf.reshape(y_true[:], (-1,1))\n",
    "    #         y_pred = tf.reshape(y_pred, (-1,1))\n",
    "            y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), self.threshold), K.floatx())        \n",
    "            possible_positives = K.sum(K.round(K.clip(y_labels, 0, 1)))\n",
    "            true_positives = K.sum(K.round(K.clip(y_labels * y_pred, 0, 1)))\n",
    "            recall = true_positives / (possible_positives + K.epsilon())\n",
    "            return recall\n",
    "\n",
    "    def precision_m(self, y_true, y_pred):\n",
    "            y_labels = y_true#tf.reshape(y_true[:], (-1,1))\n",
    "    #         y_pred = tf.reshape(y_pred, (-1,1))\n",
    "            y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), self.threshold), K.floatx())        \n",
    "            true_positives = K.sum(K.round(K.clip(y_labels * y_pred, 0, 1)))\n",
    "            predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "            precision = true_positives / (predicted_positives + K.epsilon())\n",
    "            return precision\n",
    "\n",
    "    def f1_m(self, y_true, y_pred):\n",
    "        precision = self.precision_m(y_true, y_pred)\n",
    "        recall = self.recall_m(y_true, y_pred)\n",
    "        return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def binary_crossentropy_custom(y_true, y_pred):\n",
    "    y_labels = y_true#tf.reshape(y_true[0],(1,-1))\n",
    "    return K.binary_crossentropy(y_labels, \n",
    "                                 y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopwords_list,\n",
    "                liwc_categories,\n",
    "               ignore_layer=[]):\n",
    "\n",
    "    tokens_features = Input(shape=(hyperparams_features['maxlen'],), name='word_seq')\n",
    "    embedding_layer = Embedding(hyperparams_features['max_features'], \n",
    "                                hyperparams_features['embedding_dim'], \n",
    "                                input_length=hyperparams_features['maxlen'],\n",
    "#                                 mask_zero=True,\n",
    "                                embeddings_regularizer=regularizers.l2(hyperparams['l2_embeddings']),\n",
    "                                weights=[embedding_matrix], \n",
    "                                trainable=hyperparams['trainable_embeddings'],\n",
    "                               name='embeddings_layer')(\n",
    "        tokens_features)\n",
    "#     lstm_layers = Bidirectional(LSTM(hyperparams['lstm_units']))(embedding_layer)\n",
    "    if tf.test.is_gpu_available():\n",
    "        lstm_layers = CuDNNLSTM(hyperparams['lstm_units'], \n",
    "#                             dropout=hyperparams['dropout'],\n",
    "#                       recurrent_dropout=hyperparams['dropout'],\n",
    "                      name='LSTM_layer')(embedding_layer)\n",
    "    else:\n",
    "        lstm_layers = LSTM(hyperparams['lstm_units'], \n",
    "#                             dropout=hyperparams['dropout'],\n",
    "#                       recurrent_dropout=hyperparams['dropout'],\n",
    "                      name='LSTM_layer')(embedding_layer)\n",
    "\n",
    "    dropout_layer = Dropout(hyperparams['dropout'], name='lstm_wdropout')(lstm_layers)\n",
    "    numerical_features = Input(shape=(len(emotions) + 1 + len(liwc_categories),), name='numeric_input') # emotions and pronouns\n",
    "    dense_layer = Dense(units=1,\n",
    "                        kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                        name='numerical_dense_layer',\n",
    "                       )(numerical_features)\n",
    "    sparse_features = Input(shape=(len(stopwords_list),), name='sparse_input') # stopwords\n",
    "    subjects = Input(shape=(1,), name='subjects')\n",
    "    dense_layer_sparse = Dense(units=hyperparams['dense_bow_units'],\n",
    "                              name='sparse_feat_dense_layer',\n",
    "                                kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                              )(sparse_features)\n",
    "#     # TODO: this is getting out of hand. Refactor this ablation part.\n",
    "    all_layers = {\n",
    "        'lstm_layers': dropout_layer,\n",
    "        'numerical_dense_layer': numerical_features,# dense_layer,\n",
    "        'sparse_feat_dense_layer': sparse_features#dense_layer_sparse,\n",
    "    }\n",
    "    layers_to_merge = []\n",
    "    for n, l in all_layers.items():\n",
    "        if n in ignore_layer:\n",
    "            continue\n",
    "        layers_to_merge.append(l)\n",
    "        \n",
    "    if len(layers_to_merge) == 1:\n",
    "        merged_layers = layers_to_merge[0]\n",
    "    else:\n",
    "        merged_layers = concatenate(layers_to_merge)\n",
    "    output_layer = Dense(1, activation='sigmoid',\n",
    "                         name='output_layer',\n",
    "                        kernel_regularizer=regularizers.l2(hyperparams['l2_dense']))(merged_layers)\n",
    "\n",
    "    # Compile model\n",
    "    model = Model(inputs=[tokens_features, numerical_features, sparse_features, subjects], \n",
    "                  outputs=output_layer)\n",
    "    metrics_class = Metrics(threshold=hyperparams['threshold'])\n",
    "    model.compile(hyperparams['optimizer'], binary_crossentropy_custom,\n",
    "                  metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ana/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ana/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "numeric_input (InputLayer)      (None, 75)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sparse_input (InputLayer)       (None, 179)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 254)          0           numeric_input[0][0]              \n",
      "                                                                 sparse_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 1)            255         concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 255\n",
      "Trainable params: 255\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopword_list,\n",
    "                    liwc_categories=[c for c in categories if c in writings_df.columns]\n",
    ",\n",
    "                   ignore_layer=hyperparams['ignore_layer'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, 'models/lstm_plus_ablated3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: old comet version (3.0.2) detected. current: 3.0.3 please update your comet lib with command: `pip install --no-cache-dir --upgrade comet_ml`\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/ananana/mental/e9fe9232fe96427fab21bab15736a70f\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(api_key=\"eoBdVyznAhfg3bK9pZ58ZSXfv\",\n",
    "                        project_name=\"mental\", workspace=\"ananana\")\n",
    "\n",
    "experiment.log_parameters(hyperparams_features)\n",
    "\n",
    "experiment.log_parameter('emotion_lexicon', nrc_lexicon_path)\n",
    "experiment.log_parameter('emotions', emotions)\n",
    "experiment.log_parameter('embeddings_path', pretrained_embeddings_path)\n",
    "if 'subset' in writings_df.columns:\n",
    "    experiment.add_tag('anorexia')\n",
    "\n",
    "experiment.log_parameters(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'i': 2,\n",
       " 'to': 3,\n",
       " 'a': 4,\n",
       " 'and': 5,\n",
       " 'of': 6,\n",
       " 'it': 7,\n",
       " 'in': 8,\n",
       " 'that': 9,\n",
       " 'you': 10,\n",
       " 'is': 11,\n",
       " 'for': 12,\n",
       " 's': 13,\n",
       " 'this': 14,\n",
       " 'on': 15,\n",
       " 't': 16,\n",
       " 'with': 17,\n",
       " 'but': 18,\n",
       " 'be': 19,\n",
       " 'was': 20,\n",
       " 'have': 21,\n",
       " 'my': 22,\n",
       " 'are': 23,\n",
       " 'not': 24,\n",
       " 'as': 25,\n",
       " 'they': 26,\n",
       " 'if': 27,\n",
       " 'so': 28,\n",
       " 'or': 29,\n",
       " 'like': 30,\n",
       " 'just': 31,\n",
       " 'what': 32,\n",
       " 'can': 33,\n",
       " 'he': 34,\n",
       " 'at': 35,\n",
       " 'me': 36,\n",
       " 'we': 37,\n",
       " 'from': 38,\n",
       " 'your': 39,\n",
       " 'all': 40,\n",
       " 'about': 41,\n",
       " 'do': 42,\n",
       " 'com': 43,\n",
       " 'there': 44,\n",
       " 'one': 45,\n",
       " 'm': 46,\n",
       " 'an': 47,\n",
       " 'would': 48,\n",
       " 'out': 49,\n",
       " 'up': 50,\n",
       " 'more': 51,\n",
       " 'people': 52,\n",
       " 'by': 53,\n",
       " 'get': 54,\n",
       " 'will': 55,\n",
       " 'how': 56,\n",
       " 'when': 57,\n",
       " 'don': 58,\n",
       " 'no': 59,\n",
       " 'some': 60,\n",
       " 'has': 61,\n",
       " 'r': 62,\n",
       " 'https': 63,\n",
       " 'think': 64,\n",
       " 'www': 65,\n",
       " 'them': 66,\n",
       " 'time': 67,\n",
       " 'his': 68,\n",
       " 'know': 69,\n",
       " 'who': 70,\n",
       " 'because': 71,\n",
       " 'their': 72,\n",
       " 'really': 73,\n",
       " 'had': 74,\n",
       " 'been': 75,\n",
       " 'good': 76,\n",
       " 'any': 77,\n",
       " 'now': 78,\n",
       " 're': 79,\n",
       " 'also': 80,\n",
       " 'only': 81,\n",
       " 'http': 82,\n",
       " 've': 83,\n",
       " 'other': 84,\n",
       " 'even': 85,\n",
       " 'were': 86,\n",
       " 'see': 87,\n",
       " 'then': 88,\n",
       " 'than': 89,\n",
       " 'reddit': 90,\n",
       " 'she': 91,\n",
       " 'here': 92,\n",
       " 'much': 93,\n",
       " 'new': 94,\n",
       " 'make': 95,\n",
       " 'which': 96,\n",
       " 'could': 97,\n",
       " '1': 98,\n",
       " '2': 99,\n",
       " 'want': 100,\n",
       " 'well': 101,\n",
       " 'her': 102,\n",
       " 'why': 103,\n",
       " 'still': 104,\n",
       " 'go': 105,\n",
       " 'him': 106,\n",
       " 'way': 107,\n",
       " 'into': 108,\n",
       " 'too': 109,\n",
       " 'did': 110,\n",
       " 'first': 111,\n",
       " 'being': 112,\n",
       " 'its': 113,\n",
       " 'going': 114,\n",
       " 'd': 115,\n",
       " 'should': 116,\n",
       " 'after': 117,\n",
       " 'very': 118,\n",
       " 'something': 119,\n",
       " 'over': 120,\n",
       " 'most': 121,\n",
       " 'game': 122,\n",
       " 'right': 123,\n",
       " '3': 124,\n",
       " 'got': 125,\n",
       " 'back': 126,\n",
       " 'where': 127,\n",
       " 'am': 128,\n",
       " 'us': 129,\n",
       " 'work': 130,\n",
       " 'll': 131,\n",
       " 'years': 132,\n",
       " 'need': 133,\n",
       " 'never': 134,\n",
       " 'these': 135,\n",
       " 'thanks': 136,\n",
       " 'same': 137,\n",
       " 'say': 138,\n",
       " 'day': 139,\n",
       " 'lot': 140,\n",
       " 'use': 141,\n",
       " 'our': 142,\n",
       " 'off': 143,\n",
       " 'feel': 144,\n",
       " 'does': 145,\n",
       " 'before': 146,\n",
       " 'those': 147,\n",
       " 'post': 148,\n",
       " 'things': 149,\n",
       " 'actually': 150,\n",
       " 'thing': 151,\n",
       " 'sure': 152,\n",
       " 'take': 153,\n",
       " 'great': 154,\n",
       " 'said': 155,\n",
       " 'better': 156,\n",
       " 'though': 157,\n",
       " 'made': 158,\n",
       " 'love': 159,\n",
       " 'many': 160,\n",
       " 'someone': 161,\n",
       " 'didn': 162,\n",
       " 'best': 163,\n",
       " 'down': 164,\n",
       " 'two': 165,\n",
       " 'comments': 166,\n",
       " 'look': 167,\n",
       " '5': 168,\n",
       " 'year': 169,\n",
       " 'while': 170,\n",
       " 'pretty': 171,\n",
       " 'man': 172,\n",
       " 'doesn': 173,\n",
       " 'always': 174,\n",
       " 'last': 175,\n",
       " 'around': 176,\n",
       " 'every': 177,\n",
       " 'life': 178,\n",
       " 'show': 179,\n",
       " 'help': 180,\n",
       " 'find': 181,\n",
       " 'u': 182,\n",
       " 'few': 183,\n",
       " 'watch': 184,\n",
       " 'since': 185,\n",
       " 'world': 186,\n",
       " '4': 187,\n",
       " '10': 188,\n",
       " 'please': 189,\n",
       " 'removed': 190,\n",
       " 'may': 191,\n",
       " 'anything': 192,\n",
       " 'through': 193,\n",
       " 'try': 194,\n",
       " 'long': 195,\n",
       " 'trump': 196,\n",
       " 'bad': 197,\n",
       " 'probably': 198,\n",
       " 'anyone': 199,\n",
       " 'point': 200,\n",
       " 'little': 201,\n",
       " 'ever': 202,\n",
       " 'getting': 203,\n",
       " 'used': 204,\n",
       " 'yeah': 205,\n",
       " 'thought': 206,\n",
       " 'again': 207,\n",
       " 'maybe': 208,\n",
       " 'let': 209,\n",
       " 'doing': 210,\n",
       " 'looking': 211,\n",
       " 'might': 212,\n",
       " 'old': 213,\n",
       " 'play': 214,\n",
       " 'own': 215,\n",
       " 'part': 216,\n",
       " 'isn': 217,\n",
       " 'another': 218,\n",
       " 'keep': 219,\n",
       " 'different': 220,\n",
       " 'video': 221,\n",
       " 'give': 222,\n",
       " 'yes': 223,\n",
       " 'come': 224,\n",
       " 'mean': 225,\n",
       " 'without': 226,\n",
       " 'guy': 227,\n",
       " 'thank': 228,\n",
       " 'both': 229,\n",
       " 'youtube': 230,\n",
       " 'shit': 231,\n",
       " 'big': 232,\n",
       " 'read': 233,\n",
       " 'trying': 234,\n",
       " 'end': 235,\n",
       " 'makes': 236,\n",
       " 'everyone': 237,\n",
       " 'put': 238,\n",
       " 'next': 239,\n",
       " 'free': 240,\n",
       " 'such': 241,\n",
       " 'enough': 242,\n",
       " 'bit': 243,\n",
       " 'having': 244,\n",
       " 'person': 245,\n",
       " 'done': 246,\n",
       " 'games': 247,\n",
       " 'hard': 248,\n",
       " 'already': 249,\n",
       " '6': 250,\n",
       " 'using': 251,\n",
       " 'money': 252,\n",
       " 'least': 253,\n",
       " 'looks': 254,\n",
       " 'place': 255,\n",
       " 'real': 256,\n",
       " 'found': 257,\n",
       " 'else': 258,\n",
       " 'nothing': 259,\n",
       " 'start': 260,\n",
       " 'high': 261,\n",
       " 'far': 262,\n",
       " 'name': 263,\n",
       " 'everything': 264,\n",
       " 'v': 265,\n",
       " 'movie': 266,\n",
       " 'stuff': 267,\n",
       " 'able': 268,\n",
       " 'seems': 269,\n",
       " 'idea': 270,\n",
       " 'times': 271,\n",
       " 'says': 272,\n",
       " 'kind': 273,\n",
       " 'live': 274,\n",
       " 'once': 275,\n",
       " 'question': 276,\n",
       " 'tell': 277,\n",
       " 'making': 278,\n",
       " 'seen': 279,\n",
       " 'away': 280,\n",
       " 'team': 281,\n",
       " 'oh': 282,\n",
       " 'edit': 283,\n",
       " '7': 284,\n",
       " 'against': 285,\n",
       " 'story': 286,\n",
       " 'days': 287,\n",
       " 'imgur': 288,\n",
       " 'between': 289,\n",
       " 'wiki': 290,\n",
       " 'hope': 291,\n",
       " 'today': 292,\n",
       " 'news': 293,\n",
       " 'season': 294,\n",
       " 'until': 295,\n",
       " 'reason': 296,\n",
       " '31': 297,\n",
       " 'believe': 298,\n",
       " 'wrong': 299,\n",
       " 'war': 300,\n",
       " 'saying': 301,\n",
       " 'top': 302,\n",
       " 'week': 303,\n",
       " 'nice': 304,\n",
       " 'fuck': 305,\n",
       " 'started': 306,\n",
       " 'problem': 307,\n",
       " 'full': 308,\n",
       " 'set': 309,\n",
       " 'less': 310,\n",
       " 'etc': 311,\n",
       " 'community': 312,\n",
       " 'org': 313,\n",
       " 'won': 314,\n",
       " 'went': 315,\n",
       " 'each': 316,\n",
       " 'guys': 317,\n",
       " 'black': 318,\n",
       " 'im': 319,\n",
       " 'comment': 320,\n",
       " 'myself': 321,\n",
       " 'change': 322,\n",
       " 'yet': 323,\n",
       " 'either': 324,\n",
       " 'e': 325,\n",
       " 'ago': 326,\n",
       " '0': 327,\n",
       " '8': 328,\n",
       " 'case': 329,\n",
       " 'lol': 330,\n",
       " 'message': 331,\n",
       " 'home': 332,\n",
       " 'school': 333,\n",
       " 'whole': 334,\n",
       " 'sorry': 335,\n",
       " 'fucking': 336,\n",
       " 'remember': 337,\n",
       " 'interesting': 338,\n",
       " 'during': 339,\n",
       " 'music': 340,\n",
       " 'almost': 341,\n",
       " 'link': 342,\n",
       " 'true': 343,\n",
       " 'understand': 344,\n",
       " 'wanted': 345,\n",
       " 'dont': 346,\n",
       " 'left': 347,\n",
       " 'white': 348,\n",
       " 'stop': 349,\n",
       " 'guess': 350,\n",
       " 'definitely': 351,\n",
       " '20': 352,\n",
       " 'fun': 353,\n",
       " 'x': 354,\n",
       " 'working': 355,\n",
       " 'wasn': 356,\n",
       " 'run': 357,\n",
       " 'check': 358,\n",
       " 'house': 359,\n",
       " 'gets': 360,\n",
       " 'small': 361,\n",
       " 'instead': 362,\n",
       " 'subreddit': 363,\n",
       " 'questions': 364,\n",
       " 'night': 365,\n",
       " 'job': 366,\n",
       " 'state': 367,\n",
       " 'thread': 368,\n",
       " '30': 369,\n",
       " 'playing': 370,\n",
       " 'fact': 371,\n",
       " 'must': 372,\n",
       " 'friends': 373,\n",
       " 'power': 374,\n",
       " 'under': 375,\n",
       " 'call': 376,\n",
       " 'level': 377,\n",
       " 'series': 378,\n",
       " 'mind': 379,\n",
       " 'friend': 380,\n",
       " 'buy': 381,\n",
       " 'based': 382,\n",
       " 'second': 383,\n",
       " 'called': 384,\n",
       " 'wouldn': 385,\n",
       " 'however': 386,\n",
       " 'episode': 387,\n",
       " 'possible': 388,\n",
       " 'support': 389,\n",
       " 'system': 390,\n",
       " 'came': 391,\n",
       " 'months': 392,\n",
       " 'cool': 393,\n",
       " 'ask': 394,\n",
       " 'others': 395,\n",
       " 'tv': 396,\n",
       " 'side': 397,\n",
       " 'rules': 398,\n",
       " 'saw': 399,\n",
       " 'god': 400,\n",
       " 'country': 401,\n",
       " 'food': 402,\n",
       " 'told': 403,\n",
       " 'open': 404,\n",
       " 'care': 405,\n",
       " 'talking': 406,\n",
       " 'took': 407,\n",
       " 'happy': 408,\n",
       " 'tried': 409,\n",
       " '100': 410,\n",
       " 'en': 411,\n",
       " 'quite': 412,\n",
       " 'experience': 413,\n",
       " 'title': 414,\n",
       " 'talk': 415,\n",
       " 'often': 416,\n",
       " 'family': 417,\n",
       " 'google': 418,\n",
       " 'thinking': 419,\n",
       " '9': 420,\n",
       " '15': 421,\n",
       " 'history': 422,\n",
       " 'hours': 423,\n",
       " 'past': 424,\n",
       " 'means': 425,\n",
       " 'especially': 426,\n",
       " 'list': 427,\n",
       " 'three': 428,\n",
       " 'hate': 429,\n",
       " 'hey': 430,\n",
       " 'jpg': 431,\n",
       " 'issue': 432,\n",
       " 'sense': 433,\n",
       " 'happened': 434,\n",
       " 'agree': 435,\n",
       " 'w': 436,\n",
       " 'head': 437,\n",
       " 'seem': 438,\n",
       " 'coming': 439,\n",
       " 'information': 440,\n",
       " 'future': 441,\n",
       " '2017': 442,\n",
       " 'awesome': 443,\n",
       " 'heard': 444,\n",
       " 'book': 445,\n",
       " 'number': 446,\n",
       " 'later': 447,\n",
       " 'car': 448,\n",
       " 'haven': 449,\n",
       " 'sometimes': 450,\n",
       " 'aren': 451,\n",
       " 'content': 452,\n",
       " 'eat': 453,\n",
       " 'page': 454,\n",
       " 'goes': 455,\n",
       " 'super': 456,\n",
       " 'shows': 457,\n",
       " 'exactly': 458,\n",
       " 'line': 459,\n",
       " 'works': 460,\n",
       " 'example': 461,\n",
       " 'comes': 462,\n",
       " 'sub': 463,\n",
       " 'article': 464,\n",
       " 'taking': 465,\n",
       " 'pay': 466,\n",
       " 'government': 467,\n",
       " 'sounds': 468,\n",
       " 'wait': 469,\n",
       " 'character': 470,\n",
       " 'self': 471,\n",
       " 'rather': 472,\n",
       " 'half': 473,\n",
       " 'amazing': 474,\n",
       " 'discussion': 475,\n",
       " 'couple': 476,\n",
       " 'course': 477,\n",
       " 'add': 478,\n",
       " 'eating': 479,\n",
       " 'water': 480,\n",
       " 'usually': 481,\n",
       " 'city': 482,\n",
       " '2016': 483,\n",
       " 'lost': 484,\n",
       " 'deal': 485,\n",
       " 'hand': 486,\n",
       " 'similar': 487,\n",
       " '12': 488,\n",
       " 'hit': 489,\n",
       " 'media': 490,\n",
       " 'face': 491,\n",
       " 'body': 492,\n",
       " 'likely': 493,\n",
       " 'subsubject3131s': 494,\n",
       " 'american': 495,\n",
       " 'items': 496,\n",
       " 'weight': 497,\n",
       " 'hear': 498,\n",
       " 'original': 499,\n",
       " 'worth': 500,\n",
       " '000': 501,\n",
       " 'group': 502,\n",
       " 'rule': 503,\n",
       " 'close': 504,\n",
       " 'basically': 505,\n",
       " 'needs': 506,\n",
       " 'ones': 507,\n",
       " 'fine': 508,\n",
       " 'film': 509,\n",
       " 'completely': 510,\n",
       " 'matter': 511,\n",
       " 'single': 512,\n",
       " 'happen': 513,\n",
       " 'favorite': 514,\n",
       " 'become': 515,\n",
       " 'party': 516,\n",
       " 'men': 517,\n",
       " 'vs': 518,\n",
       " 'build': 519,\n",
       " 'posts': 520,\n",
       " 'die': 521,\n",
       " 'art': 522,\n",
       " 'order': 523,\n",
       " 'women': 524,\n",
       " 'low': 525,\n",
       " 'ok': 526,\n",
       " 'human': 527,\n",
       " 'added': 528,\n",
       " 'together': 529,\n",
       " 'non': 530,\n",
       " 'phone': 531,\n",
       " '2018': 532,\n",
       " 'death': 533,\n",
       " 'early': 534,\n",
       " 'front': 535,\n",
       " 'type': 536,\n",
       " 'turn': 537,\n",
       " 'yourself': 538,\n",
       " 'move': 539,\n",
       " 'b': 540,\n",
       " 'source': 541,\n",
       " 'issues': 542,\n",
       " 'de': 543,\n",
       " 'kids': 544,\n",
       " 'star': 545,\n",
       " '50': 546,\n",
       " 'literally': 547,\n",
       " 'thats': 548,\n",
       " 'uk': 549,\n",
       " 'played': 550,\n",
       " 'general': 551,\n",
       " 'song': 552,\n",
       " 'sort': 553,\n",
       " 'watching': 554,\n",
       " 'kill': 555,\n",
       " 'control': 556,\n",
       " 'fight': 557,\n",
       " 'girl': 558,\n",
       " 'easy': 559,\n",
       " 'p': 560,\n",
       " 'soon': 561,\n",
       " 'running': 562,\n",
       " 'players': 563,\n",
       " 'month': 564,\n",
       " 'dead': 565,\n",
       " 'fan': 566,\n",
       " 'company': 567,\n",
       " 'hell': 568,\n",
       " 'important': 569,\n",
       " 'online': 570,\n",
       " 'movies': 571,\n",
       " 'price': 572,\n",
       " 'due': 573,\n",
       " '11': 574,\n",
       " 'damn': 575,\n",
       " 'haha': 576,\n",
       " 'public': 577,\n",
       " 'huge': 578,\n",
       " 'wikipedia': 579,\n",
       " 'data': 580,\n",
       " 'wish': 581,\n",
       " 'per': 582,\n",
       " 'cause': 583,\n",
       " 'entire': 584,\n",
       " 'internet': 585,\n",
       " 'version': 586,\n",
       " 'seeing': 587,\n",
       " 'asked': 588,\n",
       " 'win': 589,\n",
       " 'gonna': 590,\n",
       " 'reading': 591,\n",
       " 'china': 592,\n",
       " 'minutes': 593,\n",
       " 'wants': 594,\n",
       " 'whatever': 595,\n",
       " 'leave': 596,\n",
       " 'funny': 597,\n",
       " 'posted': 598,\n",
       " 'weeks': 599,\n",
       " 'social': 600,\n",
       " 'player': 601,\n",
       " 'behind': 602,\n",
       " 'sound': 603,\n",
       " 'action': 604,\n",
       " 'site': 605,\n",
       " 'red': 606,\n",
       " 'space': 607,\n",
       " 'amount': 608,\n",
       " 'answer': 609,\n",
       " 'main': 610,\n",
       " 'c': 611,\n",
       " 'currently': 612,\n",
       " 'honestly': 613,\n",
       " 'finally': 614,\n",
       " 'president': 615,\n",
       " 'word': 616,\n",
       " 'given': 617,\n",
       " '25': 618,\n",
       " 'current': 619,\n",
       " 'felt': 620,\n",
       " 'light': 621,\n",
       " 'couldn': 622,\n",
       " 'woman': 623,\n",
       " 'children': 624,\n",
       " 'account': 625,\n",
       " 'interested': 626,\n",
       " 'card': 627,\n",
       " 'short': 628,\n",
       " 'room': 629,\n",
       " 'dark': 630,\n",
       " 'rest': 631,\n",
       " 'rock': 632,\n",
       " 'opinion': 633,\n",
       " 'certain': 634,\n",
       " 'whether': 635,\n",
       " 'q': 636,\n",
       " 'million': 637,\n",
       " 'recently': 638,\n",
       " 'subscribers': 639,\n",
       " 'event': 640,\n",
       " 'feeling': 641,\n",
       " 'age': 642,\n",
       " 'picture': 643,\n",
       " 'class': 644,\n",
       " 'weird': 645,\n",
       " 'moment': 646,\n",
       " 'large': 647,\n",
       " 'normal': 648,\n",
       " 'absolutely': 649,\n",
       " 'enjoy': 650,\n",
       " 'o': 651,\n",
       " 'op': 652,\n",
       " 'takes': 653,\n",
       " 'chance': 654,\n",
       " 'mod': 655,\n",
       " 'videos': 656,\n",
       " 'health': 657,\n",
       " 'share': 658,\n",
       " 'save': 659,\n",
       " 'extra': 660,\n",
       " 'anyway': 661,\n",
       " 'drive': 662,\n",
       " 'law': 663,\n",
       " 'within': 664,\n",
       " 'stupid': 665,\n",
       " 'shot': 666,\n",
       " 'outside': 667,\n",
       " 'kid': 668,\n",
       " 'america': 669,\n",
       " 'longer': 670,\n",
       " 'happens': 671,\n",
       " 'submission': 672,\n",
       " 'plan': 673,\n",
       " 'dude': 674,\n",
       " 'mostly': 675,\n",
       " 'app': 676,\n",
       " 'sex': 677,\n",
       " 'fire': 678,\n",
       " 'knew': 679,\n",
       " 'business': 680,\n",
       " 'area': 681,\n",
       " 'advice': 682,\n",
       " 'pc': 683,\n",
       " 'users': 684,\n",
       " 'starting': 685,\n",
       " 'police': 686,\n",
       " 'gun': 687,\n",
       " 'user': 688,\n",
       " 'bring': 689,\n",
       " 'unless': 690,\n",
       " 'clear': 691,\n",
       " 'living': 692,\n",
       " 'parents': 693,\n",
       " 'view': 694,\n",
       " 'russia': 695,\n",
       " 'seriously': 696,\n",
       " 'update': 697,\n",
       " 'stay': 698,\n",
       " 'mods': 699,\n",
       " 'gold': 700,\n",
       " 'id': 701,\n",
       " 'dog': 702,\n",
       " 'cut': 703,\n",
       " 'store': 704,\n",
       " 'states': 705,\n",
       " 'late': 706,\n",
       " 'crazy': 707,\n",
       " 'actual': 708,\n",
       " 'facebook': 709,\n",
       " 'totally': 710,\n",
       " 'form': 711,\n",
       " 'fast': 712,\n",
       " 'following': 713,\n",
       " 'themselves': 714,\n",
       " 'available': 715,\n",
       " 'moderators': 716,\n",
       " 'common': 717,\n",
       " 'okay': 718,\n",
       " 'words': 719,\n",
       " 'characters': 720,\n",
       " 'joke': 721,\n",
       " 'inside': 722,\n",
       " 'attack': 723,\n",
       " 'personal': 724,\n",
       " 'vote': 725,\n",
       " 'major': 726,\n",
       " 'young': 727,\n",
       " 'mine': 728,\n",
       " 'perfect': 729,\n",
       " 'learn': 730,\n",
       " 'plus': 731,\n",
       " 'worse': 732,\n",
       " 'trade': 733,\n",
       " 'events': 734,\n",
       " 'worst': 735,\n",
       " 'known': 736,\n",
       " 'wow': 737,\n",
       " 'taken': 738,\n",
       " 'oil': 739,\n",
       " 'asking': 740,\n",
       " 'along': 741,\n",
       " 'looked': 742,\n",
       " 'specific': 743,\n",
       " 'amazon': 744,\n",
       " 'wife': 745,\n",
       " 'quality': 746,\n",
       " '18': 747,\n",
       " 'damage': 748,\n",
       " 'style': 749,\n",
       " 'difference': 750,\n",
       " 'imagine': 751,\n",
       " 'science': 752,\n",
       " 'obviously': 753,\n",
       " 'study': 754,\n",
       " 'note': 755,\n",
       " 'situation': 756,\n",
       " 'blue': 757,\n",
       " 'break': 758,\n",
       " 'gave': 759,\n",
       " 'twitter': 760,\n",
       " 'scene': 761,\n",
       " 'simply': 762,\n",
       " 'political': 763,\n",
       " 'worked': 764,\n",
       " '2015': 765,\n",
       " 'across': 766,\n",
       " 'send': 767,\n",
       " 'college': 768,\n",
       " 'create': 769,\n",
       " 'itself': 770,\n",
       " 'pick': 771,\n",
       " 'local': 772,\n",
       " 'lose': 773,\n",
       " 'near': 774,\n",
       " 'decided': 775,\n",
       " 'knows': 776,\n",
       " 'giving': 777,\n",
       " 'needed': 778,\n",
       " 'core': 779,\n",
       " 'research': 780,\n",
       " 'special': 781,\n",
       " 'office': 782,\n",
       " 'contact': 783,\n",
       " 'middle': 784,\n",
       " 'force': 785,\n",
       " 'search': 786,\n",
       " 'report': 787,\n",
       " 'serious': 788,\n",
       " 'project': 789,\n",
       " 'problems': 790,\n",
       " 'html': 791,\n",
       " 'fair': 792,\n",
       " 'wonder': 793,\n",
       " 'child': 794,\n",
       " 'points': 795,\n",
       " 'net': 796,\n",
       " 'gone': 797,\n",
       " 'cost': 798,\n",
       " 'countries': 799,\n",
       " 'north': 800,\n",
       " 'bought': 801,\n",
       " 'above': 802,\n",
       " 'safe': 803,\n",
       " 'details': 804,\n",
       " 'glad': 805,\n",
       " 'higher': 806,\n",
       " 'review': 807,\n",
       " 'compose': 808,\n",
       " 'books': 809,\n",
       " 'alone': 810,\n",
       " 'hour': 811,\n",
       " 'product': 812,\n",
       " 'including': 813,\n",
       " 'release': 814,\n",
       " 'test': 815,\n",
       " 'green': 816,\n",
       " 'moved': 817,\n",
       " 'image': 818,\n",
       " 'air': 819,\n",
       " 'cannot': 820,\n",
       " 'screen': 821,\n",
       " 'eyes': 822,\n",
       " 'changes': 823,\n",
       " 'killed': 824,\n",
       " 'mom': 825,\n",
       " 'english': 826,\n",
       " 'anymore': 827,\n",
       " 'himself': 828,\n",
       " 'choose': 829,\n",
       " 'feels': 830,\n",
       " 'box': 831,\n",
       " 'poor': 832,\n",
       " 'simple': 833,\n",
       " 'military': 834,\n",
       " 'except': 835,\n",
       " 'bill': 836,\n",
       " 'several': 837,\n",
       " 'evidence': 838,\n",
       " 'ass': 839,\n",
       " 'building': 840,\n",
       " 'til': 841,\n",
       " 'unfortunately': 842,\n",
       " 'process': 843,\n",
       " 'meant': 844,\n",
       " 'related': 845,\n",
       " 'pro': 846,\n",
       " 'earth': 847,\n",
       " '16': 848,\n",
       " 'strong': 849,\n",
       " 'consider': 850,\n",
       " 'figure': 851,\n",
       " 'total': 852,\n",
       " 'although': 853,\n",
       " 'kinda': 854,\n",
       " 'morning': 855,\n",
       " 'canada': 856,\n",
       " 'beautiful': 857,\n",
       " 'sell': 858,\n",
       " 'personally': 859,\n",
       " 'market': 860,\n",
       " 'lots': 861,\n",
       " 'pm': 862,\n",
       " 'follow': 863,\n",
       " 'response': 864,\n",
       " 'dad': 865,\n",
       " 'welcome': 866,\n",
       " 'date': 867,\n",
       " 'speed': 868,\n",
       " 'service': 869,\n",
       " 'vr': 870,\n",
       " 'anti': 871,\n",
       " 'official': 872,\n",
       " 'cant': 873,\n",
       " 'match': 874,\n",
       " 'youtu': 875,\n",
       " 'towards': 876,\n",
       " 'piece': 877,\n",
       " 'info': 878,\n",
       " 'gives': 879,\n",
       " 'ideas': 880,\n",
       " 'proof': 881,\n",
       " 'hold': 882,\n",
       " 'john': 883,\n",
       " 'popular': 884,\n",
       " 'hi': 885,\n",
       " 'russian': 886,\n",
       " 'effect': 887,\n",
       " 'turned': 888,\n",
       " 'fans': 889,\n",
       " 'loved': 890,\n",
       " 'size': 891,\n",
       " 'hot': 892,\n",
       " 'changed': 893,\n",
       " 'photo': 894,\n",
       " 'ways': 895,\n",
       " 'code': 896,\n",
       " 'doctor': 897,\n",
       " 'allowed': 898,\n",
       " 'term': 899,\n",
       " 'co': 900,\n",
       " 'la': 901,\n",
       " 'supposed': 902,\n",
       " 'created': 903,\n",
       " 'mother': 904,\n",
       " 'expect': 905,\n",
       " 'baby': 906,\n",
       " 'sad': 907,\n",
       " 'posting': 908,\n",
       " 'access': 909,\n",
       " 'sign': 910,\n",
       " 'trending': 911,\n",
       " 'fix': 912,\n",
       " 'act': 913,\n",
       " 'explain': 914,\n",
       " 'subject': 915,\n",
       " 'king': 916,\n",
       " 'below': 917,\n",
       " 'easily': 918,\n",
       " '40': 919,\n",
       " 'race': 920,\n",
       " 'apple': 921,\n",
       " 'south': 922,\n",
       " 'complete': 923,\n",
       " 'hands': 924,\n",
       " 'clearly': 925,\n",
       " 'watched': 926,\n",
       " 'heart': 927,\n",
       " 'request': 928,\n",
       " 'multiple': 929,\n",
       " 'bunch': 930,\n",
       " 'computer': 931,\n",
       " 'stand': 932,\n",
       " 'forward': 933,\n",
       " 'considered': 934,\n",
       " 'luck': 935,\n",
       " 'hair': 936,\n",
       " 'third': 937,\n",
       " '13': 938,\n",
       " '24': 939,\n",
       " 'relationship': 940,\n",
       " 'value': 941,\n",
       " 'straight': 942,\n",
       " 'spend': 943,\n",
       " 'fake': 944,\n",
       " 'mr': 945,\n",
       " 'text': 946,\n",
       " 'thoughts': 947,\n",
       " 'choice': 948,\n",
       " 'cat': 949,\n",
       " 'paid': 950,\n",
       " 'item': 951,\n",
       " 'correct': 952,\n",
       " 'politics': 953,\n",
       " 'road': 954,\n",
       " 'writing': 955,\n",
       " 'write': 956,\n",
       " 'h': 957,\n",
       " 'final': 958,\n",
       " '14': 959,\n",
       " 'perhaps': 960,\n",
       " 'design': 961,\n",
       " 'campaign': 962,\n",
       " 'hero': 963,\n",
       " 'didnt': 964,\n",
       " 'bar': 965,\n",
       " 'eye': 966,\n",
       " 'apparently': 967,\n",
       " 'reasons': 968,\n",
       " 'results': 969,\n",
       " 'easier': 970,\n",
       " 'g': 971,\n",
       " 'y': 972,\n",
       " 'wondering': 973,\n",
       " 'hopefully': 974,\n",
       " 'n': 975,\n",
       " 'drop': 976,\n",
       " 'parts': 977,\n",
       " 'paper': 978,\n",
       " 'language': 979,\n",
       " 'fit': 980,\n",
       " 'voice': 981,\n",
       " 'windows': 982,\n",
       " 'average': 983,\n",
       " 'option': 984,\n",
       " 'four': 985,\n",
       " 'realize': 986,\n",
       " 'avoid': 987,\n",
       " 'redditgifts': 988,\n",
       " 'mentioned': 989,\n",
       " 'wars': 990,\n",
       " 'stories': 991,\n",
       " 'energy': 992,\n",
       " 'ed': 993,\n",
       " 'brain': 994,\n",
       " 'chinese': 995,\n",
       " 'national': 996,\n",
       " 'quick': 997,\n",
       " 'pop': 998,\n",
       " 'further': 999,\n",
       " 'recent': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightsHistory(callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.log_weights(0)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.log_weights(epoch)\n",
    "        \n",
    "    def log_weights(self, step):\n",
    "        for layer_name in ['LSTM_layer', 'embeddings_layer', 'sparse_feat_dense_layer', 'output_layer']:\n",
    "            try:\n",
    "                layer = self.model.get_layer(layer_name)\n",
    "                experiment.log_histogram_3d(layer.get_weights()[0], \n",
    "                                            name=layer_name,\n",
    "                                   step=step)  \n",
    "            except Exception as e:\n",
    "                logger.debug(\"Logging weights error: \" + str(e) + \"\\n\")\n",
    "                # layer probably does not exist\n",
    "                pass\n",
    "        \n",
    "class FreezeLayer(callbacks.Callback):\n",
    "    def __init__(self, logs={}, patience=5, layer='embeddings_layer', verbose=1, set_to=False):\n",
    "        super(FreezeLayer, self).__init__()\n",
    "        self.freeze_epoch = patience\n",
    "        self.freeze_layer = layer\n",
    "        self.verbose = verbose\n",
    "        self.set_to = set_to\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        if epoch == self.freeze_epoch:\n",
    "            try:\n",
    "                layer = model.get_layer(self.freeze_layer)\n",
    "                old_value = layer.trainable\n",
    "                layer.trainable = self.set_to\n",
    "                model.compile()\n",
    "                if self.verbose:\n",
    "                    logger.debug(\"Setting %s layer from %s to trainable=%s...\\n\" % (layer.name, old_value,\n",
    "                                                                   model.get_layer(self.freeze_layer).trainable))\n",
    "            except Exception as e:\n",
    "                # layer probably does not exist\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, \n",
    "                x_train, y_train, x_test, y_test, \n",
    "                batch_size, epochs, class_weight, start_epoch=0, workers=4,\n",
    "                callback_list = [],\n",
    "                model_path='/tmp/model',\n",
    "               verbose=1):\n",
    "    print('Train...')\n",
    "    experiment.log_parameter('class_weight', class_weight.values())\n",
    "    experiment.log_parameter('callbacks', callbacks)\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs, initial_epoch=start_epoch, \n",
    "              class_weight=class_weight,\n",
    "              validation_data=[x_test, y_test],\n",
    "                        verbose=verbose,\n",
    "#               validation_split=0.3,\n",
    "                       workers=workers,\n",
    "            callbacks = [\n",
    "                callbacks.ModelCheckpoint(filepath='%s_best' % model_path, verbose=1, \n",
    "                                          save_best_only=True),#, save_weights_only=True),\n",
    "                callbacks.EarlyStopping(patience=70), *callback_list\n",
    "            ])\n",
    "    model.save(model_path)#, save_weights_only=True)\n",
    "    experiment.log_parameter('model_path', model_path)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 330 samples, validate on 141 samples\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "Epoch 1/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3482 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000\n",
      "Epoch 00001: val_loss improved from inf to 0.51648, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 303us/sample - loss: 0.3851 - f1_m: 0.5542 - precision_m: 0.4022 - recall_m: 0.9545 - val_loss: 0.5165 - val_f1_m: 0.4810 - val_precision_m: 0.3698 - val_recall_m: 0.7000\n",
      "Epoch 2/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3647 - f1_m: 0.2500 - precision_m: 0.1429 - recall_m: 1.0000\n",
      "Epoch 00002: val_loss did not improve from 0.51648\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 248us/sample - loss: 0.3900 - f1_m: 0.4607 - precision_m: 0.3340 - recall_m: 0.8658 - val_loss: 0.5165 - val_f1_m: 0.4424 - val_precision_m: 0.3651 - val_recall_m: 0.5893\n",
      "Epoch 3/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4283 - f1_m: 0.6154 - precision_m: 0.5000 - recall_m: 0.8000\n",
      "Epoch 00003: val_loss improved from 0.51648 to 0.51648, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 260us/sample - loss: 0.3838 - f1_m: 0.5285 - precision_m: 0.3783 - recall_m: 0.9591 - val_loss: 0.5165 - val_f1_m: 0.4866 - val_precision_m: 0.3758 - val_recall_m: 0.7729\n",
      "Epoch 4/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3134 - f1_m: 0.6154 - precision_m: 0.4444 - recall_m: 1.0000\n",
      "Epoch 00004: val_loss improved from 0.51648 to 0.51645, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 267us/sample - loss: 0.3875 - f1_m: 0.5361 - precision_m: 0.3840 - recall_m: 0.9667 - val_loss: 0.5164 - val_f1_m: 0.5119 - val_precision_m: 0.4094 - val_recall_m: 0.8417\n",
      "Epoch 5/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4075 - f1_m: 0.5333 - precision_m: 0.3636 - recall_m: 1.0000\n",
      "Epoch 00005: val_loss improved from 0.51645 to 0.51643, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 332us/sample - loss: 0.3873 - f1_m: 0.5658 - precision_m: 0.4350 - recall_m: 0.9545 - val_loss: 0.5164 - val_f1_m: 0.5143 - val_precision_m: 0.3854 - val_recall_m: 0.7929\n",
      "Epoch 6/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4475 - f1_m: 0.2500 - precision_m: 0.1429 - recall_m: 1.0000\n",
      "Epoch 00006: val_loss did not improve from 0.51643\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 178us/sample - loss: 0.3893 - f1_m: 0.4843 - precision_m: 0.3481 - recall_m: 0.8961 - val_loss: 0.5164 - val_f1_m: 0.4172 - val_precision_m: 0.3224 - val_recall_m: 0.6433\n",
      "Epoch 7/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3575 - f1_m: 0.5455 - precision_m: 0.3750 - recall_m: 1.0000\n",
      "Epoch 00007: val_loss did not improve from 0.51643\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 249us/sample - loss: 0.3963 - f1_m: 0.5309 - precision_m: 0.3994 - recall_m: 0.8658 - val_loss: 0.5164 - val_f1_m: 0.5179 - val_precision_m: 0.4368 - val_recall_m: 0.7095\n",
      "Epoch 8/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4492 - f1_m: 0.5000 - precision_m: 0.3636 - recall_m: 0.8000\n",
      "Epoch 00008: val_loss did not improve from 0.51643\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 231us/sample - loss: 0.3873 - f1_m: 0.5203 - precision_m: 0.3765 - recall_m: 0.9636 - val_loss: 0.5164 - val_f1_m: 0.4967 - val_precision_m: 0.4958 - val_recall_m: 0.7381\n",
      "Epoch 9/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3761 - f1_m: 0.7368 - precision_m: 0.5833 - recall_m: 1.0000\n",
      "Epoch 00009: val_loss did not improve from 0.51643\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 225us/sample - loss: 0.3901 - f1_m: 0.5365 - precision_m: 0.3994 - recall_m: 0.9364 - val_loss: 0.5165 - val_f1_m: 0.5341 - val_precision_m: 0.4031 - val_recall_m: 0.7956\n",
      "Epoch 10/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4558 - f1_m: 0.4615 - precision_m: 0.3000 - recall_m: 1.0000\n",
      "Epoch 00010: val_loss did not improve from 0.51643\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 199us/sample - loss: 0.3819 - f1_m: 0.4684 - precision_m: 0.3312 - recall_m: 0.8763 - val_loss: 0.5165 - val_f1_m: 0.5418 - val_precision_m: 0.4476 - val_recall_m: 0.7500\n",
      "Epoch 11/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3345 - f1_m: 0.7143 - precision_m: 0.5556 - recall_m: 1.0000\n",
      "Epoch 00011: val_loss improved from 0.51643 to 0.51637, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 255us/sample - loss: 0.3892 - f1_m: 0.4999 - precision_m: 0.3569 - recall_m: 0.9091 - val_loss: 0.5164 - val_f1_m: 0.5533 - val_precision_m: 0.4345 - val_recall_m: 0.7800\n",
      "Epoch 12/300\n",
      "288/330 [=========================>....] - ETA: 0s - loss: 0.3931 - f1_m: 0.5311 - precision_m: 0.3923 - recall_m: 0.9444\n",
      "Epoch 00012: val_loss improved from 0.51637 to 0.51635, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 368us/sample - loss: 0.3940 - f1_m: 0.4765 - precision_m: 0.3483 - recall_m: 0.8636 - val_loss: 0.5164 - val_f1_m: 0.4031 - val_precision_m: 0.3109 - val_recall_m: 0.5762\n",
      "Epoch 13/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3153 - f1_m: 0.8235 - precision_m: 0.7000 - recall_m: 1.0000\n",
      "Epoch 00013: val_loss improved from 0.51635 to 0.51629, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 260us/sample - loss: 0.3883 - f1_m: 0.5448 - precision_m: 0.3976 - recall_m: 0.9515 - val_loss: 0.5163 - val_f1_m: 0.4886 - val_precision_m: 0.3811 - val_recall_m: 0.7933\n",
      "Epoch 14/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3168 - f1_m: 0.5714 - precision_m: 0.4000 - recall_m: 1.0000\n",
      "Epoch 00014: val_loss improved from 0.51629 to 0.51627, saving model to models/mlp_user_anorex_best\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 264us/sample - loss: 0.3789 - f1_m: 0.4847 - precision_m: 0.3476 - recall_m: 0.8788 - val_loss: 0.5163 - val_f1_m: 0.5146 - val_precision_m: 0.4212 - val_recall_m: 0.7667\n",
      "Epoch 15/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3665 - f1_m: 0.6667 - precision_m: 0.5000 - recall_m: 1.0000\n",
      "Epoch 00015: val_loss improved from 0.51627 to 0.51618, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 248us/sample - loss: 0.3892 - f1_m: 0.4737 - precision_m: 0.3382 - recall_m: 0.8409 - val_loss: 0.5162 - val_f1_m: 0.5020 - val_precision_m: 0.5011 - val_recall_m: 0.6767\n",
      "Epoch 16/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3601 - f1_m: 0.6000 - precision_m: 0.4286 - recall_m: 1.0000\n",
      "Epoch 00016: val_loss improved from 0.51618 to 0.51605, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 306us/sample - loss: 0.3896 - f1_m: 0.5431 - precision_m: 0.3943 - recall_m: 0.9394 - val_loss: 0.5160 - val_f1_m: 0.5252 - val_precision_m: 0.4114 - val_recall_m: 0.7643\n",
      "Epoch 17/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4155 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000\n",
      "Epoch 00017: val_loss improved from 0.51605 to 0.51603, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 234us/sample - loss: 0.3925 - f1_m: 0.5751 - precision_m: 0.4447 - recall_m: 0.9545 - val_loss: 0.5160 - val_f1_m: 0.4937 - val_precision_m: 0.3857 - val_recall_m: 0.7095\n",
      "Epoch 18/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4369 - f1_m: 0.6667 - precision_m: 0.5714 - recall_m: 0.8000\n",
      "Epoch 00018: val_loss did not improve from 0.51603\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 220us/sample - loss: 0.4022 - f1_m: 0.5133 - precision_m: 0.4098 - recall_m: 0.7848 - val_loss: 0.5160 - val_f1_m: 0.5101 - val_precision_m: 0.4244 - val_recall_m: 0.8100\n",
      "Epoch 19/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3672 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000\n",
      "Epoch 00019: val_loss improved from 0.51603 to 0.51601, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 282us/sample - loss: 0.3880 - f1_m: 0.5396 - precision_m: 0.3843 - recall_m: 0.9242 - val_loss: 0.5160 - val_f1_m: 0.4806 - val_precision_m: 0.3659 - val_recall_m: 0.7929\n",
      "Epoch 20/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4018 - f1_m: 0.7059 - precision_m: 0.5455 - recall_m: 1.0000\n",
      "Epoch 00020: val_loss did not improve from 0.51601\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 214us/sample - loss: 0.3899 - f1_m: 0.5708 - precision_m: 0.4420 - recall_m: 0.9394 - val_loss: 0.5161 - val_f1_m: 0.4349 - val_precision_m: 0.3342 - val_recall_m: 0.7294\n",
      "Epoch 21/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4305 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000\n",
      "Epoch 00021: val_loss did not improve from 0.51601\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 263us/sample - loss: 0.3906 - f1_m: 0.5234 - precision_m: 0.3690 - recall_m: 0.9697 - val_loss: 0.5161 - val_f1_m: 0.5911 - val_precision_m: 0.4989 - val_recall_m: 0.8050\n",
      "Epoch 22/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3807 - f1_m: 0.8000 - precision_m: 0.6667 - recall_m: 1.0000\n",
      "Epoch 00022: val_loss did not improve from 0.51601\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 258us/sample - loss: 0.3934 - f1_m: 0.5391 - precision_m: 0.4036 - recall_m: 0.9621 - val_loss: 0.5161 - val_f1_m: 0.5250 - val_precision_m: 0.4113 - val_recall_m: 0.7500\n",
      "Epoch 23/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4005 - f1_m: 0.6154 - precision_m: 0.4444 - recall_m: 1.0000\n",
      "Epoch 00023: val_loss did not improve from 0.51601\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 245us/sample - loss: 0.3867 - f1_m: 0.5148 - precision_m: 0.3703 - recall_m: 0.9697 - val_loss: 0.5161 - val_f1_m: 0.5027 - val_precision_m: 0.4038 - val_recall_m: 0.7095\n",
      "Epoch 24/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4216 - f1_m: 0.3333 - precision_m: 0.2222 - recall_m: 0.6667\n",
      "Epoch 00024: val_loss did not improve from 0.51601\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 281us/sample - loss: 0.3904 - f1_m: 0.5316 - precision_m: 0.3977 - recall_m: 0.9394 - val_loss: 0.5160 - val_f1_m: 0.5421 - val_precision_m: 0.4113 - val_recall_m: 0.8048\n",
      "Epoch 25/300\n",
      "288/330 [=========================>....] - ETA: 0s - loss: 0.4054 - f1_m: 0.4721 - precision_m: 0.3373 - recall_m: 0.8611\n",
      "Epoch 00025: val_loss did not improve from 0.51601\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 284us/sample - loss: 0.3967 - f1_m: 0.5277 - precision_m: 0.4018 - recall_m: 0.8864 - val_loss: 0.5161 - val_f1_m: 0.4433 - val_precision_m: 0.3412 - val_recall_m: 0.7229\n",
      "Epoch 26/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3626 - f1_m: 0.6154 - precision_m: 0.4444 - recall_m: 1.0000\n",
      "Epoch 00026: val_loss did not improve from 0.51601\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 205us/sample - loss: 0.3836 - f1_m: 0.5355 - precision_m: 0.3816 - recall_m: 0.9515 - val_loss: 0.5161 - val_f1_m: 0.5222 - val_precision_m: 0.4143 - val_recall_m: 0.7429\n",
      "Epoch 27/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3495 - f1_m: 0.6667 - precision_m: 0.5000 - recall_m: 1.0000\n",
      "Epoch 00027: val_loss did not improve from 0.51601\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 248us/sample - loss: 0.3875 - f1_m: 0.5841 - precision_m: 0.4512 - recall_m: 0.9697 - val_loss: 0.5161 - val_f1_m: 0.4204 - val_precision_m: 0.3312 - val_recall_m: 0.6083\n",
      "Epoch 28/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3828 - f1_m: 0.2000 - precision_m: 0.1111 - recall_m: 1.0000\n",
      "Epoch 00028: val_loss did not improve from 0.51601\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 254us/sample - loss: 0.3825 - f1_m: 0.4853 - precision_m: 0.3514 - recall_m: 0.8758 - val_loss: 0.5161 - val_f1_m: 0.4780 - val_precision_m: 0.3733 - val_recall_m: 0.7017\n",
      "Epoch 29/300\n",
      "288/330 [=========================>....] - ETA: 0s - loss: 0.3748 - f1_m: 0.5640 - precision_m: 0.4256 - recall_m: 0.9630\n",
      "Epoch 00029: val_loss did not improve from 0.51601\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 264us/sample - loss: 0.3850 - f1_m: 0.4874 - precision_m: 0.3633 - recall_m: 0.8788 - val_loss: 0.5161 - val_f1_m: 0.4800 - val_precision_m: 0.3591 - val_recall_m: 0.7679\n",
      "Epoch 30/300\n",
      "224/330 [===================>..........] - ETA: 0s - loss: 0.3829 - f1_m: 0.5234 - precision_m: 0.3706 - recall_m: 0.9286\n",
      "Epoch 00030: val_loss improved from 0.51601 to 0.51596, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 396us/sample - loss: 0.3902 - f1_m: 0.4792 - precision_m: 0.3419 - recall_m: 0.8636 - val_loss: 0.5160 - val_f1_m: 0.5210 - val_precision_m: 0.4111 - val_recall_m: 0.7629\n",
      "Epoch 31/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3488 - f1_m: 0.7143 - precision_m: 0.6250 - recall_m: 0.8333\n",
      "Epoch 00031: val_loss did not improve from 0.51596\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 190us/sample - loss: 0.3882 - f1_m: 0.5772 - precision_m: 0.4457 - recall_m: 0.9621 - val_loss: 0.5160 - val_f1_m: 0.5111 - val_precision_m: 0.3889 - val_recall_m: 0.7790\n",
      "Epoch 32/300\n",
      "224/330 [===================>..........] - ETA: 0s - loss: 0.3977 - f1_m: 0.5125 - precision_m: 0.3728 - recall_m: 0.9714\n",
      "Epoch 00032: val_loss did not improve from 0.51596\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 307us/sample - loss: 0.3896 - f1_m: 0.5731 - precision_m: 0.4440 - recall_m: 0.9591 - val_loss: 0.5160 - val_f1_m: 0.4876 - val_precision_m: 0.3778 - val_recall_m: 0.7524\n",
      "Epoch 33/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3307 - f1_m: 0.7778 - precision_m: 0.6364 - recall_m: 1.0000\n",
      "Epoch 00033: val_loss did not improve from 0.51596\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 292us/sample - loss: 0.3963 - f1_m: 0.5049 - precision_m: 0.3926 - recall_m: 0.8788 - val_loss: 0.5160 - val_f1_m: 0.5914 - val_precision_m: 0.5076 - val_recall_m: 0.8048\n",
      "Epoch 34/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4391 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000\n",
      "Epoch 00034: val_loss did not improve from 0.51596\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 226us/sample - loss: 0.3906 - f1_m: 0.5303 - precision_m: 0.3921 - recall_m: 0.8606 - val_loss: 0.5161 - val_f1_m: 0.4751 - val_precision_m: 0.3987 - val_recall_m: 0.7250\n",
      "Epoch 35/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3394 - f1_m: 0.5333 - precision_m: 0.3636 - recall_m: 1.0000\n",
      "Epoch 00035: val_loss did not improve from 0.51596\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 275us/sample - loss: 0.3919 - f1_m: 0.4497 - precision_m: 0.3279 - recall_m: 0.7727 - val_loss: 0.5160 - val_f1_m: 0.5170 - val_precision_m: 0.4137 - val_recall_m: 0.7248\n",
      "Epoch 36/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4206 - f1_m: 0.6250 - precision_m: 0.4545 - recall_m: 1.0000\n",
      "Epoch 00036: val_loss improved from 0.51596 to 0.51585, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 240us/sample - loss: 0.3845 - f1_m: 0.4743 - precision_m: 0.3407 - recall_m: 0.8561 - val_loss: 0.5158 - val_f1_m: 0.6080 - val_precision_m: 0.5202 - val_recall_m: 0.7895\n",
      "Epoch 37/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3391 - f1_m: 0.4444 - precision_m: 0.2857 - recall_m: 1.0000\n",
      "Epoch 00037: val_loss improved from 0.51585 to 0.51575, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 304us/sample - loss: 0.3858 - f1_m: 0.5385 - precision_m: 0.4217 - recall_m: 0.9515 - val_loss: 0.5157 - val_f1_m: 0.4900 - val_precision_m: 0.3690 - val_recall_m: 0.7929\n",
      "Epoch 38/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3557 - f1_m: 0.6667 - precision_m: 0.5556 - recall_m: 0.8333\n",
      "Epoch 00038: val_loss did not improve from 0.51575\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 216us/sample - loss: 0.3879 - f1_m: 0.5509 - precision_m: 0.4143 - recall_m: 0.9545 - val_loss: 0.5158 - val_f1_m: 0.4776 - val_precision_m: 0.3700 - val_recall_m: 0.6929\n",
      "Epoch 39/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4605 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000\n",
      "Epoch 00039: val_loss improved from 0.51575 to 0.51571, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 259us/sample - loss: 0.3867 - f1_m: 0.4776 - precision_m: 0.3503 - recall_m: 0.8734 - val_loss: 0.5157 - val_f1_m: 0.4297 - val_precision_m: 0.3424 - val_recall_m: 0.5810\n",
      "Epoch 40/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3872 - f1_m: 0.7500 - precision_m: 0.6667 - recall_m: 0.8571\n",
      "Epoch 00040: val_loss improved from 0.51571 to 0.51563, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 251us/sample - loss: 0.3887 - f1_m: 0.5015 - precision_m: 0.3686 - recall_m: 0.8658 - val_loss: 0.5156 - val_f1_m: 0.4176 - val_precision_m: 0.3236 - val_recall_m: 0.6000\n",
      "Epoch 41/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3984 - f1_m: 0.6667 - precision_m: 0.5000 - recall_m: 1.0000\n",
      "Epoch 00041: val_loss improved from 0.51563 to 0.51558, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 295us/sample - loss: 0.3868 - f1_m: 0.5333 - precision_m: 0.3941 - recall_m: 0.9515 - val_loss: 0.5156 - val_f1_m: 0.4804 - val_precision_m: 0.3611 - val_recall_m: 0.8286\n",
      "Epoch 42/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4425 - f1_m: 0.2500 - precision_m: 0.1667 - recall_m: 0.5000\n",
      "Epoch 00042: val_loss improved from 0.51558 to 0.51557, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 220us/sample - loss: 0.3916 - f1_m: 0.5512 - precision_m: 0.4060 - recall_m: 0.9364 - val_loss: 0.5156 - val_f1_m: 0.4937 - val_precision_m: 0.3845 - val_recall_m: 0.7095\n",
      "Epoch 43/300\n",
      "320/330 [============================>.] - ETA: 0s - loss: 0.3824 - f1_m: 0.5552 - precision_m: 0.4272 - recall_m: 0.9550\n",
      "Epoch 00043: val_loss did not improve from 0.51557\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 288us/sample - loss: 0.3858 - f1_m: 0.5502 - precision_m: 0.4187 - recall_m: 0.9591 - val_loss: 0.5156 - val_f1_m: 0.5180 - val_precision_m: 0.4156 - val_recall_m: 0.7733\n",
      "Epoch 44/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4046 - f1_m: 0.4444 - precision_m: 0.2857 - recall_m: 1.0000\n",
      "Epoch 00044: val_loss improved from 0.51557 to 0.51546, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 227us/sample - loss: 0.3949 - f1_m: 0.4488 - precision_m: 0.3406 - recall_m: 0.7597 - val_loss: 0.5155 - val_f1_m: 0.4168 - val_precision_m: 0.3238 - val_recall_m: 0.6400\n",
      "Epoch 45/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4469 - f1_m: 0.8571 - precision_m: 0.7500 - recall_m: 1.0000\n",
      "Epoch 00045: val_loss improved from 0.51546 to 0.51533, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 321us/sample - loss: 0.3944 - f1_m: 0.4877 - precision_m: 0.3648 - recall_m: 0.8455 - val_loss: 0.5153 - val_f1_m: 0.4936 - val_precision_m: 0.3769 - val_recall_m: 0.7829\n",
      "Epoch 46/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4165 - f1_m: 0.6667 - precision_m: 0.5556 - recall_m: 0.8333\n",
      "Epoch 00046: val_loss improved from 0.51533 to 0.51522, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 223us/sample - loss: 0.3871 - f1_m: 0.4744 - precision_m: 0.3372 - recall_m: 0.8636 - val_loss: 0.5152 - val_f1_m: 0.5376 - val_precision_m: 0.4113 - val_recall_m: 0.7950\n",
      "Epoch 47/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3989 - f1_m: 0.6667 - precision_m: 0.5455 - recall_m: 0.8571\n",
      "Epoch 00047: val_loss improved from 0.51522 to 0.51509, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 261us/sample - loss: 0.3937 - f1_m: 0.4649 - precision_m: 0.3413 - recall_m: 0.8734 - val_loss: 0.5151 - val_f1_m: 0.4354 - val_precision_m: 0.3298 - val_recall_m: 0.6417\n",
      "Epoch 48/300\n",
      "320/330 [============================>.] - ETA: 0s - loss: 0.3846 - f1_m: 0.5520 - precision_m: 0.3976 - recall_m: 0.9675\n",
      "Epoch 00048: val_loss improved from 0.51509 to 0.51490, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 406us/sample - loss: 0.3876 - f1_m: 0.5018 - precision_m: 0.3615 - recall_m: 0.8795 - val_loss: 0.5149 - val_f1_m: 0.4719 - val_precision_m: 0.3620 - val_recall_m: 0.7655\n",
      "Epoch 49/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3233 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000\n",
      "Epoch 00049: val_loss improved from 0.51490 to 0.51477, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 313us/sample - loss: 0.3951 - f1_m: 0.4740 - precision_m: 0.3506 - recall_m: 0.8000 - val_loss: 0.5148 - val_f1_m: 0.4969 - val_precision_m: 0.3854 - val_recall_m: 0.8012\n",
      "Epoch 50/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3486 - f1_m: 0.6667 - precision_m: 0.5000 - recall_m: 1.0000\n",
      "Epoch 00050: val_loss improved from 0.51477 to 0.51468, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 320us/sample - loss: 0.3967 - f1_m: 0.4886 - precision_m: 0.3539 - recall_m: 0.8734 - val_loss: 0.5147 - val_f1_m: 0.4901 - val_precision_m: 0.3817 - val_recall_m: 0.7429\n",
      "Epoch 51/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.5295 - f1_m: 0.1818 - precision_m: 0.1000 - recall_m: 1.0000\n",
      "Epoch 00051: val_loss improved from 0.51468 to 0.51449, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 279us/sample - loss: 0.3947 - f1_m: 0.4872 - precision_m: 0.3507 - recall_m: 0.8909 - val_loss: 0.5145 - val_f1_m: 0.4849 - val_precision_m: 0.3785 - val_recall_m: 0.7848\n",
      "Epoch 52/300\n",
      "288/330 [=========================>....] - ETA: 0s - loss: 0.3997 - f1_m: 0.5139 - precision_m: 0.3887 - recall_m: 0.9444\n",
      "Epoch 00052: val_loss did not improve from 0.51449\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 314us/sample - loss: 0.3948 - f1_m: 0.4811 - precision_m: 0.3635 - recall_m: 0.8636 - val_loss: 0.5145 - val_f1_m: 0.4426 - val_precision_m: 0.3535 - val_recall_m: 0.5962\n",
      "Epoch 53/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4209 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000\n",
      "Epoch 00053: val_loss improved from 0.51449 to 0.51440, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 306us/sample - loss: 0.3870 - f1_m: 0.4716 - precision_m: 0.3426 - recall_m: 0.8455 - val_loss: 0.5144 - val_f1_m: 0.4808 - val_precision_m: 0.3795 - val_recall_m: 0.7964\n",
      "Epoch 54/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4258 - f1_m: 0.5263 - precision_m: 0.3571 - recall_m: 1.0000\n",
      "Epoch 00054: val_loss improved from 0.51440 to 0.51421, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 180us/sample - loss: 0.3886 - f1_m: 0.4545 - precision_m: 0.3410 - recall_m: 0.7500 - val_loss: 0.5142 - val_f1_m: 0.5044 - val_precision_m: 0.3925 - val_recall_m: 0.7333\n",
      "Epoch 55/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4704 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000\n",
      "Epoch 00055: val_loss improved from 0.51421 to 0.51410, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 229us/sample - loss: 0.3909 - f1_m: 0.4936 - precision_m: 0.3609 - recall_m: 0.8909 - val_loss: 0.5141 - val_f1_m: 0.5050 - val_precision_m: 0.3998 - val_recall_m: 0.7200\n",
      "Epoch 56/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4436 - f1_m: 0.5556 - precision_m: 0.4167 - recall_m: 0.8333\n",
      "Epoch 00056: val_loss did not improve from 0.51410\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 179us/sample - loss: 0.3926 - f1_m: 0.5248 - precision_m: 0.3838 - recall_m: 0.8758 - val_loss: 0.5141 - val_f1_m: 0.5125 - val_precision_m: 0.3867 - val_recall_m: 0.7929\n",
      "Epoch 57/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3642 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000\n",
      "Epoch 00057: val_loss did not improve from 0.51410\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 159us/sample - loss: 0.3906 - f1_m: 0.5140 - precision_m: 0.3652 - recall_m: 0.9545 - val_loss: 0.5141 - val_f1_m: 0.5079 - val_precision_m: 0.3834 - val_recall_m: 0.7690\n",
      "Epoch 58/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.5166 - f1_m: 0.3077 - precision_m: 0.2000 - recall_m: 0.6667\n",
      "Epoch 00058: val_loss did not improve from 0.51410\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 212us/sample - loss: 0.3857 - f1_m: 0.5700 - precision_m: 0.4307 - recall_m: 0.9515 - val_loss: 0.5141 - val_f1_m: 0.4159 - val_precision_m: 0.3191 - val_recall_m: 0.6083\n",
      "Epoch 59/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3082 - f1_m: 0.7619 - precision_m: 0.6154 - recall_m: 1.0000\n",
      "Epoch 00059: val_loss did not improve from 0.51410\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 240us/sample - loss: 0.3928 - f1_m: 0.4922 - precision_m: 0.3597 - recall_m: 0.8727 - val_loss: 0.5142 - val_f1_m: 0.4810 - val_precision_m: 0.3773 - val_recall_m: 0.6981\n",
      "Epoch 60/300\n",
      "128/330 [==========>...................] - ETA: 0s - loss: 0.3788 - f1_m: 0.5434 - precision_m: 0.3958 - recall_m: 0.8750\n",
      "Epoch 00060: val_loss improved from 0.51410 to 0.51407, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 457us/sample - loss: 0.3869 - f1_m: 0.5412 - precision_m: 0.3903 - recall_m: 0.9545 - val_loss: 0.5141 - val_f1_m: 0.4940 - val_precision_m: 0.3709 - val_recall_m: 0.8139\n",
      "Epoch 61/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4851 - f1_m: 0.4000 - precision_m: 0.2857 - recall_m: 0.6667\n",
      "Epoch 00061: val_loss did not improve from 0.51407\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 263us/sample - loss: 0.3903 - f1_m: 0.4896 - precision_m: 0.3595 - recall_m: 0.8333 - val_loss: 0.5141 - val_f1_m: 0.5308 - val_precision_m: 0.4472 - val_recall_m: 0.7267\n",
      "Epoch 62/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3805 - f1_m: 0.6000 - precision_m: 0.4286 - recall_m: 1.0000\n",
      "Epoch 00062: val_loss improved from 0.51407 to 0.51405, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 279us/sample - loss: 0.3870 - f1_m: 0.5711 - precision_m: 0.4226 - recall_m: 0.9591 - val_loss: 0.5141 - val_f1_m: 0.4372 - val_precision_m: 0.3556 - val_recall_m: 0.6651\n",
      "Epoch 63/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4091 - f1_m: 0.2222 - precision_m: 0.1250 - recall_m: 1.0000\n",
      "Epoch 00063: val_loss improved from 0.51405 to 0.51403, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 261us/sample - loss: 0.3861 - f1_m: 0.4944 - precision_m: 0.3623 - recall_m: 0.8636 - val_loss: 0.5140 - val_f1_m: 0.4778 - val_precision_m: 0.3712 - val_recall_m: 0.7867\n",
      "Epoch 64/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4016 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000\n",
      "Epoch 00064: val_loss improved from 0.51403 to 0.51393, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 258us/sample - loss: 0.3843 - f1_m: 0.5353 - precision_m: 0.3835 - recall_m: 0.9659 - val_loss: 0.5139 - val_f1_m: 0.4542 - val_precision_m: 0.3508 - val_recall_m: 0.6743\n",
      "Epoch 65/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4370 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000\n",
      "Epoch 00065: val_loss improved from 0.51393 to 0.51392, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 221us/sample - loss: 0.3913 - f1_m: 0.4546 - precision_m: 0.3277 - recall_m: 0.8182 - val_loss: 0.5139 - val_f1_m: 0.5153 - val_precision_m: 0.3909 - val_recall_m: 0.7667\n",
      "Epoch 66/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3916 - f1_m: 0.4286 - precision_m: 0.2727 - recall_m: 1.0000\n",
      "Epoch 00066: val_loss improved from 0.51392 to 0.51388, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 234us/sample - loss: 0.3832 - f1_m: 0.4869 - precision_m: 0.3489 - recall_m: 0.8779 - val_loss: 0.5139 - val_f1_m: 0.5356 - val_precision_m: 0.4279 - val_recall_m: 0.7500\n",
      "Epoch 67/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3569 - f1_m: 0.5714 - precision_m: 0.4000 - recall_m: 1.0000\n",
      "Epoch 00067: val_loss improved from 0.51388 to 0.51383, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 238us/sample - loss: 0.3902 - f1_m: 0.5302 - precision_m: 0.3847 - recall_m: 0.9545 - val_loss: 0.5138 - val_f1_m: 0.5102 - val_precision_m: 0.4287 - val_recall_m: 0.7200\n",
      "Epoch 68/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.5385 - f1_m: 0.2857 - precision_m: 0.1818 - recall_m: 0.6667\n",
      "Epoch 00068: val_loss improved from 0.51383 to 0.51383, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 230us/sample - loss: 0.3863 - f1_m: 0.4891 - precision_m: 0.3518 - recall_m: 0.8485 - val_loss: 0.5138 - val_f1_m: 0.5102 - val_precision_m: 0.3875 - val_recall_m: 0.7933\n",
      "Epoch 69/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3767 - f1_m: 0.6667 - precision_m: 0.5000 - recall_m: 1.0000\n",
      "Epoch 00069: val_loss improved from 0.51383 to 0.51381, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 229us/sample - loss: 0.3856 - f1_m: 0.5630 - precision_m: 0.4127 - recall_m: 0.9470 - val_loss: 0.5138 - val_f1_m: 0.4357 - val_precision_m: 0.3560 - val_recall_m: 0.6190\n",
      "Epoch 70/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3547 - f1_m: 0.6667 - precision_m: 0.5000 - recall_m: 1.0000\n",
      "Epoch 00070: val_loss did not improve from 0.51381\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 178us/sample - loss: 0.3871 - f1_m: 0.5530 - precision_m: 0.4011 - recall_m: 0.9567 - val_loss: 0.5139 - val_f1_m: 0.4583 - val_precision_m: 0.3438 - val_recall_m: 0.7467\n",
      "Epoch 71/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3852 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000\n",
      "Epoch 00071: val_loss did not improve from 0.51381\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 178us/sample - loss: 0.3838 - f1_m: 0.5327 - precision_m: 0.3852 - recall_m: 0.9688 - val_loss: 0.5138 - val_f1_m: 0.5040 - val_precision_m: 0.3885 - val_recall_m: 0.7848\n",
      "Epoch 72/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3524 - f1_m: 0.6000 - precision_m: 0.4286 - recall_m: 1.0000\n",
      "Epoch 00072: val_loss did not improve from 0.51381\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 175us/sample - loss: 0.3899 - f1_m: 0.5326 - precision_m: 0.3840 - recall_m: 0.9242 - val_loss: 0.5139 - val_f1_m: 0.5464 - val_precision_m: 0.4305 - val_recall_m: 0.7762\n",
      "Epoch 73/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3844 - f1_m: 0.4615 - precision_m: 0.3000 - recall_m: 1.0000\n",
      "Epoch 00073: val_loss did not improve from 0.51381\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 188us/sample - loss: 0.3884 - f1_m: 0.4865 - precision_m: 0.3458 - recall_m: 0.8712 - val_loss: 0.5140 - val_f1_m: 0.4158 - val_precision_m: 0.3178 - val_recall_m: 0.6345\n",
      "Epoch 74/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.2914 - f1_m: 0.6154 - precision_m: 0.4444 - recall_m: 1.0000\n",
      "Epoch 00074: val_loss did not improve from 0.51381\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 234us/sample - loss: 0.3875 - f1_m: 0.5721 - precision_m: 0.4332 - recall_m: 0.9643 - val_loss: 0.5139 - val_f1_m: 0.5097 - val_precision_m: 0.4090 - val_recall_m: 0.7317\n",
      "Epoch 75/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.2924 - f1_m: 0.5714 - precision_m: 0.4000 - recall_m: 1.0000\n",
      "Epoch 00075: val_loss did not improve from 0.51381\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 186us/sample - loss: 0.3889 - f1_m: 0.5371 - precision_m: 0.3918 - recall_m: 0.9719 - val_loss: 0.5139 - val_f1_m: 0.5327 - val_precision_m: 0.4083 - val_recall_m: 0.7743\n",
      "Epoch 76/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.5390 - f1_m: 0.2222 - precision_m: 0.1429 - recall_m: 0.5000\n",
      "Epoch 00076: val_loss did not improve from 0.51381\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 179us/sample - loss: 0.3925 - f1_m: 0.4870 - precision_m: 0.3524 - recall_m: 0.8333 - val_loss: 0.5139 - val_f1_m: 0.4316 - val_precision_m: 0.3341 - val_recall_m: 0.6631\n",
      "Epoch 77/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3476 - f1_m: 0.3636 - precision_m: 0.2222 - recall_m: 1.0000\n",
      "Epoch 00077: val_loss did not improve from 0.51381\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 248us/sample - loss: 0.3818 - f1_m: 0.5814 - precision_m: 0.4446 - recall_m: 0.9643 - val_loss: 0.5138 - val_f1_m: 0.5324 - val_precision_m: 0.4330 - val_recall_m: 0.7433\n",
      "Epoch 78/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3430 - f1_m: 0.7273 - precision_m: 0.5714 - recall_m: 1.0000\n",
      "Epoch 00078: val_loss did not improve from 0.51381\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 186us/sample - loss: 0.3870 - f1_m: 0.5317 - precision_m: 0.3832 - recall_m: 0.9636 - val_loss: 0.5138 - val_f1_m: 0.5159 - val_precision_m: 0.4033 - val_recall_m: 0.7314\n",
      "Epoch 79/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4103 - f1_m: 0.5333 - precision_m: 0.3636 - recall_m: 1.0000\n",
      "Epoch 00079: val_loss did not improve from 0.51381\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 196us/sample - loss: 0.3951 - f1_m: 0.5453 - precision_m: 0.4092 - recall_m: 0.9545 - val_loss: 0.5139 - val_f1_m: 0.5891 - val_precision_m: 0.4927 - val_recall_m: 0.7950\n",
      "Epoch 80/300\n",
      "320/330 [============================>.] - ETA: 0s - loss: 0.3786 - f1_m: 0.5112 - precision_m: 0.3675 - recall_m: 0.9417\n",
      "Epoch 00080: val_loss did not improve from 0.51381\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 338us/sample - loss: 0.3839 - f1_m: 0.5102 - precision_m: 0.3644 - recall_m: 0.9470 - val_loss: 0.5138 - val_f1_m: 0.4315 - val_precision_m: 0.3406 - val_recall_m: 0.6867\n",
      "Epoch 81/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3780 - f1_m: 0.6154 - precision_m: 0.4444 - recall_m: 1.0000\n",
      "Epoch 00081: val_loss did not improve from 0.51381\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 269us/sample - loss: 0.3926 - f1_m: 0.4721 - precision_m: 0.3519 - recall_m: 0.7803 - val_loss: 0.5139 - val_f1_m: 0.4344 - val_precision_m: 0.3284 - val_recall_m: 0.6500\n",
      "Epoch 82/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.2832 - f1_m: 0.5714 - precision_m: 0.4000 - recall_m: 1.0000\n",
      "Epoch 00082: val_loss improved from 0.51381 to 0.51380, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 263us/sample - loss: 0.3864 - f1_m: 0.4953 - precision_m: 0.3593 - recall_m: 0.8485 - val_loss: 0.5138 - val_f1_m: 0.4862 - val_precision_m: 0.3761 - val_recall_m: 0.7229\n",
      "Epoch 83/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/330 [=>............................] - ETA: 0s - loss: 0.5144 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00\n",
      "Epoch 00083: val_loss improved from 0.51380 to 0.51354, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 269us/sample - loss: 0.3987 - f1_m: 0.4629 - precision_m: 0.3435 - recall_m: 0.7576 - val_loss: 0.5135 - val_f1_m: 0.5067 - val_precision_m: 0.3819 - val_recall_m: 0.7667\n",
      "Epoch 84/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4096 - f1_m: 0.5882 - precision_m: 0.4545 - recall_m: 0.8333\n",
      "Epoch 00084: val_loss improved from 0.51354 to 0.51341, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 364us/sample - loss: 0.3881 - f1_m: 0.5268 - precision_m: 0.3800 - recall_m: 0.9697 - val_loss: 0.5134 - val_f1_m: 0.4897 - val_precision_m: 0.3786 - val_recall_m: 0.8010\n",
      "Epoch 85/300\n",
      "224/330 [===================>..........] - ETA: 0s - loss: 0.3764 - f1_m: 0.4801 - precision_m: 0.3323 - recall_m: 1.0000\n",
      "Epoch 00085: val_loss improved from 0.51341 to 0.51325, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 376us/sample - loss: 0.3877 - f1_m: 0.5508 - precision_m: 0.4089 - recall_m: 0.9697 - val_loss: 0.5133 - val_f1_m: 0.5214 - val_precision_m: 0.4237 - val_recall_m: 0.7500\n",
      "Epoch 86/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3127 - f1_m: 0.4286 - precision_m: 0.2727 - recall_m: 1.0000\n",
      "Epoch 00086: val_loss improved from 0.51325 to 0.51321, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 228us/sample - loss: 0.3860 - f1_m: 0.5313 - precision_m: 0.3947 - recall_m: 0.8606 - val_loss: 0.5132 - val_f1_m: 0.5240 - val_precision_m: 0.4073 - val_recall_m: 0.7950\n",
      "Epoch 87/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4204 - f1_m: 0.3333 - precision_m: 0.2500 - recall_m: 0.5000\n",
      "Epoch 00087: val_loss did not improve from 0.51321\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 213us/sample - loss: 0.3952 - f1_m: 0.5251 - precision_m: 0.3899 - recall_m: 0.9318 - val_loss: 0.5132 - val_f1_m: 0.5243 - val_precision_m: 0.4023 - val_recall_m: 0.8083\n",
      "Epoch 88/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3791 - f1_m: 0.6000 - precision_m: 0.4286 - recall_m: 1.0000\n",
      "Epoch 00088: val_loss did not improve from 0.51321\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 233us/sample - loss: 0.3918 - f1_m: 0.5124 - precision_m: 0.3628 - recall_m: 0.9091 - val_loss: 0.5133 - val_f1_m: 0.4958 - val_precision_m: 0.3890 - val_recall_m: 0.7133\n",
      "Epoch 89/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3773 - f1_m: 0.3077 - precision_m: 0.1818 - recall_m: 1.0000\n",
      "Epoch 00089: val_loss did not improve from 0.51321\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 236us/sample - loss: 0.3895 - f1_m: 0.5309 - precision_m: 0.3790 - recall_m: 0.9515 - val_loss: 0.5134 - val_f1_m: 0.5140 - val_precision_m: 0.3985 - val_recall_m: 0.8107\n",
      "Epoch 90/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3609 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000\n",
      "Epoch 00090: val_loss did not improve from 0.51321\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 209us/sample - loss: 0.3859 - f1_m: 0.5404 - precision_m: 0.3863 - recall_m: 0.9591 - val_loss: 0.5134 - val_f1_m: 0.5063 - val_precision_m: 0.5008 - val_recall_m: 0.7017\n",
      "Epoch 91/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3909 - f1_m: 0.6000 - precision_m: 0.4286 - recall_m: 1.0000\n",
      "Epoch 00091: val_loss did not improve from 0.51321\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 257us/sample - loss: 0.3877 - f1_m: 0.5611 - precision_m: 0.4228 - recall_m: 0.9773 - val_loss: 0.5135 - val_f1_m: 0.4181 - val_precision_m: 0.3193 - val_recall_m: 0.6578\n",
      "Epoch 92/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4563 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000\n",
      "Epoch 00092: val_loss did not improve from 0.51321\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 247us/sample - loss: 0.3939 - f1_m: 0.5302 - precision_m: 0.3880 - recall_m: 0.9567 - val_loss: 0.5135 - val_f1_m: 0.4296 - val_precision_m: 0.3357 - val_recall_m: 0.5976\n",
      "Epoch 93/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4087 - f1_m: 0.5714 - precision_m: 0.4000 - recall_m: 1.0000\n",
      "Epoch 00093: val_loss did not improve from 0.51321\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 233us/sample - loss: 0.3874 - f1_m: 0.4972 - precision_m: 0.3586 - recall_m: 0.8636 - val_loss: 0.5136 - val_f1_m: 0.5127 - val_precision_m: 0.4035 - val_recall_m: 0.7267\n",
      "Epoch 94/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3302 - f1_m: 0.3077 - precision_m: 0.1818 - recall_m: 1.0000\n",
      "Epoch 00094: val_loss did not improve from 0.51321\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 219us/sample - loss: 0.3860 - f1_m: 0.4791 - precision_m: 0.3461 - recall_m: 0.8682 - val_loss: 0.5135 - val_f1_m: 0.3943 - val_precision_m: 0.3068 - val_recall_m: 0.6200\n",
      "Epoch 95/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3991 - f1_m: 0.5333 - precision_m: 0.3636 - recall_m: 1.0000\n",
      "Epoch 00095: val_loss did not improve from 0.51321\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 231us/sample - loss: 0.3906 - f1_m: 0.5209 - precision_m: 0.3767 - recall_m: 0.9515 - val_loss: 0.5134 - val_f1_m: 0.5243 - val_precision_m: 0.4278 - val_recall_m: 0.7362\n",
      "Epoch 96/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3452 - f1_m: 0.5455 - precision_m: 0.3750 - recall_m: 1.0000\n",
      "Epoch 00096: val_loss did not improve from 0.51321\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 248us/sample - loss: 0.3884 - f1_m: 0.5900 - precision_m: 0.4563 - recall_m: 0.9470 - val_loss: 0.5134 - val_f1_m: 0.5227 - val_precision_m: 0.3939 - val_recall_m: 0.8267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/300\n",
      "192/330 [================>.............] - ETA: 0s - loss: 0.3853 - f1_m: 0.5014 - precision_m: 0.3630 - recall_m: 0.9167\n",
      "Epoch 00097: val_loss did not improve from 0.51321\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 290us/sample - loss: 0.3855 - f1_m: 0.5591 - precision_m: 0.4092 - recall_m: 0.9545 - val_loss: 0.5134 - val_f1_m: 0.5191 - val_precision_m: 0.3911 - val_recall_m: 0.8143\n",
      "Epoch 98/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3713 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000\n",
      "Epoch 00098: val_loss did not improve from 0.51321\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 164us/sample - loss: 0.3831 - f1_m: 0.4860 - precision_m: 0.3426 - recall_m: 0.8758 - val_loss: 0.5135 - val_f1_m: 0.4941 - val_precision_m: 0.3849 - val_recall_m: 0.7283\n",
      "Epoch 99/300\n",
      "288/330 [=========================>....] - ETA: 0s - loss: 0.3996 - f1_m: 0.4868 - precision_m: 0.3514 - recall_m: 0.8333\n",
      "Epoch 00099: val_loss did not improve from 0.51321\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 290us/sample - loss: 0.3975 - f1_m: 0.5136 - precision_m: 0.3828 - recall_m: 0.8485 - val_loss: 0.5134 - val_f1_m: 0.5021 - val_precision_m: 0.3936 - val_recall_m: 0.7843\n",
      "Epoch 100/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4297 - f1_m: 0.5333 - precision_m: 0.3636 - recall_m: 1.0000\n",
      "Epoch 00100: val_loss did not improve from 0.51321\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 212us/sample - loss: 0.3891 - f1_m: 0.5333 - precision_m: 0.3999 - recall_m: 0.8658 - val_loss: 0.5134 - val_f1_m: 0.4710 - val_precision_m: 0.3639 - val_recall_m: 0.7905\n",
      "Epoch 101/300\n",
      "160/330 [=============>................] - ETA: 0s - loss: 0.4183 - f1_m: 0.4555 - precision_m: 0.3293 - recall_m: 0.9333\n",
      "Epoch 00101: val_loss did not improve from 0.51321\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 325us/sample - loss: 0.3906 - f1_m: 0.5173 - precision_m: 0.3744 - recall_m: 0.9596 - val_loss: 0.5134 - val_f1_m: 0.5505 - val_precision_m: 0.4464 - val_recall_m: 0.8200\n",
      "Epoch 102/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3699 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000\n",
      "Epoch 00102: val_loss did not improve from 0.51321\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 181us/sample - loss: 0.3899 - f1_m: 0.5232 - precision_m: 0.3758 - recall_m: 0.9364 - val_loss: 0.5133 - val_f1_m: 0.5086 - val_precision_m: 0.3816 - val_recall_m: 0.8250\n",
      "Epoch 103/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3597 - f1_m: 0.5714 - precision_m: 0.4000 - recall_m: 1.0000\n",
      "Epoch 00103: val_loss did not improve from 0.51321\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 232us/sample - loss: 0.3944 - f1_m: 0.5235 - precision_m: 0.3809 - recall_m: 0.9364 - val_loss: 0.5134 - val_f1_m: 0.5410 - val_precision_m: 0.4232 - val_recall_m: 0.8167\n",
      "Epoch 104/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3872 - f1_m: 0.7000 - precision_m: 0.6364 - recall_m: 0.7778\n",
      "Epoch 00104: val_loss did not improve from 0.51321\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 244us/sample - loss: 0.3934 - f1_m: 0.5555 - precision_m: 0.4420 - recall_m: 0.8889 - val_loss: 0.5134 - val_f1_m: 0.4324 - val_precision_m: 0.3682 - val_recall_m: 0.6413\n",
      "Epoch 105/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3819 - f1_m: 0.5455 - precision_m: 0.3750 - recall_m: 1.0000\n",
      "Epoch 00105: val_loss did not improve from 0.51321\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 208us/sample - loss: 0.3883 - f1_m: 0.4838 - precision_m: 0.3467 - recall_m: 0.8636 - val_loss: 0.5133 - val_f1_m: 0.4681 - val_precision_m: 0.3694 - val_recall_m: 0.8000\n",
      "Epoch 106/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4187 - f1_m: 0.4615 - precision_m: 0.3333 - recall_m: 0.7500\n",
      "Epoch 00106: val_loss improved from 0.51321 to 0.51317, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 285us/sample - loss: 0.3835 - f1_m: 0.5299 - precision_m: 0.3779 - recall_m: 0.9659 - val_loss: 0.5132 - val_f1_m: 0.5214 - val_precision_m: 0.4296 - val_recall_m: 0.8250\n",
      "Epoch 107/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3688 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000\n",
      "Epoch 00107: val_loss improved from 0.51317 to 0.51314, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 252us/sample - loss: 0.3888 - f1_m: 0.5113 - precision_m: 0.3729 - recall_m: 0.8939 - val_loss: 0.5131 - val_f1_m: 0.5063 - val_precision_m: 0.4085 - val_recall_m: 0.7589\n",
      "Epoch 108/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4230 - f1_m: 0.7692 - precision_m: 0.6250 - recall_m: 1.0000\n",
      "Epoch 00108: val_loss did not improve from 0.51314\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 261us/sample - loss: 0.3879 - f1_m: 0.5337 - precision_m: 0.3923 - recall_m: 0.8864 - val_loss: 0.5133 - val_f1_m: 0.4971 - val_precision_m: 0.4200 - val_recall_m: 0.7500\n",
      "Epoch 109/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4141 - f1_m: 0.6250 - precision_m: 0.5000 - recall_m: 0.8333\n",
      "Epoch 00109: val_loss did not improve from 0.51314\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 244us/sample - loss: 0.3838 - f1_m: 0.5061 - precision_m: 0.3557 - recall_m: 0.9621 - val_loss: 0.5133 - val_f1_m: 0.5414 - val_precision_m: 0.4322 - val_recall_m: 0.7810\n",
      "Epoch 110/300\n",
      "288/330 [=========================>....] - ETA: 0s - loss: 0.3964 - f1_m: 0.5381 - precision_m: 0.3884 - recall_m: 0.9630\n",
      "Epoch 00110: val_loss did not improve from 0.51314\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 339us/sample - loss: 0.3919 - f1_m: 0.4857 - precision_m: 0.3481 - recall_m: 0.8788 - val_loss: 0.5132 - val_f1_m: 0.4881 - val_precision_m: 0.3939 - val_recall_m: 0.8273\n",
      "Epoch 111/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3953 - f1_m: 0.6667 - precision_m: 0.5000 - recall_m: 1.0000\n",
      "Epoch 00111: val_loss did not improve from 0.51314\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 176us/sample - loss: 0.3895 - f1_m: 0.5447 - precision_m: 0.4051 - recall_m: 0.9591 - val_loss: 0.5132 - val_f1_m: 0.4754 - val_precision_m: 0.3552 - val_recall_m: 0.7500\n",
      "Epoch 112/300\n",
      "224/330 [===================>..........] - ETA: 0s - loss: 0.3848 - f1_m: 0.5742 - precision_m: 0.4284 - recall_m: 0.9643\n",
      "Epoch 00112: val_loss did not improve from 0.51314\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 398us/sample - loss: 0.3893 - f1_m: 0.5234 - precision_m: 0.3888 - recall_m: 0.8682 - val_loss: 0.5133 - val_f1_m: 0.5083 - val_precision_m: 0.3939 - val_recall_m: 0.7300\n",
      "Epoch 113/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3924 - f1_m: 0.6154 - precision_m: 0.4444 - recall_m: 1.0000\n",
      "Epoch 00113: val_loss did not improve from 0.51314\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 177us/sample - loss: 0.3912 - f1_m: 0.4754 - precision_m: 0.3513 - recall_m: 0.8455 - val_loss: 0.5133 - val_f1_m: 0.4873 - val_precision_m: 0.3717 - val_recall_m: 0.7422\n",
      "Epoch 114/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4595 - f1_m: 0.1667 - precision_m: 0.0909 - recall_m: 1.0000\n",
      "Epoch 00114: val_loss improved from 0.51314 to 0.51311, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 266us/sample - loss: 0.3827 - f1_m: 0.5476 - precision_m: 0.4068 - recall_m: 0.9545 - val_loss: 0.5131 - val_f1_m: 0.5279 - val_precision_m: 0.4335 - val_recall_m: 0.7348\n",
      "Epoch 115/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3731 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000\n",
      "Epoch 00115: val_loss improved from 0.51311 to 0.51309, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 208us/sample - loss: 0.3924 - f1_m: 0.5239 - precision_m: 0.3869 - recall_m: 0.8727 - val_loss: 0.5131 - val_f1_m: 0.4931 - val_precision_m: 0.3762 - val_recall_m: 0.7262\n",
      "Epoch 116/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3496 - f1_m: 0.8333 - precision_m: 0.7143 - recall_m: 1.0000\n",
      "Epoch 00116: val_loss improved from 0.51309 to 0.51300, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 250us/sample - loss: 0.3840 - f1_m: 0.5151 - precision_m: 0.3929 - recall_m: 0.8485 - val_loss: 0.5130 - val_f1_m: 0.4849 - val_precision_m: 0.3854 - val_recall_m: 0.8000\n",
      "Epoch 117/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3492 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000\n",
      "Epoch 00117: val_loss did not improve from 0.51300\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 243us/sample - loss: 0.3868 - f1_m: 0.5489 - precision_m: 0.3987 - recall_m: 0.9636 - val_loss: 0.5130 - val_f1_m: 0.5543 - val_precision_m: 0.4356 - val_recall_m: 0.7800\n",
      "Epoch 118/300\n",
      "288/330 [=========================>....] - ETA: 0s - loss: 0.3852 - f1_m: 0.5109 - precision_m: 0.3574 - recall_m: 0.9259\n",
      "Epoch 00118: val_loss did not improve from 0.51300\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 313us/sample - loss: 0.3899 - f1_m: 0.4907 - precision_m: 0.3531 - recall_m: 0.8485 - val_loss: 0.5131 - val_f1_m: 0.5400 - val_precision_m: 0.4106 - val_recall_m: 0.7933\n",
      "Epoch 119/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3108 - f1_m: 0.7500 - precision_m: 0.6000 - recall_m: 1.0000\n",
      "Epoch 00119: val_loss did not improve from 0.51300\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 271us/sample - loss: 0.3883 - f1_m: 0.4925 - precision_m: 0.3556 - recall_m: 0.8658 - val_loss: 0.5131 - val_f1_m: 0.4250 - val_precision_m: 0.3411 - val_recall_m: 0.5889\n",
      "Epoch 120/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.5158 - f1_m: 0.6154 - precision_m: 0.5000 - recall_m: 0.8000\n",
      "Epoch 00120: val_loss did not improve from 0.51300\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 270us/sample - loss: 0.3828 - f1_m: 0.5514 - precision_m: 0.4030 - recall_m: 0.9591 - val_loss: 0.5130 - val_f1_m: 0.5180 - val_precision_m: 0.3949 - val_recall_m: 0.7867\n",
      "Epoch 121/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3802 - f1_m: 0.6250 - precision_m: 0.4545 - recall_m: 1.0000\n",
      "Epoch 00121: val_loss did not improve from 0.51300\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 192us/sample - loss: 0.3872 - f1_m: 0.4806 - precision_m: 0.3476 - recall_m: 0.8810 - val_loss: 0.5130 - val_f1_m: 0.5662 - val_precision_m: 0.4571 - val_recall_m: 0.7843\n",
      "Epoch 122/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3614 - f1_m: 0.8421 - precision_m: 0.7273 - recall_m: 1.0000\n",
      "Epoch 00122: val_loss improved from 0.51300 to 0.51284, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 224us/sample - loss: 0.3912 - f1_m: 0.5005 - precision_m: 0.3772 - recall_m: 0.8727 - val_loss: 0.5128 - val_f1_m: 0.4829 - val_precision_m: 0.3862 - val_recall_m: 0.7217\n",
      "Epoch 123/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3648 - f1_m: 0.5333 - precision_m: 0.3636 - recall_m: 1.0000\n",
      "Epoch 00123: val_loss improved from 0.51284 to 0.51280, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 236us/sample - loss: 0.3892 - f1_m: 0.4929 - precision_m: 0.3545 - recall_m: 0.8682 - val_loss: 0.5128 - val_f1_m: 0.5067 - val_precision_m: 0.3879 - val_recall_m: 0.7581\n",
      "Epoch 124/300\n",
      "224/330 [===================>..........] - ETA: 0s - loss: 0.3977 - f1_m: 0.5034 - precision_m: 0.3618 - recall_m: 0.9714\n",
      "Epoch 00124: val_loss improved from 0.51280 to 0.51273, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 438us/sample - loss: 0.3922 - f1_m: 0.5491 - precision_m: 0.4207 - recall_m: 0.9364 - val_loss: 0.5127 - val_f1_m: 0.5008 - val_precision_m: 0.3939 - val_recall_m: 0.7417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4604 - f1_m: 0.1818 - precision_m: 0.1000 - recall_m: 1.0000\n",
      "Epoch 00125: val_loss improved from 0.51273 to 0.51264, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 208us/sample - loss: 0.3923 - f1_m: 0.5064 - precision_m: 0.3722 - recall_m: 0.9242 - val_loss: 0.5126 - val_f1_m: 0.4579 - val_precision_m: 0.3741 - val_recall_m: 0.7333\n",
      "Epoch 126/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3821 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000\n",
      "Epoch 00126: val_loss did not improve from 0.51264\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 189us/sample - loss: 0.3920 - f1_m: 0.5446 - precision_m: 0.4119 - recall_m: 0.9242 - val_loss: 0.5127 - val_f1_m: 0.4552 - val_precision_m: 0.3870 - val_recall_m: 0.6417\n",
      "Epoch 127/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4892 - f1_m: 0.3077 - precision_m: 0.2000 - recall_m: 0.6667\n",
      "Epoch 00127: val_loss did not improve from 0.51264\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 199us/sample - loss: 0.3866 - f1_m: 0.5036 - precision_m: 0.3794 - recall_m: 0.8636 - val_loss: 0.5128 - val_f1_m: 0.4178 - val_precision_m: 0.3227 - val_recall_m: 0.6800\n",
      "Epoch 128/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4498 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000\n",
      "Epoch 00128: val_loss did not improve from 0.51264\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 181us/sample - loss: 0.3943 - f1_m: 0.5267 - precision_m: 0.3976 - recall_m: 0.8636 - val_loss: 0.5127 - val_f1_m: 0.5152 - val_precision_m: 0.3981 - val_recall_m: 0.8095\n",
      "Epoch 129/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4021 - f1_m: 0.6667 - precision_m: 0.5000 - recall_m: 1.0000\n",
      "Epoch 00129: val_loss did not improve from 0.51264\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 203us/sample - loss: 0.3838 - f1_m: 0.5000 - precision_m: 0.3583 - recall_m: 0.8636 - val_loss: 0.5127 - val_f1_m: 0.5292 - val_precision_m: 0.4104 - val_recall_m: 0.7829\n",
      "Epoch 130/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3039 - f1_m: 0.6667 - precision_m: 0.5000 - recall_m: 1.0000\n",
      "Epoch 00130: val_loss improved from 0.51264 to 0.51264, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 258us/sample - loss: 0.3818 - f1_m: 0.5031 - precision_m: 0.3652 - recall_m: 0.8734 - val_loss: 0.5126 - val_f1_m: 0.4872 - val_precision_m: 0.3757 - val_recall_m: 0.7100\n",
      "Epoch 131/300\n",
      "192/330 [================>.............] - ETA: 0s - loss: 0.4003 - f1_m: 0.4285 - precision_m: 0.3072 - recall_m: 0.7583\n",
      "Epoch 00131: val_loss improved from 0.51264 to 0.51255, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 370us/sample - loss: 0.3881 - f1_m: 0.5050 - precision_m: 0.3668 - recall_m: 0.8682 - val_loss: 0.5125 - val_f1_m: 0.4513 - val_precision_m: 0.3753 - val_recall_m: 0.6029\n",
      "Epoch 132/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3914 - f1_m: 0.5714 - precision_m: 0.4000 - recall_m: 1.0000\n",
      "Epoch 00132: val_loss improved from 0.51255 to 0.51248, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 205us/sample - loss: 0.3855 - f1_m: 0.5113 - precision_m: 0.3642 - recall_m: 0.9591 - val_loss: 0.5125 - val_f1_m: 0.5127 - val_precision_m: 0.4000 - val_recall_m: 0.8543\n",
      "Epoch 133/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3324 - f1_m: 0.6667 - precision_m: 0.5000 - recall_m: 1.0000\n",
      "Epoch 00133: val_loss did not improve from 0.51248\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 229us/sample - loss: 0.3887 - f1_m: 0.5744 - precision_m: 0.4364 - recall_m: 0.9470 - val_loss: 0.5125 - val_f1_m: 0.5369 - val_precision_m: 0.4358 - val_recall_m: 0.7667\n",
      "Epoch 134/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3058 - f1_m: 0.8571 - precision_m: 0.7500 - recall_m: 1.0000\n",
      "Epoch 00134: val_loss did not improve from 0.51248\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 223us/sample - loss: 0.3931 - f1_m: 0.5033 - precision_m: 0.3781 - recall_m: 0.8000 - val_loss: 0.5126 - val_f1_m: 0.5184 - val_precision_m: 0.3933 - val_recall_m: 0.7862\n",
      "Epoch 135/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3302 - f1_m: 0.5455 - precision_m: 0.3750 - recall_m: 1.0000\n",
      "Epoch 00135: val_loss did not improve from 0.51248\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 197us/sample - loss: 0.3857 - f1_m: 0.5378 - precision_m: 0.3953 - recall_m: 0.9591 - val_loss: 0.5125 - val_f1_m: 0.4455 - val_precision_m: 0.3540 - val_recall_m: 0.6114\n",
      "Epoch 136/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3954 - f1_m: 0.6667 - precision_m: 0.5000 - recall_m: 1.0000\n",
      "Epoch 00136: val_loss improved from 0.51248 to 0.51247, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 250us/sample - loss: 0.3868 - f1_m: 0.5282 - precision_m: 0.3778 - recall_m: 0.9364 - val_loss: 0.5125 - val_f1_m: 0.5078 - val_precision_m: 0.3856 - val_recall_m: 0.7767\n",
      "Epoch 137/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4168 - f1_m: 0.5714 - precision_m: 0.4444 - recall_m: 0.8000\n",
      "Epoch 00137: val_loss did not improve from 0.51247\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 211us/sample - loss: 0.3945 - f1_m: 0.4999 - precision_m: 0.3633 - recall_m: 0.8682 - val_loss: 0.5125 - val_f1_m: 0.4554 - val_precision_m: 0.3486 - val_recall_m: 0.6867\n",
      "Epoch 138/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3951 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000\n",
      "Epoch 00138: val_loss improved from 0.51247 to 0.51244, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 240us/sample - loss: 0.3891 - f1_m: 0.5090 - precision_m: 0.3713 - recall_m: 0.9667 - val_loss: 0.5124 - val_f1_m: 0.5050 - val_precision_m: 0.3909 - val_recall_m: 0.7267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/300\n",
      "320/330 [============================>.] - ETA: 0s - loss: 0.3932 - f1_m: 0.5247 - precision_m: 0.3769 - recall_m: 0.9600\n",
      "Epoch 00139: val_loss improved from 0.51244 to 0.51240, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 396us/sample - loss: 0.3895 - f1_m: 0.5134 - precision_m: 0.3654 - recall_m: 0.9636 - val_loss: 0.5124 - val_f1_m: 0.4983 - val_precision_m: 0.3885 - val_recall_m: 0.8033\n",
      "Epoch 140/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4488 - f1_m: 0.5455 - precision_m: 0.4286 - recall_m: 0.7500\n",
      "Epoch 00140: val_loss improved from 0.51240 to 0.51236, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 270us/sample - loss: 0.3872 - f1_m: 0.5019 - precision_m: 0.3748 - recall_m: 0.8682 - val_loss: 0.5124 - val_f1_m: 0.4990 - val_precision_m: 0.3800 - val_recall_m: 0.7533\n",
      "Epoch 141/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3442 - f1_m: 0.5455 - precision_m: 0.3750 - recall_m: 1.0000\n",
      "Epoch 00141: val_loss improved from 0.51236 to 0.51228, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 313us/sample - loss: 0.3855 - f1_m: 0.5333 - precision_m: 0.3855 - recall_m: 0.9416 - val_loss: 0.5123 - val_f1_m: 0.4982 - val_precision_m: 0.4118 - val_recall_m: 0.7181\n",
      "Epoch 142/300\n",
      "320/330 [============================>.] - ETA: 0s - loss: 0.3759 - f1_m: 0.5456 - precision_m: 0.3920 - recall_m: 0.9667\n",
      "Epoch 00142: val_loss did not improve from 0.51228\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 303us/sample - loss: 0.3826 - f1_m: 0.5220 - precision_m: 0.3715 - recall_m: 0.9697 - val_loss: 0.5123 - val_f1_m: 0.4986 - val_precision_m: 0.3823 - val_recall_m: 0.7583\n",
      "Epoch 143/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3930 - f1_m: 0.5455 - precision_m: 0.4286 - recall_m: 0.7500\n",
      "Epoch 00143: val_loss did not improve from 0.51228\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 261us/sample - loss: 0.3904 - f1_m: 0.4610 - precision_m: 0.3377 - recall_m: 0.7727 - val_loss: 0.5124 - val_f1_m: 0.5473 - val_precision_m: 0.4409 - val_recall_m: 0.7750\n",
      "Epoch 144/300\n",
      "320/330 [============================>.] - ETA: 0s - loss: 0.3865 - f1_m: 0.5240 - precision_m: 0.3823 - recall_m: 0.8550\n",
      "Epoch 00144: val_loss improved from 0.51228 to 0.51224, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 378us/sample - loss: 0.3866 - f1_m: 0.4764 - precision_m: 0.3475 - recall_m: 0.7773 - val_loss: 0.5122 - val_f1_m: 0.5088 - val_precision_m: 0.4228 - val_recall_m: 0.7200\n",
      "Epoch 145/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4118 - f1_m: 0.5714 - precision_m: 0.4000 - recall_m: 1.0000\n",
      "Epoch 00145: val_loss improved from 0.51224 to 0.51210, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 289us/sample - loss: 0.3877 - f1_m: 0.4871 - precision_m: 0.3545 - recall_m: 0.8734 - val_loss: 0.5121 - val_f1_m: 0.5264 - val_precision_m: 0.4160 - val_recall_m: 0.7667\n",
      "Epoch 146/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3566 - f1_m: 0.7273 - precision_m: 0.6667 - recall_m: 0.8000\n",
      "Epoch 00146: val_loss improved from 0.51210 to 0.51203, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 302us/sample - loss: 0.3919 - f1_m: 0.4981 - precision_m: 0.3727 - recall_m: 0.8758 - val_loss: 0.5120 - val_f1_m: 0.5240 - val_precision_m: 0.4198 - val_recall_m: 0.7333\n",
      "Epoch 147/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4549 - f1_m: 0.3529 - precision_m: 0.2143 - recall_m: 1.0000\n",
      "Epoch 00147: val_loss improved from 0.51203 to 0.51196, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 283us/sample - loss: 0.3888 - f1_m: 0.5304 - precision_m: 0.3841 - recall_m: 0.9394 - val_loss: 0.5120 - val_f1_m: 0.4498 - val_precision_m: 0.3461 - val_recall_m: 0.7472\n",
      "Epoch 148/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3304 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000\n",
      "Epoch 00148: val_loss improved from 0.51196 to 0.51191, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 294us/sample - loss: 0.3859 - f1_m: 0.5326 - precision_m: 0.3848 - recall_m: 0.9591 - val_loss: 0.5119 - val_f1_m: 0.5149 - val_precision_m: 0.4083 - val_recall_m: 0.7200\n",
      "Epoch 149/300\n",
      "256/330 [======================>.......] - ETA: 0s - loss: 0.3824 - f1_m: 0.5240 - precision_m: 0.3922 - recall_m: 0.8542\n",
      "Epoch 00149: val_loss improved from 0.51191 to 0.51185, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 331us/sample - loss: 0.3935 - f1_m: 0.4417 - precision_m: 0.3307 - recall_m: 0.7121 - val_loss: 0.5118 - val_f1_m: 0.4964 - val_precision_m: 0.3838 - val_recall_m: 0.7200\n",
      "Epoch 150/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4485 - f1_m: 0.2500 - precision_m: 0.1429 - recall_m: 1.0000\n",
      "Epoch 00150: val_loss improved from 0.51185 to 0.51171, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 305us/sample - loss: 0.3937 - f1_m: 0.5044 - precision_m: 0.3602 - recall_m: 0.9242 - val_loss: 0.5117 - val_f1_m: 0.4619 - val_precision_m: 0.3532 - val_recall_m: 0.7611\n",
      "Epoch 151/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.5253 - f1_m: 0.3077 - precision_m: 0.1818 - recall_m: 1.0000\n",
      "Epoch 00151: val_loss improved from 0.51171 to 0.51170, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 277us/sample - loss: 0.3992 - f1_m: 0.5390 - precision_m: 0.4093 - recall_m: 0.9318 - val_loss: 0.5117 - val_f1_m: 0.4999 - val_precision_m: 0.3952 - val_recall_m: 0.7700\n",
      "Epoch 152/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4038 - f1_m: 0.3077 - precision_m: 0.2000 - recall_m: 0.6667\n",
      "Epoch 00152: val_loss did not improve from 0.51170\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 255us/sample - loss: 0.3941 - f1_m: 0.5331 - precision_m: 0.4000 - recall_m: 0.9242 - val_loss: 0.5117 - val_f1_m: 0.6026 - val_precision_m: 0.5090 - val_recall_m: 0.7848\n",
      "Epoch 153/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3321 - f1_m: 0.7273 - precision_m: 0.5714 - recall_m: 1.0000\n",
      "Epoch 00153: val_loss did not improve from 0.51170\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 297us/sample - loss: 0.3860 - f1_m: 0.5622 - precision_m: 0.4186 - recall_m: 0.9591 - val_loss: 0.5117 - val_f1_m: 0.4086 - val_precision_m: 0.3069 - val_recall_m: 0.6400\n",
      "Epoch 154/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4139 - f1_m: 0.6667 - precision_m: 0.5000 - recall_m: 1.0000\n",
      "Epoch 00154: val_loss did not improve from 0.51170\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 218us/sample - loss: 0.3952 - f1_m: 0.4590 - precision_m: 0.3360 - recall_m: 0.8409 - val_loss: 0.5118 - val_f1_m: 0.5003 - val_precision_m: 0.3818 - val_recall_m: 0.7667\n",
      "Epoch 155/300\n",
      "192/330 [================>.............] - ETA: 0s - loss: 0.3766 - f1_m: 0.5046 - precision_m: 0.3731 - recall_m: 0.8056\n",
      "Epoch 00155: val_loss did not improve from 0.51170\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 351us/sample - loss: 0.3857 - f1_m: 0.5032 - precision_m: 0.3651 - recall_m: 0.8636 - val_loss: 0.5117 - val_f1_m: 0.5264 - val_precision_m: 0.3951 - val_recall_m: 0.8076\n",
      "Epoch 156/300\n",
      "224/330 [===================>..........] - ETA: 0s - loss: 0.3783 - f1_m: 0.5793 - precision_m: 0.4275 - recall_m: 0.9429\n",
      "Epoch 00156: val_loss did not improve from 0.51170\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 314us/sample - loss: 0.3887 - f1_m: 0.5196 - precision_m: 0.3831 - recall_m: 0.8727 - val_loss: 0.5118 - val_f1_m: 0.4970 - val_precision_m: 0.3778 - val_recall_m: 0.7314\n",
      "Epoch 157/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3505 - f1_m: 0.7143 - precision_m: 0.5556 - recall_m: 1.0000\n",
      "Epoch 00157: val_loss improved from 0.51170 to 0.51168, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 276us/sample - loss: 0.3906 - f1_m: 0.5064 - precision_m: 0.3622 - recall_m: 0.8864 - val_loss: 0.5117 - val_f1_m: 0.5390 - val_precision_m: 0.4312 - val_recall_m: 0.7833\n",
      "Epoch 158/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3236 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000\n",
      "Epoch 00158: val_loss improved from 0.51168 to 0.51164, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 269us/sample - loss: 0.3861 - f1_m: 0.4651 - precision_m: 0.3357 - recall_m: 0.8682 - val_loss: 0.5116 - val_f1_m: 0.5228 - val_precision_m: 0.4203 - val_recall_m: 0.7367\n",
      "Epoch 159/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4816 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000\n",
      "Epoch 00159: val_loss improved from 0.51164 to 0.51153, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 272us/sample - loss: 0.3890 - f1_m: 0.4902 - precision_m: 0.3767 - recall_m: 0.7727 - val_loss: 0.5115 - val_f1_m: 0.5132 - val_precision_m: 0.3957 - val_recall_m: 0.7500\n",
      "Epoch 160/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4268 - f1_m: 0.8000 - precision_m: 0.6667 - recall_m: 1.0000\n",
      "Epoch 00160: val_loss improved from 0.51153 to 0.51142, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 276us/sample - loss: 0.3958 - f1_m: 0.5165 - precision_m: 0.3808 - recall_m: 0.8864 - val_loss: 0.5114 - val_f1_m: 0.5371 - val_precision_m: 0.4167 - val_recall_m: 0.7733\n",
      "Epoch 161/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4284 - f1_m: 0.6667 - precision_m: 0.5000 - recall_m: 1.0000\n",
      "Epoch 00161: val_loss improved from 0.51142 to 0.51142, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 304us/sample - loss: 0.3894 - f1_m: 0.5579 - precision_m: 0.4018 - recall_m: 0.9515 - val_loss: 0.5114 - val_f1_m: 0.5127 - val_precision_m: 0.4133 - val_recall_m: 0.8167\n",
      "Epoch 162/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3692 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000\n",
      "Epoch 00162: val_loss did not improve from 0.51142\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 229us/sample - loss: 0.3831 - f1_m: 0.5328 - precision_m: 0.3868 - recall_m: 0.9364 - val_loss: 0.5114 - val_f1_m: 0.4920 - val_precision_m: 0.3873 - val_recall_m: 0.7556\n",
      "Epoch 163/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3755 - f1_m: 0.5714 - precision_m: 0.4000 - recall_m: 1.0000\n",
      "Epoch 00163: val_loss did not improve from 0.51142\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 240us/sample - loss: 0.3926 - f1_m: 0.5624 - precision_m: 0.4374 - recall_m: 0.9545 - val_loss: 0.5115 - val_f1_m: 0.5179 - val_precision_m: 0.4298 - val_recall_m: 0.7514\n",
      "Epoch 164/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.5182 - f1_m: 0.4615 - precision_m: 0.3333 - recall_m: 0.7500\n",
      "Epoch 00164: val_loss did not improve from 0.51142\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 267us/sample - loss: 0.3850 - f1_m: 0.5022 - precision_m: 0.3606 - recall_m: 0.8561 - val_loss: 0.5116 - val_f1_m: 0.4855 - val_precision_m: 0.3722 - val_recall_m: 0.7083\n",
      "Epoch 165/300\n",
      "288/330 [=========================>....] - ETA: 0s - loss: 0.3866 - f1_m: 0.5483 - precision_m: 0.4110 - recall_m: 0.9259\n",
      "Epoch 00165: val_loss did not improve from 0.51142\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 386us/sample - loss: 0.3916 - f1_m: 0.5319 - precision_m: 0.3947 - recall_m: 0.9394 - val_loss: 0.5116 - val_f1_m: 0.4229 - val_precision_m: 0.3387 - val_recall_m: 0.5929\n",
      "Epoch 166/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3093 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00166: val_loss did not improve from 0.51142\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 207us/sample - loss: 0.3906 - f1_m: 0.4998 - precision_m: 0.3793 - recall_m: 0.8561 - val_loss: 0.5116 - val_f1_m: 0.4324 - val_precision_m: 0.3496 - val_recall_m: 0.5803\n",
      "Epoch 167/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3408 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000\n",
      "Epoch 00167: val_loss did not improve from 0.51142\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 262us/sample - loss: 0.3818 - f1_m: 0.5515 - precision_m: 0.4026 - recall_m: 0.9667 - val_loss: 0.5115 - val_f1_m: 0.4941 - val_precision_m: 0.3817 - val_recall_m: 0.7048\n",
      "Epoch 168/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3518 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00\n",
      "Epoch 00168: val_loss did not improve from 0.51142\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 189us/sample - loss: 0.3871 - f1_m: 0.4573 - precision_m: 0.3336 - recall_m: 0.7500 - val_loss: 0.5115 - val_f1_m: 0.5075 - val_precision_m: 0.3845 - val_recall_m: 0.8345\n",
      "Epoch 169/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4301 - f1_m: 0.4615 - precision_m: 0.3333 - recall_m: 0.7500\n",
      "Epoch 00169: val_loss did not improve from 0.51142\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 265us/sample - loss: 0.3916 - f1_m: 0.5429 - precision_m: 0.3933 - recall_m: 0.9621 - val_loss: 0.5115 - val_f1_m: 0.5173 - val_precision_m: 0.4038 - val_recall_m: 0.7695\n",
      "Epoch 170/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4374 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000\n",
      "Epoch 00170: val_loss did not improve from 0.51142\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 257us/sample - loss: 0.3923 - f1_m: 0.5214 - precision_m: 0.3811 - recall_m: 0.9242 - val_loss: 0.5115 - val_f1_m: 0.5339 - val_precision_m: 0.4135 - val_recall_m: 0.8000\n",
      "Epoch 171/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4350 - f1_m: 0.6000 - precision_m: 0.5000 - recall_m: 0.7500\n",
      "Epoch 00171: val_loss did not improve from 0.51142\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 227us/sample - loss: 0.3859 - f1_m: 0.5320 - precision_m: 0.3916 - recall_m: 0.8682 - val_loss: 0.5115 - val_f1_m: 0.5472 - val_precision_m: 0.4323 - val_recall_m: 0.7900\n",
      "Epoch 172/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4099 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000\n",
      "Epoch 00172: val_loss did not improve from 0.51142\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 218us/sample - loss: 0.3839 - f1_m: 0.5637 - precision_m: 0.4348 - recall_m: 0.9636 - val_loss: 0.5116 - val_f1_m: 0.5364 - val_precision_m: 0.4256 - val_recall_m: 0.7795\n",
      "Epoch 173/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3422 - f1_m: 0.5714 - precision_m: 0.5000 - recall_m: 0.6667\n",
      "Epoch 00173: val_loss did not improve from 0.51142\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 274us/sample - loss: 0.3907 - f1_m: 0.5001 - precision_m: 0.3696 - recall_m: 0.8636 - val_loss: 0.5117 - val_f1_m: 0.5411 - val_precision_m: 0.4240 - val_recall_m: 0.7762\n",
      "Epoch 174/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4472 - f1_m: 0.6000 - precision_m: 0.4286 - recall_m: 1.0000\n",
      "Epoch 00174: val_loss did not improve from 0.51142\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 240us/sample - loss: 0.3944 - f1_m: 0.5487 - precision_m: 0.3992 - recall_m: 0.9545 - val_loss: 0.5117 - val_f1_m: 0.5340 - val_precision_m: 0.4317 - val_recall_m: 0.7467\n",
      "Epoch 175/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3888 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000\n",
      "Epoch 00175: val_loss did not improve from 0.51142\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 243us/sample - loss: 0.3984 - f1_m: 0.4806 - precision_m: 0.3604 - recall_m: 0.7879 - val_loss: 0.5116 - val_f1_m: 0.4194 - val_precision_m: 0.3139 - val_recall_m: 0.6381\n",
      "Epoch 176/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4490 - f1_m: 0.4615 - precision_m: 0.3000 - recall_m: 1.0000\n",
      "Epoch 00176: val_loss did not improve from 0.51142\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 236us/sample - loss: 0.3911 - f1_m: 0.5293 - precision_m: 0.3847 - recall_m: 0.9432 - val_loss: 0.5116 - val_f1_m: 0.5659 - val_precision_m: 0.4522 - val_recall_m: 0.7829\n",
      "Epoch 177/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3733 - f1_m: 0.6957 - precision_m: 0.5714 - recall_m: 0.8889\n",
      "Epoch 00177: val_loss did not improve from 0.51142\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 234us/sample - loss: 0.3895 - f1_m: 0.4394 - precision_m: 0.3262 - recall_m: 0.7778 - val_loss: 0.5116 - val_f1_m: 0.4724 - val_precision_m: 0.3675 - val_recall_m: 0.7244\n",
      "Epoch 178/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4195 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00\n",
      "Epoch 00178: val_loss improved from 0.51142 to 0.51138, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 321us/sample - loss: 0.3928 - f1_m: 0.5122 - precision_m: 0.3818 - recall_m: 0.8506 - val_loss: 0.5114 - val_f1_m: 0.4790 - val_precision_m: 0.3742 - val_recall_m: 0.7595\n",
      "Epoch 179/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.2789 - f1_m: 0.8421 - precision_m: 0.7273 - recall_m: 1.0000\n",
      "Epoch 00179: val_loss did not improve from 0.51138\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 182us/sample - loss: 0.4002 - f1_m: 0.5602 - precision_m: 0.4471 - recall_m: 0.9470 - val_loss: 0.5114 - val_f1_m: 0.5020 - val_precision_m: 0.3811 - val_recall_m: 0.7600\n",
      "Epoch 180/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3571 - f1_m: 0.6667 - precision_m: 0.5000 - recall_m: 1.0000\n",
      "Epoch 00180: val_loss improved from 0.51138 to 0.51138, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 287us/sample - loss: 0.3875 - f1_m: 0.5340 - precision_m: 0.3900 - recall_m: 0.9667 - val_loss: 0.5114 - val_f1_m: 0.5425 - val_precision_m: 0.4552 - val_recall_m: 0.7314\n",
      "Epoch 181/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.5047 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00\n",
      "Epoch 00181: val_loss improved from 0.51138 to 0.51134, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 264us/sample - loss: 0.4070 - f1_m: 0.4998 - precision_m: 0.3880 - recall_m: 0.8864 - val_loss: 0.5113 - val_f1_m: 0.4965 - val_precision_m: 0.3811 - val_recall_m: 0.7362\n",
      "Epoch 182/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3281 - f1_m: 0.8750 - precision_m: 0.7778 - recall_m: 1.0000\n",
      "Epoch 00182: val_loss improved from 0.51134 to 0.51134, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 249us/sample - loss: 0.3901 - f1_m: 0.5295 - precision_m: 0.4021 - recall_m: 0.9091 - val_loss: 0.5113 - val_f1_m: 0.5451 - val_precision_m: 0.4231 - val_recall_m: 0.7917\n",
      "Epoch 183/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.2677 - f1_m: 0.5714 - precision_m: 0.4000 - recall_m: 1.0000\n",
      "Epoch 00183: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 163us/sample - loss: 0.3824 - f1_m: 0.5422 - precision_m: 0.3856 - recall_m: 0.9591 - val_loss: 0.5114 - val_f1_m: 0.4288 - val_precision_m: 0.3318 - val_recall_m: 0.6262\n",
      "Epoch 184/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3995 - f1_m: 0.6667 - precision_m: 0.5556 - recall_m: 0.8333\n",
      "Epoch 00184: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 148us/sample - loss: 0.3905 - f1_m: 0.5517 - precision_m: 0.4175 - recall_m: 0.8712 - val_loss: 0.5115 - val_f1_m: 0.5543 - val_precision_m: 0.4423 - val_recall_m: 0.8048\n",
      "Epoch 185/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.5046 - f1_m: 0.5000 - precision_m: 0.3636 - recall_m: 0.8000\n",
      "Epoch 00185: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 223us/sample - loss: 0.3905 - f1_m: 0.5283 - precision_m: 0.3767 - recall_m: 0.9515 - val_loss: 0.5116 - val_f1_m: 0.4125 - val_precision_m: 0.3310 - val_recall_m: 0.6083\n",
      "Epoch 186/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3550 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000\n",
      "Epoch 00186: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 303us/sample - loss: 0.3871 - f1_m: 0.5587 - precision_m: 0.4051 - recall_m: 0.9667 - val_loss: 0.5116 - val_f1_m: 0.4657 - val_precision_m: 0.3712 - val_recall_m: 0.6770\n",
      "Epoch 187/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3838 - f1_m: 0.4706 - precision_m: 0.3077 - recall_m: 1.0000\n",
      "Epoch 00187: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 194us/sample - loss: 0.3838 - f1_m: 0.5179 - precision_m: 0.3848 - recall_m: 0.8758 - val_loss: 0.5116 - val_f1_m: 0.5319 - val_precision_m: 0.4178 - val_recall_m: 0.7433\n",
      "Epoch 188/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4391 - f1_m: 0.5333 - precision_m: 0.3636 - recall_m: 1.0000\n",
      "Epoch 00188: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 232us/sample - loss: 0.3898 - f1_m: 0.4811 - precision_m: 0.3504 - recall_m: 0.8758 - val_loss: 0.5116 - val_f1_m: 0.5198 - val_precision_m: 0.3994 - val_recall_m: 0.7833\n",
      "Epoch 189/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3325 - f1_m: 0.5333 - precision_m: 0.3636 - recall_m: 1.0000\n",
      "Epoch 00189: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 252us/sample - loss: 0.3907 - f1_m: 0.5223 - precision_m: 0.3846 - recall_m: 0.8750 - val_loss: 0.5114 - val_f1_m: 0.5014 - val_precision_m: 0.4012 - val_recall_m: 0.7133\n",
      "Epoch 190/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3680 - f1_m: 0.4444 - precision_m: 0.2857 - recall_m: 1.0000\n",
      "Epoch 00190: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 271us/sample - loss: 0.3936 - f1_m: 0.4963 - precision_m: 0.3626 - recall_m: 0.8658 - val_loss: 0.5114 - val_f1_m: 0.5287 - val_precision_m: 0.4282 - val_recall_m: 0.7829\n",
      "Epoch 191/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3384 - f1_m: 0.7143 - precision_m: 0.5556 - recall_m: 1.0000\n",
      "Epoch 00191: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 294us/sample - loss: 0.3919 - f1_m: 0.5187 - precision_m: 0.3721 - recall_m: 0.9364 - val_loss: 0.5114 - val_f1_m: 0.4825 - val_precision_m: 0.3703 - val_recall_m: 0.7667\n",
      "Epoch 192/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3858 - f1_m: 0.5714 - precision_m: 0.4286 - recall_m: 0.8571\n",
      "Epoch 00192: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 222us/sample - loss: 0.3854 - f1_m: 0.5596 - precision_m: 0.4038 - recall_m: 0.9688 - val_loss: 0.5114 - val_f1_m: 0.4865 - val_precision_m: 0.3727 - val_recall_m: 0.7114\n",
      "Epoch 193/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3629 - f1_m: 0.6667 - precision_m: 0.5000 - recall_m: 1.0000\n",
      "Epoch 00193: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 252us/sample - loss: 0.3900 - f1_m: 0.5477 - precision_m: 0.4100 - recall_m: 0.8712 - val_loss: 0.5114 - val_f1_m: 0.5269 - val_precision_m: 0.4195 - val_recall_m: 0.7467\n",
      "Epoch 194/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3417 - f1_m: 0.4444 - precision_m: 0.2857 - recall_m: 1.0000\n",
      "Epoch 00194: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330/330 [==============================] - 0s 211us/sample - loss: 0.3878 - f1_m: 0.5411 - precision_m: 0.3976 - recall_m: 0.9621 - val_loss: 0.5114 - val_f1_m: 0.5542 - val_precision_m: 0.4330 - val_recall_m: 0.7790\n",
      "Epoch 195/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3853 - f1_m: 0.7143 - precision_m: 0.5556 - recall_m: 1.0000\n",
      "Epoch 00195: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 242us/sample - loss: 0.3836 - f1_m: 0.5264 - precision_m: 0.3756 - recall_m: 0.9364 - val_loss: 0.5115 - val_f1_m: 0.4185 - val_precision_m: 0.3243 - val_recall_m: 0.6044\n",
      "Epoch 196/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4686 - f1_m: 0.5455 - precision_m: 0.3750 - recall_m: 1.0000\n",
      "Epoch 00196: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 227us/sample - loss: 0.3929 - f1_m: 0.5383 - precision_m: 0.3932 - recall_m: 0.9515 - val_loss: 0.5115 - val_f1_m: 0.4740 - val_precision_m: 0.3722 - val_recall_m: 0.7083\n",
      "Epoch 197/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3648 - f1_m: 0.4286 - precision_m: 0.3000 - recall_m: 0.7500\n",
      "Epoch 00197: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 235us/sample - loss: 0.3865 - f1_m: 0.5695 - precision_m: 0.4175 - recall_m: 0.9545 - val_loss: 0.5116 - val_f1_m: 0.5458 - val_precision_m: 0.4424 - val_recall_m: 0.7700\n",
      "Epoch 198/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4339 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000\n",
      "Epoch 00198: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 235us/sample - loss: 0.3977 - f1_m: 0.4947 - precision_m: 0.3632 - recall_m: 0.8636 - val_loss: 0.5116 - val_f1_m: 0.4800 - val_precision_m: 0.3711 - val_recall_m: 0.7089\n",
      "Epoch 199/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3842 - f1_m: 0.7368 - precision_m: 0.6364 - recall_m: 0.8750\n",
      "Epoch 00199: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 238us/sample - loss: 0.3817 - f1_m: 0.5318 - precision_m: 0.3850 - recall_m: 0.9705 - val_loss: 0.5116 - val_f1_m: 0.4828 - val_precision_m: 0.4176 - val_recall_m: 0.6750\n",
      "Epoch 200/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3229 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000\n",
      "Epoch 00200: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 226us/sample - loss: 0.3833 - f1_m: 0.5529 - precision_m: 0.3990 - recall_m: 0.9591 - val_loss: 0.5115 - val_f1_m: 0.5060 - val_precision_m: 0.4163 - val_recall_m: 0.6933\n",
      "Epoch 201/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3617 - f1_m: 0.7143 - precision_m: 0.5556 - recall_m: 1.0000\n",
      "Epoch 00201: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 242us/sample - loss: 0.3889 - f1_m: 0.5497 - precision_m: 0.4005 - recall_m: 0.9545 - val_loss: 0.5116 - val_f1_m: 0.4638 - val_precision_m: 0.3762 - val_recall_m: 0.7800\n",
      "Epoch 202/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3373 - f1_m: 0.6250 - precision_m: 0.4545 - recall_m: 1.0000\n",
      "Epoch 00202: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 220us/sample - loss: 0.3893 - f1_m: 0.5320 - precision_m: 0.3893 - recall_m: 0.9318 - val_loss: 0.5117 - val_f1_m: 0.5367 - val_precision_m: 0.4714 - val_recall_m: 0.8083\n",
      "Epoch 203/300\n",
      "320/330 [============================>.] - ETA: 0s - loss: 0.3889 - f1_m: 0.5505 - precision_m: 0.4051 - recall_m: 0.9250\n",
      "Epoch 00203: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 288us/sample - loss: 0.3940 - f1_m: 0.5004 - precision_m: 0.3683 - recall_m: 0.8409 - val_loss: 0.5117 - val_f1_m: 0.5686 - val_precision_m: 0.4867 - val_recall_m: 0.7433\n",
      "Epoch 204/300\n",
      "320/330 [============================>.] - ETA: 0s - loss: 0.3898 - f1_m: 0.5139 - precision_m: 0.3785 - recall_m: 0.8524\n",
      "Epoch 00204: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 321us/sample - loss: 0.3905 - f1_m: 0.5278 - precision_m: 0.3895 - recall_m: 0.8658 - val_loss: 0.5117 - val_f1_m: 0.4861 - val_precision_m: 0.3792 - val_recall_m: 0.7429\n",
      "Epoch 205/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3367 - f1_m: 0.6000 - precision_m: 0.4286 - recall_m: 1.0000\n",
      "Epoch 00205: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 278us/sample - loss: 0.3938 - f1_m: 0.5295 - precision_m: 0.3805 - recall_m: 0.9318 - val_loss: 0.5116 - val_f1_m: 0.4987 - val_precision_m: 0.3811 - val_recall_m: 0.8143\n",
      "Epoch 206/300\n",
      "160/330 [=============>................] - ETA: 0s - loss: 0.4009 - f1_m: 0.5156 - precision_m: 0.3694 - recall_m: 0.8833\n",
      "Epoch 00206: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 458us/sample - loss: 0.3874 - f1_m: 0.5098 - precision_m: 0.3685 - recall_m: 0.8561 - val_loss: 0.5117 - val_f1_m: 0.5186 - val_precision_m: 0.3903 - val_recall_m: 0.8050\n",
      "Epoch 207/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.2973 - f1_m: 0.6667 - precision_m: 0.5000 - recall_m: 1.0000\n",
      "Epoch 00207: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 232us/sample - loss: 0.3856 - f1_m: 0.5041 - precision_m: 0.3667 - recall_m: 0.8961 - val_loss: 0.5117 - val_f1_m: 0.5236 - val_precision_m: 0.4004 - val_recall_m: 0.7750\n",
      "Epoch 208/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3599 - f1_m: 0.7692 - precision_m: 0.6250 - recall_m: 1.0000\n",
      "Epoch 00208: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 219us/sample - loss: 0.3910 - f1_m: 0.5602 - precision_m: 0.4242 - recall_m: 0.9318 - val_loss: 0.5117 - val_f1_m: 0.5323 - val_precision_m: 0.4074 - val_recall_m: 0.7867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 209/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4056 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00\n",
      "Epoch 00209: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 200us/sample - loss: 0.3896 - f1_m: 0.5166 - precision_m: 0.3983 - recall_m: 0.7773 - val_loss: 0.5117 - val_f1_m: 0.5238 - val_precision_m: 0.4060 - val_recall_m: 0.7600\n",
      "Epoch 210/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3399 - f1_m: 0.4615 - precision_m: 0.3000 - recall_m: 1.0000\n",
      "Epoch 00210: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "\n",
      "Epoch 00210: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "330/330 [==============================] - 0s 223us/sample - loss: 0.3844 - f1_m: 0.5307 - precision_m: 0.3929 - recall_m: 0.9545 - val_loss: 0.5117 - val_f1_m: 0.5106 - val_precision_m: 0.3973 - val_recall_m: 0.8095\n",
      "Epoch 211/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4396 - f1_m: 0.3750 - precision_m: 0.2308 - recall_m: 1.0000\n",
      "Epoch 00211: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 205us/sample - loss: 0.3898 - f1_m: 0.5136 - precision_m: 0.3739 - recall_m: 0.8758 - val_loss: 0.5117 - val_f1_m: 0.5328 - val_precision_m: 0.4384 - val_recall_m: 0.7600\n",
      "Epoch 212/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4606 - f1_m: 0.4286 - precision_m: 0.2727 - recall_m: 1.0000\n",
      "Epoch 00212: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 214us/sample - loss: 0.3900 - f1_m: 0.5506 - precision_m: 0.4023 - recall_m: 0.9364 - val_loss: 0.5117 - val_f1_m: 0.5006 - val_precision_m: 0.3983 - val_recall_m: 0.7929\n",
      "Epoch 213/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3343 - f1_m: 0.6000 - precision_m: 0.4286 - recall_m: 1.0000\n",
      "Epoch 00213: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 238us/sample - loss: 0.3841 - f1_m: 0.5585 - precision_m: 0.4142 - recall_m: 0.9667 - val_loss: 0.5117 - val_f1_m: 0.5077 - val_precision_m: 0.4107 - val_recall_m: 0.7429\n",
      "Epoch 214/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3949 - f1_m: 0.4286 - precision_m: 0.2727 - recall_m: 1.0000\n",
      "Epoch 00214: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 220us/sample - loss: 0.3966 - f1_m: 0.4992 - precision_m: 0.3728 - recall_m: 0.8485 - val_loss: 0.5117 - val_f1_m: 0.4988 - val_precision_m: 0.4420 - val_recall_m: 0.7700\n",
      "Epoch 215/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3534 - f1_m: 0.2222 - precision_m: 0.1250 - recall_m: 1.0000\n",
      "Epoch 00215: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 237us/sample - loss: 0.3895 - f1_m: 0.4445 - precision_m: 0.3298 - recall_m: 0.7727 - val_loss: 0.5117 - val_f1_m: 0.4497 - val_precision_m: 0.3605 - val_recall_m: 0.6171\n",
      "Epoch 216/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3822 - f1_m: 0.6667 - precision_m: 0.5000 - recall_m: 1.0000\n",
      "Epoch 00216: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 244us/sample - loss: 0.3864 - f1_m: 0.5126 - precision_m: 0.3876 - recall_m: 0.8606 - val_loss: 0.5116 - val_f1_m: 0.4411 - val_precision_m: 0.3434 - val_recall_m: 0.6333\n",
      "Epoch 217/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.2738 - f1_m: 0.6667 - precision_m: 0.5000 - recall_m: 1.0000\n",
      "Epoch 00217: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 251us/sample - loss: 0.3841 - f1_m: 0.5197 - precision_m: 0.3689 - recall_m: 0.9667 - val_loss: 0.5116 - val_f1_m: 0.5280 - val_precision_m: 0.4295 - val_recall_m: 0.7350\n",
      "Epoch 218/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3520 - f1_m: 0.6000 - precision_m: 0.4286 - recall_m: 1.0000\n",
      "Epoch 00218: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 241us/sample - loss: 0.3875 - f1_m: 0.5471 - precision_m: 0.3910 - recall_m: 0.9470 - val_loss: 0.5116 - val_f1_m: 0.4525 - val_precision_m: 0.3576 - val_recall_m: 0.6600\n",
      "Epoch 219/300\n",
      "256/330 [======================>.......] - ETA: 0s - loss: 0.3901 - f1_m: 0.5392 - precision_m: 0.3924 - recall_m: 0.9792\n",
      "Epoch 00219: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 299us/sample - loss: 0.3915 - f1_m: 0.5056 - precision_m: 0.3722 - recall_m: 0.8758 - val_loss: 0.5116 - val_f1_m: 0.4613 - val_precision_m: 0.3681 - val_recall_m: 0.7306\n",
      "Epoch 220/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3023 - f1_m: 0.8000 - precision_m: 0.6667 - recall_m: 1.0000\n",
      "Epoch 00220: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 202us/sample - loss: 0.3863 - f1_m: 0.5462 - precision_m: 0.3932 - recall_m: 0.9545 - val_loss: 0.5115 - val_f1_m: 0.5258 - val_precision_m: 0.4150 - val_recall_m: 0.7333\n",
      "Epoch 221/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4091 - f1_m: 0.3636 - precision_m: 0.2222 - recall_m: 1.0000\n",
      "Epoch 00221: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 237us/sample - loss: 0.3954 - f1_m: 0.5304 - precision_m: 0.4220 - recall_m: 0.8909 - val_loss: 0.5115 - val_f1_m: 0.4844 - val_precision_m: 0.3722 - val_recall_m: 0.7000\n",
      "Epoch 222/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4507 - f1_m: 0.2222 - precision_m: 0.1250 - recall_m: 1.0000\n",
      "Epoch 00222: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 225us/sample - loss: 0.3874 - f1_m: 0.4463 - precision_m: 0.3284 - recall_m: 0.7818 - val_loss: 0.5115 - val_f1_m: 0.5139 - val_precision_m: 0.3992 - val_recall_m: 0.7679\n",
      "Epoch 223/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3698 - f1_m: 0.4444 - precision_m: 0.2857 - recall_m: 1.0000\n",
      "Epoch 00223: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 239us/sample - loss: 0.3849 - f1_m: 0.5013 - precision_m: 0.3613 - recall_m: 0.8674 - val_loss: 0.5115 - val_f1_m: 0.5434 - val_precision_m: 0.4230 - val_recall_m: 0.8000\n",
      "Epoch 224/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.2929 - f1_m: 0.6000 - precision_m: 0.4286 - recall_m: 1.0000\n",
      "Epoch 00224: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 193us/sample - loss: 0.3871 - f1_m: 0.4855 - precision_m: 0.3575 - recall_m: 0.8506 - val_loss: 0.5115 - val_f1_m: 0.5210 - val_precision_m: 0.4073 - val_recall_m: 0.7348\n",
      "Epoch 225/300\n",
      "320/330 [============================>.] - ETA: 0s - loss: 0.3868 - f1_m: 0.5568 - precision_m: 0.4177 - recall_m: 0.9417\n",
      "Epoch 00225: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 271us/sample - loss: 0.3877 - f1_m: 0.5062 - precision_m: 0.3797 - recall_m: 0.8561 - val_loss: 0.5115 - val_f1_m: 0.5445 - val_precision_m: 0.4312 - val_recall_m: 0.8155\n",
      "Epoch 226/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.5677 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00\n",
      "Epoch 00226: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 251us/sample - loss: 0.3930 - f1_m: 0.5542 - precision_m: 0.4076 - recall_m: 0.8864 - val_loss: 0.5114 - val_f1_m: 0.5292 - val_precision_m: 0.4219 - val_recall_m: 0.7762\n",
      "Epoch 227/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3551 - f1_m: 0.5714 - precision_m: 0.4000 - recall_m: 1.0000\n",
      "Epoch 00227: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 243us/sample - loss: 0.3820 - f1_m: 0.5153 - precision_m: 0.3796 - recall_m: 0.8455 - val_loss: 0.5114 - val_f1_m: 0.4869 - val_precision_m: 0.4293 - val_recall_m: 0.7512\n",
      "Epoch 228/300\n",
      "224/330 [===================>..........] - ETA: 0s - loss: 0.3941 - f1_m: 0.5311 - precision_m: 0.3933 - recall_m: 0.9405\n",
      "Epoch 00228: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 307us/sample - loss: 0.3896 - f1_m: 0.5349 - precision_m: 0.3872 - recall_m: 0.9621 - val_loss: 0.5115 - val_f1_m: 0.4848 - val_precision_m: 0.3796 - val_recall_m: 0.7295\n",
      "Epoch 229/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3009 - f1_m: 0.7778 - precision_m: 0.6364 - recall_m: 1.0000\n",
      "Epoch 00229: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 244us/sample - loss: 0.3933 - f1_m: 0.5259 - precision_m: 0.3779 - recall_m: 0.9318 - val_loss: 0.5115 - val_f1_m: 0.5176 - val_precision_m: 0.4064 - val_recall_m: 0.8143\n",
      "Epoch 230/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3243 - f1_m: 0.6316 - precision_m: 0.4615 - recall_m: 1.0000\n",
      "Epoch 00230: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 300us/sample - loss: 0.3867 - f1_m: 0.5668 - precision_m: 0.4183 - recall_m: 0.9636 - val_loss: 0.5115 - val_f1_m: 0.5032 - val_precision_m: 0.3885 - val_recall_m: 0.7357\n",
      "Epoch 231/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3828 - f1_m: 0.6000 - precision_m: 0.4286 - recall_m: 1.0000\n",
      "Epoch 00231: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 258us/sample - loss: 0.3928 - f1_m: 0.4797 - precision_m: 0.3531 - recall_m: 0.8523 - val_loss: 0.5115 - val_f1_m: 0.5513 - val_precision_m: 0.4306 - val_recall_m: 0.7867\n",
      "Epoch 232/300\n",
      "256/330 [======================>.......] - ETA: 0s - loss: 0.3989 - f1_m: 0.5372 - precision_m: 0.3946 - recall_m: 0.9688\n",
      "Epoch 00232: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 267us/sample - loss: 0.3894 - f1_m: 0.4962 - precision_m: 0.3665 - recall_m: 0.8682 - val_loss: 0.5115 - val_f1_m: 0.4018 - val_precision_m: 0.3184 - val_recall_m: 0.6318\n",
      "Epoch 233/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4441 - f1_m: 0.3636 - precision_m: 0.2222 - recall_m: 1.0000\n",
      "Epoch 00233: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 221us/sample - loss: 0.3913 - f1_m: 0.4989 - precision_m: 0.3611 - recall_m: 0.8561 - val_loss: 0.5115 - val_f1_m: 0.5095 - val_precision_m: 0.4050 - val_recall_m: 0.7333\n",
      "Epoch 234/300\n",
      "320/330 [============================>.] - ETA: 0s - loss: 0.3858 - f1_m: 0.5416 - precision_m: 0.3908 - recall_m: 0.9333\n",
      "Epoch 00234: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 309us/sample - loss: 0.3877 - f1_m: 0.5227 - precision_m: 0.3734 - recall_m: 0.9394 - val_loss: 0.5114 - val_f1_m: 0.4915 - val_precision_m: 0.3794 - val_recall_m: 0.7600\n",
      "Epoch 235/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4382 - f1_m: 0.3636 - precision_m: 0.2222 - recall_m: 1.0000\n",
      "Epoch 00235: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 223us/sample - loss: 0.3839 - f1_m: 0.5596 - precision_m: 0.4213 - recall_m: 0.9636 - val_loss: 0.5114 - val_f1_m: 0.5211 - val_precision_m: 0.4367 - val_recall_m: 0.7595\n",
      "Epoch 236/300\n",
      "320/330 [============================>.] - ETA: 0s - loss: 0.3910 - f1_m: 0.5369 - precision_m: 0.3913 - recall_m: 0.9633\n",
      "Epoch 00236: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 303us/sample - loss: 0.3891 - f1_m: 0.5608 - precision_m: 0.4164 - recall_m: 0.9667 - val_loss: 0.5115 - val_f1_m: 0.5467 - val_precision_m: 0.4127 - val_recall_m: 0.8350\n",
      "Epoch 237/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3406 - f1_m: 0.5882 - precision_m: 0.4167 - recall_m: 1.0000\n",
      "Epoch 00237: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 218us/sample - loss: 0.3956 - f1_m: 0.5097 - precision_m: 0.3793 - recall_m: 0.8939 - val_loss: 0.5115 - val_f1_m: 0.4190 - val_precision_m: 0.3312 - val_recall_m: 0.5750\n",
      "Epoch 238/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4257 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00\n",
      "Epoch 00238: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 243us/sample - loss: 0.3962 - f1_m: 0.4776 - precision_m: 0.3582 - recall_m: 0.7576 - val_loss: 0.5115 - val_f1_m: 0.5214 - val_precision_m: 0.4056 - val_recall_m: 0.7729\n",
      "Epoch 239/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4000 - f1_m: 0.5882 - precision_m: 0.4167 - recall_m: 1.0000\n",
      "Epoch 00239: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 215us/sample - loss: 0.3915 - f1_m: 0.4949 - precision_m: 0.3643 - recall_m: 0.8606 - val_loss: 0.5115 - val_f1_m: 0.5149 - val_precision_m: 0.3965 - val_recall_m: 0.8095\n",
      "Epoch 240/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3660 - f1_m: 0.5455 - precision_m: 0.3750 - recall_m: 1.0000\n",
      "Epoch 00240: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 266us/sample - loss: 0.3855 - f1_m: 0.5174 - precision_m: 0.3754 - recall_m: 0.9591 - val_loss: 0.5114 - val_f1_m: 0.5123 - val_precision_m: 0.4288 - val_recall_m: 0.7233\n",
      "Epoch 241/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4680 - f1_m: 0.5455 - precision_m: 0.3750 - recall_m: 1.0000\n",
      "Epoch 00241: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 242us/sample - loss: 0.3842 - f1_m: 0.5678 - precision_m: 0.4154 - recall_m: 0.9591 - val_loss: 0.5114 - val_f1_m: 0.4222 - val_precision_m: 0.3414 - val_recall_m: 0.6100\n",
      "Epoch 242/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3646 - f1_m: 0.6667 - precision_m: 0.5000 - recall_m: 1.0000\n",
      "Epoch 00242: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 230us/sample - loss: 0.3884 - f1_m: 0.5107 - precision_m: 0.3716 - recall_m: 0.9394 - val_loss: 0.5115 - val_f1_m: 0.5562 - val_precision_m: 0.5426 - val_recall_m: 0.7195\n",
      "Epoch 243/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3439 - f1_m: 0.5455 - precision_m: 0.3750 - recall_m: 1.0000\n",
      "Epoch 00243: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 215us/sample - loss: 0.3842 - f1_m: 0.4696 - precision_m: 0.3381 - recall_m: 0.8727 - val_loss: 0.5114 - val_f1_m: 0.5247 - val_precision_m: 0.4628 - val_recall_m: 0.7833\n",
      "Epoch 244/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3030 - f1_m: 0.4444 - precision_m: 0.2857 - recall_m: 1.0000\n",
      "Epoch 00244: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 207us/sample - loss: 0.3848 - f1_m: 0.5546 - precision_m: 0.4002 - recall_m: 0.9545 - val_loss: 0.5114 - val_f1_m: 0.5185 - val_precision_m: 0.3939 - val_recall_m: 0.7889\n",
      "Epoch 245/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3727 - f1_m: 0.7368 - precision_m: 0.5833 - recall_m: 1.0000\n",
      "Epoch 00245: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 254us/sample - loss: 0.3870 - f1_m: 0.4885 - precision_m: 0.3467 - recall_m: 0.8788 - val_loss: 0.5114 - val_f1_m: 0.4917 - val_precision_m: 0.3836 - val_recall_m: 0.7889\n",
      "Epoch 246/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3835 - f1_m: 0.5455 - precision_m: 0.3750 - recall_m: 1.0000\n",
      "Epoch 00246: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 255us/sample - loss: 0.3792 - f1_m: 0.5128 - precision_m: 0.3630 - recall_m: 0.9659 - val_loss: 0.5114 - val_f1_m: 0.4902 - val_precision_m: 0.3827 - val_recall_m: 0.7167\n",
      "Epoch 247/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4381 - f1_m: 0.3333 - precision_m: 0.2222 - recall_m: 0.6667\n",
      "Epoch 00247: val_loss did not improve from 0.51134\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 223us/sample - loss: 0.3951 - f1_m: 0.4610 - precision_m: 0.3410 - recall_m: 0.7697 - val_loss: 0.5113 - val_f1_m: 0.4819 - val_precision_m: 0.3833 - val_recall_m: 0.7000\n",
      "Epoch 248/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4068 - f1_m: 0.5714 - precision_m: 0.4000 - recall_m: 1.0000\n",
      "Epoch 00248: val_loss improved from 0.51134 to 0.51128, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 287us/sample - loss: 0.3889 - f1_m: 0.5144 - precision_m: 0.3729 - recall_m: 0.9591 - val_loss: 0.5113 - val_f1_m: 0.5141 - val_precision_m: 0.4140 - val_recall_m: 0.7600\n",
      "Epoch 249/300\n",
      "192/330 [================>.............] - ETA: 0s - loss: 0.3598 - f1_m: 0.6126 - precision_m: 0.4537 - recall_m: 1.0000\n",
      "Epoch 00249: val_loss improved from 0.51128 to 0.51125, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 376us/sample - loss: 0.3910 - f1_m: 0.5160 - precision_m: 0.3820 - recall_m: 0.8864 - val_loss: 0.5112 - val_f1_m: 0.5053 - val_precision_m: 0.3962 - val_recall_m: 0.7581\n",
      "Epoch 250/300\n",
      "256/330 [======================>.......] - ETA: 0s - loss: 0.3719 - f1_m: 0.5794 - precision_m: 0.4341 - recall_m: 0.9583\n",
      "Epoch 00250: val_loss did not improve from 0.51125\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 362us/sample - loss: 0.3859 - f1_m: 0.5391 - precision_m: 0.3935 - recall_m: 0.9394 - val_loss: 0.5113 - val_f1_m: 0.4636 - val_precision_m: 0.3561 - val_recall_m: 0.6667\n",
      "Epoch 251/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3411 - f1_m: 0.7778 - precision_m: 0.6364 - recall_m: 1.0000\n",
      "Epoch 00251: val_loss did not improve from 0.51125\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330/330 [==============================] - 0s 208us/sample - loss: 0.3884 - f1_m: 0.5233 - precision_m: 0.3726 - recall_m: 0.9545 - val_loss: 0.5113 - val_f1_m: 0.5448 - val_precision_m: 0.5159 - val_recall_m: 0.7195\n",
      "Epoch 252/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3718 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000\n",
      "Epoch 00252: val_loss did not improve from 0.51125\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 214us/sample - loss: 0.3935 - f1_m: 0.5095 - precision_m: 0.3835 - recall_m: 0.8682 - val_loss: 0.5113 - val_f1_m: 0.5331 - val_precision_m: 0.4280 - val_recall_m: 0.7583\n",
      "Epoch 253/300\n",
      "288/330 [=========================>....] - ETA: 0s - loss: 0.3875 - f1_m: 0.5341 - precision_m: 0.3831 - recall_m: 0.9471\n",
      "Epoch 00253: val_loss did not improve from 0.51125\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 379us/sample - loss: 0.3861 - f1_m: 0.5521 - precision_m: 0.3979 - recall_m: 0.9567 - val_loss: 0.5113 - val_f1_m: 0.5022 - val_precision_m: 0.4027 - val_recall_m: 0.8000\n",
      "Epoch 254/300\n",
      "320/330 [============================>.] - ETA: 0s - loss: 0.3961 - f1_m: 0.5043 - precision_m: 0.3718 - recall_m: 0.8333\n",
      "Epoch 00254: val_loss did not improve from 0.51125\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 297us/sample - loss: 0.3926 - f1_m: 0.5312 - precision_m: 0.3986 - recall_m: 0.8485 - val_loss: 0.5113 - val_f1_m: 0.4443 - val_precision_m: 0.3524 - val_recall_m: 0.6143\n",
      "Epoch 255/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3954 - f1_m: 0.6000 - precision_m: 0.4286 - recall_m: 1.0000\n",
      "Epoch 00255: val_loss did not improve from 0.51125\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 263us/sample - loss: 0.3934 - f1_m: 0.4917 - precision_m: 0.3621 - recall_m: 0.8409 - val_loss: 0.5113 - val_f1_m: 0.5409 - val_precision_m: 0.4533 - val_recall_m: 0.7833\n",
      "Epoch 256/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4718 - f1_m: 0.6250 - precision_m: 0.5000 - recall_m: 0.8333\n",
      "Epoch 00256: val_loss did not improve from 0.51125\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 218us/sample - loss: 0.3827 - f1_m: 0.5122 - precision_m: 0.3597 - recall_m: 0.9545 - val_loss: 0.5113 - val_f1_m: 0.5254 - val_precision_m: 0.3983 - val_recall_m: 0.7833\n",
      "Epoch 257/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.5240 - f1_m: 0.4000 - precision_m: 0.3000 - recall_m: 0.6000\n",
      "Epoch 00257: val_loss did not improve from 0.51125\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 242us/sample - loss: 0.3841 - f1_m: 0.5467 - precision_m: 0.3896 - recall_m: 0.9636 - val_loss: 0.5113 - val_f1_m: 0.5255 - val_precision_m: 0.4000 - val_recall_m: 0.7917\n",
      "Epoch 258/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3172 - f1_m: 0.6667 - precision_m: 0.5000 - recall_m: 1.0000\n",
      "Epoch 00258: val_loss did not improve from 0.51125\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 197us/sample - loss: 0.3926 - f1_m: 0.5231 - precision_m: 0.3751 - recall_m: 0.9545 - val_loss: 0.5113 - val_f1_m: 0.4258 - val_precision_m: 0.3400 - val_recall_m: 0.6333\n",
      "Epoch 259/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3824 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000\n",
      "Epoch 00259: val_loss did not improve from 0.51125\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 216us/sample - loss: 0.3851 - f1_m: 0.5502 - precision_m: 0.4391 - recall_m: 0.9242 - val_loss: 0.5113 - val_f1_m: 0.4333 - val_precision_m: 0.3464 - val_recall_m: 0.6000\n",
      "Epoch 260/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3336 - f1_m: 0.7059 - precision_m: 0.5455 - recall_m: 1.0000\n",
      "Epoch 00260: val_loss did not improve from 0.51125\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 220us/sample - loss: 0.3901 - f1_m: 0.4765 - precision_m: 0.3414 - recall_m: 0.8333 - val_loss: 0.5113 - val_f1_m: 0.5299 - val_precision_m: 0.4053 - val_recall_m: 0.7950\n",
      "Epoch 261/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3493 - f1_m: 0.5455 - precision_m: 0.3750 - recall_m: 1.0000\n",
      "Epoch 00261: val_loss did not improve from 0.51125\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 305us/sample - loss: 0.3891 - f1_m: 0.5659 - precision_m: 0.4331 - recall_m: 0.9394 - val_loss: 0.5113 - val_f1_m: 0.4856 - val_precision_m: 0.3791 - val_recall_m: 0.6889\n",
      "Epoch 262/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3879 - f1_m: 0.7059 - precision_m: 0.5455 - recall_m: 1.0000\n",
      "Epoch 00262: val_loss did not improve from 0.51125\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 253us/sample - loss: 0.3923 - f1_m: 0.4556 - precision_m: 0.3278 - recall_m: 0.7955 - val_loss: 0.5113 - val_f1_m: 0.5185 - val_precision_m: 0.4056 - val_recall_m: 0.7667\n",
      "Epoch 263/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3797 - f1_m: 0.5714 - precision_m: 0.4000 - recall_m: 1.0000\n",
      "Epoch 00263: val_loss did not improve from 0.51125\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 279us/sample - loss: 0.3888 - f1_m: 0.5424 - precision_m: 0.4004 - recall_m: 0.9567 - val_loss: 0.5113 - val_f1_m: 0.4708 - val_precision_m: 0.3719 - val_recall_m: 0.6743\n",
      "Epoch 264/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3999 - f1_m: 0.5000 - precision_m: 0.3750 - recall_m: 0.7500\n",
      "Epoch 00264: val_loss did not improve from 0.51125\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 212us/sample - loss: 0.3891 - f1_m: 0.4829 - precision_m: 0.3592 - recall_m: 0.7773 - val_loss: 0.5113 - val_f1_m: 0.5029 - val_precision_m: 0.3876 - val_recall_m: 0.7750\n",
      "Epoch 265/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4286 - f1_m: 0.3636 - precision_m: 0.2500 - recall_m: 0.6667\n",
      "Epoch 00265: val_loss did not improve from 0.51125\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 232us/sample - loss: 0.3910 - f1_m: 0.5674 - precision_m: 0.4409 - recall_m: 0.9583 - val_loss: 0.5113 - val_f1_m: 0.5408 - val_precision_m: 0.4250 - val_recall_m: 0.7862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 266/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4435 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000\n",
      "Epoch 00266: val_loss improved from 0.51125 to 0.51124, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 358us/sample - loss: 0.3930 - f1_m: 0.5301 - precision_m: 0.3863 - recall_m: 0.8909 - val_loss: 0.5112 - val_f1_m: 0.4921 - val_precision_m: 0.3767 - val_recall_m: 0.7850\n",
      "Epoch 267/300\n",
      "288/330 [=========================>....] - ETA: 0s - loss: 0.3766 - f1_m: 0.5129 - precision_m: 0.3651 - recall_m: 0.9722\n",
      "Epoch 00267: val_loss improved from 0.51124 to 0.51122, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 377us/sample - loss: 0.3851 - f1_m: 0.5362 - precision_m: 0.3897 - recall_m: 0.9591 - val_loss: 0.5112 - val_f1_m: 0.5373 - val_precision_m: 0.4298 - val_recall_m: 0.7500\n",
      "Epoch 268/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4154 - f1_m: 0.5333 - precision_m: 0.3636 - recall_m: 1.0000\n",
      "Epoch 00268: val_loss did not improve from 0.51122\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 197us/sample - loss: 0.3908 - f1_m: 0.4866 - precision_m: 0.3473 - recall_m: 0.8682 - val_loss: 0.5113 - val_f1_m: 0.5467 - val_precision_m: 0.4783 - val_recall_m: 0.7467\n",
      "Epoch 269/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3098 - f1_m: 0.7500 - precision_m: 0.6000 - recall_m: 1.0000\n",
      "Epoch 00269: val_loss improved from 0.51122 to 0.51120, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 252us/sample - loss: 0.3872 - f1_m: 0.5332 - precision_m: 0.3790 - recall_m: 0.9591 - val_loss: 0.5112 - val_f1_m: 0.5034 - val_precision_m: 0.3885 - val_recall_m: 0.7381\n",
      "Epoch 270/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3228 - f1_m: 0.6667 - precision_m: 0.5000 - recall_m: 1.0000\n",
      "Epoch 00270: val_loss improved from 0.51120 to 0.51119, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 236us/sample - loss: 0.3834 - f1_m: 0.4886 - precision_m: 0.3623 - recall_m: 0.7765 - val_loss: 0.5112 - val_f1_m: 0.5401 - val_precision_m: 0.4231 - val_recall_m: 0.8000\n",
      "Epoch 271/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4673 - f1_m: 0.4286 - precision_m: 0.3000 - recall_m: 0.7500\n",
      "Epoch 00271: val_loss improved from 0.51119 to 0.51113, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 268us/sample - loss: 0.3940 - f1_m: 0.4753 - precision_m: 0.3745 - recall_m: 0.7803 - val_loss: 0.5111 - val_f1_m: 0.5275 - val_precision_m: 0.4426 - val_recall_m: 0.7367\n",
      "Epoch 272/300\n",
      "288/330 [=========================>....] - ETA: 0s - loss: 0.3970 - f1_m: 0.5412 - precision_m: 0.3896 - recall_m: 0.9444\n",
      "Epoch 00272: val_loss improved from 0.51113 to 0.51107, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 311us/sample - loss: 0.3931 - f1_m: 0.5567 - precision_m: 0.4142 - recall_m: 0.9242 - val_loss: 0.5111 - val_f1_m: 0.4817 - val_precision_m: 0.3750 - val_recall_m: 0.7470\n",
      "Epoch 273/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3836 - f1_m: 0.7059 - precision_m: 0.5455 - recall_m: 1.0000\n",
      "Epoch 00273: val_loss improved from 0.51107 to 0.51104, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 308us/sample - loss: 0.3903 - f1_m: 0.5237 - precision_m: 0.4079 - recall_m: 0.8712 - val_loss: 0.5110 - val_f1_m: 0.3782 - val_precision_m: 0.3093 - val_recall_m: 0.5400\n",
      "Epoch 274/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4470 - f1_m: 0.5714 - precision_m: 0.4444 - recall_m: 0.8000\n",
      "Epoch 00274: val_loss did not improve from 0.51104\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 229us/sample - loss: 0.3917 - f1_m: 0.5095 - precision_m: 0.3766 - recall_m: 0.8779 - val_loss: 0.5111 - val_f1_m: 0.5350 - val_precision_m: 0.4427 - val_recall_m: 0.7414\n",
      "Epoch 275/300\n",
      "192/330 [================>.............] - ETA: 0s - loss: 0.3949 - f1_m: 0.4915 - precision_m: 0.3611 - recall_m: 0.9444\n",
      "Epoch 00275: val_loss improved from 0.51104 to 0.51102, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 384us/sample - loss: 0.3885 - f1_m: 0.5708 - precision_m: 0.4405 - recall_m: 0.9697 - val_loss: 0.5110 - val_f1_m: 0.5156 - val_precision_m: 0.3958 - val_recall_m: 0.7514\n",
      "Epoch 276/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3262 - f1_m: 0.8333 - precision_m: 0.7143 - recall_m: 1.0000\n",
      "Epoch 00276: val_loss improved from 0.51102 to 0.51099, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 295us/sample - loss: 0.3825 - f1_m: 0.5598 - precision_m: 0.4242 - recall_m: 0.9667 - val_loss: 0.5110 - val_f1_m: 0.5196 - val_precision_m: 0.4396 - val_recall_m: 0.7250\n",
      "Epoch 277/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4271 - f1_m: 0.5000 - precision_m: 0.5000 - recall_m: 0.5000\n",
      "Epoch 00277: val_loss did not improve from 0.51099\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 298us/sample - loss: 0.3889 - f1_m: 0.5599 - precision_m: 0.4201 - recall_m: 0.9318 - val_loss: 0.5110 - val_f1_m: 0.5383 - val_precision_m: 0.4441 - val_recall_m: 0.7714\n",
      "Epoch 278/300\n",
      " 96/330 [=======>......................] - ETA: 0s - loss: 0.3583 - f1_m: 0.5908 - precision_m: 0.4318 - recall_m: 1.0000\n",
      "Epoch 00278: val_loss did not improve from 0.51099\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 362us/sample - loss: 0.3808 - f1_m: 0.5738 - precision_m: 0.4240 - recall_m: 0.9591 - val_loss: 0.5110 - val_f1_m: 0.5326 - val_precision_m: 0.4169 - val_recall_m: 0.8133\n",
      "Epoch 279/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3836 - f1_m: 0.6667 - precision_m: 0.5000 - recall_m: 1.0000\n",
      "Epoch 00279: val_loss did not improve from 0.51099\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330/330 [==============================] - 0s 174us/sample - loss: 0.3880 - f1_m: 0.5583 - precision_m: 0.4099 - recall_m: 0.9515 - val_loss: 0.5111 - val_f1_m: 0.5456 - val_precision_m: 0.4239 - val_recall_m: 0.7810\n",
      "Epoch 280/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3387 - f1_m: 0.6154 - precision_m: 0.4444 - recall_m: 1.0000\n",
      "Epoch 00280: val_loss did not improve from 0.51099\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 210us/sample - loss: 0.3845 - f1_m: 0.4569 - precision_m: 0.3310 - recall_m: 0.7922 - val_loss: 0.5111 - val_f1_m: 0.4957 - val_precision_m: 0.3897 - val_recall_m: 0.7933\n",
      "Epoch 281/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3016 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000\n",
      "Epoch 00281: val_loss did not improve from 0.51099\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 208us/sample - loss: 0.3799 - f1_m: 0.5396 - precision_m: 0.3823 - recall_m: 0.9591 - val_loss: 0.5110 - val_f1_m: 0.4408 - val_precision_m: 0.3526 - val_recall_m: 0.6200\n",
      "Epoch 282/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3843 - f1_m: 0.6667 - precision_m: 0.5000 - recall_m: 1.0000\n",
      "Epoch 00282: val_loss did not improve from 0.51099\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 207us/sample - loss: 0.3877 - f1_m: 0.5268 - precision_m: 0.3901 - recall_m: 0.8750 - val_loss: 0.5110 - val_f1_m: 0.5181 - val_precision_m: 0.4140 - val_recall_m: 0.7788\n",
      "Epoch 283/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4561 - f1_m: 0.6250 - precision_m: 0.5000 - recall_m: 0.8333\n",
      "Epoch 00283: val_loss did not improve from 0.51099\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 222us/sample - loss: 0.3886 - f1_m: 0.5502 - precision_m: 0.4137 - recall_m: 0.9667 - val_loss: 0.5110 - val_f1_m: 0.4649 - val_precision_m: 0.3849 - val_recall_m: 0.6267\n",
      "Epoch 284/300\n",
      "320/330 [============================>.] - ETA: 0s - loss: 0.3907 - f1_m: 0.5168 - precision_m: 0.3716 - recall_m: 0.9524\n",
      "Epoch 00284: val_loss did not improve from 0.51099\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 376us/sample - loss: 0.3919 - f1_m: 0.5153 - precision_m: 0.3681 - recall_m: 0.9567 - val_loss: 0.5110 - val_f1_m: 0.4805 - val_precision_m: 0.3692 - val_recall_m: 0.7584\n",
      "Epoch 285/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4434 - f1_m: 0.4444 - precision_m: 0.2857 - recall_m: 1.0000\n",
      "Epoch 00285: val_loss did not improve from 0.51099\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 195us/sample - loss: 0.3923 - f1_m: 0.5649 - precision_m: 0.4318 - recall_m: 0.9242 - val_loss: 0.5110 - val_f1_m: 0.5206 - val_precision_m: 0.3921 - val_recall_m: 0.7957\n",
      "Epoch 286/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4738 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000\n",
      "Epoch 00286: val_loss did not improve from 0.51099\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 229us/sample - loss: 0.3950 - f1_m: 0.5341 - precision_m: 0.4162 - recall_m: 0.8909 - val_loss: 0.5110 - val_f1_m: 0.5366 - val_precision_m: 0.4162 - val_recall_m: 0.7917\n",
      "Epoch 287/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3693 - f1_m: 0.5455 - precision_m: 0.3750 - recall_m: 1.0000\n",
      "Epoch 00287: val_loss did not improve from 0.51099\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 210us/sample - loss: 0.3883 - f1_m: 0.4894 - precision_m: 0.3503 - recall_m: 0.8561 - val_loss: 0.5111 - val_f1_m: 0.4881 - val_precision_m: 0.3696 - val_recall_m: 0.7529\n",
      "Epoch 288/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3596 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00\n",
      "Epoch 00288: val_loss did not improve from 0.51099\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 243us/sample - loss: 0.3839 - f1_m: 0.4564 - precision_m: 0.3311 - recall_m: 0.7900 - val_loss: 0.5110 - val_f1_m: 0.4999 - val_precision_m: 0.3881 - val_recall_m: 0.7464\n",
      "Epoch 289/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3749 - f1_m: 0.8333 - precision_m: 0.7143 - recall_m: 1.0000\n",
      "Epoch 00289: val_loss improved from 0.51099 to 0.51098, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 373us/sample - loss: 0.3850 - f1_m: 0.5536 - precision_m: 0.4113 - recall_m: 0.9545 - val_loss: 0.5110 - val_f1_m: 0.5132 - val_precision_m: 0.4078 - val_recall_m: 0.7743\n",
      "Epoch 290/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3924 - f1_m: 0.5455 - precision_m: 0.3750 - recall_m: 1.0000\n",
      "Epoch 00290: val_loss did not improve from 0.51098\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 199us/sample - loss: 0.3961 - f1_m: 0.5587 - precision_m: 0.4246 - recall_m: 0.9545 - val_loss: 0.5110 - val_f1_m: 0.5234 - val_precision_m: 0.4230 - val_recall_m: 0.7895\n",
      "Epoch 291/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3297 - f1_m: 0.7692 - precision_m: 0.6250 - recall_m: 1.0000\n",
      "Epoch 00291: val_loss did not improve from 0.51098\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 272us/sample - loss: 0.3911 - f1_m: 0.5786 - precision_m: 0.4510 - recall_m: 0.9591 - val_loss: 0.5110 - val_f1_m: 0.4167 - val_precision_m: 0.3203 - val_recall_m: 0.6233\n",
      "Epoch 292/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.2890 - f1_m: 0.6000 - precision_m: 0.4286 - recall_m: 1.0000\n",
      "Epoch 00292: val_loss did not improve from 0.51098\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 246us/sample - loss: 0.3895 - f1_m: 0.4764 - precision_m: 0.3471 - recall_m: 0.8606 - val_loss: 0.5110 - val_f1_m: 0.5317 - val_precision_m: 0.4163 - val_recall_m: 0.7767\n",
      "Epoch 293/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4135 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000\n",
      "Epoch 00293: val_loss did not improve from 0.51098\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 230us/sample - loss: 0.3917 - f1_m: 0.5310 - precision_m: 0.3914 - recall_m: 0.9591 - val_loss: 0.5110 - val_f1_m: 0.4528 - val_precision_m: 0.3611 - val_recall_m: 0.6190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 294/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3822 - f1_m: 0.8000 - precision_m: 0.6667 - recall_m: 1.0000\n",
      "Epoch 00294: val_loss improved from 0.51098 to 0.51095, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 253us/sample - loss: 0.3838 - f1_m: 0.4514 - precision_m: 0.3316 - recall_m: 0.7749 - val_loss: 0.5109 - val_f1_m: 0.4763 - val_precision_m: 0.3666 - val_recall_m: 0.7600\n",
      "Epoch 295/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.3354 - f1_m: 0.6667 - precision_m: 0.5000 - recall_m: 1.0000\n",
      "Epoch 00295: val_loss improved from 0.51095 to 0.51091, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 262us/sample - loss: 0.3939 - f1_m: 0.5110 - precision_m: 0.3857 - recall_m: 0.8485 - val_loss: 0.5109 - val_f1_m: 0.5235 - val_precision_m: 0.4072 - val_recall_m: 0.7933\n",
      "Epoch 296/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4528 - f1_m: 0.2857 - precision_m: 0.2000 - recall_m: 0.5000\n",
      "Epoch 00296: val_loss improved from 0.51091 to 0.51085, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 271us/sample - loss: 0.3865 - f1_m: 0.5169 - precision_m: 0.3755 - recall_m: 0.9364 - val_loss: 0.5108 - val_f1_m: 0.4411 - val_precision_m: 0.3413 - val_recall_m: 0.6362\n",
      "Epoch 297/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4184 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000\n",
      "Epoch 00297: val_loss improved from 0.51085 to 0.51082, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 282us/sample - loss: 0.3857 - f1_m: 0.4886 - precision_m: 0.3527 - recall_m: 0.8864 - val_loss: 0.5108 - val_f1_m: 0.5258 - val_precision_m: 0.4217 - val_recall_m: 0.7381\n",
      "Epoch 298/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4103 - f1_m: 0.7500 - precision_m: 0.6000 - recall_m: 1.0000\n",
      "Epoch 00298: val_loss did not improve from 0.51082\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 227us/sample - loss: 0.3878 - f1_m: 0.5135 - precision_m: 0.3742 - recall_m: 0.8788 - val_loss: 0.5109 - val_f1_m: 0.4862 - val_precision_m: 0.3746 - val_recall_m: 0.7833\n",
      "Epoch 299/300\n",
      " 32/330 [=>............................] - ETA: 0s - loss: 0.4172 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000\n",
      "Epoch 00299: val_loss did not improve from 0.51082\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 289us/sample - loss: 0.4017 - f1_m: 0.4581 - precision_m: 0.3374 - recall_m: 0.7727 - val_loss: 0.5109 - val_f1_m: 0.4910 - val_precision_m: 0.3756 - val_recall_m: 0.7750\n",
      "Epoch 300/300\n",
      "256/330 [======================>.......] - ETA: 0s - loss: 0.3914 - f1_m: 0.5626 - precision_m: 0.4414 - recall_m: 0.9583\n",
      "Epoch 00300: val_loss did not improve from 0.51082\n",
      "Logging weights error: No such layer: LSTM_layer\n",
      "\n",
      "Logging weights error: No such layer: embeddings_layer\n",
      "\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "330/330 [==============================] - 0s 344us/sample - loss: 0.3921 - f1_m: 0.5268 - precision_m: 0.3991 - recall_m: 0.9697 - val_loss: 0.5109 - val_f1_m: 0.5045 - val_precision_m: 0.3957 - val_recall_m: 0.7333\n",
      "CPU times: user 25.9 s, sys: 1.99 s, total: 27.9 s\n",
      "Wall time: 29.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "freeze_layer = FreezeLayer(hyperparams['freeze_patience'], set_to=not hyperparams['trainable_embeddings'])\n",
    "weights_history = WeightsHistory()\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=hyperparams['reduce_lr_factor'],\n",
    "                          patience=hyperparams['reduce_lr_patience'], min_lr=0.000001, verbose=1)\n",
    "\n",
    "model, history = train_model(model, x_train, y_train, x_valid, y_valid,\n",
    "           epochs=300, batch_size=hyperparams['batch_size'],\n",
    "                      class_weight={0:0.5, 1:5}, start_epoch=0,\n",
    "                      callback_list = [freeze_layer, weights_history, reduce_lr],\n",
    "                      model_path='models/mlp_user_anorex', workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.13346808e-01],\n",
       "        [ 1.49938107e-01],\n",
       "        [-4.02613357e-02],\n",
       "        [-4.96538496e-03],\n",
       "        [-1.31627843e-01],\n",
       "        [ 1.80921361e-01],\n",
       "        [ 5.91040961e-02],\n",
       "        [ 8.27158988e-02],\n",
       "        [-1.11491986e-01],\n",
       "        [-9.24129561e-02],\n",
       "        [ 1.72996402e-01],\n",
       "        [ 1.82813600e-01],\n",
       "        [-3.86944227e-02],\n",
       "        [ 3.28002982e-02],\n",
       "        [ 6.13929220e-02],\n",
       "        [ 1.47554502e-01],\n",
       "        [ 1.34893045e-01],\n",
       "        [ 1.24246009e-01],\n",
       "        [-1.52474254e-01],\n",
       "        [ 7.69707561e-02],\n",
       "        [ 5.58837987e-02],\n",
       "        [ 5.96134402e-02],\n",
       "        [-6.24312973e-03],\n",
       "        [-1.55915285e-03],\n",
       "        [ 8.63053873e-02],\n",
       "        [ 4.75549996e-02],\n",
       "        [-7.87871256e-02],\n",
       "        [-1.49628788e-01],\n",
       "        [ 8.46894681e-02],\n",
       "        [ 8.07042792e-02],\n",
       "        [ 1.17896684e-01],\n",
       "        [ 1.13564476e-01],\n",
       "        [-3.49921137e-02],\n",
       "        [ 1.46593889e-02],\n",
       "        [-3.47690051e-03],\n",
       "        [ 4.25638892e-02],\n",
       "        [-9.71212089e-02],\n",
       "        [ 5.64015321e-02],\n",
       "        [ 7.16500357e-02],\n",
       "        [-1.08462475e-01],\n",
       "        [ 1.33519862e-02],\n",
       "        [ 2.22153589e-03],\n",
       "        [-4.60857302e-02],\n",
       "        [ 6.66336864e-02],\n",
       "        [-4.60681356e-02],\n",
       "        [ 8.37811008e-02],\n",
       "        [-1.95291251e-01],\n",
       "        [ 9.98888165e-02],\n",
       "        [-1.25741586e-02],\n",
       "        [ 6.38722163e-03],\n",
       "        [ 3.69350053e-03],\n",
       "        [-6.87789768e-02],\n",
       "        [ 2.13629276e-01],\n",
       "        [-1.95351336e-02],\n",
       "        [-5.37343025e-02],\n",
       "        [ 2.23885521e-01],\n",
       "        [ 1.48967400e-01],\n",
       "        [-1.25706531e-02],\n",
       "        [ 4.98889163e-02],\n",
       "        [-4.89821509e-02],\n",
       "        [ 3.96624170e-02],\n",
       "        [ 1.49418265e-01],\n",
       "        [ 4.33165729e-02],\n",
       "        [ 2.60219909e-02],\n",
       "        [ 1.35336146e-01],\n",
       "        [ 1.49108022e-01],\n",
       "        [-6.28161877e-02],\n",
       "        [-4.51554768e-02],\n",
       "        [-1.20844632e-01],\n",
       "        [ 1.26302913e-01],\n",
       "        [ 2.25514313e-03],\n",
       "        [ 6.29298240e-02],\n",
       "        [ 1.62328392e-01],\n",
       "        [ 1.90966316e-02],\n",
       "        [-1.42699433e-02],\n",
       "        [ 1.19457603e-01],\n",
       "        [ 1.05168574e-01],\n",
       "        [-8.66559371e-02],\n",
       "        [ 6.24882206e-02],\n",
       "        [ 1.31257884e-02],\n",
       "        [ 6.11451901e-02],\n",
       "        [-1.36410519e-01],\n",
       "        [-1.56491458e-01],\n",
       "        [-9.77962166e-02],\n",
       "        [ 2.19149898e-09],\n",
       "        [-3.16750421e-03],\n",
       "        [ 1.24134047e-09],\n",
       "        [ 1.87178713e-07],\n",
       "        [ 1.57135278e-01],\n",
       "        [ 1.06525980e-01],\n",
       "        [-2.26811338e-02],\n",
       "        [ 4.18008827e-02],\n",
       "        [-1.62558272e-01],\n",
       "        [ 1.64229590e-02],\n",
       "        [-9.99655724e-02],\n",
       "        [ 1.05370432e-01],\n",
       "        [ 8.38235393e-02],\n",
       "        [ 2.94409547e-04],\n",
       "        [ 1.86582226e-02],\n",
       "        [-6.33901507e-02],\n",
       "        [-3.20388302e-02],\n",
       "        [-2.14377176e-02],\n",
       "        [-1.77424497e-09],\n",
       "        [-8.61299336e-02],\n",
       "        [ 8.12208056e-02],\n",
       "        [ 1.12673894e-01],\n",
       "        [-4.34028581e-02],\n",
       "        [-1.59247026e-01],\n",
       "        [-1.63344234e-01],\n",
       "        [-4.84324880e-02],\n",
       "        [-2.22532614e-03],\n",
       "        [-5.52749820e-02],\n",
       "        [-9.28694233e-02],\n",
       "        [-2.63031006e-01],\n",
       "        [-1.20948873e-01],\n",
       "        [ 3.01445127e-02],\n",
       "        [-9.72760206e-10],\n",
       "        [ 5.28047495e-02],\n",
       "        [ 1.81494921e-01],\n",
       "        [-1.19286459e-02],\n",
       "        [ 9.45564359e-02],\n",
       "        [ 1.42100817e-02],\n",
       "        [-1.22201331e-01],\n",
       "        [ 9.81219113e-02],\n",
       "        [ 1.26256391e-01],\n",
       "        [ 1.07745171e-01],\n",
       "        [-3.81029248e-02],\n",
       "        [-4.60895933e-02],\n",
       "        [-9.63097066e-02],\n",
       "        [ 3.01120859e-02],\n",
       "        [ 4.29925211e-02],\n",
       "        [ 1.38276611e-02],\n",
       "        [-1.64101794e-02],\n",
       "        [-3.34986970e-02],\n",
       "        [ 2.18760651e-02],\n",
       "        [ 1.43932253e-01],\n",
       "        [-6.37985989e-02],\n",
       "        [-7.48375878e-02],\n",
       "        [ 1.57065034e-01],\n",
       "        [-8.44406243e-03],\n",
       "        [-2.66874004e-02],\n",
       "        [-1.36280432e-02],\n",
       "        [-3.22498865e-02],\n",
       "        [-1.30448103e-01],\n",
       "        [-8.58247504e-02],\n",
       "        [ 1.56724993e-02],\n",
       "        [ 5.79428300e-03],\n",
       "        [-1.66645441e-02],\n",
       "        [-1.35751560e-01],\n",
       "        [ 7.85717368e-02],\n",
       "        [-3.83953825e-02],\n",
       "        [ 1.39426947e-01],\n",
       "        [ 1.19833298e-01],\n",
       "        [ 5.48574738e-02],\n",
       "        [-8.08766186e-02],\n",
       "        [-9.81495380e-02],\n",
       "        [-7.93948919e-02],\n",
       "        [-7.93120936e-02],\n",
       "        [ 4.44845222e-02],\n",
       "        [-1.19267583e-01],\n",
       "        [-1.21423520e-01],\n",
       "        [ 8.95863548e-02],\n",
       "        [-1.50607675e-01],\n",
       "        [-7.24039674e-02],\n",
       "        [ 1.49001375e-01],\n",
       "        [ 1.61926791e-01],\n",
       "        [-5.12878671e-02],\n",
       "        [ 9.21329707e-02],\n",
       "        [ 3.87384966e-02],\n",
       "        [ 1.34020120e-01],\n",
       "        [-4.48516123e-02],\n",
       "        [ 8.93616769e-03],\n",
       "        [-8.65443498e-02],\n",
       "        [ 8.69182963e-03],\n",
       "        [-2.29907501e-02],\n",
       "        [ 4.52838317e-02],\n",
       "        [-1.03498317e-01],\n",
       "        [-7.38420188e-02],\n",
       "        [-1.05392337e-01],\n",
       "        [-8.71607959e-02],\n",
       "        [-1.29405066e-01],\n",
       "        [-6.22687163e-04],\n",
       "        [-6.15527406e-02],\n",
       "        [-1.60848033e-02],\n",
       "        [-1.85059816e-01],\n",
       "        [ 8.76245871e-02],\n",
       "        [ 3.24902590e-03],\n",
       "        [ 3.89152467e-02],\n",
       "        [-4.65949364e-02],\n",
       "        [ 2.89901346e-02],\n",
       "        [ 1.12154074e-01],\n",
       "        [ 8.04428849e-03],\n",
       "        [ 1.06879473e-01],\n",
       "        [ 1.22096203e-01],\n",
       "        [ 1.14465535e-01],\n",
       "        [-8.97411108e-02],\n",
       "        [-1.64673496e-02],\n",
       "        [-1.21778389e-02],\n",
       "        [-7.99962413e-03],\n",
       "        [-7.13739768e-02],\n",
       "        [ 1.43412411e-01],\n",
       "        [-1.15511611e-01],\n",
       "        [ 9.38293859e-02],\n",
       "        [ 1.31361008e-01],\n",
       "        [ 1.71429515e-01],\n",
       "        [ 5.76935224e-02],\n",
       "        [-4.19502631e-02],\n",
       "        [-1.00140972e-03],\n",
       "        [-8.76636580e-02],\n",
       "        [-7.01169716e-04],\n",
       "        [-1.31890355e-02],\n",
       "        [-4.91886698e-02],\n",
       "        [ 1.91561922e-01],\n",
       "        [-1.06010683e-01],\n",
       "        [-1.91037744e-01],\n",
       "        [-1.35131210e-01],\n",
       "        [-1.10110253e-04],\n",
       "        [ 3.67756374e-02],\n",
       "        [-1.95506677e-01],\n",
       "        [ 1.17719762e-01],\n",
       "        [ 3.41542838e-09],\n",
       "        [ 3.96404006e-02],\n",
       "        [ 1.57728337e-08],\n",
       "        [-1.95929743e-02],\n",
       "        [-4.16693391e-09],\n",
       "        [-5.26794456e-02],\n",
       "        [-1.67643908e-08],\n",
       "        [ 1.46674201e-01],\n",
       "        [ 1.73045875e-04],\n",
       "        [-2.32114956e-01],\n",
       "        [ 4.92477259e-09],\n",
       "        [ 1.42844573e-01],\n",
       "        [-2.44983193e-03],\n",
       "        [-8.66675377e-02],\n",
       "        [ 1.48491585e-03],\n",
       "        [-1.69914335e-01],\n",
       "        [ 1.74436465e-08],\n",
       "        [-2.96116687e-09],\n",
       "        [-6.88543427e-04],\n",
       "        [-5.39601974e-10],\n",
       "        [ 8.70251260e-10],\n",
       "        [ 1.10177777e-03],\n",
       "        [-2.23759837e-08],\n",
       "        [ 5.69516256e-09],\n",
       "        [-2.07367558e-02],\n",
       "        [-1.36672296e-09],\n",
       "        [ 5.27007803e-02],\n",
       "        [-1.79660802e-07],\n",
       "        [-8.14630240e-02],\n",
       "        [-1.05777499e-03],\n",
       "        [ 1.15063593e-01],\n",
       "        [ 1.10638132e-09],\n",
       "        [ 4.24930751e-02],\n",
       "        [-1.67647549e-08]], dtype=float32), array([0.00897142], dtype=float32)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='models/lstm_plus4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_class = Metrics(threshold=hyperparams['threshold'])\n",
    "dependencies = {\n",
    "    'f1_m': metrics_class.f1_m,\n",
    "    'precision_m': metrics_class.precision_m,\n",
    "    'recall_m': metrics_class.recall_m,\n",
    "    'binary_crossentropy_custom': binary_crossentropy_custom\n",
    "}\n",
    "model = load_model('models/mlp_user_anorex_best', custom_objects=dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected numeric_input to have shape (75,) but got array with shape (100,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-113a54cbdef7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0msteps_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'steps'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     if (self.run_eagerly or (isinstance(x, iterator_ops.EagerIterator) and\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\u001b[0m\n\u001b[1;32m   2380\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2382\u001b[0;31m         exception_prefix='input')\n\u001b[0m\u001b[1;32m   2383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    360\u001b[0m                 \u001b[0;34m'Error when checking '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexception_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m                 ' but got array with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    363\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected numeric_input to have shape (75,) but got array with shape (100,)"
     ]
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179 179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('himself', 'stopword'), -0.7284296154975891),\n",
       " (('against', 'stopword'), -0.677932858467102),\n",
       " (('themselves', 'stopword'), -0.6269885897636414),\n",
       " (('myself', 'stopword'), 0.5944074988365173),\n",
       " (('me', 'stopword'), 0.5808129906654358),\n",
       " (('which', 'stopword'), -0.5802603363990784),\n",
       " (('y', 'stopword'), -0.4890809953212738),\n",
       " (('yours', 'stopword'), -0.45802047848701477),\n",
       " (('shouldn', 'stopword'), -0.44473156332969666),\n",
       " (('herself', 'stopword'), -0.44048914313316345),\n",
       " (('ain', 'stopword'), 0.43934911489486694),\n",
       " (('as', 'stopword'), -0.42869824171066284),\n",
       " (('or', 'stopword'), 0.42340654134750366),\n",
       " (('by', 'stopword'), -0.4231763184070587),\n",
       " (('hasn', 'stopword'), -0.4093189239501953),\n",
       " (('from', 'stopword'), -0.40467941761016846),\n",
       " (('during', 'stopword'), -0.40312299132347107),\n",
       " (('no', 'stopword'), 0.39973184466362),\n",
       " (('am', 'stopword'), 0.39801648259162903),\n",
       " (('re', 'stopword'), -0.39346882700920105),\n",
       " (('once', 'stopword'), -0.3780168890953064),\n",
       " (('below', 'stopword'), -0.3677508234977722),\n",
       " (('i', 'stopword'), 0.36311987042427063),\n",
       " (('don', 'stopword'), 0.3617483377456665),\n",
       " (('haven', 'stopword'), 0.3546900451183319),\n",
       " (('my', 'stopword'), 0.33312052488327026),\n",
       " (('itself', 'stopword'), -0.32148054242134094),\n",
       " (('ma', 'stopword'), -0.3166116774082184),\n",
       " (('where', 'stopword'), -0.3089277744293213),\n",
       " (('isn', 'stopword'), 0.2894366383552551),\n",
       " (('our', 'stopword'), -0.27598702907562256),\n",
       " (('but', 'stopword'), 0.2721453905105591),\n",
       " (('their', 'stopword'), -0.2711140513420105),\n",
       " (('will', 'stopword'), -0.267701119184494),\n",
       " (('couldn', 'stopword'), -0.26473215222358704),\n",
       " (('above', 'stopword'), -0.26369601488113403),\n",
       " (('has', 'stopword'), -0.26124489307403564),\n",
       " (('too', 'stopword'), 0.26051756739616394),\n",
       " (('he', 'stopword'), -0.2553723156452179),\n",
       " (('doesn', 'stopword'), -0.24496737122535706),\n",
       " (('being', 'stopword'), 0.2437523603439331),\n",
       " (('those', 'stopword'), -0.23140552639961243),\n",
       " (('had', 'stopword'), 0.2304789274930954),\n",
       " (('few', 'stopword'), 0.22835423052310944),\n",
       " (('in', 'stopword'), -0.22744475305080414),\n",
       " (('such', 'stopword'), 0.22520944476127625),\n",
       " (('been', 'stopword'), 0.21875424683094025),\n",
       " (('having', 'stopword'), 0.21869756281375885),\n",
       " (('wouldn', 'stopword'), -0.21862567961215973),\n",
       " (('before', 'stopword'), 0.21012906730175018),\n",
       " (('we', 'stopword'), -0.20108430087566376),\n",
       " (('ourselves', 'stopword'), -0.19850486516952515),\n",
       " (('there', 'stopword'), -0.19686132669448853),\n",
       " (('other', 'stopword'), 0.19673803448677063),\n",
       " (('some', 'stopword'), 0.18504102528095245),\n",
       " (('so', 'stopword'), 0.18480679392814636),\n",
       " (('here', 'stopword'), -0.18447595834732056),\n",
       " (('what', 'stopword'), -0.1844455450773239),\n",
       " (('does', 'stopword'), 0.17731575667858124),\n",
       " (('do', 'stopword'), 0.17503418028354645),\n",
       " (('won', 'stopword'), 0.17335145175457),\n",
       " (('whom', 'stopword'), -0.1720760464668274),\n",
       " (('are', 'stopword'), -0.16836398839950562),\n",
       " (('that', 'stopword'), -0.1671752631664276),\n",
       " (('to', 'stopword'), -0.16634982824325562),\n",
       " (('how', 'stopword'), -0.16420386731624603),\n",
       " (('out', 'stopword'), 0.16412632167339325),\n",
       " (('theirs', 'stopword'), -0.16323165595531464),\n",
       " (('s', 'stopword'), -0.16315224766731262),\n",
       " (('both', 'stopword'), -0.16060587763786316),\n",
       " (('who', 'stopword'), -0.1551540493965149),\n",
       " (('and', 'stopword'), 0.15454809367656708),\n",
       " (('not', 'stopword'), -0.14919698238372803),\n",
       " (('same', 'stopword'), 0.1489904373884201),\n",
       " (('him', 'stopword'), 0.14691443741321564),\n",
       " (('weren', 'stopword'), -0.14680826663970947),\n",
       " (('an', 'stopword'), -0.14651371538639069),\n",
       " (('have', 'stopword'), 0.14613273739814758),\n",
       " (('between', 'stopword'), -0.14189879596233368),\n",
       " (('on', 'stopword'), -0.13969027996063232),\n",
       " (('yourself', 'stopword'), 0.13830257952213287),\n",
       " (('didn', 'stopword'), -0.13764648139476776),\n",
       " (('ll', 'stopword'), 0.1372218132019043),\n",
       " (('they', 'stopword'), 0.13642309606075287),\n",
       " (('because', 'stopword'), 0.13503842055797577),\n",
       " (('hers', 'stopword'), -0.13339081406593323),\n",
       " (('into', 'stopword'), 0.12987807393074036),\n",
       " (('very', 'stopword'), 0.12931254506111145),\n",
       " (('m', 'stopword'), 0.12383969873189926),\n",
       " (('you', 'stopword'), -0.1210186555981636),\n",
       " (('them', 'stopword'), 0.11946829408407211),\n",
       " (('each', 'stopword'), -0.11706925928592682),\n",
       " (('just', 'stopword'), 0.1131720021367073),\n",
       " (('this', 'stopword'), -0.11145927011966705),\n",
       " (('the', 'stopword'), -0.10931143164634705),\n",
       " (('more', 'stopword'), -0.10882273316383362),\n",
       " (('over', 'stopword'), -0.10315610468387604),\n",
       " (('should', 'stopword'), 0.10081365704536438),\n",
       " (('yourselves', 'stopword'), -0.0988498404622078),\n",
       " (('doing', 'stopword'), -0.09709588438272476),\n",
       " (('she', 'stopword'), -0.0968303382396698),\n",
       " (('ours', 'stopword'), -0.09657665342092514),\n",
       " (('only', 'stopword'), 0.09469287842512131),\n",
       " (('now', 'stopword'), 0.08953463286161423),\n",
       " (('nor', 'stopword'), -0.08595557510852814),\n",
       " (('hadn', 'stopword'), 0.08587336540222168),\n",
       " (('through', 'stopword'), -0.08433729410171509),\n",
       " (('under', 'stopword'), -0.08411584794521332),\n",
       " (('aren', 'stopword'), 0.08257634937763214),\n",
       " (('own', 'stopword'), -0.07760537415742874),\n",
       " (('was', 'stopword'), -0.07583646476268768),\n",
       " (('at', 'stopword'), -0.07479090988636017),\n",
       " (('these', 'stopword'), -0.07382287830114365),\n",
       " (('most', 'stopword'), -0.07372624427080154),\n",
       " (('than', 'stopword'), 0.0693732500076294),\n",
       " (('why', 'stopword'), 0.06750459969043732),\n",
       " (('it', 'stopword'), 0.06707317382097244),\n",
       " (('ve', 'stopword'), 0.06469342112541199),\n",
       " (('wasn', 'stopword'), -0.06450213491916656),\n",
       " (('while', 'stopword'), 0.06333070993423462),\n",
       " (('can', 'stopword'), 0.058743420988321304),\n",
       " (('until', 'stopword'), -0.058417029678821564),\n",
       " (('o', 'stopword'), 0.05738965421915054),\n",
       " (('of', 'stopword'), -0.05538037419319153),\n",
       " (('further', 'stopword'), -0.051090940833091736),\n",
       " (('t', 'stopword'), 0.04609139263629913),\n",
       " (('its', 'stopword'), -0.042768996208906174),\n",
       " (('your', 'stopword'), 0.03739121928811073),\n",
       " (('his', 'stopword'), -0.034853242337703705),\n",
       " (('down', 'stopword'), 0.03455885127186775),\n",
       " (('did', 'stopword'), 0.03330613672733307),\n",
       " (('is', 'stopword'), 0.032258592545986176),\n",
       " (('about', 'stopword'), 0.02921261638402939),\n",
       " (('if', 'stopword'), -0.028307614848017693),\n",
       " (('again', 'stopword'), -0.022284070029854774),\n",
       " (('d', 'stopword'), -0.022185973823070526),\n",
       " (('then', 'stopword'), 0.021076273173093796),\n",
       " (('when', 'stopword'), 0.02044433355331421),\n",
       " (('up', 'stopword'), -0.019743449985980988),\n",
       " (('after', 'stopword'), -0.019444486126303673),\n",
       " (('be', 'stopword'), 0.01701318472623825),\n",
       " (('with', 'stopword'), 0.013119727373123169),\n",
       " (('were', 'stopword'), 0.012461096048355103),\n",
       " (('all', 'stopword'), -0.01203831098973751),\n",
       " (('needn', 'stopword'), -0.008970396593213081),\n",
       " (('her', 'stopword'), 0.0054072244092822075),\n",
       " (('any', 'stopword'), -0.002541568363085389),\n",
       " (('a', 'stopword'), -0.002423858502879739),\n",
       " (('off', 'stopword'), -0.001514786621555686),\n",
       " (('shan', 'stopword'), -0.0011344060767441988),\n",
       " (('for', 'stopword'), 0.00031475667492486537),\n",
       " ((\"wouldn't\", 'stopword'), 5.176468636734443e-32),\n",
       " ((\"shan't\", 'stopword'), -5.095613433307483e-32),\n",
       " ((\"should've\", 'stopword'), -4.964383594970385e-32),\n",
       " ((\"weren't\", 'stopword'), 4.772837965998244e-32),\n",
       " ((\"won't\", 'stopword'), -4.63614354829776e-32),\n",
       " ((\"that'll\", 'stopword'), 4.584724780530504e-32),\n",
       " ((\"it's\", 'stopword'), -4.3887857414221647e-32),\n",
       " ((\"you'd\", 'stopword'), -4.3158663003576057e-32),\n",
       " ((\"couldn't\", 'stopword'), -4.2802335401011297e-32),\n",
       " ((\"hasn't\", 'stopword'), -4.041534424613691e-32),\n",
       " (('mightn', 'stopword'), 3.896663855956063e-32),\n",
       " ((\"hadn't\", 'stopword'), 3.5740990960092086e-32),\n",
       " ((\"don't\", 'stopword'), -3.12946688844274e-32),\n",
       " ((\"didn't\", 'stopword'), 3.1185759332823714e-32),\n",
       " ((\"shouldn't\", 'stopword'), -3.057398799161764e-32),\n",
       " ((\"wasn't\", 'stopword'), 3.0061598820301835e-32),\n",
       " ((\"mightn't\", 'stopword'), 2.937466930904006e-32),\n",
       " ((\"mustn't\", 'stopword'), 2.8985042890252383e-32),\n",
       " ((\"doesn't\", 'stopword'), 2.7697121320981575e-32),\n",
       " ((\"you're\", 'stopword'), 2.1849133903924637e-32),\n",
       " ((\"needn't\", 'stopword'), 1.6041142682154428e-32),\n",
       " ((\"you've\", 'stopword'), -1.4975576154917225e-32),\n",
       " ((\"you'll\", 'stopword'), 1.1830921585588274e-32),\n",
       " ((\"aren't\", 'stopword'), 1.0727144879793623e-32),\n",
       " ((\"isn't\", 'stopword'), 5.661518013502029e-33),\n",
       " (('mustn', 'stopword'), -3.381039136439621e-33),\n",
       " ((\"haven't\", 'stopword'), 1.1777872994903602e-33),\n",
       " ((\"she's\", 'stopword'), -2.1108939804891228e-35)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [\n",
    "#     (e, 'nrc') for e in emotions] + ['pers_pronouns'] + [\n",
    "#     (c, 'liwc') for c in list(categories) if c in writings_df.columns] + [\n",
    "(st, 'stopword') for st in stopword_list]\n",
    "weights = model.get_layer('output_layer').get_weights()[0].tolist()[-(len(features)):]\n",
    "\n",
    "print(len(weights), len(features))\n",
    "feature_importance = {}\n",
    "for (i, f) in enumerate(features):\n",
    "    feature_importance[f] = weights[i][0]\n",
    "\n",
    "sorted(feature_importance.items(), key=lambda t: abs(t[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_layer('output_layer').get_weights()[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/lstm_plus_ablated_user_anorexia')#, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5631"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5644"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "voc2=pickle.load(open('vocabulary20K_selfharm.pkl', 'rb'))\n",
    "voc2['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(predictions.flatten()).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([327])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(predictions>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([488])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(predictions<0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_point(subject, voc, hyperparams_features=hyperparams_features, nrc_lexicon=nrc_lexicon,\n",
    "                      emotions=emotions):\n",
    "    eval_writings_df = writings_df[writings_df['subject']==subject]\n",
    "    correct_label = eval_writings_df.label.values[0]\n",
    "    (x_train, y_train), (x_valid, y_valid), (x_test, y_test), voc = load_erisk_data(eval_writings_df,\n",
    "                        seq_len=hyperparams_features['maxlen'],\n",
    "                        voc_size=hyperparams_features['max_features'],\n",
    "                        emotion_lexicon=nrc_lexicon,\n",
    "                        emotions=emotions, user_level=False,\n",
    "                        train_prop=0.0, vocabulary=voc)\n",
    "    return x_test, y_test, correct_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_per_user(writings_df, majority_prop=0.2, train_prop=0.7, majority_nr=0, validate=False, voc=None,\n",
    "                    test_slice=2, nr_slices=5, random=False):\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    thresh=0.5\n",
    "    majority_proportion=majority_prop\n",
    "    valid_prop = 0.3\n",
    "    \n",
    "    if 'subset' in writings_df.columns:\n",
    "        training_subjects = list(set(writings_df[writings_df['subset']=='train'].subject))\n",
    "        test_subjects = list(set(writings_df[writings_df['subset']=='test'].subject))\n",
    "    else:\n",
    "        all_subjects = sorted(list(set(writings_df.subject)))\n",
    "        training_subjects_size = int(len(all_subjects) * train_prop)\n",
    "        test_subjects_size = len(all_subjects) - training_subjects_size\n",
    "        # Cross-validation, with fixed slice as input\n",
    "        test_prop = 1-train_prop\n",
    "        test_slice = min(test_slice, nr_slices)\n",
    "        logger.debug(\"start index: %f, from %f\\n\" % (\n",
    "            len(all_subjects)*(1/nr_slices)*test_slice, test_prop*test_slice))\n",
    "        start_slice = int(len(all_subjects)*(1/nr_slices)*test_slice)\n",
    "        test_subjects = all_subjects[start_slice: start_slice+test_subjects_size]\n",
    "        training_subjects = [s for s in all_subjects if s not in test_subjects]\n",
    "    training_subjects = sorted(training_subjects) # ensuring reproducibility\n",
    "    valid_subjects_size = int(len(training_subjects) * valid_prop)\n",
    "    valid_subjects = training_subjects[:valid_subjects_size]\n",
    "    training_subjects = training_subjects[valid_subjects_size:]\n",
    "    \n",
    "    if validate:\n",
    "        subjects = valid_subjects\n",
    "    else:\n",
    "        subjects = test_subjects\n",
    "    for subject in subjects:\n",
    "        x_test_user, y_test_user, label = get_data_for_point(subject, voc=voc)\n",
    "        outputs = model.predict(x_test_user)\n",
    "        if random:\n",
    "            sigma = np.std(outputs)\n",
    "            mu = np.mean(outputs)\n",
    "            print(\"generating random outputs with sigma\", sigma, \"and mu\", mu)\n",
    "            outputs = sigma*np.random.randn(len(outputs))+mu\n",
    "        positive_pred = sum(outputs>=thresh)\n",
    "        negative_pred = sum(outputs<thresh)\n",
    "        majority_pred = 0\n",
    "        if majority_proportion and positive_pred >= majority_proportion*negative_pred:\n",
    "            majority_pred = 1\n",
    "        if majority_nr and positive_pred>=majority_nr:\n",
    "            majority_pred = 1\n",
    "        if label == 1:\n",
    "            if majority_pred == 1:\n",
    "                tp+=1\n",
    "            else:\n",
    "                fn+=1\n",
    "        else:\n",
    "            if majority_pred == 0:\n",
    "                tn+=1\n",
    "            else:\n",
    "                fp+=1\n",
    "        print(negative_pred, positive_pred, majority_pred)\n",
    "        all_predictions.append(majority_pred)\n",
    "        all_labels.append(label)\n",
    "    def prec_recall_f1(tp, fp, tn, fn):\n",
    "        recall = tp/(tp+fn+0.0000001)\n",
    "        precision = tp/(tp+fp+0.0000001)\n",
    "        f1 = 2*precision*recall/(precision+recall+0.0000001)\n",
    "        print(\"Recall\", recall, \"Precision\", precision, \"F1\", f1)\n",
    "    if majority_prop:\n",
    "        print(\"Vote proportion\", majority_prop)\n",
    "    if majority_nr:\n",
    "        print(\"Vote points\", majority_nr)\n",
    "    prec_recall_f1(tp, fp, tn, fn)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[417] [22] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[78] [7] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[496] [7] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1285] [62] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[172] [14] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[112] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1231] [27] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1866] [10] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[282] [10] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[113] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[478] [2] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[9] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[155] [3] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[197] [2] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[989] [33] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[872] [12] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[105] [2] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[70] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[42] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[688] [37] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[965] [14] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[168] [4] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[321] [7] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[100] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[379] [21] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[31] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1099] [16] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[363] [3] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[87] [9] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[19] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[160] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[62] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1181] [5] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[25] [2] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[160] [6] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[562] [23] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1188] [50] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[285] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[14] [2] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[962] [8] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[935] [13] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[391] [3] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[22] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[848] [42] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1356] [24] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1165] [7] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[25] [4] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[19] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[46] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[14] [2] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[52] [4] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1124] [29] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[289] [20] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[119] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[237] [41] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[261] [7] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[41] [14] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[27] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1132] [16] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[569] [13] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[440] [31] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[48] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[51] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[97] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1079] [12] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[239] [5] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[33] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[314] [2] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[918] [7] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1375] [31] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[322] [9] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1586] [13] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[25] [4] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[59] [46] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[407] [13] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[51] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[481] [8] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[36] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[131] [4] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[143] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[34] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[37] [2] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[491] [39] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1539] [23] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[32] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[22] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[93] [2] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[34] [12] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[139] [2] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[12] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[53] [2] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[35] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[120] [7] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[800] [18] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[98] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[243] [7] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[122] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[90] [4] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[72] [76] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[282] [7] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1039] [24] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[587] [4] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[95] [5] 0\n",
      "Vote proportion 0.2\n",
      "Recall 0.23529411626297578 Precision 0.9999999750000006 F1 0.3809523464852629\n"
     ]
    }
   ],
   "source": [
    "predict_per_user(writings_df=writings_df, voc=voc, majority_prop=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_per_slice = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "103/103 [==============================] - 0s 48us/sample - loss: 0.4739 - f1_m: 0.3333 - precision_m: 0.3000 - recall_m: 0.5000     \n",
      "Results for slice 0: [0.47392676351139845, 0.33333328, 0.29999998, 0.49999994]\n",
      "\n",
      "Train...\n",
      "103/103 [==============================] - 0s 54us/sample - loss: 0.4942 - f1_m: 0.3576 - precision_m: 0.3806 - recall_m: 0.5250     \n",
      "Results for slice 1: [0.49420061707496643, 0.3575757, 0.3805555, 0.525]\n",
      "\n",
      "Train...\n",
      "103/103 [==============================] - 0s 216us/sample - loss: 0.4024 - f1_m: 0.6373 - precision_m: 0.5214 - recall_m: 0.9500\n",
      "Results for slice 2: [0.4023708821211046, 0.6373015, 0.5214286, 0.9499999]\n",
      "\n",
      "Train...\n",
      "103/103 [==============================] - 0s 148us/sample - loss: 0.3093 - f1_m: 0.8264 - precision_m: 0.7250 - recall_m: 1.0000\n",
      "Results for slice 3: [0.3093455121355149, 0.82638884, 0.725, 1.0]\n",
      "\n",
      "Train...\n",
      "68/68 [==============================] - 0s 130us/sample - loss: 0.3460 - f1_m: 0.8611 - precision_m: 0.7714 - recall_m: 1.0000\n",
      "Results for slice 4: [0.34599672871477466, 0.8611111, 0.7714286, 1.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nr_slices=5\n",
    "logger.setLevel(logging.INFO)\n",
    "for tslice in range(nr_slices): \n",
    "    (x_train, y_train), (x_valid, y_valid), (x_test, y_test), voc = load_erisk_data(writings_df, \n",
    "                                                                seq_len=hyperparams_features['maxlen'],\n",
    "                                                                voc_size=hyperparams_features['max_features'],\n",
    "                                                               emotion_lexicon=nrc_lexicon,\n",
    "                                                               emotions=emotions,\n",
    "                                                               user_level=hyperparams_features['user_level'],\n",
    "                                                                                    test_slice=tslice,\n",
    "                                                                                    nr_slices=nr_slices,\n",
    "    #                                                            vocabulary=pickle.load(open('vocabulary20K_selfharm.pkl', 'rb'))\n",
    "                                                                                   logger=logger)\n",
    "    model, history = train_model(model, x_train, y_train, x_valid, y_valid,\n",
    "           epochs=200, batch_size=hyperparams['batch_size'],\n",
    "                      class_weight={0:0.5, 1:5}, start_epoch=0,\n",
    "                      callback_list = [freeze_layer, weights_history, reduce_lr],\n",
    "                      workers=2, verbose=0)\n",
    "    results_per_slice[tslice] = model.evaluate(x_test, y_test)\n",
    "    logger.info(\"Results for slice %d: %s\\n\" % (tslice, results_per_slice[tslice]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 score:  0.60314214 all F1 scores:  {0: 0.33333328, 1: 0.3575757, 2: 0.6373015, 3: 0.82638884, 4: 0.8611111}\n"
     ]
    }
   ],
   "source": [
    "print(\"Average F1 score: \", np.array([results_per_slice[s][1] for s in results_per_slice.keys()]).mean(),\n",
    "     \"all F1 scores: \", {s: v[1] for (s,v) in results_per_slice.items()} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tokens(row):\n",
    "    tokens = []\n",
    "    if row.tokenized_text:\n",
    "        tokens += row.tokenized_text\n",
    "    if row.tokenized_title:\n",
    "        tokens += row.tokenized_title\n",
    "    return tokens\n",
    "writings_df['all_tokens'] = writings_df.apply (lambda row: merge_tokens(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: include the title\n",
    "def extract_emotions(tokens, emotion, relative=True):\n",
    "    if not tokens:\n",
    "        return None\n",
    "    emotion_words = [t for t in tokens \n",
    "                     if t in nrc_lexicon[emotion]]\n",
    "    if relative:\n",
    "        return len(emotion_words) / len(tokens)\n",
    "    else:\n",
    "        return len(emotion_words)\n",
    "    \n",
    "    return encoded_emotions\n",
    "\n",
    "from functools import partial\n",
    "for emotion in emotions:\n",
    "    writings_df[emotion] = writings_df['all_tokens'].apply(partial(extract_emotions, emotion=emotion, \n",
    "                                                                   relative=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['pronouns'] = writings_df['all_tokens'].apply(partial(encode_pronouns, relative=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>text_len</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.104269</td>\n",
       "      <td>0.011986</td>\n",
       "      <td>0.020197</td>\n",
       "      <td>0.031982</td>\n",
       "      <td>0.031271</td>\n",
       "      <td>0.019335</td>\n",
       "      <td>0.040782</td>\n",
       "      <td>0.023853</td>\n",
       "      <td>0.023621</td>\n",
       "      <td>0.032969</td>\n",
       "      <td>0.020421</td>\n",
       "      <td>0.023590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pronouns</th>\n",
       "      <td>0.104269</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.636745</td>\n",
       "      <td>0.449384</td>\n",
       "      <td>0.567496</td>\n",
       "      <td>0.452098</td>\n",
       "      <td>0.464899</td>\n",
       "      <td>0.548570</td>\n",
       "      <td>0.513029</td>\n",
       "      <td>0.571303</td>\n",
       "      <td>0.524614</td>\n",
       "      <td>0.461328</td>\n",
       "      <td>0.538335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_len</th>\n",
       "      <td>0.011986</td>\n",
       "      <td>0.636745</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.708853</td>\n",
       "      <td>0.791715</td>\n",
       "      <td>0.642980</td>\n",
       "      <td>0.738146</td>\n",
       "      <td>0.728836</td>\n",
       "      <td>0.823974</td>\n",
       "      <td>0.867609</td>\n",
       "      <td>0.723653</td>\n",
       "      <td>0.650420</td>\n",
       "      <td>0.834939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.020197</td>\n",
       "      <td>0.449384</td>\n",
       "      <td>0.708853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.643459</td>\n",
       "      <td>0.762591</td>\n",
       "      <td>0.858442</td>\n",
       "      <td>0.564162</td>\n",
       "      <td>0.835345</td>\n",
       "      <td>0.681573</td>\n",
       "      <td>0.774846</td>\n",
       "      <td>0.583704</td>\n",
       "      <td>0.671042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anticipation</th>\n",
       "      <td>0.031982</td>\n",
       "      <td>0.567496</td>\n",
       "      <td>0.791715</td>\n",
       "      <td>0.643459</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.573916</td>\n",
       "      <td>0.668326</td>\n",
       "      <td>0.834784</td>\n",
       "      <td>0.684882</td>\n",
       "      <td>0.849864</td>\n",
       "      <td>0.668269</td>\n",
       "      <td>0.727331</td>\n",
       "      <td>0.818885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.031271</td>\n",
       "      <td>0.452098</td>\n",
       "      <td>0.642980</td>\n",
       "      <td>0.762591</td>\n",
       "      <td>0.573916</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.729799</td>\n",
       "      <td>0.526733</td>\n",
       "      <td>0.765865</td>\n",
       "      <td>0.603013</td>\n",
       "      <td>0.737717</td>\n",
       "      <td>0.540439</td>\n",
       "      <td>0.589641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.019335</td>\n",
       "      <td>0.464899</td>\n",
       "      <td>0.738146</td>\n",
       "      <td>0.858442</td>\n",
       "      <td>0.668326</td>\n",
       "      <td>0.729799</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.570632</td>\n",
       "      <td>0.862778</td>\n",
       "      <td>0.706676</td>\n",
       "      <td>0.824782</td>\n",
       "      <td>0.569688</td>\n",
       "      <td>0.687232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.040782</td>\n",
       "      <td>0.548570</td>\n",
       "      <td>0.728836</td>\n",
       "      <td>0.564162</td>\n",
       "      <td>0.834784</td>\n",
       "      <td>0.526733</td>\n",
       "      <td>0.570632</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.604964</td>\n",
       "      <td>0.850961</td>\n",
       "      <td>0.603296</td>\n",
       "      <td>0.722710</td>\n",
       "      <td>0.811529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0.023853</td>\n",
       "      <td>0.513029</td>\n",
       "      <td>0.823974</td>\n",
       "      <td>0.835345</td>\n",
       "      <td>0.684882</td>\n",
       "      <td>0.765865</td>\n",
       "      <td>0.862778</td>\n",
       "      <td>0.604964</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.735431</td>\n",
       "      <td>0.840379</td>\n",
       "      <td>0.597634</td>\n",
       "      <td>0.706808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>0.023621</td>\n",
       "      <td>0.571303</td>\n",
       "      <td>0.867609</td>\n",
       "      <td>0.681573</td>\n",
       "      <td>0.849864</td>\n",
       "      <td>0.603013</td>\n",
       "      <td>0.706676</td>\n",
       "      <td>0.850961</td>\n",
       "      <td>0.735431</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.702751</td>\n",
       "      <td>0.678778</td>\n",
       "      <td>0.916526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.032969</td>\n",
       "      <td>0.524614</td>\n",
       "      <td>0.723653</td>\n",
       "      <td>0.774846</td>\n",
       "      <td>0.668269</td>\n",
       "      <td>0.737717</td>\n",
       "      <td>0.824782</td>\n",
       "      <td>0.603296</td>\n",
       "      <td>0.840379</td>\n",
       "      <td>0.702751</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.584816</td>\n",
       "      <td>0.665750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.020421</td>\n",
       "      <td>0.461328</td>\n",
       "      <td>0.650420</td>\n",
       "      <td>0.583704</td>\n",
       "      <td>0.727331</td>\n",
       "      <td>0.540439</td>\n",
       "      <td>0.569688</td>\n",
       "      <td>0.722710</td>\n",
       "      <td>0.597634</td>\n",
       "      <td>0.678778</td>\n",
       "      <td>0.584816</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.660681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust</th>\n",
       "      <td>0.023590</td>\n",
       "      <td>0.538335</td>\n",
       "      <td>0.834939</td>\n",
       "      <td>0.671042</td>\n",
       "      <td>0.818885</td>\n",
       "      <td>0.589641</td>\n",
       "      <td>0.687232</td>\n",
       "      <td>0.811529</td>\n",
       "      <td>0.706808</td>\n",
       "      <td>0.916526</td>\n",
       "      <td>0.665750</td>\n",
       "      <td>0.660681</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 label  pronouns  text_len     anger  anticipation   disgust  \\\n",
       "label         1.000000  0.104269  0.011986  0.020197      0.031982  0.031271   \n",
       "pronouns      0.104269  1.000000  0.636745  0.449384      0.567496  0.452098   \n",
       "text_len      0.011986  0.636745  1.000000  0.708853      0.791715  0.642980   \n",
       "anger         0.020197  0.449384  0.708853  1.000000      0.643459  0.762591   \n",
       "anticipation  0.031982  0.567496  0.791715  0.643459      1.000000  0.573916   \n",
       "disgust       0.031271  0.452098  0.642980  0.762591      0.573916  1.000000   \n",
       "fear          0.019335  0.464899  0.738146  0.858442      0.668326  0.729799   \n",
       "joy           0.040782  0.548570  0.728836  0.564162      0.834784  0.526733   \n",
       "negative      0.023853  0.513029  0.823974  0.835345      0.684882  0.765865   \n",
       "positive      0.023621  0.571303  0.867609  0.681573      0.849864  0.603013   \n",
       "sadness       0.032969  0.524614  0.723653  0.774846      0.668269  0.737717   \n",
       "surprise      0.020421  0.461328  0.650420  0.583704      0.727331  0.540439   \n",
       "trust         0.023590  0.538335  0.834939  0.671042      0.818885  0.589641   \n",
       "\n",
       "                  fear       joy  negative  positive   sadness  surprise  \\\n",
       "label         0.019335  0.040782  0.023853  0.023621  0.032969  0.020421   \n",
       "pronouns      0.464899  0.548570  0.513029  0.571303  0.524614  0.461328   \n",
       "text_len      0.738146  0.728836  0.823974  0.867609  0.723653  0.650420   \n",
       "anger         0.858442  0.564162  0.835345  0.681573  0.774846  0.583704   \n",
       "anticipation  0.668326  0.834784  0.684882  0.849864  0.668269  0.727331   \n",
       "disgust       0.729799  0.526733  0.765865  0.603013  0.737717  0.540439   \n",
       "fear          1.000000  0.570632  0.862778  0.706676  0.824782  0.569688   \n",
       "joy           0.570632  1.000000  0.604964  0.850961  0.603296  0.722710   \n",
       "negative      0.862778  0.604964  1.000000  0.735431  0.840379  0.597634   \n",
       "positive      0.706676  0.850961  0.735431  1.000000  0.702751  0.678778   \n",
       "sadness       0.824782  0.603296  0.840379  0.702751  1.000000  0.584816   \n",
       "surprise      0.569688  0.722710  0.597634  0.678778  0.584816  1.000000   \n",
       "trust         0.687232  0.811529  0.706808  0.916526  0.665750  0.660681   \n",
       "\n",
       "                 trust  \n",
       "label         0.023590  \n",
       "pronouns      0.538335  \n",
       "text_len      0.834939  \n",
       "anger         0.671042  \n",
       "anticipation  0.818885  \n",
       "disgust       0.589641  \n",
       "fear          0.687232  \n",
       "joy           0.811529  \n",
       "negative      0.706808  \n",
       "positive      0.916526  \n",
       "sadness       0.665750  \n",
       "surprise      0.660681  \n",
       "trust         1.000000  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len'] + emotions].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pronouns</th>\n",
       "      <th>text_len</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.868213</td>\n",
       "      <td>32.031615</td>\n",
       "      <td>0.386069</td>\n",
       "      <td>0.58984</td>\n",
       "      <td>0.263683</td>\n",
       "      <td>0.478014</td>\n",
       "      <td>0.479908</td>\n",
       "      <td>0.818800</td>\n",
       "      <td>1.280788</td>\n",
       "      <td>0.385315</td>\n",
       "      <td>0.284790</td>\n",
       "      <td>0.830560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.484271</td>\n",
       "      <td>36.398389</td>\n",
       "      <td>0.529232</td>\n",
       "      <td>0.86985</td>\n",
       "      <td>0.416203</td>\n",
       "      <td>0.654371</td>\n",
       "      <td>0.769766</td>\n",
       "      <td>1.152422</td>\n",
       "      <td>1.717428</td>\n",
       "      <td>0.627088</td>\n",
       "      <td>0.375418</td>\n",
       "      <td>1.128341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pronouns   text_len     anger  anticipation   disgust      fear  \\\n",
       "label                                                                    \n",
       "0      0.868213  32.031615  0.386069       0.58984  0.263683  0.478014   \n",
       "1      2.484271  36.398389  0.529232       0.86985  0.416203  0.654371   \n",
       "\n",
       "            joy  negative  positive   sadness  surprise     trust  \n",
       "label                                                              \n",
       "0      0.479908  0.818800  1.280788  0.385315  0.284790  0.830560  \n",
       "1      0.769766  1.152422  1.717428  0.627088  0.375418  1.128341  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len'] + emotions].groupby('label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentAnalyzer, SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.66, 'pos': 0.34, 'compound': 0.5574}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid.polarity_scores(\"We are here today happiness is all around\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['neg_vader'] = writings_df.text.apply(lambda t: sid.polarity_scores(t)['neg']\n",
    "                                                 if type(t)==str else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>title_len</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>text_len</th>\n",
       "      <th>emotions</th>\n",
       "      <th>...</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>all_tokens</th>\n",
       "      <th>neg_vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>If anyone could help with which sub to put thi...</td>\n",
       "      <td>2016-08-02 09:22:12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[if, anyone, could, help, with, which, sub, to...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[if, anyone, could, help, with, which, sub, to...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>I'm literally never gonna stop waiting...</td>\n",
       "      <td>2016-08-05 09:35:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, m, literally, never, gonna, stop, waiting]</td>\n",
       "      <td>7.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[i, m, literally, never, gonna, stop, waiting]</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>This is a really interesting study! Makes sens...</td>\n",
       "      <td>2016-08-05 21:36:24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[this, is, a, really, interesting, study, make...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[this, is, a, really, interesting, study, make...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>The only thing Frank is building ...</td>\n",
       "      <td>2016-08-07 23:35:23</td>\n",
       "      <td>... Is hype. Think about it, every time he wor...</td>\n",
       "      <td>0</td>\n",
       "      <td>[the, only, thing, frank, is, building]</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[is, hype, think, about, it, every, time, he, ...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[is, hype, think, about, it, every, time, he, ...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>Mostly always me during this whole charade</td>\n",
       "      <td>2016-08-09 08:39:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[mostly, always, me, during, this, whole, char...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[mostly, always, me, during, this, whole, char...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170693</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 11:29:21</td>\n",
       "      <td>this is my personal experience ,it may not ref...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[this, is, my, personal, experience, it, may, ...</td>\n",
       "      <td>153.0</td>\n",
       "      <td>0.026144</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[this, is, my, personal, experience, it, may, ...</td>\n",
       "      <td>0.089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170694</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 16:17:34</td>\n",
       "      <td>stop looking at 20 million saudis as one entit...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[stop, looking, at, 20, million, saudis, as, o...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[stop, looking, at, 20, million, saudis, as, o...</td>\n",
       "      <td>0.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170695</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 20:00:31</td>\n",
       "      <td>i am aware of stats now and then. i was just s...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[i, am, aware, of, stats, now, and, then, i, w...</td>\n",
       "      <td>198.0</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>[i, am, aware, of, stats, now, and, then, i, w...</td>\n",
       "      <td>0.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170696</th>\n",
       "      <td>subject217</td>\n",
       "      <td>WHAT DID YOU SAY TO ME?</td>\n",
       "      <td>2018-08-20 10:54:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[what, did, you, say, to, me]</td>\n",
       "      <td>6.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[what, did, you, say, to, me]</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170697</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-20 12:07:44</td>\n",
       "      <td>me smellz fish,me find no fish!...what the fuc...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[me, smellz, fish, me, find, no, fish, what, t...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[me, smellz, fish, me, find, no, fish, what, t...</td>\n",
       "      <td>0.484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170698 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            subject                                              title  \\\n",
       "0       subject8292  If anyone could help with which sub to put thi...   \n",
       "1       subject8292          I'm literally never gonna stop waiting...   \n",
       "2       subject8292  This is a really interesting study! Makes sens...   \n",
       "3       subject8292               The only thing Frank is building ...   \n",
       "4       subject8292         Mostly always me during this whole charade   \n",
       "...             ...                                                ...   \n",
       "170693   subject217                                                NaN   \n",
       "170694   subject217                                                NaN   \n",
       "170695   subject217                                                NaN   \n",
       "170696   subject217                            WHAT DID YOU SAY TO ME?   \n",
       "170697   subject217                                                NaN   \n",
       "\n",
       "                       date  \\\n",
       "0       2016-08-02 09:22:12   \n",
       "1       2016-08-05 09:35:55   \n",
       "2       2016-08-05 21:36:24   \n",
       "3       2016-08-07 23:35:23   \n",
       "4       2016-08-09 08:39:41   \n",
       "...                     ...   \n",
       "170693  2018-08-19 11:29:21   \n",
       "170694  2018-08-19 16:17:34   \n",
       "170695  2018-08-19 20:00:31   \n",
       "170696  2018-08-20 10:54:11   \n",
       "170697  2018-08-20 12:07:44   \n",
       "\n",
       "                                                     text  label  \\\n",
       "0                                                     NaN      0   \n",
       "1                                                     NaN      0   \n",
       "2                                                     NaN      0   \n",
       "3       ... Is hype. Think about it, every time he wor...      0   \n",
       "4                                                     NaN      0   \n",
       "...                                                   ...    ...   \n",
       "170693  this is my personal experience ,it may not ref...      0   \n",
       "170694  stop looking at 20 million saudis as one entit...      0   \n",
       "170695  i am aware of stats now and then. i was just s...      0   \n",
       "170696                                                NaN      0   \n",
       "170697  me smellz fish,me find no fish!...what the fuc...      0   \n",
       "\n",
       "                                          tokenized_title  title_len  \\\n",
       "0       [if, anyone, could, help, with, which, sub, to...       11.0   \n",
       "1          [i, m, literally, never, gonna, stop, waiting]        7.0   \n",
       "2       [this, is, a, really, interesting, study, make...        9.0   \n",
       "3                 [the, only, thing, frank, is, building]        6.0   \n",
       "4       [mostly, always, me, during, this, whole, char...        7.0   \n",
       "...                                                   ...        ...   \n",
       "170693                                               None        NaN   \n",
       "170694                                               None        NaN   \n",
       "170695                                               None        NaN   \n",
       "170696                      [what, did, you, say, to, me]        6.0   \n",
       "170697                                               None        NaN   \n",
       "\n",
       "                                           tokenized_text  text_len  emotions  \\\n",
       "0                                                    None       NaN       NaN   \n",
       "1                                                    None       NaN       NaN   \n",
       "2                                                    None       NaN       NaN   \n",
       "3       [is, hype, think, about, it, every, time, he, ...      26.0  0.000000   \n",
       "4                                                    None       NaN       NaN   \n",
       "...                                                   ...       ...       ...   \n",
       "170693  [this, is, my, personal, experience, it, may, ...     153.0  0.026144   \n",
       "170694  [stop, looking, at, 20, million, saudis, as, o...      15.0  0.000000   \n",
       "170695  [i, am, aware, of, stats, now, and, then, i, w...     198.0  0.030303   \n",
       "170696                                               None       NaN       NaN   \n",
       "170697  [me, smellz, fish, me, find, no, fish, what, t...      11.0  0.000000   \n",
       "\n",
       "        ...  fear  joy  negative  positive  sadness  surprise  trust  \\\n",
       "0       ...   0.0  0.0       0.0       0.0      0.0       0.0    0.0   \n",
       "1       ...   0.0  0.0       0.0       0.0      0.0       0.0    0.0   \n",
       "2       ...   0.0  0.0       0.0       3.0      0.0       0.0    0.0   \n",
       "3       ...   0.0  0.0       3.0       3.0      0.0       0.0    1.0   \n",
       "4       ...   0.0  0.0       1.0       0.0      0.0       0.0    0.0   \n",
       "...     ...   ...  ...       ...       ...      ...       ...    ...   \n",
       "170693  ...   1.0  1.0       1.0       7.0      0.0       1.0    4.0   \n",
       "170694  ...   1.0  0.0       1.0       0.0      0.0       0.0    0.0   \n",
       "170695  ...   2.0  3.0       4.0      11.0      3.0       0.0    6.0   \n",
       "170696  ...   0.0  0.0       0.0       0.0      0.0       0.0    0.0   \n",
       "170697  ...   0.0  0.0       0.0       0.0      0.0       0.0    0.0   \n",
       "\n",
       "        pronouns                                         all_tokens  neg_vader  \n",
       "0            0.0  [if, anyone, could, help, with, which, sub, to...      0.000  \n",
       "1            1.0     [i, m, literally, never, gonna, stop, waiting]      0.000  \n",
       "2            0.0  [this, is, a, really, interesting, study, make...      0.000  \n",
       "3            0.0  [is, hype, think, about, it, every, time, he, ...      0.000  \n",
       "4            1.0  [mostly, always, me, during, this, whole, char...      0.000  \n",
       "...          ...                                                ...        ...  \n",
       "170693       4.0  [this, is, my, personal, experience, it, may, ...      0.089  \n",
       "170694       0.0  [stop, looking, at, 20, million, saudis, as, o...      0.145  \n",
       "170695      16.0  [i, am, aware, of, stats, now, and, then, i, w...      0.070  \n",
       "170696       1.0                      [what, did, you, say, to, me]      0.000  \n",
       "170697       2.0  [me, smellz, fish, me, find, no, fish, what, t...      0.484  \n",
       "\n",
       "[170698 rows x 23 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['pos_vader'] = writings_df.text.apply(lambda t: sid.polarity_scores(t)['pos']\n",
    "                                                 if type(t)==str else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pronouns</th>\n",
       "      <th>text_len</th>\n",
       "      <th>neg_vader</th>\n",
       "      <th>pos_vader</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.868213</td>\n",
       "      <td>32.031615</td>\n",
       "      <td>0.054259</td>\n",
       "      <td>0.109981</td>\n",
       "      <td>0.386069</td>\n",
       "      <td>0.58984</td>\n",
       "      <td>0.263683</td>\n",
       "      <td>0.478014</td>\n",
       "      <td>0.479908</td>\n",
       "      <td>0.818800</td>\n",
       "      <td>1.280788</td>\n",
       "      <td>0.385315</td>\n",
       "      <td>0.284790</td>\n",
       "      <td>0.830560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.484271</td>\n",
       "      <td>36.398389</td>\n",
       "      <td>0.079191</td>\n",
       "      <td>0.148154</td>\n",
       "      <td>0.529232</td>\n",
       "      <td>0.86985</td>\n",
       "      <td>0.416203</td>\n",
       "      <td>0.654371</td>\n",
       "      <td>0.769766</td>\n",
       "      <td>1.152422</td>\n",
       "      <td>1.717428</td>\n",
       "      <td>0.627088</td>\n",
       "      <td>0.375418</td>\n",
       "      <td>1.128341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pronouns   text_len  neg_vader  pos_vader     anger  anticipation  \\\n",
       "label                                                                      \n",
       "0      0.868213  32.031615   0.054259   0.109981  0.386069       0.58984   \n",
       "1      2.484271  36.398389   0.079191   0.148154  0.529232       0.86985   \n",
       "\n",
       "        disgust      fear       joy  negative  positive   sadness  surprise  \\\n",
       "label                                                                         \n",
       "0      0.263683  0.478014  0.479908  0.818800  1.280788  0.385315  0.284790   \n",
       "1      0.416203  0.654371  0.769766  1.152422  1.717428  0.627088  0.375418   \n",
       "\n",
       "          trust  \n",
       "label            \n",
       "0      0.830560  \n",
       "1      1.128341  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len', 'neg_vader', 'pos_vader'] + emotions].groupby('label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>text_len</th>\n",
       "      <th>neg_vader</th>\n",
       "      <th>pos_vader</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.097800</td>\n",
       "      <td>0.033477</td>\n",
       "      <td>0.067170</td>\n",
       "      <td>0.065211</td>\n",
       "      <td>0.022057</td>\n",
       "      <td>0.025666</td>\n",
       "      <td>0.030664</td>\n",
       "      <td>0.019114</td>\n",
       "      <td>0.033977</td>\n",
       "      <td>0.022934</td>\n",
       "      <td>0.019590</td>\n",
       "      <td>0.032641</td>\n",
       "      <td>0.018109</td>\n",
       "      <td>0.024014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pronouns</th>\n",
       "      <td>0.097800</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.332071</td>\n",
       "      <td>0.193938</td>\n",
       "      <td>0.221419</td>\n",
       "      <td>0.076345</td>\n",
       "      <td>0.128030</td>\n",
       "      <td>0.094069</td>\n",
       "      <td>0.063176</td>\n",
       "      <td>0.144011</td>\n",
       "      <td>0.076670</td>\n",
       "      <td>0.106055</td>\n",
       "      <td>0.100827</td>\n",
       "      <td>0.106790</td>\n",
       "      <td>0.122914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_len</th>\n",
       "      <td>0.033477</td>\n",
       "      <td>0.332071</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.343154</td>\n",
       "      <td>0.159673</td>\n",
       "      <td>0.360460</td>\n",
       "      <td>0.386351</td>\n",
       "      <td>0.312393</td>\n",
       "      <td>0.381410</td>\n",
       "      <td>0.339398</td>\n",
       "      <td>0.370250</td>\n",
       "      <td>0.330075</td>\n",
       "      <td>0.384031</td>\n",
       "      <td>0.349498</td>\n",
       "      <td>0.389620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_vader</th>\n",
       "      <td>0.067170</td>\n",
       "      <td>0.193938</td>\n",
       "      <td>0.343154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.169624</td>\n",
       "      <td>0.384510</td>\n",
       "      <td>0.141868</td>\n",
       "      <td>0.362582</td>\n",
       "      <td>0.339245</td>\n",
       "      <td>0.126042</td>\n",
       "      <td>0.431111</td>\n",
       "      <td>0.099767</td>\n",
       "      <td>0.374256</td>\n",
       "      <td>0.159302</td>\n",
       "      <td>0.143060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_vader</th>\n",
       "      <td>0.065211</td>\n",
       "      <td>0.221419</td>\n",
       "      <td>0.159673</td>\n",
       "      <td>0.169624</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.079693</td>\n",
       "      <td>0.225925</td>\n",
       "      <td>0.087309</td>\n",
       "      <td>0.071450</td>\n",
       "      <td>0.323148</td>\n",
       "      <td>0.058266</td>\n",
       "      <td>0.270687</td>\n",
       "      <td>0.095040</td>\n",
       "      <td>0.186243</td>\n",
       "      <td>0.231954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.022057</td>\n",
       "      <td>0.076345</td>\n",
       "      <td>0.360460</td>\n",
       "      <td>0.384510</td>\n",
       "      <td>0.079693</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.196795</td>\n",
       "      <td>0.583864</td>\n",
       "      <td>0.587460</td>\n",
       "      <td>0.157202</td>\n",
       "      <td>0.631708</td>\n",
       "      <td>0.128169</td>\n",
       "      <td>0.528980</td>\n",
       "      <td>0.273195</td>\n",
       "      <td>0.169261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anticipation</th>\n",
       "      <td>0.025666</td>\n",
       "      <td>0.128030</td>\n",
       "      <td>0.386351</td>\n",
       "      <td>0.141868</td>\n",
       "      <td>0.225925</td>\n",
       "      <td>0.196795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.164649</td>\n",
       "      <td>0.241958</td>\n",
       "      <td>0.583107</td>\n",
       "      <td>0.178827</td>\n",
       "      <td>0.452457</td>\n",
       "      <td>0.198972</td>\n",
       "      <td>0.460851</td>\n",
       "      <td>0.469028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.030664</td>\n",
       "      <td>0.094069</td>\n",
       "      <td>0.312393</td>\n",
       "      <td>0.362582</td>\n",
       "      <td>0.087309</td>\n",
       "      <td>0.583864</td>\n",
       "      <td>0.164649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.440376</td>\n",
       "      <td>0.152731</td>\n",
       "      <td>0.552021</td>\n",
       "      <td>0.116588</td>\n",
       "      <td>0.490181</td>\n",
       "      <td>0.232166</td>\n",
       "      <td>0.153723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.019114</td>\n",
       "      <td>0.063176</td>\n",
       "      <td>0.381410</td>\n",
       "      <td>0.339245</td>\n",
       "      <td>0.071450</td>\n",
       "      <td>0.587460</td>\n",
       "      <td>0.241958</td>\n",
       "      <td>0.440376</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.159907</td>\n",
       "      <td>0.576962</td>\n",
       "      <td>0.141985</td>\n",
       "      <td>0.583703</td>\n",
       "      <td>0.248160</td>\n",
       "      <td>0.184240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.033977</td>\n",
       "      <td>0.144011</td>\n",
       "      <td>0.339398</td>\n",
       "      <td>0.126042</td>\n",
       "      <td>0.323148</td>\n",
       "      <td>0.157202</td>\n",
       "      <td>0.583107</td>\n",
       "      <td>0.152731</td>\n",
       "      <td>0.159907</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.113400</td>\n",
       "      <td>0.645827</td>\n",
       "      <td>0.176440</td>\n",
       "      <td>0.477317</td>\n",
       "      <td>0.582920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0.022934</td>\n",
       "      <td>0.076670</td>\n",
       "      <td>0.370250</td>\n",
       "      <td>0.431111</td>\n",
       "      <td>0.058266</td>\n",
       "      <td>0.631708</td>\n",
       "      <td>0.178827</td>\n",
       "      <td>0.552021</td>\n",
       "      <td>0.576962</td>\n",
       "      <td>0.113400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.105821</td>\n",
       "      <td>0.612781</td>\n",
       "      <td>0.226230</td>\n",
       "      <td>0.145220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>0.019590</td>\n",
       "      <td>0.106055</td>\n",
       "      <td>0.330075</td>\n",
       "      <td>0.099767</td>\n",
       "      <td>0.270687</td>\n",
       "      <td>0.128169</td>\n",
       "      <td>0.452457</td>\n",
       "      <td>0.116588</td>\n",
       "      <td>0.141985</td>\n",
       "      <td>0.645827</td>\n",
       "      <td>0.105821</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.139827</td>\n",
       "      <td>0.333998</td>\n",
       "      <td>0.648163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.032641</td>\n",
       "      <td>0.100827</td>\n",
       "      <td>0.384031</td>\n",
       "      <td>0.374256</td>\n",
       "      <td>0.095040</td>\n",
       "      <td>0.528980</td>\n",
       "      <td>0.198972</td>\n",
       "      <td>0.490181</td>\n",
       "      <td>0.583703</td>\n",
       "      <td>0.176440</td>\n",
       "      <td>0.612781</td>\n",
       "      <td>0.139827</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.265026</td>\n",
       "      <td>0.171245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.018109</td>\n",
       "      <td>0.106790</td>\n",
       "      <td>0.349498</td>\n",
       "      <td>0.159302</td>\n",
       "      <td>0.186243</td>\n",
       "      <td>0.273195</td>\n",
       "      <td>0.460851</td>\n",
       "      <td>0.232166</td>\n",
       "      <td>0.248160</td>\n",
       "      <td>0.477317</td>\n",
       "      <td>0.226230</td>\n",
       "      <td>0.333998</td>\n",
       "      <td>0.265026</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.354746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust</th>\n",
       "      <td>0.024014</td>\n",
       "      <td>0.122914</td>\n",
       "      <td>0.389620</td>\n",
       "      <td>0.143060</td>\n",
       "      <td>0.231954</td>\n",
       "      <td>0.169261</td>\n",
       "      <td>0.469028</td>\n",
       "      <td>0.153723</td>\n",
       "      <td>0.184240</td>\n",
       "      <td>0.582920</td>\n",
       "      <td>0.145220</td>\n",
       "      <td>0.648163</td>\n",
       "      <td>0.171245</td>\n",
       "      <td>0.354746</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 label  pronouns  text_len  neg_vader  pos_vader     anger  \\\n",
       "label         1.000000  0.097800  0.033477   0.067170   0.065211  0.022057   \n",
       "pronouns      0.097800  1.000000  0.332071   0.193938   0.221419  0.076345   \n",
       "text_len      0.033477  0.332071  1.000000   0.343154   0.159673  0.360460   \n",
       "neg_vader     0.067170  0.193938  0.343154   1.000000   0.169624  0.384510   \n",
       "pos_vader     0.065211  0.221419  0.159673   0.169624   1.000000  0.079693   \n",
       "anger         0.022057  0.076345  0.360460   0.384510   0.079693  1.000000   \n",
       "anticipation  0.025666  0.128030  0.386351   0.141868   0.225925  0.196795   \n",
       "disgust       0.030664  0.094069  0.312393   0.362582   0.087309  0.583864   \n",
       "fear          0.019114  0.063176  0.381410   0.339245   0.071450  0.587460   \n",
       "joy           0.033977  0.144011  0.339398   0.126042   0.323148  0.157202   \n",
       "negative      0.022934  0.076670  0.370250   0.431111   0.058266  0.631708   \n",
       "positive      0.019590  0.106055  0.330075   0.099767   0.270687  0.128169   \n",
       "sadness       0.032641  0.100827  0.384031   0.374256   0.095040  0.528980   \n",
       "surprise      0.018109  0.106790  0.349498   0.159302   0.186243  0.273195   \n",
       "trust         0.024014  0.122914  0.389620   0.143060   0.231954  0.169261   \n",
       "\n",
       "              anticipation   disgust      fear       joy  negative  positive  \\\n",
       "label             0.025666  0.030664  0.019114  0.033977  0.022934  0.019590   \n",
       "pronouns          0.128030  0.094069  0.063176  0.144011  0.076670  0.106055   \n",
       "text_len          0.386351  0.312393  0.381410  0.339398  0.370250  0.330075   \n",
       "neg_vader         0.141868  0.362582  0.339245  0.126042  0.431111  0.099767   \n",
       "pos_vader         0.225925  0.087309  0.071450  0.323148  0.058266  0.270687   \n",
       "anger             0.196795  0.583864  0.587460  0.157202  0.631708  0.128169   \n",
       "anticipation      1.000000  0.164649  0.241958  0.583107  0.178827  0.452457   \n",
       "disgust           0.164649  1.000000  0.440376  0.152731  0.552021  0.116588   \n",
       "fear              0.241958  0.440376  1.000000  0.159907  0.576962  0.141985   \n",
       "joy               0.583107  0.152731  0.159907  1.000000  0.113400  0.645827   \n",
       "negative          0.178827  0.552021  0.576962  0.113400  1.000000  0.105821   \n",
       "positive          0.452457  0.116588  0.141985  0.645827  0.105821  1.000000   \n",
       "sadness           0.198972  0.490181  0.583703  0.176440  0.612781  0.139827   \n",
       "surprise          0.460851  0.232166  0.248160  0.477317  0.226230  0.333998   \n",
       "trust             0.469028  0.153723  0.184240  0.582920  0.145220  0.648163   \n",
       "\n",
       "               sadness  surprise     trust  \n",
       "label         0.032641  0.018109  0.024014  \n",
       "pronouns      0.100827  0.106790  0.122914  \n",
       "text_len      0.384031  0.349498  0.389620  \n",
       "neg_vader     0.374256  0.159302  0.143060  \n",
       "pos_vader     0.095040  0.186243  0.231954  \n",
       "anger         0.528980  0.273195  0.169261  \n",
       "anticipation  0.198972  0.460851  0.469028  \n",
       "disgust       0.490181  0.232166  0.153723  \n",
       "fear          0.583703  0.248160  0.184240  \n",
       "joy           0.176440  0.477317  0.582920  \n",
       "negative      0.612781  0.226230  0.145220  \n",
       "positive      0.139827  0.333998  0.648163  \n",
       "sadness       1.000000  0.265026  0.171245  \n",
       "surprise      0.265026  1.000000  0.354746  \n",
       "trust         0.171245  0.354746  1.000000  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len', 'neg_vader', 'pos_vader'] + emotions].corr('spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from liwc_readDict import readDict\n",
    "\n",
    "liwc = readDict('/home/ana/resources/FakeOrFact/features/LIWC/LIWC/liwc.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'achieve',\n",
       " 'adverb',\n",
       " 'affect',\n",
       " 'anger',\n",
       " 'anx',\n",
       " 'article',\n",
       " 'assent',\n",
       " 'auxverb',\n",
       " 'bio',\n",
       " 'body',\n",
       " 'cause',\n",
       " 'certain',\n",
       " 'cogmech',\n",
       " 'conj',\n",
       " 'death',\n",
       " 'discrep',\n",
       " 'excl',\n",
       " 'family',\n",
       " 'feel',\n",
       " 'filler',\n",
       " 'friend',\n",
       " 'funct',\n",
       " 'future',\n",
       " 'health',\n",
       " 'hear',\n",
       " 'home',\n",
       " 'humans',\n",
       " 'i',\n",
       " 'incl',\n",
       " 'ingest',\n",
       " 'inhib',\n",
       " 'insight',\n",
       " 'ipron',\n",
       " 'leisure',\n",
       " 'money',\n",
       " 'motion',\n",
       " 'negate',\n",
       " 'negemo',\n",
       " 'nonfl',\n",
       " 'number',\n",
       " 'past',\n",
       " 'percept',\n",
       " 'posemo',\n",
       " 'ppron',\n",
       " 'preps',\n",
       " 'present',\n",
       " 'pronoun',\n",
       " 'quant',\n",
       " 'relativ',\n",
       " 'relig',\n",
       " 'sad',\n",
       " 'see',\n",
       " 'sexual',\n",
       " 'shehe',\n",
       " 'social',\n",
       " 'space',\n",
       " 'swear',\n",
       " 'tentat',\n",
       " 'they',\n",
       " 'time',\n",
       " 'verb',\n",
       " 'we',\n",
       " 'work',\n",
       " 'you'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = [c for (w,c) in liwc]\n",
    "set(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 'funct'],\n",
       " ['a', 'article'],\n",
       " ['abandon*', 'affect'],\n",
       " ['abandon*', 'negemo'],\n",
       " ['abandon*', 'sad'],\n",
       " ['abandon*', 'cogmech'],\n",
       " ['abandon*', 'inhib'],\n",
       " ['abdomen*', 'bio'],\n",
       " ['abdomen*', 'body'],\n",
       " ['abilit*', 'achieve'],\n",
       " ['able*', 'achieve'],\n",
       " ['abortion*', 'bio'],\n",
       " ['abortion*', 'health'],\n",
       " ['abortion*', 'sexual'],\n",
       " ['about', 'funct'],\n",
       " ['about', 'adverb'],\n",
       " ['about', 'preps'],\n",
       " ['above', 'funct'],\n",
       " ['above', 'preps'],\n",
       " ['above', 'space'],\n",
       " ['above', 'relativ'],\n",
       " ['abrupt*', 'time'],\n",
       " ['abrupt*', 'relativ'],\n",
       " ['abs', 'bio'],\n",
       " ['abs', 'body'],\n",
       " ['absent*', 'work'],\n",
       " ['absolute', 'cogmech'],\n",
       " ['absolute', 'certain'],\n",
       " ['absolutely', 'funct'],\n",
       " ['absolutely', 'adverb'],\n",
       " ['absolutely', 'cogmech'],\n",
       " ['absolutely', 'certain'],\n",
       " ['absolutely', 'assent'],\n",
       " ['abstain*', 'cogmech'],\n",
       " ['abstain*', 'inhib'],\n",
       " ['abuse*', 'affect'],\n",
       " ['abuse*', 'negemo'],\n",
       " ['abuse*', 'anger'],\n",
       " ['abusi*', 'affect'],\n",
       " ['abusi*', 'negemo'],\n",
       " ['abusi*', 'anger'],\n",
       " ['academ*', 'work'],\n",
       " ['accept', 'affect'],\n",
       " ['accept', 'posemo'],\n",
       " ['accept', 'cogmech'],\n",
       " ['accept', 'insight'],\n",
       " ['accepta*', 'affect'],\n",
       " ['accepta*', 'posemo'],\n",
       " ['accepta*', 'cogmech'],\n",
       " ['accepta*', 'insight'],\n",
       " ['accepted', 'verb'],\n",
       " ['accepted', 'past'],\n",
       " ['accepted', 'affect'],\n",
       " ['accepted', 'posemo'],\n",
       " ['accepted', 'cogmech'],\n",
       " ['accepted', 'insight'],\n",
       " ['accepting', 'affect'],\n",
       " ['accepting', 'posemo'],\n",
       " ['accepting', 'cogmech'],\n",
       " ['accepting', 'insight'],\n",
       " ['accepts', 'affect'],\n",
       " ['accepts', 'posemo'],\n",
       " ['accepts', 'cogmech'],\n",
       " ['accepts', 'insight'],\n",
       " ['accomplish*', 'work'],\n",
       " ['accomplish*', 'achieve'],\n",
       " ['account*', 'money'],\n",
       " ['accura*', 'cogmech'],\n",
       " ['accura*', 'certain'],\n",
       " ['ace', 'achieve'],\n",
       " ['ache*', 'affect'],\n",
       " ['ache*', 'negemo'],\n",
       " ['ache*', 'sad'],\n",
       " ['ache*', 'bio'],\n",
       " ['ache*', 'health'],\n",
       " ['achiev*', 'work'],\n",
       " ['achiev*', 'achieve'],\n",
       " ['aching', 'affect'],\n",
       " ['aching', 'negemo'],\n",
       " ['aching', 'sad'],\n",
       " ['aching', 'bio'],\n",
       " ['aching', 'health'],\n",
       " ['acid*', 'percept'],\n",
       " ['acknowledg*', 'cogmech'],\n",
       " ['acknowledg*', 'insight'],\n",
       " ['acne', 'bio'],\n",
       " ['acne', 'health'],\n",
       " ['acquainta*', 'social'],\n",
       " ['acquainta*', 'friend'],\n",
       " ['acquir*', 'achieve'],\n",
       " ['acquisition*', 'achieve'],\n",
       " ['acrid*', 'percept'],\n",
       " ['across', 'funct'],\n",
       " ['across', 'preps'],\n",
       " ['across', 'space'],\n",
       " ['across', 'relativ'],\n",
       " ['act', 'relativ'],\n",
       " ['act', 'motion'],\n",
       " ['action*', 'motion'],\n",
       " ['action*', 'relativ'],\n",
       " ['activat*', 'cogmech'],\n",
       " ['activat*', 'cause'],\n",
       " ['active*', 'affect'],\n",
       " ['active*', 'posemo'],\n",
       " ['actor*', 'leisure'],\n",
       " ['actress*', 'leisure'],\n",
       " ['actually', 'funct'],\n",
       " ['actually', 'adverb'],\n",
       " ['add', 'cogmech'],\n",
       " ['add', 'incl'],\n",
       " ['addict*', 'bio'],\n",
       " ['addict*', 'health'],\n",
       " ['addit*', 'cogmech'],\n",
       " ['addit*', 'incl'],\n",
       " ['address', 'home'],\n",
       " ['adequa*', 'achieve'],\n",
       " ['adjust*', 'cogmech'],\n",
       " ['adjust*', 'insight'],\n",
       " ['administrat*', 'work'],\n",
       " ['admir*', 'affect'],\n",
       " ['admir*', 'posemo'],\n",
       " ['admit', 'verb'],\n",
       " ['admit', 'present'],\n",
       " ['admit', 'social'],\n",
       " ['admit', 'cogmech'],\n",
       " ['admit', 'insight'],\n",
       " ['admits', 'verb'],\n",
       " ['admits', 'present'],\n",
       " ['admits', 'social'],\n",
       " ['admits', 'cogmech'],\n",
       " ['admits', 'insight'],\n",
       " ['admitted', 'verb'],\n",
       " ['admitted', 'past'],\n",
       " ['admitted', 'social'],\n",
       " ['admitted', 'cogmech'],\n",
       " ['admitted', 'insight'],\n",
       " ['admitting', 'social'],\n",
       " ['admitting', 'cogmech'],\n",
       " ['admitting', 'insight'],\n",
       " ['ador*', 'affect'],\n",
       " ['ador*', 'posemo'],\n",
       " ['adult', 'social'],\n",
       " ['adult', 'humans'],\n",
       " ['adults', 'social'],\n",
       " ['adults', 'humans'],\n",
       " ['advanc*', 'motion'],\n",
       " ['advanc*', 'relativ'],\n",
       " ['advanc*', 'achieve'],\n",
       " ['advantag*', 'affect'],\n",
       " ['advantag*', 'posemo'],\n",
       " ['advantag*', 'achieve'],\n",
       " ['adventur*', 'affect'],\n",
       " ['adventur*', 'posemo'],\n",
       " ['advers*', 'affect'],\n",
       " ['advers*', 'negemo'],\n",
       " ['advertising', 'work'],\n",
       " ['advice', 'social'],\n",
       " ['advil', 'bio'],\n",
       " ['advil', 'health'],\n",
       " ['advis*', 'social'],\n",
       " ['advis*', 'work'],\n",
       " ['aerobic*', 'leisure'],\n",
       " ['affair*', 'social'],\n",
       " ['affect', 'cogmech'],\n",
       " ['affect', 'cause'],\n",
       " ['affected', 'verb'],\n",
       " ['affected', 'past'],\n",
       " ['affected', 'cogmech'],\n",
       " ['affected', 'cause'],\n",
       " ['affecting', 'cogmech'],\n",
       " ['affecting', 'cause'],\n",
       " ['affection*', 'affect'],\n",
       " ['affection*', 'posemo'],\n",
       " ['affects', 'cogmech'],\n",
       " ['affects', 'cause'],\n",
       " ['afraid', 'affect'],\n",
       " ['afraid', 'negemo'],\n",
       " ['afraid', 'anx'],\n",
       " ['after', 'funct'],\n",
       " ['after', 'preps'],\n",
       " ['after', 'time'],\n",
       " ['after', 'relativ'],\n",
       " ['afterlife*', 'time'],\n",
       " ['afterlife*', 'relativ'],\n",
       " ['afterlife*', 'relig'],\n",
       " ['aftermath*', 'time'],\n",
       " ['aftermath*', 'relativ'],\n",
       " ['afternoon*', 'time'],\n",
       " ['afternoon*', 'relativ'],\n",
       " ['afterthought*', 'cogmech'],\n",
       " ['afterthought*', 'insight'],\n",
       " ['afterthought*', 'time'],\n",
       " ['afterthought*', 'relativ'],\n",
       " ['afterward*', 'time'],\n",
       " ['afterward*', 'relativ'],\n",
       " ['again', 'funct'],\n",
       " ['again', 'adverb'],\n",
       " ['again', 'time'],\n",
       " ['again', 'relativ'],\n",
       " ['against', 'funct'],\n",
       " ['against', 'preps'],\n",
       " ['age', 'time'],\n",
       " ['age', 'relativ'],\n",
       " ['aged', 'time'],\n",
       " ['aged', 'relativ'],\n",
       " ['agent', 'work'],\n",
       " ['agents', 'work'],\n",
       " ['ages', 'time'],\n",
       " ['ages', 'relativ'],\n",
       " ['aggravat*', 'affect'],\n",
       " ['aggravat*', 'negemo'],\n",
       " ['aggravat*', 'anger'],\n",
       " ['aggravat*', 'cogmech'],\n",
       " ['aggravat*', 'cause'],\n",
       " ['aggress*', 'affect'],\n",
       " ['aggress*', 'negemo'],\n",
       " ['aggress*', 'anger'],\n",
       " ['aging', 'time'],\n",
       " ['aging', 'relativ'],\n",
       " ['agitat*', 'affect'],\n",
       " ['agitat*', 'negemo'],\n",
       " ['agitat*', 'anger'],\n",
       " ['agnost*', 'relig'],\n",
       " ['ago', 'time'],\n",
       " ['ago', 'relativ'],\n",
       " ['agoniz*', 'affect'],\n",
       " ['agoniz*', 'negemo'],\n",
       " ['agoniz*', 'sad'],\n",
       " ['agony', 'affect'],\n",
       " ['agony', 'negemo'],\n",
       " ['agony', 'sad'],\n",
       " ['agree', 'affect'],\n",
       " ['agree', 'posemo'],\n",
       " ['agree', 'assent'],\n",
       " ['agreeab*', 'affect'],\n",
       " ['agreeab*', 'posemo'],\n",
       " ['agreed', 'affect'],\n",
       " ['agreed', 'posemo'],\n",
       " ['agreeing', 'affect'],\n",
       " ['agreeing', 'posemo'],\n",
       " ['agreement*', 'affect'],\n",
       " ['agreement*', 'posemo'],\n",
       " ['agrees', 'affect'],\n",
       " ['agrees', 'posemo'],\n",
       " ['ah', 'assent'],\n",
       " ['ahead', 'funct'],\n",
       " ['ahead', 'preps'],\n",
       " ['ahead', 'time'],\n",
       " ['ahead', 'relativ'],\n",
       " ['ahead', 'achieve'],\n",
       " ['aids', 'bio'],\n",
       " ['aids', 'health'],\n",
       " ['aids', 'sexual'],\n",
       " [\"ain't\", 'verb'],\n",
       " [\"ain't\", 'funct'],\n",
       " [\"ain't\", 'auxverb'],\n",
       " [\"ain't\", 'present'],\n",
       " [\"ain't\", 'negate'],\n",
       " ['aint', 'verb'],\n",
       " ['aint', 'funct'],\n",
       " ['aint', 'auxverb'],\n",
       " ['aint', 'present'],\n",
       " ['aint', 'negate'],\n",
       " ['air', 'relativ'],\n",
       " ['air', 'space'],\n",
       " ['alarm*', 'affect'],\n",
       " ['alarm*', 'negemo'],\n",
       " ['alarm*', 'anx'],\n",
       " ['alcohol*', 'bio'],\n",
       " ['alcohol*', 'health'],\n",
       " ['alcohol*', 'ingest'],\n",
       " ['alive', 'bio'],\n",
       " ['alive', 'health'],\n",
       " ['alive', 'death'],\n",
       " ['all', 'funct'],\n",
       " ['all', 'quant'],\n",
       " ['all', 'cogmech'],\n",
       " ['all', 'certain'],\n",
       " ['alla', 'relig'],\n",
       " ['allah*', 'relig'],\n",
       " ['allerg*', 'bio'],\n",
       " ['allerg*', 'health'],\n",
       " ['allot', 'funct'],\n",
       " ['allot', 'quant'],\n",
       " ['allot', 'cogmech'],\n",
       " ['allot', 'tentat'],\n",
       " ['allow*', 'cogmech'],\n",
       " ['allow*', 'cause'],\n",
       " ['almost', 'cogmech'],\n",
       " ['almost', 'tentat'],\n",
       " ['alone', 'affect'],\n",
       " ['alone', 'negemo'],\n",
       " ['alone', 'sad'],\n",
       " ['along', 'funct'],\n",
       " ['along', 'preps'],\n",
       " ['along', 'cogmech'],\n",
       " ['along', 'incl'],\n",
       " ['alot', 'funct'],\n",
       " ['alot', 'article'],\n",
       " ['alot', 'quant'],\n",
       " ['alot', 'cogmech'],\n",
       " ['alot', 'tentat'],\n",
       " ['already', 'time'],\n",
       " ['already', 'relativ'],\n",
       " ['alright*', 'affect'],\n",
       " ['alright*', 'posemo'],\n",
       " ['alright*', 'assent'],\n",
       " ['also', 'funct'],\n",
       " ['also', 'adverb'],\n",
       " ['also', 'conj'],\n",
       " ['altar*', 'relig'],\n",
       " ['although', 'funct'],\n",
       " ['although', 'conj'],\n",
       " ['altogether', 'cogmech'],\n",
       " ['altogether', 'certain'],\n",
       " ['always', 'cogmech'],\n",
       " ['always', 'certain'],\n",
       " ['always', 'time'],\n",
       " ['always', 'relativ'],\n",
       " ['am', 'verb'],\n",
       " ['am', 'funct'],\n",
       " ['am', 'auxverb'],\n",
       " ['am', 'present'],\n",
       " ['amaz*', 'affect'],\n",
       " ['amaz*', 'posemo'],\n",
       " ['ambigu*', 'cogmech'],\n",
       " ['ambigu*', 'tentat'],\n",
       " ['ambiti*', 'work'],\n",
       " ['ambiti*', 'achieve'],\n",
       " ['amen', 'relig'],\n",
       " ['amigo*', 'social'],\n",
       " ['amigo*', 'friend'],\n",
       " ['amish', 'relig'],\n",
       " ['among*', 'funct'],\n",
       " ['among*', 'preps'],\n",
       " ['among*', 'space'],\n",
       " ['among*', 'relativ'],\n",
       " ['amor*', 'affect'],\n",
       " ['amor*', 'posemo'],\n",
       " ['amount*', 'quant'],\n",
       " ['amput*', 'bio'],\n",
       " ['amput*', 'health'],\n",
       " ['amus*', 'affect'],\n",
       " ['amus*', 'posemo'],\n",
       " ['amus*', 'leisure'],\n",
       " ['an', 'funct'],\n",
       " ['an', 'article'],\n",
       " ['anal', 'cogmech'],\n",
       " ['anal', 'inhib'],\n",
       " ['anal', 'bio'],\n",
       " ['anal', 'body'],\n",
       " ['analy*', 'cogmech'],\n",
       " ['analy*', 'insight'],\n",
       " ['ancient*', 'time'],\n",
       " ['ancient*', 'relativ'],\n",
       " ['and', 'funct'],\n",
       " ['and', 'conj'],\n",
       " ['and', 'cogmech'],\n",
       " ['and', 'incl'],\n",
       " ['angel', 'relig'],\n",
       " ['angelic*', 'relig'],\n",
       " ['angels', 'relig'],\n",
       " ['anger*', 'affect'],\n",
       " ['anger*', 'negemo'],\n",
       " ['anger*', 'anger'],\n",
       " ['angr*', 'affect'],\n",
       " ['angr*', 'negemo'],\n",
       " ['angr*', 'anger'],\n",
       " ['anguish*', 'affect'],\n",
       " ['anguish*', 'negemo'],\n",
       " ['anguish*', 'anx'],\n",
       " ['ankle*', 'bio'],\n",
       " ['ankle*', 'body'],\n",
       " ['annoy*', 'affect'],\n",
       " ['annoy*', 'negemo'],\n",
       " ['annoy*', 'anger'],\n",
       " ['annual*', 'time'],\n",
       " ['annual*', 'relativ'],\n",
       " ['anorexi*', 'bio'],\n",
       " ['anorexi*', 'health'],\n",
       " ['anorexi*', 'ingest'],\n",
       " ['another', 'funct'],\n",
       " ['another', 'quant'],\n",
       " ['answer*', 'cogmech'],\n",
       " ['answer*', 'insight'],\n",
       " ['antacid*', 'bio'],\n",
       " ['antacid*', 'health'],\n",
       " ['antagoni*', 'affect'],\n",
       " ['antagoni*', 'negemo'],\n",
       " ['antagoni*', 'anger'],\n",
       " ['antidepressant*', 'bio'],\n",
       " ['antidepressant*', 'health'],\n",
       " ['anus*', 'bio'],\n",
       " ['anus*', 'body'],\n",
       " ['anxi*', 'affect'],\n",
       " ['anxi*', 'negemo'],\n",
       " ['anxi*', 'anx'],\n",
       " ['any', 'funct'],\n",
       " ['any', 'quant'],\n",
       " ['any', 'cogmech'],\n",
       " ['any', 'tentat'],\n",
       " ['anybod*', 'funct'],\n",
       " ['anybod*', 'pronoun'],\n",
       " ['anybod*', 'ipron'],\n",
       " ['anybod*', 'social'],\n",
       " ['anybod*', 'cogmech'],\n",
       " ['anybod*', 'tentat'],\n",
       " ['anyhow', 'cogmech'],\n",
       " ['anyhow', 'tentat'],\n",
       " ['anymore', 'funct'],\n",
       " ['anymore', 'quant'],\n",
       " ['anymore', 'relativ'],\n",
       " ['anymore', 'time'],\n",
       " ['anyone*', 'funct'],\n",
       " ['anyone*', 'pronoun'],\n",
       " ['anyone*', 'ipron'],\n",
       " ['anyone*', 'social'],\n",
       " ['anyone*', 'cogmech'],\n",
       " ['anyone*', 'tentat'],\n",
       " ['anything', 'funct'],\n",
       " ['anything', 'pronoun'],\n",
       " ['anything', 'ipron'],\n",
       " ['anything', 'cogmech'],\n",
       " ['anything', 'tentat'],\n",
       " ['anytime', 'cogmech'],\n",
       " ['anytime', 'tentat'],\n",
       " ['anytime', 'time'],\n",
       " ['anytime', 'relativ'],\n",
       " ['anyway*', 'funct'],\n",
       " ['anyway*', 'adverb'],\n",
       " ['anywhere', 'funct'],\n",
       " ['anywhere', 'adverb'],\n",
       " ['anywhere', 'cogmech'],\n",
       " ['anywhere', 'tentat'],\n",
       " ['anywhere', 'space'],\n",
       " ['anywhere', 'relativ'],\n",
       " ['aok', 'affect'],\n",
       " ['aok', 'posemo'],\n",
       " ['aok', 'assent'],\n",
       " ['apart', 'space'],\n",
       " ['apart', 'relativ'],\n",
       " ['apartment*', 'leisure'],\n",
       " ['apartment*', 'home'],\n",
       " ['apath*', 'affect'],\n",
       " ['apath*', 'negemo'],\n",
       " ['apolog*', 'social'],\n",
       " ['appall*', 'affect'],\n",
       " ['appall*', 'negemo'],\n",
       " ['apparent', 'cogmech'],\n",
       " ['apparent', 'certain'],\n",
       " ['apparently', 'funct'],\n",
       " ['apparently', 'adverb'],\n",
       " ['apparently', 'cogmech'],\n",
       " ['apparently', 'tentat'],\n",
       " ['appear', 'verb'],\n",
       " ['appear', 'present'],\n",
       " ['appear', 'cogmech'],\n",
       " ['appear', 'tentat'],\n",
       " ['appear', 'motion'],\n",
       " ['appear', 'relativ'],\n",
       " ['appeared', 'verb'],\n",
       " ['appeared', 'past'],\n",
       " ['appeared', 'cogmech'],\n",
       " ['appeared', 'tentat'],\n",
       " ['appeared', 'motion'],\n",
       " ['appeared', 'relativ'],\n",
       " ['appearing', 'cogmech'],\n",
       " ['appearing', 'tentat'],\n",
       " ['appearing', 'motion'],\n",
       " ['appearing', 'relativ'],\n",
       " ['appears', 'verb'],\n",
       " ['appears', 'present'],\n",
       " ['appears', 'cogmech'],\n",
       " ['appears', 'tentat'],\n",
       " ['appears', 'motion'],\n",
       " ['appears', 'relativ'],\n",
       " ['appendic*', 'bio'],\n",
       " ['appendic*', 'health'],\n",
       " ['appendix', 'bio'],\n",
       " ['appendix', 'body'],\n",
       " ['appeti*', 'bio'],\n",
       " ['appeti*', 'ingest'],\n",
       " ['applicant*', 'work'],\n",
       " ['applicat*', 'work'],\n",
       " ['appreciat*', 'affect'],\n",
       " ['appreciat*', 'posemo'],\n",
       " ['appreciat*', 'cogmech'],\n",
       " ['appreciat*', 'insight'],\n",
       " ['apprehens*', 'affect'],\n",
       " ['apprehens*', 'negemo'],\n",
       " ['apprehens*', 'anx'],\n",
       " ['apprentic*', 'work'],\n",
       " ['approach*', 'motion'],\n",
       " ['approach*', 'relativ'],\n",
       " ['approv*', 'achieve'],\n",
       " ['approximat*', 'cogmech'],\n",
       " ['approximat*', 'tentat'],\n",
       " ['april', 'time'],\n",
       " ['april', 'relativ'],\n",
       " ['arbitrar*', 'cogmech'],\n",
       " ['arbitrar*', 'tentat'],\n",
       " ['arch', 'bio'],\n",
       " ['arch', 'body'],\n",
       " ['are', 'verb'],\n",
       " ['are', 'funct'],\n",
       " ['are', 'auxverb'],\n",
       " ['are', 'present'],\n",
       " ['area*', 'space'],\n",
       " ['area*', 'relativ'],\n",
       " [\"aren't\", 'verb'],\n",
       " [\"aren't\", 'funct'],\n",
       " [\"aren't\", 'auxverb'],\n",
       " [\"aren't\", 'present'],\n",
       " [\"aren't\", 'negate'],\n",
       " ['arent', 'verb'],\n",
       " ['arent', 'funct'],\n",
       " ['arent', 'auxverb'],\n",
       " ['arent', 'present'],\n",
       " ['arent', 'negate'],\n",
       " ['argh*', 'affect'],\n",
       " ['argh*', 'negemo'],\n",
       " ['argh*', 'anger'],\n",
       " ['argu*', 'social'],\n",
       " ['argu*', 'affect'],\n",
       " ['argu*', 'negemo'],\n",
       " ['argu*', 'anger'],\n",
       " ['arm', 'bio'],\n",
       " ['arm', 'body'],\n",
       " ['armies', 'social'],\n",
       " ['armpit*', 'bio'],\n",
       " ['armpit*', 'body'],\n",
       " ['arms*', 'bio'],\n",
       " ['arms*', 'body'],\n",
       " ['army', 'social'],\n",
       " ['aroma*', 'percept'],\n",
       " ['around', 'funct'],\n",
       " ['around', 'adverb'],\n",
       " ['around', 'preps'],\n",
       " ['around', 'cogmech'],\n",
       " ['around', 'incl'],\n",
       " ['around', 'space'],\n",
       " ['around', 'relativ'],\n",
       " ['arous*', 'bio'],\n",
       " ['arous*', 'body'],\n",
       " ['arous*', 'sexual'],\n",
       " ['arrival*', 'motion'],\n",
       " ['arrival*', 'relativ'],\n",
       " ['arrive', 'verb'],\n",
       " ['arrive', 'present'],\n",
       " ['arrive', 'motion'],\n",
       " ['arrive', 'relativ'],\n",
       " ['arrived', 'verb'],\n",
       " ['arrived', 'past'],\n",
       " ['arrived', 'motion'],\n",
       " ['arrived', 'relativ'],\n",
       " ['arrives', 'verb'],\n",
       " ['arrives', 'present'],\n",
       " ['arrives', 'motion'],\n",
       " ['arrives', 'relativ'],\n",
       " ['arriving', 'motion'],\n",
       " ['arriving', 'relativ'],\n",
       " ['arrogan*', 'affect'],\n",
       " ['arrogan*', 'negemo'],\n",
       " ['arrogan*', 'anger'],\n",
       " ['arse', 'bio'],\n",
       " ['arse', 'body'],\n",
       " ['arse', 'swear'],\n",
       " ['arsehole*', 'swear'],\n",
       " ['arses', 'bio'],\n",
       " ['arses', 'body'],\n",
       " ['arses', 'swear'],\n",
       " ['art', 'leisure'],\n",
       " ['arter*', 'bio'],\n",
       " ['arter*', 'body'],\n",
       " ['arthr*', 'bio'],\n",
       " ['arthr*', 'health'],\n",
       " ['artist*', 'leisure'],\n",
       " ['arts', 'leisure'],\n",
       " ['as', 'funct'],\n",
       " ['as', 'preps'],\n",
       " ['as', 'conj'],\n",
       " ['asham*', 'affect'],\n",
       " ['asham*', 'negemo'],\n",
       " ['asham*', 'anx'],\n",
       " ['ask', 'verb'],\n",
       " ['ask', 'present'],\n",
       " ['ask', 'social'],\n",
       " ['asked', 'verb'],\n",
       " ['asked', 'past'],\n",
       " ['asked', 'social'],\n",
       " ['asking', 'social'],\n",
       " ['asks', 'verb'],\n",
       " ['asks', 'present'],\n",
       " ['asks', 'social'],\n",
       " ['asleep', 'bio'],\n",
       " ['asleep', 'body'],\n",
       " ['aspirin*', 'bio'],\n",
       " ['aspirin*', 'health'],\n",
       " ['ass', 'bio'],\n",
       " ['ass', 'body'],\n",
       " ['ass', 'sexual'],\n",
       " ['ass', 'swear'],\n",
       " ['assault*', 'affect'],\n",
       " ['assault*', 'negemo'],\n",
       " ['assault*', 'anger'],\n",
       " ['assembl*', 'social'],\n",
       " ['asses', 'bio'],\n",
       " ['asses', 'body'],\n",
       " ['asses', 'sexual'],\n",
       " ['asses', 'swear'],\n",
       " ['asshole*', 'affect'],\n",
       " ['asshole*', 'negemo'],\n",
       " ['asshole*', 'anger'],\n",
       " ['asshole*', 'swear'],\n",
       " ['assign*', 'work'],\n",
       " ['assistan*', 'work'],\n",
       " ['associat*', 'work'],\n",
       " ['assum*', 'cogmech'],\n",
       " ['assum*', 'insight'],\n",
       " ['assum*', 'tentat'],\n",
       " ['assur*', 'affect'],\n",
       " ['assur*', 'posemo'],\n",
       " ['assur*', 'cogmech'],\n",
       " ['assur*', 'certain'],\n",
       " ['asthma*', 'bio'],\n",
       " ['asthma*', 'health'],\n",
       " ['at', 'funct'],\n",
       " ['at', 'preps'],\n",
       " ['at', 'space'],\n",
       " ['at', 'relativ'],\n",
       " ['ate', 'verb'],\n",
       " ['ate', 'past'],\n",
       " ['ate', 'bio'],\n",
       " ['ate', 'ingest'],\n",
       " ['athletic*', 'leisure'],\n",
       " ['atho', 'funct'],\n",
       " ['atho', 'conj'],\n",
       " ['atm', 'money'],\n",
       " ['atms', 'money'],\n",
       " ['atop', 'funct'],\n",
       " ['atop', 'preps'],\n",
       " ['atop', 'space'],\n",
       " ['atop', 'relativ'],\n",
       " ['attachment*', 'affect'],\n",
       " ['attachment*', 'posemo'],\n",
       " ['attack*', 'affect'],\n",
       " ['attack*', 'negemo'],\n",
       " ['attack*', 'anger'],\n",
       " ['attain*', 'achieve'],\n",
       " ['attempt*', 'achieve'],\n",
       " ['attend', 'motion'],\n",
       " ['attend', 'relativ'],\n",
       " ['attended', 'motion'],\n",
       " ['attended', 'relativ'],\n",
       " ['attending', 'motion'],\n",
       " ['attending', 'relativ'],\n",
       " ['attends', 'motion'],\n",
       " ['attends', 'relativ'],\n",
       " ['attent*', 'cogmech'],\n",
       " ['attent*', 'insight'],\n",
       " ['attract*', 'affect'],\n",
       " ['attract*', 'posemo'],\n",
       " ['attribut*', 'cogmech'],\n",
       " ['attribut*', 'cause'],\n",
       " ['auction*', 'money'],\n",
       " ['audibl*', 'percept'],\n",
       " ['audibl*', 'hear'],\n",
       " ['audio*', 'percept'],\n",
       " ['audio*', 'hear'],\n",
       " ['audit', 'money'],\n",
       " ['audited', 'money'],\n",
       " ['auditing', 'money'],\n",
       " ['auditor', 'money'],\n",
       " ['auditorium*', 'work'],\n",
       " ['auditors', 'money'],\n",
       " ['audits', 'money'],\n",
       " ['august', 'time'],\n",
       " ['august', 'relativ'],\n",
       " ['aunt*', 'social'],\n",
       " ['aunt*', 'family'],\n",
       " ['authorit*', 'achieve'],\n",
       " ['autops*', 'death'],\n",
       " ['autumn', 'time'],\n",
       " ['autumn', 'relativ'],\n",
       " ['aversi*', 'affect'],\n",
       " ['aversi*', 'negemo'],\n",
       " ['aversi*', 'anx'],\n",
       " ['avert*', 'cogmech'],\n",
       " ['avert*', 'inhib'],\n",
       " ['avoid*', 'affect'],\n",
       " ['avoid*', 'negemo'],\n",
       " ['avoid*', 'anx'],\n",
       " ['avoid*', 'cogmech'],\n",
       " ['avoid*', 'inhib'],\n",
       " ['aw', 'assent'],\n",
       " ['award*', 'affect'],\n",
       " ['award*', 'posemo'],\n",
       " ['award*', 'work'],\n",
       " ['award*', 'achieve'],\n",
       " ['aware*', 'cogmech'],\n",
       " ['aware*', 'insight'],\n",
       " ['away', 'funct'],\n",
       " ['away', 'preps'],\n",
       " ['away', 'space'],\n",
       " ['away', 'relativ'],\n",
       " ['awesome', 'affect'],\n",
       " ['awesome', 'posemo'],\n",
       " ['awesome', 'assent'],\n",
       " ['awful', 'affect'],\n",
       " ['awful', 'negemo'],\n",
       " ['awhile', 'time'],\n",
       " ['awhile', 'relativ'],\n",
       " ['awkward*', 'affect'],\n",
       " ['awkward*', 'negemo'],\n",
       " ['awkward*', 'anx'],\n",
       " ['babe*', 'social'],\n",
       " ['babe*', 'humans'],\n",
       " ['babies', 'social'],\n",
       " ['babies', 'humans'],\n",
       " ['baby*', 'social'],\n",
       " ['baby*', 'humans'],\n",
       " ['back', 'funct'],\n",
       " ['back', 'adverb'],\n",
       " ['back', 'time'],\n",
       " ['back', 'relativ'],\n",
       " ['backward*', 'space'],\n",
       " ['backward*', 'relativ'],\n",
       " ['backyard', 'home'],\n",
       " ['bad', 'affect'],\n",
       " ['bad', 'negemo'],\n",
       " ['bake*', 'bio'],\n",
       " ['bake*', 'ingest'],\n",
       " ['bake*', 'home'],\n",
       " ['baking', 'bio'],\n",
       " ['baking', 'ingest'],\n",
       " ['baking', 'home'],\n",
       " ['balcon*', 'home'],\n",
       " ['bald', 'bio'],\n",
       " ['bald', 'body'],\n",
       " ['ball', 'leisure'],\n",
       " ['ballet*', 'leisure'],\n",
       " ['bambino*', 'social'],\n",
       " ['bambino*', 'humans'],\n",
       " ['ban', 'cogmech'],\n",
       " ['ban', 'inhib'],\n",
       " ['band', 'social'],\n",
       " ['band', 'leisure'],\n",
       " ['bandage*', 'bio'],\n",
       " ['bandage*', 'health'],\n",
       " ['bandaid', 'bio'],\n",
       " ['bandaid', 'health'],\n",
       " ['bands', 'social'],\n",
       " ['bands', 'leisure'],\n",
       " ['bank*', 'money'],\n",
       " ['banned', 'cogmech'],\n",
       " ['banned', 'inhib'],\n",
       " ['banning', 'cogmech'],\n",
       " ['banning', 'inhib'],\n",
       " ['bans', 'cogmech'],\n",
       " ['bans', 'inhib'],\n",
       " ['baptis*', 'relig'],\n",
       " ['baptiz*', 'relig'],\n",
       " ['bar', 'bio'],\n",
       " ['bar', 'ingest'],\n",
       " ['bar', 'leisure'],\n",
       " ['barely', 'cogmech'],\n",
       " ['barely', 'tentat'],\n",
       " ['bargain*', 'money'],\n",
       " ['barrier*', 'cogmech'],\n",
       " ['barrier*', 'inhib'],\n",
       " ['bars', 'bio'],\n",
       " ['bars', 'ingest'],\n",
       " ['bars', 'leisure'],\n",
       " ['baseball*', 'leisure'],\n",
       " ['based', 'cogmech'],\n",
       " ['based', 'cause'],\n",
       " ['bases', 'cogmech'],\n",
       " ['bases', 'cause'],\n",
       " ['bashful*', 'affect'],\n",
       " ['bashful*', 'negemo'],\n",
       " ['basically', 'funct'],\n",
       " ['basically', 'adverb'],\n",
       " ['basis', 'cogmech'],\n",
       " ['basis', 'cause'],\n",
       " ['basketball*', 'leisure'],\n",
       " ['bastard*', 'affect'],\n",
       " ['bastard*', 'negemo'],\n",
       " ['bastard*', 'anger'],\n",
       " ['bastard*', 'swear'],\n",
       " ['bath*', 'leisure'],\n",
       " ['bath*', 'home'],\n",
       " ['battl*', 'affect'],\n",
       " ['battl*', 'negemo'],\n",
       " ['battl*', 'anger'],\n",
       " ['be', 'verb'],\n",
       " ['be', 'funct'],\n",
       " ['be', 'auxverb'],\n",
       " ['beach*', 'leisure'],\n",
       " ['beat', 'achieve'],\n",
       " ['beaten', 'affect'],\n",
       " ['beaten', 'negemo'],\n",
       " ['beaten', 'anger'],\n",
       " ['beaten', 'work'],\n",
       " ['beaten', 'achieve'],\n",
       " ['beaut*', 'affect'],\n",
       " ['beaut*', 'posemo'],\n",
       " ['beaut*', 'percept'],\n",
       " ['beaut*', 'see'],\n",
       " ['became', 'verb'],\n",
       " ['became', 'funct'],\n",
       " ['became', 'auxverb'],\n",
       " ['became', 'past'],\n",
       " ['became', 'cogmech'],\n",
       " ['became', 'insight'],\n",
       " ['because', 'funct'],\n",
       " ['because', 'conj'],\n",
       " ['because', 'cogmech'],\n",
       " ['because', 'cause'],\n",
       " ['become', 'verb'],\n",
       " ['become', 'funct'],\n",
       " ['become', 'auxverb'],\n",
       " ['become', 'present'],\n",
       " ['become', 'cogmech'],\n",
       " ['become', 'insight'],\n",
       " ['becomes', 'verb'],\n",
       " ['becomes', 'funct'],\n",
       " ['becomes', 'auxverb'],\n",
       " ['becomes', 'present'],\n",
       " ['becomes', 'cogmech'],\n",
       " ['becomes', 'insight'],\n",
       " ['becoming', 'verb'],\n",
       " ['becoming', 'funct'],\n",
       " ['becoming', 'auxverb'],\n",
       " ['becoming', 'cogmech'],\n",
       " ['becoming', 'insight'],\n",
       " ['bed', 'home'],\n",
       " ['bedding', 'home'],\n",
       " ['bedroom*', 'home'],\n",
       " ['beds', 'home'],\n",
       " ['been', 'verb'],\n",
       " ['been', 'funct'],\n",
       " ['been', 'auxverb'],\n",
       " ['been', 'past'],\n",
       " ['beer*', 'bio'],\n",
       " ['beer*', 'ingest'],\n",
       " ['beer*', 'leisure'],\n",
       " ['before', 'funct'],\n",
       " ['before', 'preps'],\n",
       " ['before', 'time'],\n",
       " ['before', 'relativ'],\n",
       " ['began', 'verb'],\n",
       " ['began', 'past'],\n",
       " ['began', 'time'],\n",
       " ['began', 'relativ'],\n",
       " ['beggar*', 'money'],\n",
       " ['begging', 'money'],\n",
       " ['begin', 'verb'],\n",
       " ['begin', 'present'],\n",
       " ['begin', 'time'],\n",
       " ['begin', 'relativ'],\n",
       " ['beginn*', 'time'],\n",
       " ['beginn*', 'relativ'],\n",
       " ['begins', 'verb'],\n",
       " ['begins', 'present'],\n",
       " ['begins', 'time'],\n",
       " ['begins', 'relativ'],\n",
       " ['begun', 'time'],\n",
       " ['begun', 'relativ'],\n",
       " ['behavio*', 'relativ'],\n",
       " ['behavio*', 'motion'],\n",
       " ['behind', 'funct'],\n",
       " ['behind', 'preps'],\n",
       " ['being', 'verb'],\n",
       " ['being', 'funct'],\n",
       " ['being', 'auxverb'],\n",
       " ['belief*', 'cogmech'],\n",
       " ['belief*', 'insight'],\n",
       " ['belief*', 'relig'],\n",
       " ['believe', 'verb'],\n",
       " ['believe', 'present'],\n",
       " ['believe', 'cogmech'],\n",
       " ['believe', 'insight'],\n",
       " ['believed', 'verb'],\n",
       " ['believed', 'past'],\n",
       " ['believed', 'cogmech'],\n",
       " ['believed', 'insight'],\n",
       " ['believes', 'verb'],\n",
       " ['believes', 'present'],\n",
       " ['believes', 'cogmech'],\n",
       " ['believes', 'insight'],\n",
       " ['believing', 'cogmech'],\n",
       " ['believing', 'insight'],\n",
       " ['bellies', 'bio'],\n",
       " ['bellies', 'body'],\n",
       " ['belly', 'bio'],\n",
       " ['belly', 'body'],\n",
       " ['beloved', 'affect'],\n",
       " ['beloved', 'posemo'],\n",
       " ['below', 'funct'],\n",
       " ['below', 'preps'],\n",
       " ['below', 'space'],\n",
       " ['below', 'relativ'],\n",
       " ['bend', 'space'],\n",
       " ['bend', 'relativ'],\n",
       " ['bending', 'space'],\n",
       " ['bending', 'relativ'],\n",
       " ['bends', 'space'],\n",
       " ['bends', 'relativ'],\n",
       " ['beneath', 'funct'],\n",
       " ['beneath', 'preps'],\n",
       " ['beneath', 'space'],\n",
       " ['beneath', 'relativ'],\n",
       " ['benefic*', 'affect'],\n",
       " ['benefic*', 'posemo'],\n",
       " ['benefit', 'affect'],\n",
       " ['benefit', 'posemo'],\n",
       " ['benefits', 'affect'],\n",
       " ['benefits', 'posemo'],\n",
       " ['benefits', 'work'],\n",
       " ['benefitt*', 'affect'],\n",
       " ['benefitt*', 'posemo'],\n",
       " ['benevolen*', 'affect'],\n",
       " ['benevolen*', 'posemo'],\n",
       " ['benign*', 'affect'],\n",
       " ['benign*', 'posemo'],\n",
       " ['bent', 'space'],\n",
       " ['bent', 'relativ'],\n",
       " ['bereave*', 'death'],\n",
       " ['beside', 'funct'],\n",
       " ['beside', 'preps'],\n",
       " ['beside', 'space'],\n",
       " ['beside', 'relativ'],\n",
       " ['besides', 'funct'],\n",
       " ['besides', 'preps'],\n",
       " ['besides', 'quant'],\n",
       " ['besides', 'cogmech'],\n",
       " ['besides', 'discrep'],\n",
       " ['best', 'affect'],\n",
       " ['best', 'posemo'],\n",
       " ['best', 'achieve'],\n",
       " ['best', 'funct'],\n",
       " ['best', 'quant'],\n",
       " ['bet', 'cogmech'],\n",
       " ['bet', 'tentat'],\n",
       " ['bet', 'money'],\n",
       " ['bets', 'cogmech'],\n",
       " ['bets', 'tentat'],\n",
       " ['bets', 'money'],\n",
       " ['better', 'affect'],\n",
       " ['better', 'posemo'],\n",
       " ['better', 'achieve'],\n",
       " ['betting', 'cogmech'],\n",
       " ['betting', 'tentat'],\n",
       " ['betting', 'money'],\n",
       " ['between', 'funct'],\n",
       " ['between', 'preps'],\n",
       " ['beyond', 'funct'],\n",
       " ['beyond', 'adverb'],\n",
       " ['beyond', 'preps'],\n",
       " ['beyond', 'space'],\n",
       " ['beyond', 'relativ'],\n",
       " ['bf*', 'social'],\n",
       " ['bf*', 'friend'],\n",
       " ['bi', 'bio'],\n",
       " ['bi', 'sexual'],\n",
       " ['biannu*', 'time'],\n",
       " ['biannu*', 'relativ'],\n",
       " ['bible*', 'relig'],\n",
       " ['biblic*', 'relig'],\n",
       " ['bicep*', 'bio'],\n",
       " ['bicep*', 'body'],\n",
       " ['bicyc*', 'leisure'],\n",
       " ['big', 'space'],\n",
       " ['big', 'relativ'],\n",
       " ['bigger', 'space'],\n",
       " ['bigger', 'relativ'],\n",
       " ['biggest', 'space'],\n",
       " ['biggest', 'relativ'],\n",
       " ['bike*', 'leisure'],\n",
       " ['bill', 'money'],\n",
       " ['billed', 'money'],\n",
       " ['billing*', 'money'],\n",
       " ['billion*', 'funct'],\n",
       " ['billion*', 'number'],\n",
       " ['bills', 'money'],\n",
       " ['bimonth*', 'time'],\n",
       " ['bimonth*', 'relativ'],\n",
       " ['binding', 'cogmech'],\n",
       " ['binding', 'inhib'],\n",
       " ['binge*', 'bio'],\n",
       " ['binge*', 'health'],\n",
       " ['binge*', 'ingest'],\n",
       " ['binging', 'bio'],\n",
       " ['binging', 'health'],\n",
       " ['binging', 'ingest'],\n",
       " ['biolog*', 'work'],\n",
       " ['bipolar', 'bio'],\n",
       " ['bipolar', 'health'],\n",
       " ['birdie*', 'leisure'],\n",
       " ['birth*', 'time'],\n",
       " ...]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liwc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_dict = {}\n",
    "for (w, c) in liwc:\n",
    "    if c not in liwc_dict:\n",
    "        liwc_dict[c] = []\n",
    "    liwc_dict[c].append(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anybod*',\n",
       " 'anyone*',\n",
       " 'anything',\n",
       " 'everybod*',\n",
       " 'everyone*',\n",
       " 'everything*',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he's\",\n",
       " 'hed',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'hes',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'id',\n",
       " 'im',\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'itd',\n",
       " 'itll',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'ive',\n",
       " \"let's\",\n",
       " 'lets',\n",
       " 'me',\n",
       " 'mine',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'nobod*',\n",
       " 'oneself',\n",
       " 'other',\n",
       " 'others',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'shes',\n",
       " 'somebod*',\n",
       " 'someone*',\n",
       " 'something*',\n",
       " 'somewhere',\n",
       " 'stuff',\n",
       " 'that',\n",
       " \"that'd\",\n",
       " \"that'll\",\n",
       " \"that's\",\n",
       " 'thatd',\n",
       " 'thatll',\n",
       " 'thats',\n",
       " 'thee',\n",
       " 'their*',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they've\",\n",
       " 'theyd',\n",
       " 'theyll',\n",
       " 'theyve',\n",
       " 'thine',\n",
       " 'thing*',\n",
       " 'this',\n",
       " 'those',\n",
       " 'thou',\n",
       " 'thoust',\n",
       " 'thy',\n",
       " 'us',\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'weve',\n",
       " 'what',\n",
       " \"what's\",\n",
       " 'whatever',\n",
       " 'whats',\n",
       " 'which',\n",
       " 'whichever',\n",
       " 'who',\n",
       " \"who'd\",\n",
       " \"who'll\",\n",
       " 'whod',\n",
       " 'wholl',\n",
       " 'whom',\n",
       " 'whose',\n",
       " \"y'all\",\n",
       " 'ya',\n",
       " 'yall',\n",
       " 'ye',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'youd',\n",
       " 'youll',\n",
       " 'your',\n",
       " 'youre',\n",
       " 'yours',\n",
       " 'youve']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liwc_dict['pronoun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_liwc_categories(tokens, category_words, relative=True):\n",
    "    category_cnt = 0\n",
    "    if not tokens:\n",
    "        return None\n",
    "    text_len = len(tokens)\n",
    "    for t in tokens:\n",
    "        for word in category_words:\n",
    "            if t==word or (word[-1]=='*' and t.startswith(word[:-1])) \\\n",
    "            or (t==word.split(\"'\")[0]):\n",
    "                category_cnt += 1\n",
    "                break # one token cannot belong to more than one word in the category\n",
    "    if relative:\n",
    "        return category_cnt/text_len\n",
    "    else:\n",
    "        return category_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing for category pronoun...\n",
      "CPU times: user 1min 13s, sys: 37.9 ms, total: 1min 13s\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from functools import partial\n",
    "# for categ in ['negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']:#liwc_dict.keys():\n",
    "for categ in liwc_dict.keys():\n",
    "    if categ in writings_df.columns:\n",
    "        continue\n",
    "    print(\"Encoding for category %s...\" % categ)\n",
    "    writings_df[categ] = writings_df['all_tokens'].apply(partial(encode_liwc_categories, \n",
    "                                                                   category_words=liwc_dict[categ], \n",
    "                                                                   relative=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>negemo</th>\n",
       "      <th>posemo</th>\n",
       "      <th>affect</th>\n",
       "      <th>sad</th>\n",
       "      <th>anx</th>\n",
       "      <th>pronoun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.110183</td>\n",
       "      <td>0.036761</td>\n",
       "      <td>0.079594</td>\n",
       "      <td>0.165661</td>\n",
       "      <td>0.253118</td>\n",
       "      <td>0.404901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negemo</th>\n",
       "      <td>0.110183</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.169785</td>\n",
       "      <td>0.212622</td>\n",
       "      <td>0.415533</td>\n",
       "      <td>0.402437</td>\n",
       "      <td>0.143275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>posemo</th>\n",
       "      <td>0.036761</td>\n",
       "      <td>-0.169785</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.926702</td>\n",
       "      <td>-0.115436</td>\n",
       "      <td>-0.075223</td>\n",
       "      <td>0.239505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>affect</th>\n",
       "      <td>0.079594</td>\n",
       "      <td>0.212622</td>\n",
       "      <td>0.926702</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.043988</td>\n",
       "      <td>0.079093</td>\n",
       "      <td>0.292047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sad</th>\n",
       "      <td>0.165661</td>\n",
       "      <td>0.415533</td>\n",
       "      <td>-0.115436</td>\n",
       "      <td>0.043988</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.117133</td>\n",
       "      <td>0.104218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anx</th>\n",
       "      <td>0.253118</td>\n",
       "      <td>0.402437</td>\n",
       "      <td>-0.075223</td>\n",
       "      <td>0.079093</td>\n",
       "      <td>0.117133</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.156770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pronoun</th>\n",
       "      <td>0.404901</td>\n",
       "      <td>0.143275</td>\n",
       "      <td>0.239505</td>\n",
       "      <td>0.292047</td>\n",
       "      <td>0.104218</td>\n",
       "      <td>0.156770</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            label    negemo    posemo    affect       sad       anx   pronoun\n",
       "label    1.000000  0.110183  0.036761  0.079594  0.165661  0.253118  0.404901\n",
       "negemo   0.110183  1.000000 -0.169785  0.212622  0.415533  0.402437  0.143275\n",
       "posemo   0.036761 -0.169785  1.000000  0.926702 -0.115436 -0.075223  0.239505\n",
       "affect   0.079594  0.212622  0.926702  1.000000  0.043988  0.079093  0.292047\n",
       "sad      0.165661  0.415533 -0.115436  0.043988  1.000000  0.117133  0.104218\n",
       "anx      0.253118  0.402437 -0.075223  0.079093  0.117133  1.000000  0.156770\n",
       "pronoun  0.404901  0.143275  0.239505  0.292047  0.104218  0.156770  1.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.groupby('subject').mean()[['label', 'negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negemo</th>\n",
       "      <th>posemo</th>\n",
       "      <th>affect</th>\n",
       "      <th>sad</th>\n",
       "      <th>anx</th>\n",
       "      <th>pronoun</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.023493</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>0.074548</td>\n",
       "      <td>0.003242</td>\n",
       "      <td>0.002606</td>\n",
       "      <td>0.120154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.026116</td>\n",
       "      <td>0.056145</td>\n",
       "      <td>0.082611</td>\n",
       "      <td>0.003706</td>\n",
       "      <td>0.003591</td>\n",
       "      <td>0.162310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         negemo    posemo    affect       sad       anx   pronoun\n",
       "label                                                            \n",
       "0      0.023493  0.050800  0.074548  0.003242  0.002606  0.120154\n",
       "1      0.026116  0.056145  0.082611  0.003706  0.003591  0.162310"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['label', 'negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']].groupby('label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-bf3a999e8d68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwritings_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'subject'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mcorr\u001b[0;34m(self, method, min_periods)\u001b[0m\n\u001b[1;32m   7513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7514\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pearson\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7515\u001b[0;31m             \u001b[0mcorrel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibalgos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnancorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_float64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_periods\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7516\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"spearman\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7517\u001b[0m             \u001b[0mcorrel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibalgos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnancorr_spearman\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_float64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_periods\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "writings_df.groupby('subject').mean()[['label'] + categories].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: COMET_OPTIMIZER_ID=786fc2b3654047e69f492db122f55b95\n",
      "COMET INFO: Using optimizer config: {'algorithm': 'random', 'configSpaceSize': 600000000000, 'endTime': None, 'id': '786fc2b3654047e69f492db122f55b95', 'lastUpdateTime': None, 'maxCombo': 0, 'name': '786fc2b3654047e69f492db122f55b95', 'parameters': {'batch_size': {'max': 512, 'min': 10, 'scalingType': 'loguniform', 'type': 'integer'}, 'decay': {'max': 0.5, 'min': 1e-08, 'scalingType': 'loguniform', 'type': 'float'}, 'dense_bow_units': {'max': 20, 'min': 1, 'scalingType': 'uniform', 'type': 'integer'}, 'dropout': {'max': 0.7, 'min': 0, 'scalingType': 'uniform', 'type': 'float'}, 'freeze_patience': {'max': 16, 'min': 2, 'scalingType': 'uniform', 'type': 'integer'}, 'l2_dense': {'max': 0.5, 'min': 1e-05, 'scalingType': 'loguniform', 'type': 'float'}, 'lr': {'max': 0.05, 'min': 1e-05, 'scalingType': 'loguniform', 'type': 'float'}, 'lr_reduce_factor': {'max': 0.8, 'min': 0.0001, 'scalingType': 'uniform', 'type': 'float'}, 'lr_reduce_patience': {'max': 16, 'min': 2, 'scalingType': 'uniform', 'type': 'integer'}, 'lstm_units': {'max': 100, 'min': 10, 'scalingType': 'uniform', 'type': 'integer'}, 'optimizer': {'type': 'categorical', 'values': ['adam', 'adagrad', '']}, 'positive_class_weight': {'max': 25, 'min': 1, 'scalingType': 'uniform', 'type': 'integer'}, 'trainable_embeddings': {'type': 'discrete', 'values': [True, False]}}, 'predictor': None, 'spec': {'gridSize': 10, 'maxCombo': 0, 'metric': 'loss', 'minSampleSize': 100, 'objective': 'minimize', 'retryAssignLimit': 0, 'retryLimit': 20, 'seed': 3968493229}, 'startTime': 15879681523, 'state': {'sequence_i': 0, 'sequence_pid': None, 'sequence_retry': 0}, 'status': 'running', 'suggestion_count': 0, 'trials': 1, 'version': '1.0.24'}\n",
      "COMET INFO: Optimizer metrics is 'loss' but no logged values found. Experiment ignored in sweep.\n",
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary:\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     url: https://www.comet.ml/ananana/mental/1ebbfc14aa9440e1948aabac5664d837\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     batch_binary_accuracy [29]: (0.5306122303009033, 0.9572663307189941)\n",
      "COMET INFO:     batch_f1_m [29]           : (0.0030426757875829935, 0.0336134247481823)\n",
      "COMET INFO:     batch_loss [29]           : (0.30054065585136414, 1.4031785726547241)\n",
      "COMET INFO:     batch_precision_m [29]    : (0.011335793882608414, 0.09260831028223038)\n",
      "COMET INFO:     batch_recall_m [29]       : (0.002346971072256565, 0.1666666716337204)\n",
      "COMET INFO:     sys.cpu.percent.01 [20]   : (4.8, 82.9)\n",
      "COMET INFO:     sys.cpu.percent.02 [20]   : (5.2, 73.9)\n",
      "COMET INFO:     sys.cpu.percent.03 [20]   : (4.6, 60.6)\n",
      "COMET INFO:     sys.cpu.percent.04 [20]   : (3.3, 57.8)\n",
      "COMET INFO:     sys.cpu.percent.avg [20]  : (4.925, 66.2)\n",
      "COMET INFO:     sys.gpu.0.total_memory    : (1073414144.0, 1073414144.0)\n",
      "COMET INFO:     sys.load.avg [20]         : (0.16, 4.02)\n",
      "COMET INFO:     sys.ram.total [20]        : (8277307392.0, 8277307392.0)\n",
      "COMET INFO:     sys.ram.used [20]         : (7586635776.0, 7714193408.0)\n",
      "COMET INFO:   Other [count]:\n",
      "COMET INFO:     optimizer_count       : 3\n",
      "COMET INFO:     optimizer_id          : 57eb178250e2401aa14cfc6860a4217a\n",
      "COMET INFO:     optimizer_metric      : loss\n",
      "COMET INFO:     optimizer_metric_value: None\n",
      "COMET INFO:     optimizer_parameters  : {'batch_size': 245, 'decay': 0.1402033634855056, 'dense_bow_units': 4, 'dropout': 0.46002110523805456, 'freeze_patience': 15, 'l2_dense': 0.0814165473266369, 'lr': 1.6244588122197995e-05, 'lr_reduce_factor': 0.6638163734300414, 'lr_reduce_patience': 10, 'lstm_units': 92, 'optimizer': 'adagrad', 'positive_class_weight': 5, 'set_trainable': True, 'trainable_embeddings': False}\n",
      "COMET INFO:     optimizer_pid         : 681f4971a3f98a865eb3100a742e6621e62a3420\n",
      "COMET INFO:     optimizer_process     : 2985\n",
      "COMET INFO:     optimizer_trial       : 1\n",
      "COMET INFO:     optimizer_version     : 1.0.24\n",
      "COMET INFO:     trainable_params      : 2071854\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     git-patch  : 1\n",
      "COMET INFO:     histogram3d: 1\n",
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/ananana/mental/6925a9644c034cbf9cd7f12c0a96e0f3\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 111375 samples, validate on 31863 samples\n",
      "Epoch 1/15\n",
      "111320/111375 [============================>.] - ETA: 0s - loss: 0.6696 - binary_accuracy: 0.8707 - f1_m: 0.1200 - precision_m: 0.0902 - recall_m: 0.2582\n",
      "Epoch 00001: val_loss improved from inf to 0.48204, saving model to models/experiment_best\n",
      "111375/111375 [==============================] - 317s 3ms/sample - loss: 0.6695 - binary_accuracy: 0.8707 - f1_m: 0.1201 - precision_m: 0.0902 - recall_m: 0.2584 - val_loss: 0.4820 - val_binary_accuracy: 0.7968 - val_f1_m: 0.2121 - val_precision_m: 0.1454 - val_recall_m: 0.4314\n",
      "Epoch 2/15\n",
      " 47190/111375 [===========>..................] - ETA: 2:52 - loss: 0.5882 - binary_accuracy: 0.8703 - f1_m: 0.1490 - precision_m: 0.1088 - recall_m: 0.2949"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-f94476829a7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m                           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                           \u001b[0mcallback_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfreeze_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_lr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                       model_path='models/experiment')\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-124-48d6d530785b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, x_train, y_train, x_test, y_test, batch_size, epochs, class_weight, start_epoch, workers, callback_list, model_path)\u001b[0m\n\u001b[1;32m     17\u001b[0m             callbacks = [\n\u001b[1;32m     18\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%s_best'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcallback_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             ])\n\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/comet_ml/monkey_patching.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m                     )\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Call after callbacks once we have the return value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Declare your hyperparameters search:\n",
    "tune_epochs=15\n",
    "config = {\n",
    "      \"algorithm\": \"random\",\n",
    "      \"parameters\": {\n",
    "          \"lstm_units\": {\"type\": \"integer\", \"min\": 10, \"max\": 100},\n",
    "          \"dense_bow_units\": {\"type\": \"integer\", \"min\": 1, \"max\": 20},\n",
    "          \"lr\": {\"type\": \"float\", \"min\": 0.00001, \"max\": 0.05, \"scalingType\": \"loguniform\"},\n",
    "          \"l2_dense\": {\"type\": \"float\", \"min\": 0.00001, \"max\": 0.5, \"scalingType\": \"loguniform\"},\n",
    "          \"dropout\": {\"type\": \"float\", \"min\": 0, \"max\": 0.7, \"scalingType\": \"uniform\"},\n",
    "          \"optimizer\": {\"type\": \"categorical\", \"values\": [\"adam\", \"adagrad\", \"\"]},\n",
    "          \"batch_size\": {\"type\": \"integer\", \"min\": 10, \"max\": 512, \"scalingType\": \"loguniform\"},\n",
    "          \"positive_class_weight\": {\"type\": \"integer\", \"min\": 1, \"max\": 25},\n",
    "          \"trainable_embeddings\": {\"type\": \"discrete\", \"values\": [True, False]},\n",
    "          \"freeze_patience\": {\"type\": \"integer\", \"min\": 2, \"max\": tune_epochs+1},\n",
    "          \"lr_reduce_factor\": {\"type\": \"float\", \"min\": 0.0001, \"max\": 0.8},\n",
    "          \"lr_reduce_patience\": {\"type\": \"integer\", \"min\": 2, \"max\": tune_epochs+1},\n",
    "          \"decay\": {\"type\": \"float\", \"min\": 0.00000001, \"max\": 0.5, \"scalingType\": \"loguniform\"},          \n",
    "      },\n",
    "      \"spec\": {\n",
    "          \"metric\": \"loss\",\n",
    "          \"objective\": \"minimize\",\n",
    "      },\n",
    "  }\n",
    "optimizer = Optimizer(config, api_key=\"eoBdVyznAhfg3bK9pZ58ZSXfv\")\n",
    "\n",
    "for experiment in optimizer.get_experiments(project_name=\"mental\"):\n",
    "    experiment.add_tag(\"tune\")\n",
    "    \n",
    "    # Test the model\n",
    "    hyperparams_config = {\n",
    "        param: experiment.get_parameter(param) for param in config['parameters'].keys()}\n",
    "    if not hyperparams_config['optimizer']:\n",
    "        hyperparams_config['optimizer'] = optimizers.Adam(lr=hyperparams_config['lr'], \n",
    "                                   decay=hyperparams_config['decay'])\n",
    "    model = build_model(hyperparams=hyperparams_config,\n",
    "                        hyperparams_features=hyperparams_features, \n",
    "                        embedding_matrix=embedding_matrix, emotions=emotions,\n",
    "                       stopwords_list=stopword_list)\n",
    "    freeze_layer = FreezeLayer(patience=experiment.get_parameter('freeze_patience'),\n",
    "                              set_to=not experiment.get_parameter('trainable_embeddings'))\n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            factor=experiment.get_parameter('lr_reduce_factor'),\n",
    "                                            patience=experiment.get_parameter('lr_reduce_patience'), \n",
    "                                            min_lr=0.000001, verbose=1)\n",
    "    history = train_model(model, \n",
    "            x_train, y_train, x_test, y_test,\n",
    "            epochs=tune_epochs, batch_size=experiment.get_parameter('batch_size'),\n",
    "                      class_weight={0:1, 1:experiment.get_parameter('positive_class_weight')}, \n",
    "                          workers=2,\n",
    "                          callback_list = [freeze_layer, reduce_lr],\n",
    "                      model_path='models/experiment')\n",
    "    loss = history.history['loss'][-1]\n",
    "    \n",
    "    # Report the loss, if not auto-logged:\n",
    "    experiment.log_metric(\"loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
