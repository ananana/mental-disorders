{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob, os\n",
    "import numpy as np\n",
    "from comet_ml import Experiment, Optimizer\n",
    "import pickle\n",
    "import logging\n",
    "import sys\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # When cudnn implementation not found, run this\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Note: when starting kernel, for gpu_available to be true, this needs to be run\n",
    "# only reserve 1 GPU\n",
    "os.environ['TFHUB_CACHE_DIR'] = '/home/anasab/tf_cache'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "assert 'GPU' in str(device_lib.list_local_devices())\n",
    "\n",
    "# # confirm Keras sees the GPU (for TensorFlow 1.X + Keras)\n",
    "# from keras import backend\n",
    "# assert len(backend.tensorflow_backend._get_available_gpus()) > 0\n",
    "\n",
    "# # confirm PyTorch sees the GPU\n",
    "# from torch import cuda\n",
    "# assert cuda.is_available()\n",
    "# assert cuda.device_count() > 0\n",
    "# print(cuda.get_device_name(cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_KERAS'] = '1'\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Lambda, BatchNormalization, TimeDistributed, \\\n",
    "    Bidirectional, Input, InputLayer, concatenate, Flatten, RepeatVector, Activation, Multiply, Permute, \\\n",
    "    Conv1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import callbacks, optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model, Sequence\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer, TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = \"depression\"\n",
    "transfer_type = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('training')\n",
    "ch = logging.StreamHandler(sys.stdout)\n",
    "# create formatter\n",
    "formatter = logging.Formatter(\"%(asctime)s;%(levelname)s;%(message)s\")\n",
    "# add formatter to ch\n",
    "ch.setFormatter(formatter)\n",
    "# add ch to logger\n",
    "logger.addHandler(ch)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.enable_eager_execution()\n",
    "my_seed = 1234\n",
    "# tf.set_random_seed(my_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend import manual_variable_initialization \n",
    "manual_variable_initialization(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_subject_writings(subject_file):\n",
    "    writings = []\n",
    "    with open(subject_file) as sf:\n",
    "        contents = sf.read()\n",
    "        root = ET.fromstring(contents)\n",
    "        try:\n",
    "            subject = root.findall('ID')[0].text.strip()\n",
    "        except Exception:\n",
    "            print('Cannot extract ID', contents[:500], '\\n-------\\n')        \n",
    "        for w in root.iter('WRITING'):\n",
    "            subject_writings = {'subject': subject}\n",
    "            for title in w.findall('TITLE'):\n",
    "                subject_writings['title'] = title.text\n",
    "            for text in w.findall('TEXT'):\n",
    "                subject_writings['text'] = text.text\n",
    "            for date in w.findall('DATE'):\n",
    "                subject_writings['date'] = date.text\n",
    "            writings.append(subject_writings)\n",
    "    return writings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/anasab/' \n",
    "# root_dir = '/home/ana/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eRisk 2020 T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datadir_T1 = root_dir + '/eRisk/data/eRisk2020_T1_train/eRISK2020_T1_training_data/eRISK2020_training_data/data/'\n",
    "# labels_file_T1 = root_dir + '/eRisk/data//eRisk2020_T1_train/eRISK2020_T1_training_data/eRISK2020_training_data/golden_truth.txt'\n",
    "\n",
    "datadirs_T1_2020 = {\n",
    "    'train': ['./data/'],\n",
    "    'test': ['./DATA/']\n",
    "}\n",
    "datadir_root_T1_2020 = {\n",
    "    'train': root_dir + '/eRisk/data/eRisk2020_T1_train/eRISK2020_T1_training_data/eRISK2020_training_data/',\n",
    "    'test': root_dir + '/eRisk/data/2020/T1/'\n",
    "}\n",
    "    \n",
    "labels_files_T1_2020 = {\n",
    "    'train': ['golden_truth.txt'],\n",
    "    'test': ['T1_erisk_golden_truth.txt']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_2020(datadir_root_T1_2020,\n",
    "                   datadirs_T1_2020,\n",
    "                   labels_files_T1_2020,\n",
    "                   test_suffix='0000',\n",
    "                    chunked_subsets=None):\n",
    "\n",
    "\n",
    "    writings = {'train': [], 'test': []}\n",
    "    writings_df = pd.DataFrame()\n",
    "    labels_df = pd.DataFrame()\n",
    "#     for subset in ('train', 'test'):\n",
    "    for subset in ('test',):\n",
    "        for subdir in [os.path.join(datadir_root_T1_2020[subset], subp) for subp in datadirs_T1_2020[subset]]:\n",
    "\n",
    "            for subject_file in os.listdir(subdir):\n",
    "                writings[subset].extend(read_subject_writings(os.path.join(subdir, subject_file)))\n",
    "        writings_df_part = pd.DataFrame(writings[subset])\n",
    "        # add a suffix for users in the test -- the numbers are duplicated with the ones in train\n",
    "        if subset=='test':\n",
    "            writings_df_part['subject'] = writings_df_part['subject'].apply(lambda s: s+test_suffix)\n",
    "            print(subset, writings_df_part.subject)\n",
    "        writings_df_part['subset'] = subset\n",
    "        writings_df = pd.concat([writings_df, writings_df_part])\n",
    "        writings_df.reindex()\n",
    "\n",
    "        for label_file in labels_files_T1_2020[subset]:\n",
    "            labels = pd.read_csv(os.path.join(datadir_root_T1_2020[subset], label_file), \n",
    "                                 delimiter='\\s+', names=['subject', 'label'])\n",
    "            # add a suffix for users in the test -- the numbers are duplicated with the ones in train\n",
    "            if subset=='test':\n",
    "                labels['subject'] = labels['subject'].apply(lambda s: s+test_suffix)\n",
    "            labels_df = pd.concat([labels_df, labels])\n",
    "    labels_df = labels_df.drop_duplicates()\n",
    "    labels_df = labels_df.set_index('subject')\n",
    "\n",
    "    writings_df = writings_df.drop_duplicates()\n",
    "    \n",
    "    writings_df = writings_df.join(labels_df, on='subject')\n",
    "    \n",
    "    return writings_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eRisk 2019 T1 (Anorexia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadirs_T1_2019 = {\n",
    "    'train': ['2018 test/', '2018 train/positive_examples/', '2018 train/negative_examples/'],\n",
    "    'test': ['data/']\n",
    "}\n",
    "datadir_root_T1_2019 = {\n",
    "    'train': root_dir + '/eRisk/data/past/eRisk2019_T1/training data - t1/',\n",
    "    'test': root_dir + '/eRisk/data/past/eRisk2019_T1/test data - T1/'\n",
    "}\n",
    "    \n",
    "labels_files_T1_2019 = {\n",
    "    'train': ['2018 train/risk_golden_truth.txt', '2018 test/risk-golden-truth-test.txt'],\n",
    "    'test': ['T1_erisk_golden_truth.txt']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_2019(datadir_root_T1_2019,\n",
    "                   datadirs_T1_2019,\n",
    "                   labels_files_T1_2019,\n",
    "                   test_suffix='0000',\n",
    "                    chunked_subsets='train'):\n",
    "    writings = {'train': [], 'test': []}\n",
    "    writings_df = pd.DataFrame()\n",
    "    labels_df = pd.DataFrame()\n",
    "\n",
    "    for subset in ('train', 'test'):\n",
    "        for subdir in [os.path.join(datadir_root_T1_2019[subset], subp) for subp in datadirs_T1_2019[subset]]:\n",
    "            if subset in chunked_subsets:\n",
    "                chunkdirs = [os.path.join(datadir_root_T1_2019[subset], subdir, chunkdir) \n",
    "                             for chunkdir in os.listdir(subdir)]\n",
    "            else:\n",
    "                chunkdirs = [os.path.join(datadir_root_T1_2019[subset], subdir)]\n",
    "                \n",
    "            for chunkdir in chunkdirs:\n",
    "                print(chunkdir)\n",
    "                if not os.path.isdir(chunkdir):\n",
    "                    continue\n",
    "                for subject_file in os.listdir(chunkdir):\n",
    "                    writings[subset].extend(read_subject_writings(os.path.join(chunkdir, subject_file)))\n",
    "        writings_df_part = pd.DataFrame(writings[subset])\n",
    "        # add a suffix for users in the test -- the numbers are duplicated with the ones in train\n",
    "        if subset=='test':\n",
    "            writings_df_part['subject'] = writings_df_part['subject'].apply(lambda s: s+test_suffix)\n",
    "            print(subset, writings_df_part.subject)\n",
    "        writings_df_part['subset'] = subset\n",
    "        writings_df = pd.concat([writings_df, writings_df_part])\n",
    "        writings_df.reindex()\n",
    "\n",
    "        for label_file in labels_files_T1_2019[subset]:\n",
    "            labels = pd.read_csv(os.path.join(datadir_root_T1_2019[subset], label_file), \n",
    "                                 delimiter='\\s+', names=['subject', 'label'])\n",
    "            # add a suffix for users in the test -- the numbers are duplicated with the ones in train\n",
    "            if subset=='test':\n",
    "                labels['subject'] = labels['subject'].apply(lambda s: s+test_suffix)\n",
    "            labels_df = pd.concat([labels_df, labels])\n",
    "    labels_df = labels_df.drop_duplicates()\n",
    "    labels_df = labels_df.set_index('subject')\n",
    "\n",
    "    writings_df = writings_df.drop_duplicates()\n",
    "    \n",
    "    writings_df = writings_df.join(labels_df, on='subject')\n",
    "    \n",
    "    return writings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eRisk 2018 (Depression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadirs_T1_2018 = {\n",
    "    'train': ['train/positive_examples_anonymous_chunks/', 'train/positive_examples_anonymous_chunks/', 'test/'],\n",
    "    'test': ['task 1 - depression (test split, train split is 2017 data)/']\n",
    "}\n",
    "datadir_root_T1_2018 = {\n",
    "    'train': root_dir + '/eRisk/data/2017/',\n",
    "    'test': root_dir + '/eRisk/data/2018/'\n",
    "}\n",
    "    \n",
    "labels_files_T1_2018 = {\n",
    "    'train': ['train/risk_golden_truth.txt', 'test/test_golden_truth.txt'],\n",
    "    'test': ['task 1 - depression (test split, train split is 2017 data)/risk-golden-truth-test.txt']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLPsych\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadirs_clpsych = {\n",
    "    'train': [''],\n",
    "    'test': ['']\n",
    "}\n",
    "datadir_root_clpsych = {\n",
    "    'train': root_dir + '/eRisk/data/clpsych/final_training_data/',\n",
    "    'test': root_dir + '/eRisk/data/clpsych/final_testing_data/'\n",
    "}\n",
    "    \n",
    "labels_files_clpsych = [root_dir + '/eRisk/data/clpsych/anonymized_user_info_by_chunk.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_subject_data_clpsych(subject_file):\n",
    "    writings = []\n",
    "    with open(subject_file, \"rt\", encoding=\"utf-8\") as sf:\n",
    "        user = subject_file.split(\"/\")[-1].split(\".\")[0]\n",
    "        print(subject_file)\n",
    "\n",
    "        for line in sf:\n",
    "            data = json.loads(line)#.encode('utf-16','surrogatepass').decode('utf-16'))\n",
    "            data['subject'] = user\n",
    "            writings.append(data)\n",
    "    return writings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_clpsych(datadir_root_clpsych,\n",
    "                   datadirs_clpsych,\n",
    "                   labels_files_T1_2019,\n",
    "                      label_by=['depression']):\n",
    "    writings = {'train': [], 'test': []}\n",
    "    writings_df = pd.DataFrame()\n",
    "    labels_df = pd.DataFrame()\n",
    "\n",
    "    for subset in ('test',):#, 'test'):\n",
    "        for subdir in [os.path.join(datadir_root_clpsych[subset], subp) for subp in datadirs_clpsych[subset]]:\n",
    "            for subject_file in glob.glob(subdir + \"/*.tweets\"):\n",
    "#                 if subject_file.split(\"/\")[-1] != 'sZVVktDN8qqjA.tweets':\n",
    "#                     continue\n",
    "                writings[subset].extend(read_subject_data_clpsych(os.path.join(subdir, subject_file)))\n",
    "        writings_df_part = pd.DataFrame(writings[subset])\n",
    "        writings_df_part['subset'] = subset\n",
    "        writings_df = pd.concat([writings_df, writings_df_part])\n",
    "        writings_df.reindex()\n",
    "\n",
    "    for label_file in labels_files_clpsych:\n",
    "        labels = pd.read_csv(label_file, \n",
    "                    names=['subject','age','num_tweets','gender','condition','chunk_index'])\n",
    "        labels['label'] = labels['condition'].apply(lambda c: 1 if c in label_by else 0)\n",
    "        \n",
    "        labels_df = pd.concat([labels_df, labels])\n",
    "        labels_df = labels_df.drop_duplicates()\n",
    "        labels_df = labels_df.set_index('subject')\n",
    "\n",
    "        # TODO: this deduplication throws some unicode, surrogates not allowed, exception\n",
    "#     writings_df = writings_df.drop_duplicates(subset=['id', 'subject', 'subset', 'created_at', 'text'])\n",
    "    \n",
    "    writings_df = writings_df.join(labels_df, on='subject')\n",
    "    writings_df['date'] = writings_df['created_at']\n",
    "    \n",
    "    return writings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symanto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_symanto(tsv_path=\"/eRisk/data/symanto/clean_dataset_with_timeline_balancedUsers.tsv\"):\n",
    "    label_key = {'REAL_LABEL_IS_DEPRESSED': 1,\n",
    "             'REAL_LABEL_IS_NON_DEPRESSED': 0}\n",
    "    prediction_key = {'predicted_as_depressed': 1,\n",
    "                     'predicted_as_non_depressed': 0,\n",
    "                     'DEPRESS_STRING_WAS_MENTIONED': -1}\n",
    "    \n",
    "    writings_df = pd.read_csv(root_dir + tsv_path, \n",
    "                              sep='\\t', names=['subject', 'date', 'text', 'prediction_text', 'real_label_text'])\n",
    "\n",
    "    writings_df = writings_df.dropna()\n",
    "    writings_df['label'] = writings_df['real_label_text'].apply(lambda l: label_key[l])\n",
    "    writings_df['prediction'] = writings_df['prediction_text'].apply(lambda l: prediction_key[l])\n",
    "#     By ommitting -1s we are excluding tweets where \"DEPRESSED\" was mentioned, and all after\n",
    "    writings_df = writings_df.drop(writings_df[writings_df['prediction']==-1].index)\n",
    "\n",
    "    \n",
    "    return writings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df = read_texts_2020(datadir_T1, labels_file_T1)\n",
    "# writings_df = read_texts_2019(datadir_root_T1_2019,\n",
    "#                    datadirs_T1_2019,\n",
    "#                    labels_files_T1_2019)\n",
    "# writings_df_depression = read_texts_2019(datadir_root_T1_2018,\n",
    "#                    datadirs_T1_2018,\n",
    "#                    labels_files_T1_2018,\n",
    "#                              chunked_subsets=['train', 'test'])\n",
    "\n",
    "if dataset_type == \"combined\":\n",
    "    writings_df_selfharm = pickle.load(open('data/writings_df_selfharm_all', 'rb'))\n",
    "    writings_df_anorexia = pickle.load(open('data/writings_df_anorexia_liwc', 'rb'))\n",
    "    writings_df_depression = pickle.load(open('data/writings_df_depression_liwc', 'rb'))\n",
    "    writings_df_selfharm['source'] = 'selfharm'\n",
    "    writings_df_anorexia['source'] = 'anorexia'\n",
    "    writings_df_depression['source'] = 'depression'\n",
    "    writings_df = pd.DataFrame()\n",
    "    writings_df = pd.concat([writings_df, writings_df_depression])\n",
    "    writings_df = pd.concat([writings_df, writings_df_selfharm])\n",
    "    writings_df = pd.concat([writings_df, writings_df_anorexia])\n",
    "elif dataset_type == \"combined_depr\":\n",
    "    writings_df = pd.DataFrame.from_dict(json.load(open('data/writings_df_depression_all.json')))\n",
    "elif dataset_type == \"clpsych\":\n",
    "    writings_df = pd.DataFrame.from_dict(json.load(open('data/writings_df_%s_liwc_affect.json' % dataset_type)))#read_texts_clpsych(datadir_root_clpsych, datadirs_clpsych, labels_files_clpsych)\n",
    "#     writings_df_test = pd.DataFrame.from_dict(json.load(open('writings_df_%s_test.json' % dataset_type)))#read_texts_clpsych(datadir_root_clpsych, datadirs_clpsych, labels_files_clpsych)\n",
    "#     writings_df_test = read_texts_clpsych(datadir_root_clpsych, datadirs_clpsych, labels_files_clpsych)\n",
    "    label_by = ['depression', 'ptsd']\n",
    "    writings_df.drop(writings_df[writings_df['condition']=='depression'].index, inplace=True)\n",
    "    writings_df['label'] = writings_df['condition'].apply(lambda c: 1 if c in label_by else 0)\n",
    "#     writings_df['date'] = writings_df['created_at']\n",
    "elif dataset_type == \"symanto\":\n",
    "    writings_df = read_texts_symanto()\n",
    "elif dataset_type == 'selfharm':\n",
    "    writings_df = pickle.load(open('data/writings_df_%s_all' % dataset_type, 'rb'))\n",
    "elif dataset_type in [\"depression\", \"anorexia\", \"selfharm\"]:\n",
    "    writings_df = pickle.load(open('data/writings_df_%s_liwc' % dataset_type, 'rb'))\n",
    "else:\n",
    "    logger.error(\"Unknown dataset %s\" % dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df[writings_df['condition']!='control'].head(150)[['label', 'condition', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df.label.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(writings_df[writings_df['label']==1].subject))\n",
    "len(set(writings_df[writings_df['label']==0].subject))\n",
    "len(set(writings_df[writings_df['subset']=='test'].subject))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(writings_df['source'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regtokenizer = RegexpTokenizer(r'\\w+')\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "def tokenize(t, tokenizer=regtokenizer):\n",
    "    return regtokenizer.tokenize(t.lower())\n",
    "\n",
    "def tokenize_tweets(t, stop=True):\n",
    "    tokens = tweet_tokenizer.tokenize(t.lower())\n",
    "    tokens_clean = [re.sub(\"^#\", \"\", token) for token in tokens]\n",
    "    tokens_clean = [token for token in tokens_clean \n",
    "                            if re.match(\"^[a-z]*$\", token)]\n",
    "    if not stop:\n",
    "        tokens_clean = [token for token in tokens\n",
    "                       if (token not in sw)]\n",
    "    return tokens_clean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.sub(\"^#\", \"\", \"#h#ash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fields(writings_df, tokenize_fct=tokenize, columns=['title', 'text']):\n",
    "    for c in columns:\n",
    "        writings_df['tokenized_%s' % c] = writings_df['%s' % c].apply(lambda t: tokenize_fct(t) \n",
    "                                                                if type(t)==str and t else None)\n",
    "        writings_df['%s_len' % c] = writings_df['tokenized_%s' % c].apply(lambda t: len(t) \n",
    "                                                                    if type(t)==list and t else None)\n",
    "    return writings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df = tokenize_fields(writings_df, tokenize_fct=tokenize_tweets, columns=['title'])\n",
    "# writings_df = tokenize_fields(writings_df, tokenize_fct=tokenize_tweets, columns=['text'])\n",
    "# writings_df = tokenize_fields(writings_df, tokenize_fct=tokenize, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df.text_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df.title_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df.groupby('subject').mean().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df.groupby('subject').max().groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Average number of posts per user\", writings_df.groupby('subject').count().title.mean())\n",
    "# print(\"Average number of comments per user\", writings_df.groupby('subject').count().text.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df.groupby('subject').count().title.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df.groupby('subject').count().text.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df.tokenized_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_dict = writings_df.to_dict()\n",
    "# json.dump(writings_dict, open(\"writings_df_clpsych_all.json\", \"w+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Total examples\", writings_df[['subject', 'subset', 'label'\n",
    "#                                     ]].groupby('subject').min().groupby('subset').count())\n",
    "# print(\"Positive examples\", writings_df[['subject', 'subset', 'label'\n",
    "#                                     ]].groupby('subject').min().groupby('subset').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df[['label', 'source']].groupby('source').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df['text'] = writings_df['text'].fillna(\"\")\n",
    "# writings_df['title'] = writings_df['title'].fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features and encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_params(model, model_path, hyperparams, hyperparams_features):\n",
    "    model.save_weights(model_path + \"_weights.h5\", save_format='h5')\n",
    "#     model.save(model_path + \"_model.model\")\n",
    "#     model.save(model_path + \"_model.h5\")\n",
    "    with open(model_path + '.hp.json', 'w+') as hpf:\n",
    "        hpf.write(json.dumps({k:v for (k,v) in hyperparams.items() if k!='optimizer'}))\n",
    "    with open(model_path + '.hpf.json', 'w+') as hpff:\n",
    "        hpff.write(json.dumps(hyperparams_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_params(model_path):\n",
    "    with open(model_path + '.hp.json', 'r') as hpf:\n",
    "        hyperparams = json.loads(hpf.read())\n",
    "    with open(model_path + '.hpf.json', 'r') as hpff:\n",
    "        hyperparams_features = json.loads(hpff.read())\n",
    "    return hyperparams, hyperparams_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_features = {\n",
    "    \"max_features\": 20002,\n",
    "    # cut texts after this number of words\n",
    "    # (among top max_features most common words)\n",
    "    \"embedding_dim\": 300,\n",
    "    'vocabulary_path': 'data/all_vocab_clpsych_erisk_stop_20000.pkl',\n",
    "#     'embeddings_path': root_dir + '/eRisk/embeddings/finetuned_glove_clpsych_erisk_stop_normalized_20000.pkl',\n",
    "    'embeddings_path': root_dir + '/resources/glove.840B/glove.840B.300d.txt',\n",
    "    'liwc_words_cached': 'data/liwc_categories_for_vocabulary_erisk_clpsych_stop_20K.pkl',\n",
    "    \"user_level\": True, # deprecated\n",
    "    \"transfer\": transfer_type,\n",
    "#     \"pretrained_model_path\": 'models/lstm_clpsych_hierarchical64_ptsdvsdepr'\n",
    "#     \"pretrained_model_path\": 'models/lstm_clpsych_hierarchical64_ptsd'\n",
    "    \"pretrained_model_path\": 'models/lstm_symanto_hierarchical64'\n",
    "#     \"pretrained_model_path\": 'models/lstm_clpsych_hierarchical65'\n",
    "#     \"pretrained_model_path\": 'models/lstm_depression_hierarchical56'\n",
    "#     \"pretrained_model_path\": 'models/lstm_anorexia_hierarchical64'\n",
    "#     \"pretrained_model_path\": 'models/lstm_selfharm_hierarchical59'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if transfer_type:\n",
    "    pretrained_model_path = hyperparams_features['pretrained_model_path']\n",
    "    hyperparams, hyperparams_features = load_params(hyperparams_features['pretrained_model_path'])\n",
    "    hyperparams_features['pretrained_model_path'] = pretrained_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_NRC(nrc_path):\n",
    "    word_emotions = {}\n",
    "    emotion_words = {}\n",
    "    with open(nrc_path) as in_f:\n",
    "        for line in in_f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            word, emotion, label = line.split()\n",
    "            if word not in word_emotions:\n",
    "                word_emotions[word] = set()\n",
    "            if emotion not in emotion_words:\n",
    "                emotion_words[emotion] = set()\n",
    "            label = int(label)\n",
    "            if label:\n",
    "                word_emotions[word].add(emotion)\n",
    "                emotion_words[emotion].add(word)\n",
    "    return emotion_words\n",
    "\n",
    "nrc_lexicon_path = root_dir + '/resources/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'\n",
    "nrc_lexicon = load_NRC(nrc_lexicon_path)\n",
    "emotions = list(nrc_lexicon.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_emotions(tokens, emotion_lexicon, emotions, relative=True):\n",
    "    text_len = len(tokens)\n",
    "    encoded_emotions = [0 for e in emotions]\n",
    "    for i, emotion in enumerate(emotions):\n",
    "        try:\n",
    "            emotion_words = [t for t in tokens if t in emotion_lexicon[emotion]]\n",
    "            if relative and len(tokens):\n",
    "                encoded_emotions[i] = len(emotion_words) / len(tokens)\n",
    "            else:\n",
    "                encoded_emotions[i] = len(emotion_words)\n",
    "        except ValueError:\n",
    "            print(\"Emotion not found.\")\n",
    "    return encoded_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from liwc_readDict import readDict\n",
    "\n",
    "liwc_dict = {}\n",
    "for (w, c) in readDict(root_dir + '/resources/liwc.dic'):\n",
    "    if c not in liwc_dict:\n",
    "        liwc_dict[c] = []\n",
    "    liwc_dict[c].append(w)\n",
    "\n",
    "categories = set(liwc_dict.keys())\n",
    "len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_categories=[c for c in categories]# if c in writings_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_words_for_categories = pickle.load(open(\"data/liwc_categories_for_vocabulary_erisk_clpsych_stop_20K.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"liwc_categories.txt\",\"w+\") as f:\n",
    "    for cat in liwc_dict:\n",
    "        f.write(\"%s: %s\\n\" % (cat, \",\".join(liwc_dict[cat])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nrc_emotions.txt\",\"w+\") as f:\n",
    "    for cat in nrc_lexicon:\n",
    "        f.write(\"%s: %s\\n\" % (cat, \",\".join(nrc_lexicon[cat])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Char n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(tokens):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Personal pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_person_pronouns = {\"i\", \"me\", \"my\", \"mine\", \"myself\"}\n",
    "def encode_pronouns(tokens, pronouns={\"i\", \"me\", \"my\", \"mine\", \"myself\"}, relative=True):\n",
    "    if not tokens:\n",
    "        return 0\n",
    "    text_len = len(tokens)\n",
    "    nr_pronouns = len([t for t in tokens if t in pronouns])\n",
    "    if relative and text_len:\n",
    "        return nr_pronouns/text_len\n",
    "    else:\n",
    "        return nr_pronouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words(\"english\")\n",
    "def encode_stopwords(tokens, stopwords=stopword_list):\n",
    "    encoded_stopwords = [0 for s in stopword_list]\n",
    "    if not tokens:\n",
    "        return encoded_stopwords\n",
    "    for i, stopword in enumerate(stopwords):\n",
    "        if stopword in tokens:\n",
    "            encoded_stopwords[i] += 1\n",
    "    return encoded_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "from bert.tokenization import FullTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "    Args:\n",
    "      guid: Unique id for the example.\n",
    "      text_a: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "        Only must be specified for sequence pair tasks.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text_for_bert(tokenizer, example, max_seq_length=256):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "#     if isinstance(example, PaddingInputExample):\n",
    "#         input_ids = [0] * max_seq_length\n",
    "#         input_mask = [0] * max_seq_length\n",
    "#         segment_ids = [0] * max_seq_length\n",
    "#         label = 0\n",
    "#         return input_ids, input_mask, segment_ids, label\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    if len(tokens_a) > max_seq_length - 2:\n",
    "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
    "\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    return input_ids, input_mask, segment_ids, example.label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer_from_hub_module(sess):\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    bert_module =  hub.Module(bert_path)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    vocab_file, do_lower_case = sess.run(\n",
    "        [\n",
    "            tokenization_info[\"vocab_file\"],\n",
    "            tokenization_info[\"do_lower_case\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_sess(sess=None):\n",
    "    if not sess:\n",
    "        sess_config = tf.ConfigProto(\n",
    "            device_count={ 'GPU' : 1, 'CPU': 4 },\n",
    "            intra_op_parallelism_threads = 0,\n",
    "            inter_op_parallelism_threads = 4,\n",
    "            allow_soft_placement=False,\n",
    "            log_device_placement=True,\n",
    "        )\n",
    "        sess_config.gpu_options.allow_growth = True\n",
    "        sess_config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "        sess = tf.Session(config=sess_config)\n",
    "\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_model(model_path, hyperparams):\n",
    "    metrics_class = Metrics(threshold=hyperparams['threshold'])\n",
    "    dependencies = {\n",
    "    'f1_m': metrics_class.f1_m,\n",
    "    'auc': metrics_class.auc,\n",
    "    'precision_m': metrics_class.precision_m,\n",
    "    'recall_m': metrics_class.recall_m,\n",
    "    'binary_crossentropy_custom': binary_crossentropy_custom,\n",
    "    'BertLayer': BertLayer\n",
    "    }\n",
    "    loaded_model = load_model(model_path + \"_model.h5\", custom_objects=dependencies)\n",
    "#     loaded_model = load_model(model_path + \"_model.model\", custom_objects=dependencies)\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_model_weights(model_path, hyperparams, emotions, stopword_list, liwc_categories, classes, h5=False):\n",
    "    metrics_class = Metrics(threshold=hyperparams['threshold'])\n",
    "    dependencies = {\n",
    "    'f1_m': metrics_class.f1_m,\n",
    "    'auc': metrics_class.auc,\n",
    "    'precision_m': metrics_class.precision_m,\n",
    "    'recall_m': metrics_class.recall_m,\n",
    "    'binary_crossentropy_custom': binary_crossentropy_custom,\n",
    "    'BertLayer': BertLayer\n",
    "    }\n",
    "    loaded_model = initialize_model(hyperparams=hyperparams, hyperparams_features=hyperparams_features, \n",
    "                                    embedding_matrix=embedding_matrix, \n",
    "                                 emotions=emotions, stopword_list=stopword_list, liwc_categories=liwc_categories,\n",
    "                                classes=classes)\n",
    "    loaded_model.summary()\n",
    "#     loaded_model.load_weights(model_path + \"_weights\")\n",
    "    path = model_path + \"_weights\"\n",
    "    by_name = False\n",
    "    if h5:\n",
    "        path += \".h5\"\n",
    "        by_name=True\n",
    "    loaded_model.load_weights(path, by_name=by_name)\n",
    "    #     loaded_model = load_model(model_path + \"_model.model\", custom_objects=dependencies)\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def load_erisk_data(writings_df, voc_size, emotion_lexicon, emotions =  \n",
    "                    ['anger', 'anticipation', 'disgust', 'fear', 'joy', \n",
    "                     'negative', 'positive', 'sadness', 'surprise', 'trust'],\n",
    "                    liwc_categories = categories, by_subset=True,\n",
    "                    pronouns = [\"i\", \"me\", \"my\", \"mine\", \"myself\"],\n",
    "                    train_prop=0.7, valid_prop=0.3, test_slice=2,\n",
    "                    nr_slices=5,\n",
    "                    min_post_len=3, min_word_len=1, \n",
    "                    user_level=True, vocabulary=None, labelcol='label', label_index=None,\n",
    "                   logger=logger):\n",
    "    logger.debug(\"Loading data...\\n\")\n",
    "    \n",
    "    ## Build vocabulary\n",
    "    vocabulary_all = {}\n",
    "    word_freqs = Counter()\n",
    "    \n",
    "    for words in writings_df.tokenized_text:\n",
    "        word_freqs.update(words)\n",
    "    if 'tokenized_title' in writings_df.columns:\n",
    "        for words in writings_df.tokenized_title:\n",
    "            word_freqs.update(words)\n",
    "    i = 1\n",
    "    for w, f in word_freqs.most_common(voc_size-2): # keeping voc_size-1 for unk\n",
    "        if len(w) < min_word_len:\n",
    "            continue\n",
    "        vocabulary_all[w] = i\n",
    "        i += 1\n",
    "    if not vocabulary:\n",
    "        vocabulary = vocabulary_all\n",
    "    else:\n",
    "        logger.info(\"Words not found in the vocabulary: %d\\n\" % len(set(vocabulary_all.keys()).difference(\n",
    "            set(vocabulary.keys()))))\n",
    "\n",
    "    if labelcol != 'label' and not label_index:\n",
    "        label_index = {}\n",
    "        l = 0\n",
    "        for label in set(writings_df[labelcol]):\n",
    "            label_index[label] = l\n",
    "            l += 1\n",
    "        print(\"Label index\", label_index)\n",
    "   \n",
    "    if by_subset and 'subset' in writings_df.columns:\n",
    "        training_subjects = list(set(writings_df[writings_df['subset']=='train'].subject))\n",
    "        test_subjects = list(set(writings_df[writings_df['subset']=='test'].subject))\n",
    "    else:\n",
    "        all_subjects = sorted(list(set(writings_df.subject)))\n",
    "        training_subjects_size = int(len(all_subjects) * train_prop)\n",
    "        test_subjects_size = len(all_subjects) - training_subjects_size\n",
    "        logger.info(\"%d training subjects, %d test subjects\\n\" % (training_subjects_size, test_subjects_size))\n",
    "        # Cross-validation, with fixed slice as input\n",
    "        test_prop = 1-train_prop\n",
    "        test_slice = min(test_slice, nr_slices)\n",
    "        logger.debug(\"start index: %f, from %f\\n\" % (\n",
    "            len(all_subjects)*(1/nr_slices)*test_slice, test_prop*test_slice))\n",
    "        start_slice = int(len(all_subjects)*(1/nr_slices)*test_slice)\n",
    "        test_subjects = all_subjects[start_slice: start_slice+test_subjects_size]\n",
    "        training_subjects = [s for s in all_subjects if s not in test_subjects]\n",
    "    training_subjects = sorted(training_subjects) # ensuring reproducibility\n",
    "    valid_subjects_size = int(len(training_subjects) * valid_prop)\n",
    "    valid_subjects = training_subjects[:valid_subjects_size]\n",
    "    training_subjects = training_subjects[valid_subjects_size:]\n",
    "    categories = [c for c in liwc_categories if c in writings_df.columns]\n",
    "    logger.debug(\"%d training users, %d validation users, %d test users.\" % (\n",
    "        len(training_subjects), \n",
    "          len(valid_subjects),\n",
    "          len(test_subjects)))\n",
    "    subjects_split = {'train': training_subjects, \n",
    "                      'valid': valid_subjects, \n",
    "                      'test': test_subjects}\n",
    "\n",
    "    user_level_texts = {}\n",
    "    for row in writings_df.sort_values(by='date').itertuples():\n",
    "        words = []\n",
    "        raw_text = \"\"\n",
    "        if hasattr(row, 'tokenized_title'):\n",
    "            if row.tokenized_title:\n",
    "                words.extend(row.tokenized_title)\n",
    "                raw_text += row.title\n",
    "        if hasattr(row, 'tokenized_text'):\n",
    "            if row.tokenized_text:\n",
    "                words.extend(row.tokenized_text)\n",
    "                raw_text += row.text\n",
    "        if not words or len(words)<min_post_len:\n",
    "#             logger.debug(row.subject)\n",
    "            continue\n",
    "        if labelcol == 'label':\n",
    "            label = row.label\n",
    "        else:\n",
    "            label = label_index[getattr(row, labelcol)]\n",
    "        liwc_categs = [getattr(row, categ) for categ in categories]\n",
    "        if row.subject not in user_level_texts.keys():\n",
    "            user_level_texts[row.subject] = {}\n",
    "            user_level_texts[row.subject]['texts'] = [words]\n",
    "            user_level_texts[row.subject]['label'] = label\n",
    "            user_level_texts[row.subject]['liwc'] = [liwc_categs]\n",
    "            user_level_texts[row.subject]['raw'] = [raw_text]\n",
    "        else:\n",
    "            user_level_texts[row.subject]['texts'].append(words)\n",
    "            user_level_texts[row.subject]['liwc'].append(liwc_categs)\n",
    "            user_level_texts[row.subject]['raw'].append(raw_text)\n",
    "            \n",
    "    return user_level_texts, subjects_split, vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del writings_df['tokenized_title']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_list = pickle.load(open(hyperparams_features['vocabulary_path'], 'rb'))\n",
    "vocabulary_dict={}\n",
    "for i,w in enumerate(vocabulary_list):\n",
    "    vocabulary_dict[w] = i\n",
    "user_level_data, subjects_split, vocabulary = load_erisk_data(writings_df, \n",
    "                                                            voc_size=hyperparams_features['max_features'],\n",
    "                                                           emotion_lexicon=nrc_lexicon,\n",
    "                                                           emotions=emotions,\n",
    "                                                           user_level=hyperparams_features['user_level'],\n",
    "                                                                                logger=logger,\n",
    "#                                                            vocabulary=pickle.load(open('vocabulary_40K_all.pkl', 'rb')),\n",
    "#                                                            vocabulary=pickle.load(open('vocab_clpsych_10000.pkl', 'rb')),\n",
    "                                                              vocabulary=vocabulary_dict,\n",
    "                                                              by_subset=True,\n",
    "#                                                               labelcol = 'condition',\n",
    "#                                                               label_index={'depression':1, \"ptsd\":0}\n",
    "                                                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df['subset'] = writings_df['subject'].apply(lambda s: 'test' if s in subjects_split['test'] else 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocabulary_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['tokenized_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df.text_len.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(writings_df, open('writings_df_selfharm_liwc_subsets', 'wb+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(vocabulary.items(), key=lambda t:t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session_collection = {}\n",
    "# models_collection = {}\n",
    "# hyperparams_collection = {}\n",
    "# user_level_data['subject57080000']['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, user_level_data, subjects_split, session=None, use_bert=False, set_type='train',\n",
    "                 batch_size=32, seq_len=512, vocabulary=vocabulary,\n",
    "                 voc_size=hyperparams_features['max_features'], emotion_lexicon=nrc_lexicon,\n",
    "                 hierarchical=False, pad_value=0, padding='pre',\n",
    "                 post_groups_per_user=None, posts_per_group=10, post_offset = 0,\n",
    "                 sampling_distr_alfa=0.1, sampling_distr='exp', # 'exp', 'uniform'\n",
    "                 emotions=emotions, pronouns=[\"i\", \"me\", \"my\", \"mine\", \"myself\"], liwc_categories=liwc_categories,\n",
    "                 liwc_dict=liwc_dict, compute_liwc=False, liwc_words_for_categories=None,\n",
    "                 pad_with_duplication=False,\n",
    "                 max_posts_per_user=None, sample_seqs=True,\n",
    "                 shuffle=True, return_subjects=False, keep_last_batch=True, class_weights=None,\n",
    "                classes=1):\n",
    "        'Initialization'\n",
    "        self.seq_len = seq_len\n",
    "        # Instantiate tokenizer\n",
    "        if session:\n",
    "            self.bert_tokenizer = create_tokenizer_from_hub_module(session)\n",
    "            session.run(tf.local_variables_initializer())\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            session.run(tf.tables_initializer())\n",
    "        else:\n",
    "            if use_bert:\n",
    "                logger.error(\"Need a session to use bert in data generation\")\n",
    "            self.bert_tokenizer = None\n",
    "        self.use_bert = use_bert\n",
    "        self.subjects_split = subjects_split\n",
    "        self.set = set_type\n",
    "        self.emotion_lexicon = emotion_lexicon\n",
    "        self.batch_size = batch_size\n",
    "        self.hierarchical = hierarchical\n",
    "        self.data = user_level_data\n",
    "        self.pad_value = pad_value\n",
    "        self.return_subjects = return_subjects\n",
    "        self.sampling_distr_alfa = sampling_distr_alfa\n",
    "        self.sampling_distr = sampling_distr\n",
    "        self.emotions = emotions\n",
    "        self.pronouns = pronouns\n",
    "        self.liwc_categories = liwc_categories\n",
    "        self.liwc_dict = liwc_dict\n",
    "        self.liwc_words_for_categories = liwc_words_for_categories\n",
    "        self.compute_liwc = compute_liwc\n",
    "        self.sample_seqs = sample_seqs\n",
    "        self.pad_with_duplication = pad_with_duplication\n",
    "        self.padding = padding\n",
    "        self.keep_last_batch = keep_last_batch\n",
    "        self.shuffle = shuffle\n",
    "        self.voc_size = voc_size\n",
    "        self.vocabulary = vocabulary\n",
    "        self.max_posts_per_user = max_posts_per_user\n",
    "        self.post_groups_per_user = post_groups_per_user\n",
    "        self.post_offset = post_offset\n",
    "        self.posts_per_group = posts_per_group\n",
    "        self.classes = classes\n",
    "        self.class_weights = class_weights\n",
    "        self.generated_labels = []\n",
    "        self.__post_indexes_per_user()\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    @staticmethod\n",
    "    def _random_sample(population_size, sample_size, sampling_distr, alfa=0.1, replacement=False):\n",
    "        if sampling_distr == 'exp':\n",
    "            # Exponential sampling\n",
    "            sample = sorted(np.random.choice(population_size, \n",
    "                            min(sample_size, population_size),\n",
    "                            p = DataGenerator.__generate_reverse_exponential_indices(population_size, alfa),\n",
    "                            replace=replacement))\n",
    "                                                                # if pad_with_duplication, \n",
    "                                                                # pad by adding the same post multiple times\n",
    "                                                                # if there are not enough posts\n",
    "        elif sampling_distr == 'uniform':\n",
    "            # Uniform sampling\n",
    "            sample = sorted(np.random.choice(population_size,\n",
    "                            min(sample_size, population_size),\n",
    "                            replace=replacement))\n",
    "        return sample\n",
    "    \n",
    "    @staticmethod\n",
    "    def __generate_reverse_exponential_indices(max_index, alfa=1):\n",
    "        probabilities = []\n",
    "        for x in range(max_index):\n",
    "            probabilities.append(alfa * (np.exp(alfa*x)))\n",
    "        reverse_probabilities = [p for p in probabilities]\n",
    "        sump = sum(reverse_probabilities)\n",
    "        normalized_probabilities = [p/sump for p in reverse_probabilities]\n",
    "        return normalized_probabilities\n",
    "    \n",
    "    def __post_indexes_per_user(self):\n",
    "        self.indexes_per_user = {u: [] for u in range(len(self.subjects_split[self.set]))}\n",
    "        self.indexes_with_user = []\n",
    "        self.item_weights = []\n",
    "        for u in range(len(self.subjects_split[self.set])):\n",
    "            if self.subjects_split[self.set][u] not in self.data:\n",
    "                logger.warning(\"User %s has no posts in %s set. Ignoring.\\n\" % (\n",
    "                    self.subjects_split[self.set][u], self.set))\n",
    "                continue\n",
    "            user_posts = self.data[self.subjects_split[self.set][u]]['texts']\n",
    "            if self.max_posts_per_user:\n",
    "                user_posts = user_posts[:self.max_posts_per_user]\n",
    "            nr_post_groups = int(np.ceil(len(user_posts) / self.posts_per_group))\n",
    "            \n",
    "            if self.post_groups_per_user:\n",
    "                nr_post_groups = min(self.post_groups_per_user, nr_post_groups)\n",
    "            for i in range(nr_post_groups):\n",
    "                # Generate random ordered samples of the posts\n",
    "                if self.sample_seqs:\n",
    "                    indexes_sample = DataGenerator._random_sample(population_size=len(user_posts),\n",
    "                                                         sample_size=self.posts_per_group,\n",
    "                                                         sampling_distr=self.sampling_distr,\n",
    "                                                         alfa=self.sampling_distr_alfa,\n",
    "                                                         replacement=self.pad_with_duplication)\n",
    "                    self.indexes_per_user[u].append(indexes_sample)\n",
    "                    self.indexes_with_user.append((u, indexes_sample))\n",
    "                    # break # just generate one?\n",
    "                # Generate all subsets of the posts in order\n",
    "                # TODO: Change here if you want a sliding window\n",
    "                else:\n",
    "                    self.indexes_per_user[u].append(range(i*self.posts_per_group + self.post_offset,\n",
    "                                                        min((i+1)*self.posts_per_group + self.post_offset, len(user_posts))))\n",
    "                    self.indexes_with_user.append((u, range(i*self.posts_per_group ,\n",
    "                                                        min((i+1)*self.posts_per_group + self.post_offset, len(user_posts)))))\n",
    "\n",
    "        if self.class_weights:\n",
    "            for item in self.indexes_with_user:\n",
    "                u, _ = item\n",
    "                s = self.subjects_split[self.set][u]\n",
    "                # Note: weight 1 is default.\n",
    "                try:\n",
    "                    self.item_weights.append(1./self.class_weights[self.data[s]['label']])\n",
    "                except Exception as e:\n",
    "                    self.item_weights.append(1)\n",
    "                    logger.error(\"Could not compute item weight for user %s. \" % s + str(e) + \"\\n\")\n",
    "        else:\n",
    "            self.item_weights = []\n",
    "\n",
    "    def __encode_text(self, tokens, raw_text):\n",
    "        # Using voc_size-1 value for OOV token\n",
    "        encoded_tokens = [self.vocabulary.get(w, self.voc_size-1) for w in tokens]\n",
    "        encoded_emotions = encode_emotions(tokens, self.emotion_lexicon, self.emotions)\n",
    "        encoded_pronouns = encode_pronouns(tokens, self.pronouns)\n",
    "        encoded_stopwords = encode_stopwords(tokens)\n",
    "        if not self.compute_liwc:\n",
    "            encoded_liwc = None\n",
    "        else:\n",
    "            encoded_liwc = self.__encode_liwc_categories(tokens)\n",
    "        if self.bert_tokenizer:\n",
    "            bert_ids, bert_masks, bert_segments, label = encode_text_for_bert(self.bert_tokenizer, InputExample(None, \n",
    "                                               raw_text), self.seq_len)\n",
    "        else:\n",
    "            bert_ids, bert_masks, bert_segments = [[0]*self.seq_len, [0]*self.seq_len, [0]*self.seq_len]\n",
    "        return (encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords, encoded_liwc,\n",
    "               bert_ids, bert_masks, bert_segments)\n",
    "    \n",
    "    def __encode_liwc_categories_full(self, tokens, relative=True):\n",
    "        categories_cnt = [0 for c in self.liwc_categories]\n",
    "        if not tokens:\n",
    "            return categories_cnt\n",
    "        text_len = len(tokens)\n",
    "        for i, category in enumerate(self.liwc_categories):\n",
    "            category_words = self.liwc_dict[category]\n",
    "            for t in tokens:\n",
    "                for word in category_words:\n",
    "                    if t==word or (word[-1]=='*' and t.startswith(word[:-1])) \\\n",
    "                    or (t==word.split(\"'\")[0]):\n",
    "                        categories_cnt[i] += 1\n",
    "                        break # one token cannot belong to more than one word in the category\n",
    "            if relative and text_len:\n",
    "                categories_cnt[i] = categories_cnt[i]/text_len\n",
    "        return categories_cnt\n",
    "        \n",
    "        \n",
    "    def __encode_liwc_categories(self, tokens, relative=True):\n",
    "        categories_cnt = [0 for c in self.liwc_categories]\n",
    "        if not tokens:\n",
    "            return categories_cnt\n",
    "        text_len = len(tokens)\n",
    "        for i, category in enumerate(self.liwc_categories):\n",
    "            for t in tokens:\n",
    "                if t in self.liwc_words_for_categories[category]:\n",
    "                    categories_cnt[i] += 1\n",
    "            if relative and text_len:\n",
    "                categories_cnt[i] = categories_cnt[i]/text_len\n",
    "        return categories_cnt\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        if self.keep_last_batch:\n",
    "            return int(np.ceil(len(self.indexes) / self.batch_size)) # + 1 to not discard last batch\n",
    "        return int((len(self.indexes))/self.batch_size)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Reset generated labels\n",
    "        if index == 0:\n",
    "             self.generated_labels = []\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Find users\n",
    "        user_indexes = [t[0] for t in indexes]\n",
    "        users = set([self.subjects_split[self.set][i] for i in user_indexes\n",
    "                    if self.subjects_split[self.set][i] in self.data.keys()]) # TODO: maybe needs a warning that user is missing\n",
    "        post_indexes_per_user = {u: [] for u in users}\n",
    "        # Sample post ids\n",
    "        for u, post_indexes in indexes:\n",
    "            user = self.subjects_split[self.set][u]\n",
    "            # Note: was bug here - changed it into a list\n",
    "            post_indexes_per_user[user].append(post_indexes)\n",
    "\n",
    "        # Generate data\n",
    "        if self.hierarchical:\n",
    "            X, s, y = self.__data_generation_hierarchical(users, post_indexes_per_user)\n",
    "        else:\n",
    "            X, s, y = self.__data_generation(users, post_indexes_per_user)\n",
    "\n",
    "        if self.return_subjects:\n",
    "            return X, s, y\n",
    "        else:\n",
    "            return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = self.indexes_with_user\n",
    "#         np.arange(len(self.subjects_split[self.set]))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "        if self.class_weights:\n",
    "            # Sample users according to class weight (Or do this for each batch instead?)\n",
    "            normalized_weights = [w/sum(self.item_weights) for w in self.item_weights]\n",
    "            random_user_indexes = np.random.choice(len(self.indexes_with_user), \n",
    "                            len(self.indexes_with_user),\n",
    "                            p = normalized_weights, replace=True)\n",
    "            self.indexes = [self.indexes_with_user[i] for i in random_user_indexes]\n",
    "    def __data_generation(self, users, post_indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        tokens_data = []\n",
    "        categ_data = []\n",
    "        sparse_data = []\n",
    "        subjects = []\n",
    "        bert_ids_data = []\n",
    "        bert_masks_data = []\n",
    "        bert_segments_data = []\n",
    "        labels = []\n",
    "\n",
    "        for subject in users:\n",
    "\n",
    "            if 'label' in self.data[subject]:\n",
    "                label = self.data[subject]['label']\n",
    "            else:\n",
    "                label = None\n",
    "\n",
    "            \n",
    "            all_words = []\n",
    "            all_raw_texts = []\n",
    "            liwc_aggreg = []\n",
    "\n",
    "            for post_index_range in post_indexes[subject]:\n",
    "                # Sample\n",
    "                texts = [self.data[subject]['texts'][i] for i in post_index_range]\n",
    "                if 'liwc' in self.data[subject] and not self.compute_liwc:\n",
    "                    liwc_selection = [self.data[subject]['liwc'][i] for i in post_index_range]\n",
    "                raw_texts = [self.data[subject]['raw'][i] for i in post_index_range]\n",
    "\n",
    "                all_words.append(sum(texts, [])) # merge all texts in group in one list\n",
    "                if 'liwc' in self.data[subject] and not self.compute_liwc:\n",
    "                    liwc_aggreg.append(np.array(liwc_selection).mean(axis=0).tolist())\n",
    "                all_raw_texts.append(\" \".join(raw_texts))\n",
    "            for i, words in enumerate(all_words):\n",
    "                encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords, encoded_liwc, \\\n",
    "                    bert_ids, bert_masks, bert_segments = self.__encode_text(words, all_raw_texts[i])\n",
    "                try:\n",
    "                    subject_id = int(re.findall('[0-9]+', subject)[0])\n",
    "                except IndexError:\n",
    "                    subject_id = subject\n",
    "                tokens_data.append(encoded_tokens)\n",
    "                # TODO: what will be the difference between these?\n",
    "                # I think instead of averaging for the post group, it just does it correctly\n",
    "                # for the whole post group (when computing, non-lazily)\n",
    "                if 'liwc' in self.data[subject] and not self.compute_liwc:  \n",
    "                    categ_data.append(encoded_emotions + [encoded_pronouns] + liwc_aggreg[i])\n",
    "                   \n",
    "                else:\n",
    "                    categ_data.append(encoded_emotions + [encoded_pronouns] + encoded_liwc)\n",
    "                    \n",
    "                sparse_data.append(encoded_stopwords)\n",
    "                bert_ids_data.append(bert_ids)\n",
    "                bert_masks_data.append(bert_masks)\n",
    "                bert_segments_data.append(bert_segments)\n",
    "                \n",
    "                labels.append(label)\n",
    "                subjects.append(subject_id)\n",
    "\n",
    "        \n",
    "        self.generated_labels.extend(labels)\n",
    "        # using zeros for padding\n",
    "        tokens_data_padded = sequence.pad_sequences(tokens_data, maxlen=self.seq_len, \n",
    "                                                    padding=self.padding,\n",
    "                                                   truncating=self.padding)\n",
    "\n",
    "        if self.use_bert:\n",
    "            return ([np.array(tokens_data_padded), np.array(categ_data), np.array(sparse_data),\n",
    "                 np.array(bert_ids_data), np.array(bert_masks_data), np.array(bert_segments_data),\n",
    "                ],\n",
    "                np.array(subjects),\n",
    "                np.array(labels))\n",
    "        else:\n",
    "            return ([np.array(tokens_data_padded), np.array(categ_data), np.array(sparse_data),\n",
    "                ],\n",
    "                np.array(subjects),\n",
    "                np.array(labels))\n",
    "    \n",
    "    def __data_generation_hierarchical(self, users, post_indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        user_tokens = []\n",
    "        user_categ_data = []\n",
    "        user_sparse_data = []\n",
    "        user_bert_ids_data = []\n",
    "        user_bert_masks_data = []\n",
    "        user_bert_segments_data = []\n",
    "        \n",
    "        labels = []\n",
    "        subjects = []\n",
    "        for subject in users:\n",
    "             \n",
    "            all_words = []\n",
    "            all_raw_texts = []\n",
    "            liwc_scores = []\n",
    "            \n",
    "            if 'label' in self.data[subject]:\n",
    "                if self.classes==1:\n",
    "                    label = self.data[subject]['label']\n",
    "                else:\n",
    "                    label = list(np_utils.to_categorical(self.data[subject]['label'], num_classes=self.classes))\n",
    "            else:\n",
    "                label = None\n",
    "\n",
    "            for post_index_range in post_indexes[subject]:\n",
    "                # Sample\n",
    "                texts = [self.data[subject]['texts'][i] for i in post_index_range]\n",
    "                if 'liwc' in self.data[subject] and not self.compute_liwc:\n",
    "                    liwc_selection = [self.data[subject]['liwc'][i] for i in post_index_range]\n",
    "                raw_texts = [self.data[subject]['raw'][i] for i in post_index_range]\n",
    "\n",
    "                all_words.append(texts)\n",
    "                if 'liwc' in self.data[subject] and not self.compute_liwc:\n",
    "                    liwc_scores.append(liwc_selection)\n",
    "                all_raw_texts.append(raw_texts)\n",
    "            \n",
    "#             if len(texts) < self.max_posts_per_user:\n",
    "#                 # TODO: pad with zeros\n",
    "#                 pass\n",
    "\n",
    "            for i, words in enumerate(all_words):\n",
    "                tokens_data = []\n",
    "                categ_data = []\n",
    "                sparse_data = []\n",
    "                bert_ids_data = []\n",
    "                bert_masks_data = []\n",
    "                bert_segments_data = []\n",
    "                \n",
    "                raw_text = all_raw_texts[i]\n",
    "                words = all_words[i]\n",
    "                \n",
    "                for p, posting in enumerate(words): \n",
    "                    encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords, encoded_liwc, \\\n",
    "                        bert_ids, bert_masks, bert_segments = self.__encode_text(words[p], raw_text[p])\n",
    "                    if 'liwc' in self.data[subject] and not self.compute_liwc:\n",
    "                        liwc = liwc_scores[i][p]\n",
    "                    else:\n",
    "                        liwc = encoded_liwc\n",
    "                    try:\n",
    "                        subject_id = int(re.findall('[0-9]+', subject)[0])\n",
    "                    except IndexError:\n",
    "                        subject_id = subject\n",
    "                    tokens_data.append(encoded_tokens)\n",
    "                    # using zeros for padding\n",
    "                    # TODO: there is something wrong with this\n",
    "                    categ_data.append(encoded_emotions + [encoded_pronouns] + liwc)\n",
    "                    sparse_data.append(encoded_stopwords)\n",
    "                    bert_ids_data.append(bert_ids)\n",
    "                    bert_masks_data.append(bert_masks)\n",
    "                    bert_segments_data.append(bert_segments)\n",
    "                \n",
    "                # For each range\n",
    "                tokens_data_padded = np.array(sequence.pad_sequences(tokens_data, maxlen=self.seq_len,\n",
    "                                              padding=self.padding,\n",
    "                                            truncating=self.padding))\n",
    "                user_tokens.append(tokens_data_padded)\n",
    "\n",
    "                user_categ_data.append(categ_data)\n",
    "                user_sparse_data.append(sparse_data)\n",
    "\n",
    "                user_bert_ids_data.append(bert_ids_data)\n",
    "                user_bert_masks_data.append(bert_masks_data)\n",
    "                user_bert_segments_data.append(bert_segments_data)\n",
    "\n",
    "                labels.append(label)\n",
    "                subjects.append(subject_id)\n",
    "\n",
    "        user_tokens = sequence.pad_sequences(user_tokens, \n",
    "                                             maxlen=self.posts_per_group, \n",
    "                                             value=self.pad_value)\n",
    "        user_tokens = np.rollaxis(np.dstack(user_tokens), -1)\n",
    "        user_categ_data = sequence.pad_sequences(user_categ_data,  \n",
    "                                                 maxlen=self.posts_per_group, \n",
    "                                                 value=self.pad_value, dtype='float32')\n",
    "        user_categ_data = np.rollaxis(np.dstack(user_categ_data), -1)\n",
    "        \n",
    "        user_sparse_data = sequence.pad_sequences(user_sparse_data, \n",
    "                                                  maxlen=self.posts_per_group, \n",
    "                                                  value=self.pad_value)\n",
    "        user_sparse_data = np.rollaxis(np.dstack(user_sparse_data), -1)\n",
    "        \n",
    "        user_bert_ids_data = sequence.pad_sequences(user_bert_ids_data, \n",
    "                                                    maxlen=self.posts_per_group, \n",
    "                                                    value=self.pad_value)\n",
    "        user_bert_ids_data = np.rollaxis(np.dstack(user_bert_ids_data), -1)\n",
    "        \n",
    "        user_bert_masks_data = sequence.pad_sequences(user_bert_masks_data, \n",
    "                                                      maxlen=self.posts_per_group, \n",
    "                                                      value=self.pad_value)\n",
    "        user_bert_masks_data = np.rollaxis(np.dstack(user_bert_masks_data), -1)\n",
    "        \n",
    "        user_bert_segments_data = sequence.pad_sequences(user_bert_segments_data, \n",
    "                                                         maxlen=self.posts_per_group, \n",
    "                                                         value=self.pad_value)\n",
    "        user_bert_segments_data = np.rollaxis(np.dstack(user_bert_segments_data), -1)\n",
    "\n",
    "        self.generated_labels.extend(labels)\n",
    "\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        if self.use_bert:\n",
    "            return ((user_tokens, user_categ_data, user_sparse_data, \n",
    "                 user_bert_ids_data, user_bert_masks_data, uifser_bert_segments_data),\n",
    "                np.array(subjects),\n",
    "                labels)\n",
    "        else:\n",
    "            return ((user_tokens, user_categ_data, user_sparse_data), \n",
    "                np.array(subjects),\n",
    "                labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGeneratorFromText(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, texts, batch_size=32, seq_len=512, vocabulary=vocabulary,\n",
    "                 voc_size=hyperparams_features['max_features'], emotion_lexicon=nrc_lexicon,\n",
    "                 hierarchical=False, pad_value=0, padding='pre',\n",
    "                 emotions=emotions, pronouns=[\"i\", \"me\", \"my\", \"mine\", \"myself\"], liwc_categories=liwc_categories,\n",
    "                 liwc_dict=liwc_dict, compute_liwc=False, liwc_words_for_categories=None,\n",
    "                 pad_with_duplication=False, posts_per_group=50,\n",
    "                 shuffle=True, keep_last_batch=True):\n",
    "        'Initialization'\n",
    "        self.seq_len = seq_len\n",
    "        # Instantiate tokenizer\n",
    "        \n",
    "        self.texts = texts\n",
    "        self.emotion_lexicon = emotion_lexicon\n",
    "        self.batch_size = batch_size\n",
    "        self.hierarchical = hierarchical\n",
    " \n",
    "        self.pad_value = pad_value\n",
    "\n",
    "        self.keep_last_batch = keep_last_batch\n",
    "        self.emotions = emotions\n",
    "        self.pronouns = pronouns\n",
    "        self.liwc_categories = liwc_categories\n",
    "        self.liwc_dict = liwc_dict\n",
    "        self.liwc_words_for_categories = liwc_words_for_categories\n",
    "        self.compute_liwc = compute_liwc\n",
    "        \n",
    "        self.use_bert = False\n",
    "      \n",
    "        self.pad_with_duplication = pad_with_duplication\n",
    "        self.padding = padding\n",
    "        self.shuffle = shuffle\n",
    "        self.voc_size = voc_size\n",
    "        self.vocabulary = vocabulary\n",
    "        \n",
    "        self.posts_per_group = posts_per_group\n",
    "       \n",
    "        self.on_epoch_end()\n",
    "        \n",
    "   \n",
    "\n",
    "    def __encode_text(self, tokens, raw_text):\n",
    "        # Using voc_size-1 value for OOV token\n",
    "        encoded_tokens = [self.vocabulary.get(w, self.voc_size-1) for w in tokens]\n",
    "        encoded_emotions = encode_emotions(tokens, self.emotion_lexicon, self.emotions)\n",
    "        encoded_pronouns = encode_pronouns(tokens, self.pronouns)\n",
    "        encoded_stopwords = encode_stopwords(tokens)\n",
    "\n",
    "        encoded_liwc = self.__encode_liwc_categories(tokens)\n",
    "      \n",
    "        bert_ids, bert_masks, bert_segments = [[0]*self.seq_len, [0]*self.seq_len, [0]*self.seq_len]\n",
    "        return (encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords, encoded_liwc,\n",
    "               bert_ids, bert_masks, bert_segments)\n",
    "    \n",
    "    def __encode_liwc_categories_full(self, tokens, relative=True):\n",
    "        categories_cnt = [0 for c in self.liwc_categories]\n",
    "        if not tokens:\n",
    "            return categories_cnt\n",
    "        text_len = len(tokens)\n",
    "        for i, category in enumerate(self.liwc_categories):\n",
    "            category_words = self.liwc_dict[category]\n",
    "            for t in tokens:\n",
    "                for word in category_words:\n",
    "                    if t==word or (word[-1]=='*' and t.startswith(word[:-1])) \\\n",
    "                    or (t==word.split(\"'\")[0]):\n",
    "                        categories_cnt[i] += 1\n",
    "                        break # one token cannot belong to more than one word in the category\n",
    "            if relative and text_len:\n",
    "                categories_cnt[i] = categories_cnt[i]/text_len\n",
    "        return categories_cnt\n",
    "        \n",
    "        \n",
    "    def __encode_liwc_categories(self, tokens, relative=True):\n",
    "        categories_cnt = [0 for c in self.liwc_categories]\n",
    "        if not tokens:\n",
    "            return categories_cnt\n",
    "        text_len = len(tokens)\n",
    "        for i, category in enumerate(self.liwc_categories):\n",
    "            for t in tokens:\n",
    "                if t in self.liwc_words_for_categories[category]:\n",
    "                    categories_cnt[i] += 1\n",
    "            if relative and text_len:\n",
    "                categories_cnt[i] = categories_cnt[i]/text_len\n",
    "        return categories_cnt\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        if self.keep_last_batch:\n",
    "            return int(np.ceil(len(self.indexes) / self.batch_size)) # + 1 to not discard last batch\n",
    "        return int((len(self.indexes))/self.batch_size)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "       \n",
    "        # Generate data\n",
    "        if self.hierarchical:\n",
    "            X, s, y = self.__data_generation_hierarchical(indexes)\n",
    "        else:\n",
    "            X, s, y = self.__data_generation(indexes)\n",
    "      \n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.texts))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        tokens_data = []\n",
    "        categ_data = []\n",
    "        sparse_data = []\n",
    "        subjects = []\n",
    "        bert_ids_data = []\n",
    "        bert_masks_data = []\n",
    "        bert_segments_data = []\n",
    "        labels = []\n",
    "\n",
    "        label = None\n",
    "\n",
    "\n",
    "        all_words = []\n",
    "        all_raw_texts = []\n",
    "        liwc_aggreg = []\n",
    "\n",
    "        # Sample\n",
    "        all_words = [tokenize(self.texts[i]) for i in indexes]\n",
    "\n",
    "        all_raw_texts = [self.texts[i] for i in indexes]\n",
    "\n",
    "\n",
    "        for i, words in enumerate(all_words):\n",
    "            encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords, encoded_liwc, \\\n",
    "                bert_ids, bert_masks, bert_segments = self.__encode_text(words, all_raw_texts[i])\n",
    "\n",
    "            tokens_data.append(encoded_tokens)\n",
    "            # TODO: what will be the difference between these?\n",
    "            # I think instead of averaging for the post group, it just does it correctly\n",
    "            # for the whole post group (when computing, non-lazily)\n",
    "\n",
    "            categ_data.append(encoded_emotions + [encoded_pronouns] + encoded_liwc)\n",
    "\n",
    "            sparse_data.append(encoded_stopwords)\n",
    "            bert_ids_data.append(bert_ids)\n",
    "            bert_masks_data.append(bert_masks)\n",
    "            bert_segments_data.append(bert_segments)\n",
    "\n",
    "            labels.append(None)\n",
    "            subjects.append(None)\n",
    "\n",
    "        \n",
    "        # using zeros for padding\n",
    "        tokens_data_padded = sequence.pad_sequences(tokens_data, maxlen=self.seq_len, \n",
    "                                                    padding=self.padding,\n",
    "                                                   truncating=self.padding)\n",
    "\n",
    "       \n",
    "        return ([np.array(tokens_data_padded), np.array(categ_data), np.array(sparse_data),\n",
    "                ],\n",
    "                np.array(subjects),\n",
    "                np.array(labels))\n",
    "    \n",
    "    def __data_generation_hierarchical(self, indexes):\n",
    "#         pass\n",
    "#         'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        user_tokens = []\n",
    "        user_categ_data = []\n",
    "        user_sparse_data = []\n",
    "        user_bert_ids_data = []\n",
    "        user_bert_masks_data = []\n",
    "        user_bert_segments_data = []\n",
    "\n",
    "        labels = []\n",
    "        subjects = []\n",
    "#         for subject in users:\n",
    "             \n",
    "        all_words = []\n",
    "        all_raw_texts = []\n",
    "        liwc_scores = []\n",
    "\n",
    "        label = -1\n",
    "\n",
    "#         for index_range in indexes:\n",
    "            # Sample\n",
    "        texts = [tokenize(self.texts[i]) for i in indexes] # is this good or am I repeating the same texts?\n",
    "\n",
    "        raw_texts = [self.texts[i] for i in indexes]\n",
    "        all_words.append(texts)\n",
    "\n",
    "        all_raw_texts.append(raw_texts)\n",
    "\n",
    "#             if len(texts) < self.max_posts_per_user:\n",
    "#                 # TODO: pad with zeros\n",
    "#                 pass\n",
    "\n",
    "        for i, words in enumerate(all_words):\n",
    "            tokens_data = []\n",
    "            categ_data = []\n",
    "            sparse_data = []\n",
    "            bert_ids_data = []\n",
    "            bert_masks_data = []\n",
    "            bert_segments_data = []\n",
    "\n",
    "            raw_text = all_raw_texts[i]\n",
    "            words = all_words[i]\n",
    "\n",
    "            for p, posting in enumerate(words): \n",
    "                encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords, encoded_liwc, \\\n",
    "                    bert_ids, bert_masks, bert_segments = self.__encode_text(words[p], raw_text[p])\n",
    "                \n",
    "\n",
    "                liwc = encoded_liwc\n",
    "      \n",
    "                tokens_data.append(encoded_tokens)\n",
    "                # using zeros for padding\n",
    "                # TODO: there is something wrong with this\n",
    "                categ_data.append(encoded_emotions + [encoded_pronouns] + liwc)\n",
    "                sparse_data.append(encoded_stopwords)\n",
    "                bert_ids_data.append(bert_ids)\n",
    "                bert_masks_data.append(bert_masks)\n",
    "                bert_segments_data.append(bert_segments)\n",
    "                \n",
    "                labels.append(None)\n",
    "                subjects.append(None)\n",
    "\n",
    "                # For each range\n",
    "                tokens_data_padded = np.array(sequence.pad_sequences(tokens_data, maxlen=self.seq_len,\n",
    "                                              padding=self.padding,\n",
    "                                            truncating=self.padding))\n",
    "                user_tokens.append(tokens_data_padded)\n",
    "\n",
    "                user_categ_data.append(categ_data)\n",
    "                user_sparse_data.append(sparse_data)\n",
    "\n",
    "                user_bert_ids_data.append(bert_ids_data)\n",
    "                user_bert_masks_data.append(bert_masks_data)\n",
    "                user_bert_segments_data.append(bert_segments_data)\n",
    "\n",
    "\n",
    "\n",
    "        user_tokens = sequence.pad_sequences(user_tokens, \n",
    "                                             maxlen=self.posts_per_group, \n",
    "                                             value=self.pad_value)\n",
    "        user_tokens = np.rollaxis(np.dstack(user_tokens), -1)\n",
    "        user_categ_data = sequence.pad_sequences(user_categ_data,  \n",
    "                                                 maxlen=self.posts_per_group, \n",
    "                                                 value=self.pad_value, dtype='float32')\n",
    "        user_categ_data = np.rollaxis(np.dstack(user_categ_data), -1)\n",
    "        \n",
    "        user_sparse_data = sequence.pad_sequences(user_sparse_data, \n",
    "                                                  maxlen=self.posts_per_group, \n",
    "                                                  value=self.pad_value)\n",
    "        user_sparse_data = np.rollaxis(np.dstack(user_sparse_data), -1)\n",
    "        \n",
    "        user_bert_ids_data = sequence.pad_sequences(user_bert_ids_data, \n",
    "                                                    maxlen=self.posts_per_group, \n",
    "                                                    value=self.pad_value)\n",
    "        user_bert_ids_data = np.rollaxis(np.dstack(user_bert_ids_data), -1)\n",
    "        \n",
    "        user_bert_masks_data = sequence.pad_sequences(user_bert_masks_data, \n",
    "                                                      maxlen=self.posts_per_group, \n",
    "                                                      value=self.pad_value)\n",
    "        user_bert_masks_data = np.rollaxis(np.dstack(user_bert_masks_data), -1)\n",
    "        \n",
    "        user_bert_segments_data = sequence.pad_sequences(user_bert_segments_data, \n",
    "                                                         maxlen=self.posts_per_group, \n",
    "                                                         value=self.pad_value)\n",
    "        user_bert_segments_data = np.rollaxis(np.dstack(user_bert_segments_data), -1)\n",
    "        \n",
    "\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        return ([user_tokens, user_categ_data, user_sparse_data], \n",
    "            np.array(subjects),\n",
    "            np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in DataGeneratorFromText([\"This is the first text\", \"and this is the second text\"], \n",
    "                                  liwc_words_for_categories=liwc_words_for_categories, \n",
    "                                  compute_liwc=True):\n",
    "    print(x[0].shape, x[1].shape, x[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = DataGenerator(user_level_data, subjects_split, \n",
    "#                                          set_type='test', \n",
    "#                                     seq_len=hyperparams_collection[key]['maxlen'],   \n",
    "# #                                         batch_size=len(subjects_split['test']), # on all data at once\n",
    "#                                         batch_size=hyperparams_collection[key]['batch_size'], # on all data at once\n",
    "#                                        hierarchical=hyperparams_collection[key]['hierarchical'],\n",
    "#                                          max_posts_per_user=None,\n",
    "#                                        pad_with_duplication=False,\n",
    "#                                         posts_per_group=hyperparams_collection[key]['posts_per_group'],\n",
    "#                                         post_groups_per_user=1,  compute_liwc=True,\n",
    "#                                         liwc_words_for_categories=liwc_words_for_categories,\n",
    "#                                         emotions=emotions, liwc_categories=liwc_categories,\n",
    "#                                          sample_seqs=False, shuffle=False, classes=classes,\n",
    "#                  class_weights={0:2,1:1,2:1})\n",
    "# # g = DataGenerator(user_level_data, subjects_split, sample_seqs=False, max_posts_per_user=None,\n",
    "# #                                           set_type='test', hierarchical=True, post_groups_per_user=None,\n",
    "# #                               posts_per_group=50, shuffle=False,\n",
    "# #                              sampling_distr='exp', liwc_words_for_categories=liwc_words_for_categories,\n",
    "# #                           \"Co\"   compute_liwc=True, classes=3)\n",
    "# for cnt, x in enumerate(g):\n",
    "#     continue\n",
    "# print(cnt)\n",
    "# print(g.generated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(g.generated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# TODO: it is skipping the last batch\n",
    "x_data = {'train': [], 'valid': [], 'test': []}\n",
    "y_data = {'train': [], 'valid': [], 'test': []}\n",
    "for set_type in ['train', 'valid', 'test']:\n",
    "    total_positive = 0\n",
    "    for x, y in DataGenerator(user_level_data, subjects_split, sample_seqs=False, max_posts_per_user=None,\n",
    "                                          set_type=set_type, hierarchical=True, post_groups_per_user=1,\n",
    "                              posts_per_group=50, shuffle=False,\n",
    "                             sampling_distr='exp', liwc_words_for_categories=liwc_words_for_categories,\n",
    "                             compute_liwc=True, classes=3):\n",
    "#         total_positive += pd.Series(y).sum()\n",
    "        x_data[set_type].append(x)\n",
    "        y_data[set_type].append(y)\n",
    "        logger.info(\"%s %s positive examples\\n\" % (total_positive, set_type))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# TODO: it is skipping the last batch\n",
    "x_data2 = {'train': [], 'valid': [], 'test': []}\n",
    "y_data2 = {'train': [], 'valid': [], 'test': []}\n",
    "for set_type in ['train', 'valid', 'test']:\n",
    "    total_positive = 0\n",
    "    for x, y in DataGenerator(user_level_data, subjects_split, sample_seqs=False, max_posts_per_user=None,\n",
    "                                          set_type=set_type, hierarchical=False, post_groups_per_user=1,\n",
    "                              posts_per_group=50, shuffle=False,\n",
    "                             sampling_distr='exp', liwc_words_for_categories=liwc_words_for_categories,\n",
    "                             compute_liwc=False):\n",
    "        total_positive += pd.Series(y).sum()\n",
    "        x_data2[set_type].append(x)\n",
    "        y_data2[set_type].append(y)\n",
    "        logger.info(\"%d %s positive examples\\n\" % (total_positive, set_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data['train'][0][1][0]\n",
    "y_data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data2['train'][0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #x_data['train'][]\n",
    "# featureindex = 1\n",
    "\n",
    "# from scipy.stats import pearsonr, spearmanr\n",
    "# for i in range(len(x_data['train'][0][featureindex])):\n",
    "#     print(spearmanr(x_data['train'][0][featureindex][i], x_data2['train'][0][featureindex][i]))    \n",
    "#     plt.scatter(x_data['train'][0][featureindex][i], x_data2['train'][0][featureindex][i])\n",
    "# #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(liwc_words_for_categories[c]) for c in categories])\n",
    "len(categories)\n",
    "len(set(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x_data['valid'][0][0].shape, x_data['valid'][0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_for_bert = encode_text_for_bert(bert_tokenizer, InputExample(None, \n",
    "#                                                \"Ana are mere\"), 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids, masks, segments, label = encoded_for_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weights = class_weight.compute_class_weight('balanced',\n",
    "#                                                  np.unique(y_data['train']),\n",
    "#                                                  y_data['train'])\n",
    "# class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path, embedding_dim, voc):\n",
    "    # random matrix with mean value = 0\n",
    "    embedding_matrix = np.random.random((len(voc)+2, embedding_dim)) - 0.5 # voc + unk + pad value(0)\n",
    "    cnt_inv = 0\n",
    "    f = open(path, encoding='utf8')\n",
    "    for i, line in enumerate(f):\n",
    "#         print(i)\n",
    "        values = line.split()\n",
    "        word = ''.join(values[:-hyperparams_features['embedding_dim']])\n",
    "        coefs = np.asarray(values[-hyperparams_features['embedding_dim']:], dtype='float32')\n",
    "        word_i = voc.get(word)\n",
    "        if word_i is not None:\n",
    "            embedding_matrix[word_i] = coefs\n",
    "            cnt_inv += 1\n",
    "    f.close()\n",
    "\n",
    "    print('Total %s word vectors.' % len(embedding_matrix))\n",
    "    print('Words not found in embedding space %d' % (len(embedding_matrix)-cnt_inv))\n",
    " \n",
    "    return embedding_matrix\n",
    "\n",
    "def load_embeddings2(path, embedding_dim, voc):\n",
    "    # random matrix with mean value = 0\n",
    "    embedding_matrix = np.random.random((len(voc)+2, embedding_dim)) #- 0.5 # voc + unk + pad value(0)\n",
    "    cnt_inv = 0\n",
    "    with open(path, \"rb\") as f:\n",
    "        embedding_dict = pickle.load(f)\n",
    "    for word, coefs in embedding_dict.items():\n",
    "        word_i = voc.get(word)\n",
    "        if word_i is not None:\n",
    "            embedding_matrix[word_i] = coefs\n",
    "            cnt_inv += 1\n",
    "    print('Total %s word vectors.' % len(embedding_matrix))\n",
    "    print('Words not found in embedding space %d' % (len(embedding_matrix)-cnt_inv))\n",
    " \n",
    "    return embedding_matrix\n",
    "# \n",
    "# pretrained_embeddings_path = root_dir + '/resources/glove.twitter.27B/glove.twitter.27B.%dd.txt' % hyperparams_features['embedding_dim']\n",
    "# pretrained_embeddings_path = root_dir + '/resources/glove.840B/glove.840B.%dd.txt' % hyperparams_features['embedding_dim']\n",
    "pretrained_embeddings_path = hyperparams_features['embeddings_path']#root_dir + '/eRisk/finetuned_glove_clpsych_erisk_normalized_2_20000.pkl'\n",
    "embedding_matrix = load_embeddings(pretrained_embeddings_path, hyperparams_features['embedding_dim'], \n",
    "                                    voc=vocabulary_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.mean()\n",
    "# hyperparams_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics():\n",
    "    def __init__(self, threshold=0.5):\n",
    "        self.threshold=threshold\n",
    "        \n",
    "    def recall_m(self, y_true, y_pred):\n",
    "            y_labels = y_true\n",
    "            y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), self.threshold), K.floatx())        \n",
    "            possible_positives = K.sum(K.round(K.clip(y_labels, 0, 1)))\n",
    "            true_positives = K.sum(K.round(K.clip(y_labels * y_pred, 0, 1)))\n",
    "            recall = true_positives / (possible_positives + K.epsilon())\n",
    "            return recall\n",
    "\n",
    "    def precision_m(self, y_true, y_pred):\n",
    "            y_labels = y_true\n",
    "            y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), self.threshold), K.floatx())        \n",
    "            true_positives = K.sum(K.round(K.clip(y_labels * y_pred, 0, 1)))\n",
    "            predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "            precision = true_positives / (predicted_positives + K.epsilon())\n",
    "            return precision\n",
    "\n",
    "    def f1_m(self, y_true, y_pred):\n",
    "        precision = self.precision_m(y_true, y_pred)\n",
    "        recall = self.recall_m(y_true, y_pred)\n",
    "        return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "    \n",
    "    def auc2(self, y_true, y_pred):\n",
    "        auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        return auc\n",
    "\n",
    "    def auc(self, y_true, y_pred):\n",
    "#         has_true_examples = K.greater(K.cast(K.sum(y_true), K.floatx()),0)\n",
    "#         has_false_examples = K.less(K.cast(K.mean(y_true), K.floatx()),1)\n",
    "#         score = tf.cond(tf.logical_and(has_true_examples, has_false_examples), \n",
    "#                         lambda:tf.py_function(roc_auc_score, (\n",
    "#                             K.cast(y_true, K.floatx()), \n",
    "#                             K.cast(y_pred, K.floatx())), tf.float32), \n",
    "#                         lambda:0.0)\n",
    "        return 0\n",
    "        \n",
    "def binary_crossentropy_custom(y_true, y_pred):\n",
    "    y_labels = y_true\n",
    "    return K.binary_crossentropy(y_labels, \n",
    "                                 y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fine_tune_layers=10,\n",
    "        pooling=\"first\",\n",
    "        trainable=True,\n",
    "        bert_path=\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\", \n",
    "        **kwargs\n",
    "    ):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = trainable\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_path = bert_path\n",
    "        if self.pooling not in [\"first\", \"mean\"]:\n",
    "            raise NameError(\n",
    "               \"Undefined pooling type (must be either first or mean, but is %s)\" % self.pooling\n",
    "            )\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def get_config(self):\n",
    "\n",
    "        config = super(BertLayer, self).get_config().copy()\n",
    "        config.update({\n",
    "            'n_fine_tune_layers': self.n_fine_tune_layers,\n",
    "            'trainable': self.trainable,\n",
    "            'output_size': self.output_size,\n",
    "            'pooling': self.pooling,\n",
    "            'bert_path': self.bert_path,\n",
    "        })\n",
    "\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_path, trainable=self.trainable, name=\"%s_module\" % self.name\n",
    "        )\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        if self.pooling == \"first\":\n",
    "            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "            trainable_layers = [\"pooler/dense\"]\n",
    "\n",
    "        elif self.pooling == \"mean\":\n",
    "            trainable_vars = [\n",
    "                var\n",
    "                for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "            ]\n",
    "            trainable_layers = []\n",
    "        else:\n",
    "            raise NameError(\n",
    "                \"Undefined pooling type (must be either first or mean, but is %s)\" % self.pooling\n",
    "            )\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(\"encoder/layer_%s\" % str(11 - i))\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        if self.pooling == \"first\":\n",
    "            pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"pooled_output\"\n",
    "            ]\n",
    "        elif self.pooling == \"mean\":\n",
    "            result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"sequence_output\"\n",
    "            ]\n",
    "\n",
    "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
    "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
    "            input_mask = tf.cast(input_mask, tf.float32)\n",
    "            pooled = masked_reduce_mean(result, input_mask)\n",
    "        else:\n",
    "            raise NameError(\"Undefined pooling type (must be either first or mean, but is %s)\" % self.pooling)\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopwords_list,\n",
    "                liwc_categories,\n",
    "               ignore_layer=[], classes=1):\n",
    "    def attention(xin):\n",
    "        return K.sum(xin, axis=1) \n",
    "    tokens_features = Input(shape=(hyperparams['maxlen'],), name='word_seq')\n",
    "    embedding_layer = Embedding(hyperparams_features['max_features'], \n",
    "                                hyperparams_features['embedding_dim'], \n",
    "                                input_length=hyperparams['maxlen'],\n",
    "                                embeddings_regularizer=regularizers.l2(hyperparams['l2_embeddings']),\n",
    "                                weights=[embedding_matrix], \n",
    "                                trainable=hyperparams['trainable_embeddings'],\n",
    "                               name='embeddings_layer')(\n",
    "        tokens_features)\n",
    "#     if 'batchnorm' not in ignore_layer:\n",
    "#         embedding_layer_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "#                                                      name='embeddings_layer_norm')(embedding_layer)\n",
    "    embedding_layer = Dropout(hyperparams['dropout'], name='embedding_dropout')(embedding_layer)\n",
    "\n",
    "    if 'lstm' not in hyperparams['ignore_layer']:\n",
    "        if False: #tf.test.is_gpu_available():\n",
    "            lstm_layers = Bidirectional(CuDNNLSTM(\n",
    "                            hyperparams['lstm_units'], \n",
    "                                    return_sequences='attention' not in hyperparams['ignore_layer'], # only True if using attention\n",
    "                          name='LSTM_layer'), name='bilstm')(embedding_layer)\n",
    "        else:\n",
    "            lstm_layers = Bidirectional(LSTM(hyperparams['lstm_units'], \n",
    "                               return_sequences='attention' not in ignore_layer,\n",
    "                          name='LSTM_layer'), name='bilstm')(embedding_layer)\n",
    "#             lstm_layers = LSTM(hyperparams['lstm_units'], \n",
    "#                                return_sequences='attention' not in hyperparams['ignore_layer'],\n",
    "#                           name='LSTM_layer')(embedding_layer)\n",
    "            \n",
    "    elif 'cnn' not in hyperparams['ignore_layer']:\n",
    "        cnn_layers = Conv1D(hyperparams['filters'],\n",
    "                             hyperparams['kernel_size'],\n",
    "                             padding='valid',\n",
    "                             activation='relu',\n",
    "                             strides=1, name='convolution')(embedding_layer)\n",
    "        # we use max pooling:\n",
    "        cnn_layers = GlobalMaxPooling1D()(cnn_layers)\n",
    "    \n",
    "    # Attention\n",
    "    if 'attention' not in hyperparams['ignore_layer']:\n",
    "        attention = Dense(1, activation='tanh', name='attention')(lstm_layers)\n",
    "        attention = Flatten()(attention)\n",
    "        attention = Activation('softmax')(attention)\n",
    "        attention = RepeatVector(hyperparams['lstm_units']*2)(attention)\n",
    "        attention = Permute([2, 1])(attention)\n",
    "\n",
    "        sent_representation = Multiply()([lstm_layers, attention])\n",
    "        sent_representation = Lambda(lambda xin: K.sum(xin, axis=1), \n",
    "                                     output_shape=(hyperparams['lstm_units'],)\n",
    "                                    )(sent_representation)\n",
    "#         sent_representation = Lambda(attention, \n",
    "#                                          output_shape=(hyperparams['lstm_units'],\n",
    "#                                         ))(sent_representation)\n",
    "\n",
    "        \n",
    "    elif 'lstm' not in hyperparams['ignore_layer']:\n",
    "        sent_representation = lstm_layers\n",
    "    elif 'cnn' not in hyperparams['ignore_layer']:\n",
    "        sent_representation = cnn_layers\n",
    "    else:\n",
    "        sent_representation = None\n",
    "        \n",
    "    if sent_representation is not None:\n",
    "        sent_representation = Dropout(hyperparams['dropout'], name='lstm_att_dropout')(sent_representation)\n",
    "        if hyperparams['dense_sentence_units']:\n",
    "            sent_representation = Dense(units=hyperparams['dense_sentence_units'], activation='relu',\n",
    "                                       name='dense_sent_representation')(sent_representation)\n",
    "    \n",
    "    # Other features\n",
    "    numerical_features = Input(shape=(len(emotions) + 1 + len(liwc_categories),), name='numeric_input') # emotions and pronouns\n",
    "    sparse_features = Input(shape=(len(stopwords_list),), name='sparse_input') # stopwords\n",
    "\n",
    "    dense_layer_sparse = Dense(units=hyperparams['dense_bow_units'],\n",
    "                              name='sparse_feat_dense_layer', activation='relu',\n",
    "                                kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                              )(sparse_features)\n",
    "    \n",
    "    # BERT encoder\n",
    "    if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "        in_id_bert = Input(shape=(hyperparams['maxlen'],), name=\"input_ids_bert\")\n",
    "        in_mask_bert = Input(shape=(hyperparams['maxlen'],), name=\"input_masks_bert\")\n",
    "        in_segment_bert = Input(shape=(hyperparams['maxlen'],), name=\"segment_ids_bert\")\n",
    "        bert_inputs = [in_id_bert, in_mask_bert, in_segment_bert]\n",
    "\n",
    "        bert_output = BertLayer(n_fine_tune_layers=hyperparams['bert_finetune_layers'], \n",
    "                                pooling=hyperparams['bert_pooling'],\n",
    "                               trainable=hyperparams['bert_trainable'],\n",
    "                               name='bert_layer')(bert_inputs)\n",
    "        dense_bert = Dense(hyperparams['bert_dense_units'], activation='relu',\n",
    "                           kernel_regularizer=regularizers.l2(hyperparams['l2_bert']),\n",
    "                          name='bert_dense_layer')(bert_output)\n",
    "    else:\n",
    "        dense_bert = None\n",
    "\n",
    "\n",
    "    \n",
    "    # Batch normalization\n",
    "    if 'batchnorm' not in hyperparams['ignore_layer']:\n",
    "        numerical_features_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                     name='numerical_features_norm')(numerical_features)\n",
    "        sent_representation_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                      name='sent_repr_norm')(sent_representation)\n",
    "        dense_layer_sparse_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                     name='sparse_features_norm')(dense_layer_sparse)\n",
    "        if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "            if hyperparams['bert_dense_units']:\n",
    "                dense_bert_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                         name='bert_layer_norm')(dense_bert)\n",
    "            else:\n",
    "                dense_bert_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                         name='bert_layer_norm')(bert_output)\n",
    "        else:\n",
    "            dense_bert_norm = None\n",
    "        \n",
    "#     subjects = Input(shape=(1,), name='subjects')\n",
    "    \n",
    "\n",
    "    all_layers = {\n",
    "        'user_encoded': sent_representation,\n",
    "        'numerical_dense_layer': numerical_features,\n",
    "        'sparse_feat_dense_layer': dense_layer_sparse,\n",
    "        'bert_layer': dense_bert if hyperparams['bert_dense_units'] else bert_output,\n",
    "    }\n",
    "    if 'batchnorm' not in hyperparams['ignore_layer']:\n",
    "        all_layers = {\n",
    "            'user_encoded': sent_representation_norm,\n",
    "            'numerical_dense_layer': numerical_features_norm,\n",
    "            'sparse_feat_dense_layer': dense_layer_sparse_norm,\n",
    "            'bert_layer': dense_bert_norm\n",
    "        }\n",
    "    layers_to_merge = []\n",
    "    for n, l in all_layers.items():\n",
    "        if n in hyperparams['ignore_layer'] or l is None:\n",
    "            continue\n",
    "        layers_to_merge.append(l)\n",
    "        \n",
    "    if len(layers_to_merge) == 1:\n",
    "        merged_layers = layers_to_merge[0]\n",
    "    else:\n",
    "        merged_layers = concatenate(layers_to_merge)\n",
    "    output_layer = Dense(classes, activation='sigmoid' if classes==1 else 'softmax',\n",
    "                         name='output_layer',\n",
    "                        kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                        )(merged_layers)\n",
    "\n",
    "    # Compile model\n",
    "    if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "        inputs=[tokens_features, numerical_features, sparse_features, \n",
    "                              in_id_bert, in_mask_bert, in_segment_bert,\n",
    "    #                           subjects\n",
    "                             ]\n",
    "    else:\n",
    "        inputs=[tokens_features, numerical_features, sparse_features]\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "        \n",
    "\n",
    "    metrics_class = Metrics(threshold=hyperparams['threshold'])\n",
    "    model.compile(hyperparams['optimizer'], binary_crossentropy_custom,\n",
    "                  metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m,\n",
    "                          metrics_class.auc, \n",
    "                           tf.keras.metrics.AUC(name=\"AUC_keras\"), \n",
    "                           tf.keras.metrics.FalsePositives(), \n",
    "                           tf.keras.metrics.FalseNegatives()])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tl_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopwords_list,\n",
    "                liwc_categories,\n",
    "               ignore_layer=[]):\n",
    "    def attention(xin):\n",
    "        return K.sum(xin, axis=1) \n",
    "    tokens_features = Input(shape=(hyperparams['maxlen'],), name='word_seq')\n",
    "    embedding_layer = Embedding(hyperparams_features['max_features'], \n",
    "                                hyperparams_features['embedding_dim'], \n",
    "                                input_length=hyperparams['maxlen'],\n",
    "                                embeddings_regularizer=regularizers.l2(hyperparams['l2_embeddings']),\n",
    "                                weights=[embedding_matrix], \n",
    "                                trainable=False,\n",
    "                               name='embeddings_layer')(\n",
    "        tokens_features)\n",
    "#     if 'batchnorm' not in ignore_layer:\n",
    "#         embedding_layer_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "#                                                      name='embeddings_layer_norm')(embedding_layer)\n",
    "    embedding_layer = Dropout(hyperparams['dropout'], name='embedding_dropout')(embedding_layer)\n",
    "\n",
    "    if 'lstm' not in ignore_layer:\n",
    "        if False: #tf.test.is_gpu_available():\n",
    "            lstm_layers = Bidirectional(CuDNNLSTM(hyperparams['lstm_units'], trainable=False,\n",
    "                                    return_sequences='attention' not in ignore_layer, # only True if using attention\n",
    "                          name='LSTM_layer'), name='bilstm')(embedding_layer)\n",
    "        else:\n",
    "            lstm_layers = Bidirectional(LSTM(hyperparams['lstm_units'], trainable=False,\n",
    "                               return_sequences='attention' not in ignore_layer,\n",
    "                          name='LSTM_layer'), name='bilstm')(embedding_layer)\n",
    "            \n",
    "    elif 'cnn' not in ignore_layer:\n",
    "        cnn_layers = Conv1D(hyperparams['filters'],\n",
    "                             hyperparams['kernel_size'],\n",
    "                             padding='valid',\n",
    "                             activation='relu',\n",
    "                             strides=1,\n",
    "                           trainable=False,\n",
    "                           name='convolution')(embedding_layer)\n",
    "        # we use max pooling:\n",
    "        cnn_layers = GlobalMaxPooling1D(trainable=False)(cnn_layers)\n",
    "    \n",
    "    # Attention\n",
    "    if 'attention' not in ignore_layer:\n",
    "        attention = Dense(1, activation='tanh', name='attention', trainable=False)(lstm_layers)\n",
    "        attention = Flatten()(attention)\n",
    "        attention = Activation('softmax')(attention)\n",
    "        attention = RepeatVector(hyperparams['lstm_units']*2)(attention)\n",
    "        attention = Permute([2, 1])(attention)\n",
    "\n",
    "        sent_representation = Multiply()([lstm_layers, attention])\n",
    "        sent_representation = Lambda(lambda xin: K.sum(xin, axis=1), \n",
    "                                     output_shape=(hyperparams['lstm_units'],)\n",
    "                                    )(sent_representation)\n",
    "#         sent_representation = Lambda(attention, \n",
    "#                                          output_shape=(hyperparams['lstm_units'],\n",
    "#                                         ))(sent_representation)\n",
    "\n",
    "        \n",
    "    elif 'lstm' not in ignore_layer:\n",
    "        sent_representation = lstm_layers\n",
    "    elif 'cnn' not in ignore_layer:\n",
    "        sent_representation = cnn_layers\n",
    "        \n",
    "    \n",
    "    sent_representation = Dropout(hyperparams['dropout'], name='lstm_att_dropout')(sent_representation)\n",
    "    if hyperparams['dense_sentence_units']:\n",
    "        sent_representation = Dense(units=hyperparams['dense_sentence_units'], activation='relu',\n",
    "                                   name='dense_sent_representation',\n",
    "                                   trainable=False)(sent_representation)\n",
    "    \n",
    "    # Other features\n",
    "    numerical_features = Input(shape=(len(emotions) + 1 + len(liwc_categories),), name='numeric_input') # emotions and pronouns\n",
    "    sparse_features = Input(shape=(len(stopwords_list),), name='sparse_input') # stopwords\n",
    "\n",
    "    dense_layer_sparse = Dense(units=hyperparams['dense_bow_units'],\n",
    "                              name='sparse_feat_dense_layer', activation='relu',\n",
    "                                kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                               trainable=False,\n",
    "                              )(sparse_features)\n",
    "    \n",
    "    # BERT encoder\n",
    "    if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "        in_id_bert = Input(shape=(hyperparams['maxlen'],), name=\"input_ids_bert\")\n",
    "        in_mask_bert = Input(shape=(hyperparams['maxlen'],), name=\"input_masks_bert\")\n",
    "        in_segment_bert = Input(shape=(hyperparams['maxlen'],), name=\"segment_ids_bert\")\n",
    "        bert_inputs = [in_id_bert, in_mask_bert, in_segment_bert]\n",
    "\n",
    "        bert_output = BertLayer(n_fine_tune_layers=hyperparams['bert_finetune_layers'], \n",
    "                                pooling=hyperparams['bert_pooling'],\n",
    "                               trainable=False,\n",
    "                               name='bert_layer')(bert_inputs)\n",
    "        dense_bert = Dense(hyperparams['bert_dense_units'], activation='relu',\n",
    "                           kernel_regularizer=regularizers.l2(hyperparams['l2_bert']),\n",
    "                          name='bert_dense_layer',\n",
    "                          trainable=False)(bert_output)\n",
    "    else:\n",
    "        dense_bert = None\n",
    "\n",
    "\n",
    "    \n",
    "    # Batch normalization\n",
    "    if 'batchnorm' not in ignore_layer:\n",
    "        numerical_features_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                     name='numerical_features_norm')(numerical_features)\n",
    "        sent_representation_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                      name='sent_repr_norm')(sent_representation)\n",
    "        dense_layer_sparse_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                     name='sparse_features_norm')(dense_layer_sparse)\n",
    "        if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "            if hyperparams['bert_dense_units']:\n",
    "                dense_bert_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                         name='bert_layer_norm')(dense_bert)\n",
    "            else:\n",
    "                dense_bert_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                         name='bert_layer_norm')(bert_output)\n",
    "        else:\n",
    "            dense_bert_norm = None\n",
    "        \n",
    "#     subjects = Input(shape=(1,), name='subjects')\n",
    "    \n",
    "\n",
    "    all_layers = {\n",
    "        'user_encoded': sent_representation,\n",
    "        'numerical_dense_layer': numerical_features,\n",
    "        'sparse_feat_dense_layer': dense_layer_sparse,\n",
    "        'bert_layer': dense_bert if hyperparams['bert_dense_units'] else bert_output,\n",
    "    }\n",
    "    if 'batchnorm' not in ignore_layer:\n",
    "        all_layers = {\n",
    "            'user_encoded': sent_representation_norm,\n",
    "            'numerical_dense_layer': numerical_features_norm,\n",
    "            'sparse_feat_dense_layer': dense_layer_sparse_norm,\n",
    "            'bert_layer': dense_bert_norm\n",
    "        }\n",
    "    layers_to_merge = []\n",
    "    for n, l in all_layers.items():\n",
    "        if n in ignore_layer:\n",
    "            continue\n",
    "        layers_to_merge.append(l)\n",
    "        \n",
    "    if len(layers_to_merge) == 1:\n",
    "        merged_layers = layers_to_merge[0]\n",
    "    else:\n",
    "        merged_layers = concatenate(layers_to_merge)\n",
    "        \n",
    "    TL_layer = Dense(hyperparams['transfer_units'], activation='sigmoid',\n",
    "                         name='tl_layer',\n",
    "                        kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                         trainable=True\n",
    "                        )(merged_layers)\n",
    "    output_layer = Dense(1, activation='sigmoid',\n",
    "                         name='output_layer_tl',\n",
    "                        kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                         trainable=True,\n",
    "                        )(TL_layer)\n",
    "\n",
    "    # Compile model\n",
    "    if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "        inputs=[tokens_features, numerical_features, sparse_features, \n",
    "                              in_id_bert, in_mask_bert, in_segment_bert,\n",
    "    #                           subjects\n",
    "                             ]\n",
    "    else:\n",
    "        inputs=[tokens_features, numerical_features, sparse_features]\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    if classes==1:\n",
    "        metrics_class = Metrics(threshold=hyperparams['threshold'])\n",
    "        model.compile(hyperparams['optimizer'], binary_crossentropy_custom,\n",
    "                      metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m,\n",
    "                              metrics_class.auc, \n",
    "                               tf.keras.metrics.AUC(name=\"AUC_keras\"), \n",
    "                               tf.keras.metrics.FalsePositives(), \n",
    "                               tf.keras.metrics.FalseNegatives()])\n",
    "    else:\n",
    "        model.compile(hyperparams['optimizer'], K.categorical_crossentropy,\n",
    "                     metrics=[tf.keras.metrics.CategoricalAccuracy(name='cat_acc'),\n",
    "#                              tf.keras.metrics.Precision(name='prec'), tf.keras.metrics.Recall(name='rec'), \n",
    "                              ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hierarchical_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopwords_list,\n",
    "                liwc_categories,\n",
    "               ignore_layer=[], activations=None, classes=1):\n",
    "    def attention(xin):\n",
    "        return K.sum(xin, axis=1) \n",
    "\n",
    "\n",
    "    # Post/sentence representation - word sequence\n",
    "    tokens_features = Input(shape=(hyperparams['maxlen'],), name='word_seq')\n",
    "    embedding_layer = Embedding(hyperparams_features['max_features'], \n",
    "                                hyperparams_features['embedding_dim'], \n",
    "                                input_length=hyperparams['maxlen'],\n",
    "                                embeddings_regularizer=regularizers.l2(hyperparams['l2_embeddings']),\n",
    "                                weights=[embedding_matrix], \n",
    "                                trainable=hyperparams['trainable_embeddings'],\n",
    "                               name='embeddings_layer')(\n",
    "        tokens_features)\n",
    "    embedding_layer = Dropout(hyperparams['dropout'], name='embedding_dropout')(embedding_layer)\n",
    "\n",
    "    \n",
    "    if 'lstm' not in ignore_layer:\n",
    "        if False: #tf.test.is_gpu_available():\n",
    "            lstm_layers = CuDNNLSTM(hyperparams['lstm_units'], \n",
    "                                    return_sequences='attention' not in ignore_layer, # only True if using attention\n",
    "                          name='LSTM_layer')(embedding_layer)\n",
    "        else:\n",
    "            lstm_layers = LSTM(hyperparams['lstm_units'], \n",
    "                               return_sequences='attention' not in ignore_layer,\n",
    "                          name='LSTM_layer')(embedding_layer)\n",
    "\n",
    "        # Attention\n",
    "        if 'attention' not in ignore_layer:\n",
    "            attention_layer = Dense(1, activation='tanh', name='attention')\n",
    "            attention = attention_layer(lstm_layers)\n",
    "            attention = Flatten()(attention)\n",
    "            attention_output = Activation('softmax')(attention)\n",
    "            attention = RepeatVector(hyperparams['lstm_units'])(attention_output)\n",
    "            attention = Permute([2, 1])(attention)\n",
    "\n",
    "            sent_representation = Multiply()([lstm_layers, attention])\n",
    "            sent_representation = Lambda(lambda xin: K.sum(xin, axis=1), \n",
    "                                     output_shape=(hyperparams['lstm_units'],)\n",
    "                                    )(sent_representation)\n",
    "\n",
    "#             sent_representation = Lambda(attention, \n",
    "#                                          output_shape=(hyperparams['lstm_units'],\n",
    "#                                         ))(sent_representation)\n",
    "        else:\n",
    "            sent_representation = lstm_layers\n",
    "\n",
    "    elif 'cnn' not in ignore_layer:\n",
    "        cnn_layers = Conv1D(hyperparams['filters'],\n",
    "                             hyperparams['kernel_size'],\n",
    "                             padding='valid',\n",
    "                             activation='relu',\n",
    "                             strides=1)(embedding_layer)\n",
    "        # we use max pooling:\n",
    "        sent_representation = GlobalMaxPooling1D()(cnn_layers)\n",
    "    \n",
    "    \n",
    "    if 'batchnorm' not in ignore_layer:\n",
    "        sent_representation = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                          name='sent_repr_norm')(sent_representation)\n",
    "    sent_representation = Dropout(hyperparams['dropout'], name='sent_repr_dropout')(sent_representation)\n",
    "\n",
    "            # Other features \n",
    "    numerical_features_history = Input(shape=(\n",
    "            hyperparams['posts_per_group'],\n",
    "            len(emotions) + 1 + len(liwc_categories)\n",
    "        ), name='numeric_input_hist') # emotions and pronouns\n",
    "    sparse_features_history = Input(shape=(\n",
    "            hyperparams['posts_per_group'],\n",
    "            len(stopwords_list)\n",
    "        ), name='sparse_input_hist') # stopwords\n",
    "\n",
    "\n",
    "\n",
    "    if activations == 'attention':\n",
    "#         sent_representation = Flatten()(attention_layer.output)\n",
    "        sent_representation = attention_output\n",
    "\n",
    "\n",
    "    posts_history_input = Input(shape=(hyperparams['posts_per_group'], \n",
    "                                     hyperparams['maxlen']\n",
    "                                          ), name='hierarchical_word_seq_input')\n",
    "\n",
    "    # Hierarchy\n",
    "    sentEncoder = Model(inputs=tokens_features, \n",
    "                        outputs=sent_representation)\n",
    "    sentEncoder.summary()\n",
    "\n",
    "    user_encoder = TimeDistributed(sentEncoder, name='user_encoder')(posts_history_input)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    if activations != 'attention':\n",
    "        \n",
    "        \n",
    "        # BERT encoder\n",
    "        if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "            in_id_bert = Input(shape=(hyperparams['maxlen'],), name=\"input_ids_bert\")\n",
    "            in_mask_bert = Input(shape=(hyperparams['maxlen'],), name=\"input_masks_bert\")\n",
    "            in_segment_bert = Input(shape=(hyperparams['maxlen'],), name=\"segment_ids_bert\")\n",
    "            bert_inputs = [in_id_bert, in_mask_bert, in_segment_bert]\n",
    "\n",
    "            bert_output = BertLayer(n_fine_tune_layers=hyperparams['bert_finetune_layers'], \n",
    "                                    pooling=hyperparams['bert_pooling'],\n",
    "                                   trainable=hyperparams['bert_trainable'],\n",
    "                                   name='bert_layer')(bert_inputs)\n",
    "            dense_bert = Dense(hyperparams['bert_dense_units'], \n",
    "                               activation='relu',\n",
    "                              kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                              name='bert_dense_layer')(bert_output)\n",
    "\n",
    "            bertSentEncoder = Model(bert_inputs, dense_bert)\n",
    "\n",
    "\n",
    "            in_id_bert_history = Input(shape=(hyperparams['posts_per_group'],\n",
    "                                                              hyperparams['maxlen'],), name=\"input_ids_bert_hist\")\n",
    "            in_mask_bert_history = Input(shape=(hyperparams['posts_per_group'],\n",
    "                                                                hyperparams['maxlen'],), name=\"input_masks_bert_hist\")\n",
    "            in_segment_bert_history = Input(shape=(hyperparams['posts_per_group'],\n",
    "                                                                   hyperparams['maxlen'],), name=\"segment_ids_bert_hist\")\n",
    "            bert_inputs_history = [in_id_bert_history, in_mask_bert_history, in_segment_bert_history]\n",
    "            bert_inputs_concatenated = concatenate(bert_inputs_history)\n",
    "            inputs_indices = [hyperparams['maxlen']*i for i in range(3)]\n",
    "            # slice the input in equal slices on the last dimension\n",
    "            bert_encoder_layer = TimeDistributed(Lambda(lambda x: bertSentEncoder([x[:,inputs_indices[0]:inputs_indices[1]], \n",
    "                                                                          x[:,inputs_indices[1]:inputs_indices[2]],\n",
    "                                                                                  x[:,inputs_indices[2]:]])),\n",
    "                                                name='bert_distributed_layer')(\n",
    "                                bert_inputs_concatenated)\n",
    "            bertUserEncoder = Model(bert_inputs_history, bert_encoder_layer)\n",
    "            bertUserEncoder.summary()\n",
    "\n",
    "            bert_user_encoder = bertUserEncoder(bert_inputs_history)\n",
    "        else:\n",
    "            bert_user_encoder = None\n",
    "\n",
    "\n",
    "        dense_layer_sparse = Dense(units=hyperparams['dense_bow_units'],\n",
    "                                  name='sparse_feat_dense_layer', activation='relu',\n",
    "                                    kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                                  )\n",
    "        dense_layer_sparse_user = TimeDistributed(dense_layer_sparse,\n",
    "                                                 name='sparse_dense_layer_user')(sparse_features_history)\n",
    "        \n",
    "                \n",
    "        dense_layer_numerical = Dense(units=hyperparams['dense_numerical_units'],\n",
    "                                  name='numerical_feat_dense_layer', activation='relu',\n",
    "                                    kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                                  )\n",
    "        dense_layer_numerical_user = TimeDistributed(dense_layer_numerical,\n",
    "                                                 name='numerical_dense_layer_user')(numerical_features_history)\n",
    "\n",
    "\n",
    "        # Concatenate features\n",
    "        if 'batchnorm' not in ignore_layer:\n",
    "#             numerical_features_history_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "#                                                          name='numerical_features_norm')(numerical_features_history)\n",
    "            dense_layer_numerical_user = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                         name='numerical_features_norm')(dense_layer_numerical_user)\n",
    "            dense_layer_sparse_user = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                         name='sparse_features_norm')(dense_layer_sparse_user)\n",
    "        all_layers = {\n",
    "            'user_encoded': user_encoder,\n",
    "            'bert_layer': bert_user_encoder,\n",
    "#             'numerical_dense_layer': numerical_features_history if 'batchnorm' in ignore_layer \\\n",
    "#                         else numerical_features_history_norm\n",
    "            'numerical_dense_layer': dense_layer_numerical_user,\n",
    "\n",
    "            'sparse_feat_dense_layer': dense_layer_sparse_user,\n",
    "        }\n",
    "\n",
    "        layers_to_merge = [l for n,l in all_layers.items() if n not in ignore_layer]\n",
    "        if len(layers_to_merge) == 1:\n",
    "            merged_layers = layers_to_merge[0]\n",
    "        else:\n",
    "            merged_layers = concatenate(layers_to_merge)\n",
    "\n",
    "        if 'lstm_user' not in ignore_layer:\n",
    "\n",
    "            if False:#tf.test.is_gpu_available():\n",
    "                lstm_user_layers = CuDNNLSTM(hyperparams['lstm_units_user'], \n",
    "                                        return_sequences='attention_user' not in ignore_layer, # only True if using attention\n",
    "                              name='LSTM_layer_user')(merged_layers)\n",
    "            else:\n",
    "                lstm_user_layers = LSTM(hyperparams['lstm_units_user'], \n",
    "                                   return_sequences='attention_user' not in ignore_layer,\n",
    "                              name='LSTM_layer_user')(merged_layers)\n",
    "\n",
    "            # Attention\n",
    "            if 'attention_user' not in ignore_layer:\n",
    "                attention_user_layer = Dense(1, activation='tanh', name='attention_user')\n",
    "                attention_user = attention_user_layer(lstm_user_layers)\n",
    "                attention_user = Flatten()(attention_user)\n",
    "                attention_user_output = Activation('softmax')(attention_user)\n",
    "                attention_user = RepeatVector(hyperparams['lstm_units_user'])(attention_user_output)\n",
    "                attention_user = Permute([2, 1])(attention_user)\n",
    "\n",
    "                user_representation = Multiply()([lstm_user_layers, attention_user])\n",
    "                user_representation = Lambda(lambda xin: K.sum(xin, axis=1), \n",
    "                                             output_shape=(hyperparams['lstm_units_user'],))(user_representation)\n",
    "    #             user_representation = Lambda(attention, \n",
    "    #                                          output_shape=(hyperparams['lstm_units_user'],\n",
    "    #                                         ))(user_representation)\n",
    "            else:\n",
    "                user_representation = lstm_user_layers\n",
    "\n",
    "\n",
    "        elif 'cnn_user' not in ignore_layer:\n",
    "            cnn_layers_user = Conv1D(hyperparams['filters_user'],\n",
    "                                 hyperparams['kernel_size_user'],\n",
    "                                 padding='valid',\n",
    "                                 activation='relu',\n",
    "                                 strides=1)(merged_layers)\n",
    "            # we use max pooling:\n",
    "            user_representation = GlobalMaxPooling1D()(cnn_layers_user)\n",
    "    #         user_representation = Flatten()(user_representation)\n",
    "\n",
    "\n",
    "        user_representation = Dropout(hyperparams['dropout'], name='user_repr_dropout')(user_representation)\n",
    "\n",
    "\n",
    "        if hyperparams['dense_user_units']:\n",
    "            user_representation = Dense(units=hyperparams['dense_user_units'], activation='relu',\n",
    "                                       name='dense_user_representation')(user_representation)\n",
    "\n",
    "        output_layer = Dense(classes, activation='sigmoid' if classes==1 else 'softmax',\n",
    "                             name='output_layer',\n",
    "                            kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                             input_shape=(hyperparams['lstm_units_user'],),\n",
    "                            )(user_representation)\n",
    "\n",
    "    # Compile model\n",
    "\n",
    "#     elif activations == 'attention':\n",
    "#         outputs = attention_layer.output\n",
    "    if activations == 'attention':\n",
    "        outputs = user_encoder\n",
    "\n",
    "        \n",
    "    elif activations == 'attention_user':\n",
    "#         outputs = attention_user.output\n",
    "        outputs = attention_user_output\n",
    "    elif activations == 'output_layer':\n",
    "        outputs = user_representation\n",
    "\n",
    "    \n",
    "    else:\n",
    "        outputs = output_layer\n",
    "    if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "\n",
    "        hierarchical_model = Model(inputs=[posts_history_input, \n",
    "                                       numerical_features_history, sparse_features_history,\n",
    "                                      in_id_bert_history, in_mask_bert_history, in_segment_bert_history], \n",
    "                  outputs=outputs)\n",
    "    else:\n",
    "        hierarchical_model = Model(inputs=[posts_history_input, \n",
    "                                       numerical_features_history, sparse_features_history,\n",
    "                                      ], \n",
    "                  outputs=outputs)\n",
    "    hierarchical_model.summary()\n",
    "    \n",
    "    if classes==1:\n",
    "        metrics_class = Metrics(threshold=hyperparams['threshold'])\n",
    "        hierarchical_model.compile(hyperparams['optimizer'], binary_crossentropy_custom,\n",
    "                      metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m,\n",
    "                              metrics_class.auc])\n",
    "    else:\n",
    "        \n",
    "        hierarchical_model.compile(hyperparams['optimizer'], K.categorical_crossentropy,\n",
    "                     metrics=[ tf.keras.metrics.CategoricalAccuracy(name='cat_acc'),\n",
    "#                              tf.keras.metrics.Precision(name='prec'), tf.keras.metrics.Recall(name='rec'), \n",
    "                              ])\n",
    "    return hierarchical_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_tl_model(pretrained_model, hyperparams, units=100,  emotions=emotions, stopwords_list=stopword_list,\n",
    "#                 liwc_categories=liwc_categories):\n",
    "#     tokens_features = Input(shape=(hyperparams['maxlen'],), name='word_seq_tl')\n",
    "#     numerical_features = Input(shape=(len(emotions) + 1 + len(liwc_categories),), name='numeric_input_tl') # emotions and pronouns\n",
    "#     sparse_features = Input(shape=(len(stopwords_list),), name='sparse_input_tl') # stopwords\n",
    "#     pretrained_model.summary()\n",
    "#     print(\"inputs\", tokens_features, numerical_features, sparse_features)\n",
    "#     pretrained_output = pretrained_model(inputs=[tokens_features, numerical_features, sparse_features])\n",
    "#     # TODO: set trainable to false\n",
    "#     tl_layer = Dense(units=units, activation='relu',\n",
    "#                                    name='tl_layer', trainable=False,\n",
    "#                         kernel_regularizer=regularizers.l2(hyperparams['l2_dense'])\n",
    "#                     )(pretrained_output)\n",
    "#     output_layer = Dense(1, activation='sigmoid',\n",
    "#                          name='tl_output_layer',\n",
    "#                         kernel_regularizer=regularizers.l2(hyperparams['l2_dense'])\n",
    "#                         )(tl_layer)\n",
    "# #     pretrained_model.outputs = output_layer ## I DUNNO\n",
    "#     tl_model = Model(inputs=[tokens_features, numerical_features, sparse_features], \n",
    "#           outputs=output_layer)\n",
    "#     metrics_class = Metrics(threshold=hyperparams['threshold'])\n",
    "#     tl_model.compile(hyperparams['optimizer'], binary_crossentropy_custom,\n",
    "#                   metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m,\n",
    "#                           metrics_class.auc, \n",
    "#                            tf.keras.metrics.AUC(name=\"AUC_keras\"), \n",
    "#                            tf.keras.metrics.FalsePositives(), \n",
    "#                            tf.keras.metrics.FalseNegatives()])\n",
    "\n",
    "#     return tl_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopword_list,\n",
    "#                     liwc_categories=[c for c in categories if c in writings_df.columns]\n",
    "# ,\n",
    "#                    ignore_layer=hyperparams['ignore_layer'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchical_model = build_hierarchical_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopword_list,\n",
    "#                     liwc_categories=[c for c in categories if c in writings_df.columns]\n",
    "# ,\n",
    "#                    ignore_layer=hyperparams['ignore_layer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(model, 'models/sequential_bert_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize_vars(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightsHistory(callbacks.Callback):\n",
    "    def __init__(self, logs={}):\n",
    "        super(WeightsHistory, self).__init__()\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.log_weights(0)\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % 10 == 0:\n",
    "            self.log_weights(epoch)\n",
    "    def log_weights(self, step):\n",
    "        for layer in self.model.layers:\n",
    "            try:\n",
    "                experiment.log_histogram_3d(layer.get_weights()[0], \n",
    "                                            name=layer.name + \"_weight\", step=step)\n",
    "            except Exception as e:\n",
    "#                 logger.debug(\"Logging weights error: \" + layer.name + \"; \" + str(e) + \"\\n\")\n",
    "                # Layer probably does not exist\n",
    "                pass\n",
    "\n",
    "class OutputsHistory(callbacks.Callback):\n",
    "    def __init__(self, logs={}, generator=None, generator_type=\"\"):\n",
    "        super(OutputsHistory, self).__init__()\n",
    "        self.generator_type = generator_type\n",
    "        if generator:\n",
    "            self.generator = generator\n",
    "        elif generator_type:\n",
    "            self.generator = DataGenerator(user_level_data, subjects_split, \n",
    "                                     set_type=generator_type, \n",
    "                                   hierarchical=hyperparams['hierarchical'],\n",
    "                                seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
    "                                     max_posts_per_user=None,\n",
    "                                   pad_with_duplication=False,\n",
    "                                    posts_per_group=hyperparams['posts_per_group'],\n",
    "                                    post_groups_per_user=None, \n",
    "                                           liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                           compute_liwc=True,\n",
    "                                         emotions=emotions, liwc_categories=liwc_categories,\n",
    "                                     sample_seqs=False, shuffle=False)\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.log_outputs(0)\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % 2 == 0:\n",
    "            self.log_outputs(epoch)\n",
    "    def log_outputs(self, step):\n",
    "        try:\n",
    "            experiment.log_histogram_3d(self.model.predict(self.generator,  verbose=1, steps=2),\n",
    "                                        name='output_%s' % self.generator_type, step=step)\n",
    "        except Exception as e:\n",
    "            logger.debug(\"Logging outputs error: \" + str(e) + \"\\n\")\n",
    "#                 Layer probably does not exist\n",
    "            pass\n",
    "\n",
    "class ActivationsAttention(callbacks.Callback):\n",
    "    def __init__(self, logs={}, generator=None, generator_type=\"\"):\n",
    "        super(ActivationsAttention, self).__init__()\n",
    "        self.generator_type = generator_type\n",
    "        if generator:\n",
    "            self.generator = generator\n",
    "        elif generator_type:\n",
    "            self.generator = DataGenerator(user_level_data, subjects_split, \n",
    "                                     set_type=generator_type, \n",
    "                                   hierarchical=hyperparams['hierarchical'],\n",
    "                                seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
    "                                     max_posts_per_user=None,\n",
    "                                   pad_with_duplication=False,\n",
    "                                    posts_per_group=hyperparams['posts_per_group'],\n",
    "                                    post_groups_per_user=None, \n",
    "                                     sample_seqs=False, shuffle=False)\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.log_outputs(0)\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % 10 == 0:\n",
    "            self.log_outputs(epoch)\n",
    "    def log_outputs(self, step):\n",
    "        try:\n",
    "            experiment.log_histogram_3d(self.model.get_layer('attention_user').output.eval())\n",
    "        except Exception as e:\n",
    "            logger.debug(\"Logging activations error: \" + str(e) + \"\\n\")\n",
    "            pass\n",
    "\n",
    "class LRHistory(callbacks.Callback):\n",
    "    def __init__(self, logs={}):\n",
    "        super(LRHistory, self).__init__()\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.log_lr()\n",
    "        \n",
    "    def log_lr(self):\n",
    "        lr = K.eval(self.model.optimizer.lr)\n",
    "        logger.debug(\"Learning rate is %f...\\n\" % lr)\n",
    "        experiment.log_parameter('lr', lr)\n",
    "\n",
    "class FreezeLayer(callbacks.Callback):\n",
    "    def __init__(self, logs={}, patience=5, layer={'user_encoder':'embeddings_layer'}, verbose=1, set_to=False):\n",
    "        super(FreezeLayer, self).__init__()\n",
    "        self.freeze_epoch = patience\n",
    "        self.freeze_layer = layer\n",
    "        self.verbose = verbose\n",
    "        self.set_to = set_to\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        if type(self.freeze_layer)==dict:\n",
    "            submodel = self.model.get_layer(list(self.freeze_layer.keys())[0])\n",
    "        else:\n",
    "            submodel = self.model\n",
    "        logging.debug(\"Trainable embeddings\", submodel.get_layer(self.freeze_layer).trainable)\n",
    "        if epoch == self.freeze_epoch:\n",
    "            try:\n",
    "                layer = submodel.get_layer(self.freeze_layer)\n",
    "                old_value = layer.trainable\n",
    "                layer.trainable = self.set_to\n",
    "                # TODO: does this reset the optimizer? should I also compile the top-level model?\n",
    "                self.model.compile(hyperparams['optimizer'], binary_crossentropy_custom,\n",
    "                  metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m])\n",
    "                if self.verbose:\n",
    "                    logging.debug(\"Setting %s layer from %s to trainable=%s...\\n\" % (layer.name, old_value,\n",
    "                                                                   submodel.get_layer(self.freeze_layer).trainable))\n",
    "            except Exception as e:\n",
    "                # layer probably does not exist\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, hyperparams,\n",
    "                data_generator_train, data_generator_valid,\n",
    "                epochs, class_weight, start_epoch=0, workers=4,\n",
    "                callback_list = [],\n",
    "                \n",
    "                model_path='/tmp/model',\n",
    "                validation_set='valid',\n",
    "               verbose=1):\n",
    "    \n",
    "    logger.info(\"Initializing callbacks...\\n\")\n",
    "    # Initialize callbacks\n",
    "    freeze_layer = FreezeLayer(patience=hyperparams['freeze_patience'], set_to=not hyperparams['trainable_embeddings'])\n",
    "    weights_history = WeightsHistory()\n",
    "    outputs_history_valid = OutputsHistory(generator_type=validation_set)\n",
    "    outputs_history_train = OutputsHistory(generator_type='train')\n",
    "    activations_history_train = ActivationsAttention(generator_type='train')\n",
    "    lr_history = LRHistory()\n",
    "\n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=hyperparams['reduce_lr_factor'],\n",
    "                              patience=hyperparams['reduce_lr_patience'], min_lr=0.000001, verbose=1)\n",
    "    lr_schedule = callbacks.LearningRateScheduler(lambda epoch, lr: \n",
    "                                                  lr if (epoch+1)%hyperparams['scheduled_reduce_lr_freq']!=0 else\n",
    "                                                  lr*hyperparams['scheduled_reduce_lr_factor'], verbose=1)\n",
    "    callbacks_dict = {'freeze_layer': freeze_layer, 'weights_history': weights_history,\n",
    "           'outputs_history_valid': outputs_history_valid, 'outputs_history_train': outputs_history_train,\n",
    "           'lr_history': lr_history,\n",
    "            'activations': activations_history_train,\n",
    "           'reduce_lr_plateau': reduce_lr,\n",
    "            'lr_schedule': lr_schedule}\n",
    "\n",
    "    \n",
    "    logging.info('Train...')\n",
    "\n",
    "\n",
    "    history = model.fit_generator(data_generator_train,\n",
    "                steps_per_epoch=100,\n",
    "              epochs=epochs, initial_epoch=start_epoch, \n",
    "              class_weight=class_weight,\n",
    "              validation_data=data_generator_valid,\n",
    "                        verbose=verbose,\n",
    "#               validation_split=0.3,\n",
    "                       workers=workers,\n",
    "            callbacks = [\n",
    "                callbacks.ModelCheckpoint(filepath='%s_best.h5' % model_path, verbose=1, \n",
    "                                          save_best_only=True, save_weights_only=True),\n",
    "#                 callbacks.EarlyStopping(patience=hyperparams['early_stopping_patience'],\n",
    "#                                        restore_best_weights=True)\n",
    "            ] + [\n",
    "                callbacks_dict[c] for c in [\n",
    "#                     'weights_history', \n",
    "                    'outputs_history_valid', \n",
    "#                     'outputs_history_train', \n",
    "#                     'reduce_lr_plateau', \n",
    "#                     'lr_schedule', \n",
    "#                     'activations'\n",
    "                ]])\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network_type(hyperparams):\n",
    "    if 'lstm' in hyperparams['ignore_layer']:\n",
    "        network_type = 'cnn'\n",
    "    else:\n",
    "        network_type = 'lstm'\n",
    "    if 'user_encoded' in hyperparams['ignore_layer']:\n",
    "        if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "            network_type = 'bert'\n",
    "        else:\n",
    "            network_type = 'extfeatures'\n",
    "    if hyperparams['hierarchical']:\n",
    "        hierarch_type = 'hierarchical'\n",
    "    else:\n",
    "        hierarch_type = 'seq'\n",
    "    return network_type, hierarch_type\n",
    "\n",
    "def initialize_experiment(hyperparams, nrc_lexicon_path, emotions, pretrained_embeddings_path, \n",
    "                          dataset_type, transfer_type):\n",
    "\n",
    "    experiment = Experiment(api_key=\"eoBdVyznAhfg3bK9pZ58ZSXfv\",\n",
    "                            project_name=\"mental\", workspace=\"ananana\", disabled=False)\n",
    "\n",
    "    experiment.log_parameters(hyperparams_features)\n",
    "\n",
    "    experiment.log_parameter('emotion_lexicon', nrc_lexicon_path)\n",
    "    experiment.log_parameter('emotions', emotions)\n",
    "    experiment.log_parameter('embeddings_path', pretrained_embeddings_path)\n",
    "    experiment.log_parameter('dataset_type', dataset_type)\n",
    "    experiment.log_parameter('transfer_type', transfer_type)\n",
    "    experiment.add_tag(dataset_type)\n",
    "    experiment.log_parameters(hyperparams)\n",
    "    network_type, hierarch_type = get_network_type(hyperparams)\n",
    "    experiment.add_tag(network_type)\n",
    "    experiment.add_tag(hierarch_type)\n",
    "    \n",
    "    return experiment\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_datasets(user_level_data, subjects_split, hyperparams, hyperparams_features, \n",
    "                        validation_set, emotions, liwc_categories, session=None, classes=1):\n",
    "    liwc_words_for_categories = pickle.load(open(hyperparams_features['liwc_words_cached'], 'rb'))\n",
    "    if classes!=1 and 'class_weights' in hyperparams:\n",
    "        class_weights = hyperparams['class_weights']\n",
    "    else:\n",
    "        class_weights = None\n",
    "    data_generator_train = DataGenerator(user_level_data, subjects_split, set_type='train',\n",
    "                                        seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
    "                                        sample_seqs=hyperparams['sample_seqs'], sampling_distr=hyperparams['sampling_distr'],\n",
    "                                        posts_per_group=hyperparams['posts_per_group'], post_groups_per_user=hyperparams['post_groups_per_user'],\n",
    "                                        max_posts_per_user=hyperparams['posts_per_user'], \n",
    "                                         hierarchical=hyperparams['hierarchical'], \n",
    "                                         use_bert='bert_layer' not in hyperparams['ignore_layer'],\n",
    "                                         compute_liwc=True, liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                         emotions=emotions, liwc_categories=liwc_categories,\n",
    "                                        session=session, classes=classes, class_weights=class_weights)\n",
    "    data_generator_valid = DataGenerator(user_level_data, subjects_split, set_type=validation_set,\n",
    "                                        seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
    "                                        posts_per_group=hyperparams['posts_per_group'], \n",
    "                                         post_groups_per_user=1,#hyperparams['post_groups_per_user'],\n",
    "                                        max_posts_per_user=None, \n",
    "                                        sample_seqs=False, shuffle=False, hierarchical=hyperparams['hierarchical'],\n",
    "                                         use_bert='bert_layer' not in hyperparams['ignore_layer'],\n",
    "                                         compute_liwc=True, liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                        emotions=emotions, liwc_categories=liwc_categories,\n",
    "                                        session=session, classes=classes)\n",
    "\n",
    "    return data_generator_train, data_generator_valid\n",
    "\n",
    "def initialize_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopword_list,\n",
    "                    liwc_categories, session=None, transfer=False, classes=1):\n",
    "\n",
    "    logger.info(\"Initializing model...\\n\")\n",
    "    # Initialize model\n",
    "    if hyperparams['hierarchical']:\n",
    "        model = build_hierarchical_model(hyperparams, hyperparams_features, embedding_matrix, \n",
    "                                         emotions, stopword_list, liwc_categories,\n",
    "                       ignore_layer=hyperparams['ignore_layer'], classes=classes)\n",
    "    else:\n",
    "        model = build_model(hyperparams, hyperparams_features, embedding_matrix, \n",
    "                                        emotions, stopword_list, liwc_categories,\n",
    "                       ignore_layer=hyperparams['ignore_layer'], classes=classes)\n",
    "    if transfer:\n",
    "        model = build_tl_model(hyperparams, hyperparams_features, embedding_matrix, \n",
    "                                        emotions, stopword_list, liwc_categories,\n",
    "                       ignore_layer=hyperparams['ignore_layer'])\n",
    "        model.load_weights(hyperparams_features['pretrained_model_path'] + '_weights.h5', by_name=True)\n",
    "    # Needed just for bert\n",
    "    if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "        initialize_sess(session)                  \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(user_level_data, subjects_split, \n",
    "          hyperparams, hyperparams_features, \n",
    "          embedding_matrix, emotions, stopword_list, liwc_categories,\n",
    "          experiment, validation_set='valid',\n",
    "          version=0, epochs=50, start_epoch=0,\n",
    "         session=None, model=None, transfer_layer=False,\n",
    "         classes=1):\n",
    "    network_type, hierarch_type = get_network_type(hyperparams)\n",
    "    for feature in ['LIWC', 'emotions', 'numerical_dense_layer', 'sparse_feat_dense_layer', 'user_encoded']:\n",
    "        if feature in hyperparams['ignore_layer']:\n",
    "            network_type += \"no%s\" % feature\n",
    "    if not transfer_layer:\n",
    "        model_path='models/%s_%s_%s%d' % (network_type, dataset_type, hierarch_type, version)\n",
    "    else:\n",
    "        model_path='models/%s_%s_%s_transfer_%s%d' % (network_type, dataset_type, hierarch_type, transfer_type, version)\n",
    "        \n",
    "    \n",
    "    # Ablate emotions or LIWC\n",
    "    if 'emotions' in hyperparams['ignore_layer']:\n",
    "        emotions = []\n",
    "    if 'LIWC' in hyperparams['ignore_layer']:\n",
    "        liwc_categories = []\n",
    "\n",
    "    logger.info(\"Initializing datasets...\\n\")\n",
    "    data_generator_train, data_generator_valid = initialize_datasets(user_level_data, subjects_split, \n",
    "                                                                     hyperparams,hyperparams_features,\n",
    "                                                                     validation_set=validation_set,\n",
    "                                                                     emotions=emotions, liwc_categories=liwc_categories, \n",
    "                                                                    session=session, classes=classes)\n",
    "    if not model:\n",
    "        if transfer_layer:\n",
    "            logger.info(\"Initializing pretrained model...\\n\")\n",
    "        else:\n",
    "            logger.info(\"Initializing model...\\n\")\n",
    "        model = initialize_model(hyperparams, hyperparams_features, embedding_matrix, \n",
    "                                 emotions, stopword_list, liwc_categories, session=session, transfer=transfer_layer,\n",
    "                                classes=classes)\n",
    "\n",
    "       \n",
    "    print(model_path)\n",
    "    logger.info(\"Training model...\\n\")\n",
    "    model, history = train_model(model, hyperparams,\n",
    "                                 data_generator_train, data_generator_valid,\n",
    "                       epochs=epochs, start_epoch=start_epoch,\n",
    "                      class_weight={0:1, 1:hyperparams['positive_class_weight']} if classes==1 else None,\n",
    "                      callback_list = [\n",
    "                          'weights_history',\n",
    "                          'lr_history',\n",
    "                          'outputs_history_valid',\n",
    "                          'outputs_history_train',\n",
    "                          'reduce_lr_plateau',\n",
    "                          'lr_schedule'\n",
    "                                      ],\n",
    "                      model_path=model_path, workers=4,\n",
    "                                validation_set=validation_set)\n",
    "    logger.info(\"Saving model...\\n\")\n",
    "    try:\n",
    "        save_model_and_params(model, model_path, hyperparams, hyperparams_features)\n",
    "        experiment.log_parameter(\"model_path\", model_path)\n",
    "    except:\n",
    "        logger.error(\"Could not save model.\\n\")\n",
    "\n",
    "    return model, history\n",
    "# except Exception as e:# tf.errors.ResourceExhaustedError:\n",
    "#     print(e)\n",
    "#     sess.close()\n",
    "#     sess = tf.Session(config=sess_config)\n",
    "#     initialize_vars(sess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'lstm_seq'\n",
    "# # non-duplicate key\n",
    "# while key in models_collection.keys():\n",
    "#     key=key + \"1\"\n",
    "# # models_collection[key] = cur_model\n",
    "# # session.close()\n",
    "# session = initialize_sess()\n",
    "# # all_sessions.append(session)\n",
    "# session_collection[key] = session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_collection = {}\n",
    "hyperparams_collection = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(liwc_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df.groupby('source').subject.nunique()\n",
    "# Counter(writings_df.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    # Network parmeters\n",
    "    \n",
    "    # Sequential + hierarchical layers\n",
    "    'trainable_embeddings': True,\n",
    "\n",
    "    'lstm_units': 128,\n",
    "#     'lstm_units': 256,\n",
    "    \n",
    "    'dense_bow_units': 20,\n",
    "    'dense_sentence_units': 0,\n",
    "    'dense_numerical_units': 20,\n",
    "\n",
    "    \n",
    "    # CNN\n",
    "    'filters': 100,\n",
    "    'kernel_size': 5,\n",
    "    \n",
    "    # Just hierarchical layers\n",
    "    'lstm_units_user': 32,\n",
    "    'dense_user_units': 0,\n",
    "    \n",
    "    'filters_user': 10,\n",
    "    'kernel_size_user': 3,\n",
    "        \n",
    "    # BERT layers\n",
    "    'bert_dense_units': 256,\n",
    "    'bert_finetune_layers': 0,\n",
    "    'bert_trainable': False ,\n",
    "    'bert_pooling': 'first', # mean, first\n",
    "    \n",
    "    'transfer_units': 20,\n",
    "\n",
    "    # Regularization etc\n",
    "    'dropout': 0.1,\n",
    "#     'dropout': 0,\n",
    "    'l2_dense': 0.00011,\n",
    "    'l2_embeddings': 0.0000001,\n",
    "    'l2_bert': 0.0001,\n",
    "    'norm_momentum': 0.1,\n",
    "    \n",
    "    'ignore_layer': [\n",
    "#         'lstm', 'attention', \n",
    "#         'cnn',\n",
    "#         'user_encoded',\n",
    "#         'lstm_user', 'attention_user', \n",
    "#                      'batchnorm',\n",
    "#                      'user_encoded', # remove LSTM/CNN\n",
    "                     'bert_layer',\n",
    "#                      'numerical_dense_layer', \n",
    "#         'sparse_feat_dense_layer', # remove extracted features\n",
    "#         'LIWC',\n",
    "#         'emotions'\n",
    "                    ],\n",
    "\n",
    "    # Learning parameters\n",
    "    'optimizer': None, #'adam',\n",
    "    'decay': 0.001,\n",
    "    'lr': 0.00005,\n",
    "#     'lr': 0.00005,#     'lr': 0.01,\n",
    "#     'lr': 0.00001,#     'lr': 0.01,\n",
    "#     'lr': 0.0001,\n",
    "    \"reduce_lr_factor\": 0.5,\n",
    "    \"reduce_lr_patience\": 55,\n",
    "    'scheduled_reduce_lr_freq': 95,\n",
    "    'scheduled_reduce_lr_factor': 0.5,\n",
    "    \"freeze_patience\": 2000,\n",
    "    'threshold': 0.5,\n",
    "    'early_stopping_patience': 5,\n",
    "    'positive_class_weight': 1,\n",
    "    # set this to None for uniform class sampling (only applies to multiclass setting)\n",
    "    'class_weights': {0:1304, 1:1287, 2:763},\n",
    "#     'positive_class_weight': 2,\n",
    "    \n",
    "    # Generator parameters\n",
    "    \n",
    "    # Note: average text length in eRisk: 300\n",
    "    #       average text length in CLPsych: 13\n",
    "#     \"maxlen\": 512,\n",
    "#     \"maxlen\": 128,\n",
    "#     \"maxlen\": 512,\n",
    "    \"maxlen\": 256,\n",
    "    \"posts_per_user\": None, # if you want to limit total nr of posts considered per user\n",
    "    \"post_groups_per_user\": None, # if you want a fixed number of post groups per user\n",
    "                                  # to even out user weights\n",
    "    \"posts_per_group\": 50, # how long are the \"batches\" of posts. maxlen/avglen~=posts_per_group\n",
    "    \"batch_size\": 32,\n",
    "    \"padding\": \"pre\",\n",
    "    \"hierarchical\": True,\n",
    "    'sample_seqs': False,\n",
    "    'sampling_distr': 'exp',\n",
    "\n",
    "}\n",
    "if not hyperparams['optimizer']:\n",
    "    hyperparams['optimizer'] = optimizers.Adam(lr=hyperparams['lr'], #beta_1=0.9, beta_2=0.999, epsilon=0.0001,\n",
    "                                   decay=hyperparams['decay'])\n",
    "    \n",
    "if transfer_type:\n",
    "#     hyperparams, _ = load_params(hyperparams_features['pretrained_model_path'])\n",
    "    if 'optimizer' not in hyperparams:\n",
    "        hyperparams['optimizer'] = optimizers.Adam(lr=hyperparams['lr'], #beta_1=0.9, beta_2=0.999, epsilon=0.0001,\n",
    "                                       decay=hyperparams['decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_collection[key] = hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# session_collection['cnn_hierarch'].close()\n",
    "# K.clear_session()\n",
    "# transfer_type=None\n",
    "dataset_type\n",
    "# transfer_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams['optimizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# transfer_type=\"depression\"\n",
    "transfer_type=None\n",
    "classes=1\n",
    "# transfer_type=\"depression\"\n",
    "# with session_collection[key].as_default():\n",
    "#     with session_collection[key].graph.as_default():\n",
    "hyperparams_collection[key] = hyperparams\n",
    "\n",
    "\n",
    "if transfer_type:\n",
    "    # TODO: try to compile? with proper optimizer?\n",
    "    hyperparams_collection[key]['trainable_embeddings'] = True\n",
    "    hyperparams_collection[key]['transfer_units'] = 20\n",
    "    hyperparams_collection[key]['positive_class_weight'] = 1\n",
    "#     hyperparams_collection[key]['positive_class_weight'] = 5\n",
    "    hyperparams_collection[key]['lr'] = 0.001\n",
    "    hyperparams_collection[key]['optimizer'] = optimizers.Adam(lr=hyperparams_collection[key]['lr'], #beta_1=0.9, beta_2=0.999, epsilon=0.0001,\n",
    "                                   decay=hyperparams_collection[key]['decay'])\n",
    "    hyperparams_features['pretrained_model_path'] = 'models/lstmnonumerical_dense_layer_clpsych_hierarchical62'\n",
    "\n",
    "#     hyperparams_collection[key]['lr'] = 0.01\n",
    "\n",
    "#     models_collection[key] = load_saved_model(hyperparams_features['pretrained_model_path'], \n",
    "#                                               hyperparams_collection[key])\n",
    "    models_collection[key] = load_saved_model_weights(hyperparams_features['pretrained_model_path'], \n",
    "                                                      hyperparams_collection[key], \n",
    "                                                      emotions, stopword_list, liwc_categories, classes, \n",
    "                                                      h5=True)\n",
    "\n",
    "#     metrics_class = Metrics(threshold=hyperparams_collection[key]['threshold'])\n",
    "#     models_collection[key].compile(hyperparams_collection[key]['optimizer'], binary_crossentropy_custom,\n",
    "#                   metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m,\n",
    "#                           metrics_class.auc, \n",
    "#                            tf.keras.metrics.AUC(name=\"AUC_keras\"), \n",
    "#                            tf.keras.metrics.FalsePositives(), \n",
    "#                            tf.keras.metrics.FalseNegatives()])\n",
    "\n",
    "\n",
    "    experiment = initialize_experiment(hyperparams_collection[key], nrc_lexicon_path, emotions, pretrained_embeddings_path, \n",
    "                                  dataset_type, transfer_type)\n",
    "    models_collection[key], history = train(user_level_data, subjects_split, \n",
    "              hyperparams_collection[key], hyperparams_features, \n",
    "              embedding_matrix, emotions, stopword_list, liwc_categories,\n",
    "                  experiment,\n",
    "                  validation_set='valid',\n",
    "              version=63, epochs=90, start_epoch=80, transfer_layer=False,\n",
    "#                     session=session_collection[key],\n",
    "                model=models_collection[key]                        # Remove this to use transfer layer(?)\n",
    "                                           )\n",
    "else:\n",
    "    experiment = initialize_experiment(hyperparams, nrc_lexicon_path, emotions, pretrained_embeddings_path, \n",
    "                          dataset_type, transfer_type)\n",
    "    models_collection[key], history = train(user_level_data, subjects_split, \n",
    "              hyperparams_collection[key], hyperparams_features, \n",
    "              embedding_matrix, emotions, stopword_list, liwc_categories,\n",
    "                  experiment,\n",
    "                  validation_set='valid',\n",
    "              version=102, epochs=25, start_epoch=0, classes=classes,\n",
    "#                     session=session_collection[key],\n",
    "#                     model=models_collection[key]\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(emotions))\n",
    "print(len(liwc_categories))\n",
    "# models_collection[key].get_layer('LSTM_layer_user').weights\n",
    "models_collection[key].summary()\n",
    "# hyperparams_collection[key]['ignore_layer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model_path = \"models/lstm_%s_hierarchical53\" % dataset_type\n",
    "# models_collection[key] = initialize_model(hyperparams_collection[key], \n",
    "#                                           hyperparams_features, embedding_matrix, emotions, stopword_list,\n",
    "#                     liwc_categories)\n",
    "# # models_collection[key].load_weights(model_path + \"_weights\")\n",
    "\n",
    "\n",
    "# # embedding_matrix.shape\n",
    "# models_collection[key].load_weights(hyperparams_features['pretrained_model_path'] + \"_weights\")\n",
    "# metrics_class = Metrics(threshold=hyperparams_collection[key]['threshold'])\n",
    "# # models_collection[key].compile(hyperparams_collection[key]['optimizer'], binary_crossentropy_custom,\n",
    "# #               metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m,\n",
    "# #                       metrics_class.auc, \n",
    "# #                        tf.keras.metrics.AUC(name=\"AUC_keras\"), \n",
    "# #                        tf.keras.metrics.FalsePositives(), \n",
    "# #                        tf.keras.metrics.FalseNegatives()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_collection[key] = initialize_model(hyperparams_collection[key], \n",
    "#                                           hyperparams_features, embedding_matrix, emotions, stopword_list,\n",
    "#                     liwc_categories)\n",
    "# models_collection[key] = load_weights(hyperparams_features['pretrained_model_path'] + \"_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_collection[key] = depression_model\n",
    "# hyperparams_collection[key]\n",
    "# classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pickle\n",
    "\n",
    "# # Evaluate on entire posts history, final F1-score\n",
    "# print(\"Evaluating on same nr of groups as train (%d)...\" % hyperparams['post_groups_per_user'] if \n",
    "#       hyperparams['post_groups_per_user'] else 0)\n",
    "# cur_model.evaluate_generator(DataGenerator(user_level_data, subjects_split, \n",
    "#                                          set_type='test', \n",
    "#                                     seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
    "#                                        hierarchical=hyperparams['hierarchical'],\n",
    "#                                          max_posts_per_user=None,\n",
    "#                                        pad_with_duplication=False,\n",
    "#                                         posts_per_group=hyperparams['posts_per_group'],\n",
    "#                                         post_groups_per_user=hyperparams['post_groups_per_user'], \n",
    "#                                          sample_seqs=False, shuffle=False), verbose=1)\n",
    "# print(\"Evaluating on entire posts history...\")\n",
    "# cur_model.evaluate_generator(DataGenerator(user_level_data, subjects_split, \n",
    "#                                          set_type='test', \n",
    "#                                     seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
    "#                                        hierarchical=hyperparams['hierarchical'],\n",
    "#                                          max_posts_per_user=None,\n",
    "#                                        pad_with_duplication=False,\n",
    "#                                         posts_per_group=hyperparams['posts_per_group'],\n",
    "#                                         post_groups_per_user=hyperparams['post_groups_per_user'], \n",
    "#                                          sample_seqs=False, shuffle=False), verbose=1)\n",
    "# hyperparams_collection[key] = hyperparams\n",
    "\n",
    "\n",
    "liwc_words_for_categories = pickle.load(open(hyperparams_features['liwc_words_cached'], 'rb'))\n",
    "if 'emotions' in hyperparams_collection[key]['ignore_layer']:\n",
    "    emotions = []\n",
    "if 'LIWC' in hyperparams_collection[key]['ignore_layer']:\n",
    "    liwc_categories = []\n",
    "# # \n",
    "# with session_collection[key].as_default():\n",
    "#     with session_collection[key].graph.as_default():\n",
    "print(\"Evaluating only on last group (1)...\")\n",
    "if classes==1:\n",
    "    metrics_class = Metrics(threshold=0.5)\n",
    "    models_collection[key].compile(hyperparams_collection[key]['optimizer'], binary_crossentropy_custom,\n",
    "                      metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m,\n",
    "                              metrics_class.auc, \n",
    "                               tf.keras.metrics.AUC(name=\"AUC_keras\"), \n",
    "                               tf.keras.metrics.FalsePositives(), \n",
    "                               tf.keras.metrics.FalseNegatives()])\n",
    "else:\n",
    "    models_collection[key].compile(hyperparams_collection[key]['optimizer'], K.categorical_crossentropy,\n",
    "                     metrics=[tf.keras.metrics.CategoricalAccuracy(name='cat_acc'),\n",
    "                             tf.keras.metrics.Precision(name='prec'), tf.keras.metrics.Recall(name='rec'), \n",
    "                              ])\n",
    "\n",
    "test_generator = DataGenerator(user_level_data, subjects_split, \n",
    "                                         set_type='test', \n",
    "                                    seq_len=hyperparams_collection[key]['maxlen'],   \n",
    "#                                         batch_size=len(subjects_split['test']), # on all data at once\n",
    "                                        batch_size=hyperparams_collection[key]['batch_size'], # on all data at once\n",
    "                                       hierarchical=hyperparams_collection[key]['hierarchical'],\n",
    "                                         max_posts_per_user=None,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams_collection[key]['posts_per_group'],\n",
    "                                        post_groups_per_user=1,  compute_liwc=True,\n",
    "                                        liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                        emotions=emotions, liwc_categories=liwc_categories, keep_last_batch=False,\n",
    "                                         sample_seqs=False, shuffle=False, classes=classes,\n",
    "                              )\n",
    "print(\"Evaluating...\")\n",
    "results1 = models_collection[key].evaluate_generator(test_generator, verbose=1)\n",
    "\n",
    "if classes>1:\n",
    "    test_labels_sparse = list(test_generator.generated_labels) # need to get these after iterating\n",
    "    print(len(test_labels_sparse), len(test_generator.generated_labels))\n",
    "    print(\"Predicting\")\n",
    "    Y_pred = models_collection[key].predict_generator(test_generator)#, num_of_test_samples // batch_size+1)\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "    \n",
    "    test_labels_sparse = list(test_generator.generated_labels) # need to get these after iterating\n",
    "    print(len(test_labels_sparse), len(test_generator.generated_labels))\n",
    "    print('Confusion Matrix')\n",
    "    # TODO: check\n",
    "    # keeping as many batches as we have for predictions, hopefully they match... I got one more\n",
    "    # TODO: no, can't assume it, have to fix it\n",
    "\n",
    "    test_labels = np.argmax(test_generator.generated_labels, axis=1) \n",
    "    print(test_labels.shape, y_pred.shape)\n",
    "    print(confusion_matrix(test_labels, y_pred))\n",
    "    print('Classification Report')\n",
    "    target_names = ['selfharm', 'anorexia', 'depression']\n",
    "    print(classification_report(test_labels, y_pred, target_names=target_names))\n",
    "    # experiment.log_metric(\"test_f1\", results1[1])\n",
    "# experiment.log_metric(\"test_prec\", results1[2])\n",
    "# experiment.log_metric(\"test_recall\", results1[3])\n",
    "# experiment.log_metric(\"test_auc\", results1[4])\n",
    "\n",
    "\n",
    "#     Optimal thresh\n",
    "if classes==1:\n",
    "    print(\"Finding optimal threshold...\")\n",
    "    f1s = {}\n",
    "    eval_generator = DataGenerator(user_level_data, subjects_split, \n",
    "                                             set_type='valid', compute_liwc=True,\n",
    "                                            liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                            emotions=emotions, liwc_categories=liwc_categories,\n",
    "                                        seq_len=hyperparams_collection[key]['maxlen'], \n",
    "                                            batch_size=hyperparams_collection[key]['batch_size'],\n",
    "                                           hierarchical=hyperparams_collection[key]['hierarchical'],\n",
    "                                             max_posts_per_user=None,\n",
    "                                           pad_with_duplication=False,\n",
    "                                            posts_per_group=hyperparams_collection[key]['posts_per_group'],\n",
    "                                            post_groups_per_user=1, \n",
    "                                                     sample_seqs=False, shuffle=False)\n",
    "    for thresh in range(0, 10, 1):\n",
    "        print(thresh)\n",
    "        # model1.load_weights(model_path + \"_weights\", by_name=True)\n",
    "        metrics_class = Metrics(threshold=thresh/10)\n",
    "        models_collection[key].compile(hyperparams_collection[key]['optimizer'], binary_crossentropy_custom,\n",
    "                          metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m,\n",
    "                                metrics_class.auc, \n",
    "                               tf.keras.metrics.AUC(name=\"AUC_keras\"), \n",
    "                               tf.keras.metrics.FalsePositives(), \n",
    "                               tf.keras.metrics.FalseNegatives()])\n",
    "    \n",
    "        results = models_collection[key].evaluate_generator(eval_generator, verbose=1)\n",
    "        f1s[thresh] = results[1]\n",
    "    best_f1 = 0\n",
    "    best_thresh = 0.5\n",
    "    for t, f1 in f1s.items():\n",
    "        if f1 >= best_f1:\n",
    "            best_thresh = t\n",
    "            best_f1 = f1\n",
    "    print(f1s, best_thresh)\n",
    "    hyperparams_features['best_thresh'] = best_thresh/10\n",
    "    # experiment.log_metric(\"best_thresh\", best_thresh/10)\n",
    "\n",
    "    print(\"Computing with best threshold...\")\n",
    "    metrics_class = Metrics(best_thresh/10)\n",
    "    models_collection[key].compile(hyperparams_collection[key]['optimizer'], binary_crossentropy_custom,\n",
    "                          metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m,\n",
    "                                metrics_class.auc, \n",
    "                               tf.keras.metrics.AUC(name=\"AUC_keras\"), \n",
    "                               tf.keras.metrics.FalsePositives(), \n",
    "                               tf.keras.metrics.FalseNegatives()])\n",
    "\n",
    "    results = models_collection[key].evaluate_generator(DataGenerator(user_level_data, subjects_split, \n",
    "                                             set_type='test', compute_liwc=True,\n",
    "                                            liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                            emotions = emotions, liwc_categories=liwc_categories,\n",
    "\n",
    "                                        seq_len=hyperparams_collection[key]['maxlen'], \n",
    "                                           batch_size=hyperparams_collection[key]['batch_size'],\n",
    "                                           hierarchical=hyperparams_collection[key]['hierarchical'],\n",
    "                                             max_posts_per_user=None,\n",
    "                                           pad_with_duplication=False,\n",
    "                                            posts_per_group=hyperparams_collection[key]['posts_per_group'],\n",
    "                                            post_groups_per_user=1, \n",
    "                                                     sample_seqs=False, shuffle=False), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.argmax(test_generator.generated_labels, axis=1)\n",
    "test_labels_sparse = list(test_generator.generated_labels)\n",
    "len(test_generator.generated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_level_data2, subjects_split2, vocabulary = load_erisk_data(writings_df[writings_df['source']=='depression'], \n",
    "                                                            voc_size=hyperparams_features['max_features'],\n",
    "                                                           emotion_lexicon=nrc_lexicon,\n",
    "                                                           emotions=emotions,\n",
    "                                                           user_level=hyperparams_features['user_level'],\n",
    "                                                                                logger=logger,\n",
    "#                                                            vocabulary=pickle.load(open('vocabulary_40K_all.pkl', 'rb')),\n",
    "#                                                            vocabulary=pickle.load(open('vocab_clpsych_10000.pkl', 'rb')),\n",
    "                                                              vocabulary=vocabulary_dict,\n",
    "                                                              by_subset=False\n",
    "                                                                               )\n",
    "results2 = models_collection[key].evaluate_generator(DataGenerator(user_level_data2, subjects_split2, \n",
    "                                         set_type='test', \n",
    "                                    seq_len=hyperparams_collection[key]['maxlen'],   \n",
    "#                                         batch_size=len(subjects_split['test']), # on all data at once\n",
    "                                        batch_size=hyperparams_collection[key]['batch_size'], # on all data at once\n",
    "                                       hierarchical=hyperparams_collection[key]['hierarchical'],\n",
    "                                         max_posts_per_user=None,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams_collection[key]['posts_per_group'],\n",
    "                                        post_groups_per_user=1,  compute_liwc=True,\n",
    "                                        liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                         sample_seqs=False, shuffle=False), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 1500\n",
    "\n",
    "# Evaluate on partial post history, simulating stream\n",
    "print(\"Evaluating on partial posts history...\")\n",
    "cnt=0\n",
    "cnt+=1\n",
    "f1_per_iteration = []\n",
    "auc_per_iteration = []\n",
    "precision_per_iteration = []\n",
    "recall_per_iteration = []\n",
    "predictions_per_iteration = []\n",
    "for iteration in range(0, iterations, hyperparams_collection[key]['posts_per_group']):\n",
    "    print(\"Iteration\", iteration)\n",
    "    results = models_collection[key].evaluate_generator(DataGenerator(user_level_data, subjects_split, \n",
    "                                         set_type='test', \n",
    "                                       hierarchical=hyperparams_collection[key]['hierarchical'],\n",
    "                                        seq_len=hyperparams['maxlen'], \n",
    "#                                         batch_size=len(subjects_split['test'])//2, # on all data at once\n",
    "                                        batch_size=hyperparams_collection[key]['batch_size'], # on all data at once\n",
    "                                         max_posts_per_user=iteration,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams_collection[key]['posts_per_group'],\n",
    "                                        post_groups_per_user=1, \n",
    "                                        post_offset=iteration,\n",
    "                                        compute_liwc=True,\n",
    "                                        liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                        emotions=emotions, liwc_categories=liwc_categories,\n",
    "                                         sample_seqs=False, shuffle=False), verbose=1)\n",
    "    f1_per_iteration.append(results[1])\n",
    "    precision_per_iteration.append(results[2])\n",
    "    recall_per_iteration.append(results[3])\n",
    "    auc_per_iteration.append(results[5])\n",
    "    predictions = models_collection[key].predict(DataGenerator(user_level_data, subjects_split, \n",
    "                                         set_type='test', \n",
    "                                       hierarchical=hyperparams_collection[key]['hierarchical'],\n",
    "                                        seq_len=hyperparams['maxlen'], \n",
    "#                                         batch_size=len(subjects_split['test'])//2, # on all data at once\n",
    "                                        batch_size=hyperparams_collection[key]['batch_size'], # on all data at once\n",
    "                                         max_posts_per_user=iteration,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams_collection[key]['posts_per_group'],\n",
    "                                        post_groups_per_user=1, \n",
    "                                        post_offset=iteration,\n",
    "                                        compute_liwc=True,\n",
    "                                        liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                        emotions=emotions, liwc_categories=liwc_categories,\n",
    "                                         sample_seqs=False, shuffle=False), verbose=1)\n",
    "    predictions_per_iteration.extend(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_per_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"average precision across %d iterations: %f;\n",
    "average recall across %d iterations: %f;\n",
    "average f1 across %d iterations: %f;\n",
    "average auc across %d iterations: %f\"\"\" % (\n",
    "    iterations, np.mean(precision_per_iteration[:iterations]),\n",
    "    iterations, np.mean(recall_per_iteration[:iterations]),\n",
    "    iterations, np.mean(f1_per_iteration[:iterations]),\n",
    "    iterations, np.mean(auc_per_iteration[:iterations])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(range(0, len(auc_per_iteration)*hyperparams_collection[key]['posts_per_group'], hyperparams_collection[key]['posts_per_group']), \n",
    "#          auc_per_iteration)\n",
    "pd.Series(auc_per_iteration, \n",
    "          index=range(0, len(auc_per_iteration)*hyperparams_collection[key]['posts_per_group'], hyperparams_collection[key]['posts_per_group'])\n",
    "                                                                    ).rolling(20).mean().plot()\n",
    "plt.xlabel(\"Posts offset\")\n",
    "plt.ylabel(\"AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([p[0] for p in predictions_per_iteration]).hist(bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(range(0, iteration*hyperparams_collection[key]['posts_per_group'], \n",
    "#                hyperparams_collection[key]['posts_per_group']), \n",
    "#          predictions_per_iteration)\n",
    "pd.Series([p[0] for p in predictions_per_iteration]).rolling(5000).std().plot()\n",
    "plt.xlabel(\"Posts offset\")\n",
    "plt.ylabel(\"Predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_user = [v for v in models_collection[key].get_layer('attention_user').get_weights()[0].flatten()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = models_collection[key].get_layer('user_encoder').get_weights()[-2]\n",
    "assert(attention_layer.shape == (128, 1))\n",
    "pd.Series([v for v in attention_layer]).rolling(30).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(auc_per_iteration, open(\"auc_per_iteration_selfharm53.pkl\", \"wb+\"))\n",
    "# pickle.dump(f1_per_iteration, open(\"f1_per_iteration_selfharm53.pkl\", \"wb+\"))\n",
    "# pickle.dump(predictions_per_iteration, open(\"predictions_per_iteration_selfharm53.pkl\", \"wb+\"))\n",
    "# pickle.dump(attention_user, open(\"attention_user_selfharm53.pkl\", \"wb+\"))\n",
    "# pickle.dump(attention_layer, open(\"attention_post_selfharm53.pkl\", \"wb+\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_saved_model(\"models/lstm_depression_hierarchical52\", hyperparams_collection[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0, len(scores_per_iteration[:15])*hyperparams_collection[key]['posts_per_group'], hyperparams_collection[key]['posts_per_group']), \n",
    "         scores_per_iteration[:15])\n",
    "plt.xlabel(\"Posts offset\")\n",
    "plt.ylabel(\"AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_user = [v for v in models_collection[key].get_layer('attention_user').get_activations()[0].flatten()]\n",
    "l = models_collection[key].get_layer('attention_user')\n",
    "# l.activation(samplegenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_per_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in nrc_lexicon:\n",
    "    print(e)\n",
    "    print(nrc_lexicon[e].intersection(set(tokenize(text.lower()))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model_and_params(cur_model, 'models/lstm_selfharm_seq16_2', hyperparams, hyperparams_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_collection = {}\n",
    "# hyperparams_collection = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([v for v in models_collection[key].get_layer('attention_user').get_weights()[0].flatten()]).rolling(10\n",
    "                                                                                                             ).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([v for v in models_collection[key].get_layer('attention_user').get_weights()[0].flatten()]).abs().rolling(10\n",
    "                                                                                                             ).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series([v for v in models_collection[key].get_layer('attention_user').get_weights()[0].flatten()]).to_csv(\n",
    "#     'attention_user_weights_lstm_selfharm_erisk_all_1group.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([v for v in models_collection[key].get_layer('attention').get_weights()[0].flatten()]).rolling(5).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([abs(v) for v in models_collection[key].get_layer('output_layer').get_weights()[0].flatten()]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1, d2 = initialize_datasets(user_level_data, subjects_split, hyperparams, hyperparams_features, \n",
    "                        validation_set='valid', session=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df.source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention and predictions analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nr = 56\n",
    "model_name = \"models/lstm_%s_hierarchical53\" % dataset_type\n",
    "hyperparams, _ = load_params(model_name)\n",
    "model = load_saved_model(model_name, hyperparams)\n",
    "auc_per_iteration = pickle.load(open(\"auc_per_iteration_depression53.pkl\", \"rb\"))\n",
    "f1_per_iteration = pickle.load(open(\"f1_per_iteration_depression53.pkl\", \"rb\"))\n",
    "predictions_per_iteration = pickle.load(open(\"predictions_per_iteration_depression53.pkl\", \"rb\"))\n",
    "attention_user = pickle.load(open(\"attention_user_depression53.pkl\", \"rb\"))\n",
    "attention_layer = pickle.load(open(\"attention_post_depression53.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(attention_user).rolling(10).mean().plot()\n",
    "plt.ylabel(\"Attention weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([abs(l[0]) for l in attention_layer]).rolling(50).mean().plot()\n",
    "plt.ylabel(\"Attention weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = range(0, min(len(auc_per_iteration)*hyperparams['posts_per_group'], 2000), \n",
    "                      hyperparams['posts_per_group'])\n",
    "pd.Series(auc_per_iteration, \n",
    "          index=index\n",
    "                                                                    ).rolling(5).mean().plot()\n",
    "plt.xlabel(\"Posts offset\")\n",
    "plt.ylabel(\"AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[l.shape for l in model.get_layer('user_encoder').get_weights()]\n",
    "# hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.get_layer('attention_user').get_weights()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(l.name, len(l.get_weights()), len(l.get_weights()[0]) if len(l.get_weights()) else 0) for l in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(len(l), len(l[0]) if len(l) and type(l[0])==list else 0) for \n",
    " l in model.get_layer('user_encoder').get_weights()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userl = model.get_layer('user_encoder').layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userl.get_layer('lstm_user').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_weights = userl.get_layer(index=3).get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_weights[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nr=101\n",
    "# dataset_type = \"anorexia\"\n",
    "\n",
    "hyperparams = json.load(open(\"models/lstm_%s_hierarchical%d.hp.json\" % (dataset_type, model_nr)))\n",
    "\n",
    "if 'optimizer' not in hyperparams:\n",
    "    hyperparams['optimizer'] = optimizers.Adam(lr=hyperparams['lr'], #beta_1=0.9, beta_2=0.999, epsilon=0.0001,\n",
    "                                   decay=hyperparams['decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model = build_hierarchical_model(hyperparams, hyperparams_features, embedding_matrix, \n",
    "                                         emotions, stopword_list, liwc_categories,\n",
    "                       ignore_layer=hyperparams['ignore_layer'], activations=False)\n",
    "original_model.load_weights(\"models/lstm_%s_hierarchical%d\" % (dataset_type, model_nr) + '_weights.h5', by_name=True)\n",
    "\n",
    "activation_model_user = build_hierarchical_model(hyperparams, hyperparams_features, embedding_matrix, \n",
    "                                         emotions, stopword_list, liwc_categories,\n",
    "                       ignore_layer=hyperparams['ignore_layer'], activations='attention_user')\n",
    "# activation_model_user.load_weights(\"models/lstm_%s_hierarchical%d\"  % (dataset_type, model_nr) + '_weights.h5', by_name=True)\n",
    "\n",
    "user_representation_model = build_hierarchical_model(hyperparams, hyperparams_features, embedding_matrix, \n",
    "                                         emotions, stopword_list, liwc_categories,\n",
    "                       ignore_layer=hyperparams['ignore_layer'], activations='output_layer')\n",
    "user_representation_model.load_weights(\"models/lstm_%s_hierarchical%d\"  % (dataset_type, model_nr) + '_weights.h5', by_name=True)\n",
    "\n",
    "\n",
    "activation_model = build_hierarchical_model(hyperparams, hyperparams_features, embedding_matrix, \n",
    "                                         emotions, stopword_list, liwc_categories,\n",
    "                       ignore_layer=hyperparams['ignore_layer'], activations='attention')\n",
    "# activation_model.load_weights(\"models/lstm_%s_hierarchical%d\" % (dataset_type, model_nr) + '_weights.h5', by_name=True)\n",
    "\n",
    "\n",
    "\n",
    "# activation_outputs = [layer.output for layer in original_model.layers]\n",
    "# activation_model = tf.keras.Model(inputs, activation_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_representation_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_subjects = list(set(writings_df[writings_df['label']==1].subject))\n",
    "negative_subjects = list(set(writings_df[writings_df['label']==0].subject)) #[:len(positive_subjects)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(positive_subjects), len(negative_subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subji = 15\n",
    "subjects_split_test_positive = {'test':\n",
    "                                 [s for s in subjects_split['test'] if s in positive_subjects][:]}\n",
    "subjects_split_test_negative = {'test':\n",
    "                                 [s for s in subjects_split['test'] if s in negative_subjects][:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplegenerator_positive = DataGenerator(user_level_data, subjects_split_test_positive, \n",
    "                                     set_type='test', \n",
    "                                   hierarchical=True,\n",
    "                                seq_len=hyperparams['maxlen'], #batch_size=hyperparams['batch_size'],\n",
    "                                batch_size=1,\n",
    "                                max_posts_per_user=None,\n",
    "                                   pad_with_duplication=False,\n",
    "                                    posts_per_group=hyperparams['posts_per_group'],\n",
    "#                                     post_groups_per_user=None, \n",
    "                                    post_groups_per_user=1, \n",
    "                                     sample_seqs=False, shuffle=False)\n",
    "samplegenerator_negative = DataGenerator(user_level_data, subjects_split_test_negative, \n",
    "                                     set_type='test', \n",
    "                                   hierarchical=True,\n",
    "                                seq_len=hyperparams['maxlen'], #batch_size=hyperparams['batch_size'],\n",
    "                                batch_size=1,\n",
    "                                max_posts_per_user=None,\n",
    "                                   pad_with_duplication=False,\n",
    "                                    posts_per_group=hyperparams['posts_per_group'],\n",
    "#                                     post_groups_per_user=None, \n",
    "                                    post_groups_per_user=1, \n",
    "                                     sample_seqs=False, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(samplegenerator_positive), len(samplegenerator_negative))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2 encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df_t2=pickle.load(open('data/writings_df_T2_liwc.pkl', 'rb'))\n",
    "# writings_df_t2=pickle.load(open('data/writings_df_t2_test_liwc.pkl', 'rb'))\n",
    "user_level_data_t2, subjects_split_t2, vocabulary = load_erisk_data(writings_df_t2, \n",
    "                                                            voc_size=hyperparams_features['max_features'],\n",
    "                                                           emotion_lexicon=nrc_lexicon,\n",
    "                                                           emotions=emotions,\n",
    "                                                           user_level=hyperparams_features['user_level'],\n",
    "                                                                                logger=logger,\n",
    "#                                                            vocabulary=pickle.load(open('vocabulary_40K_all.pkl', 'rb')),\n",
    "#                                                            vocabulary=pickle.load(open('vocab_clpsych_10000.pkl', 'rb')),\n",
    "                                                              vocabulary=vocabulary_dict,\n",
    "                                                              by_subset=True,\n",
    "                                                              labelcol = 'anx',\n",
    "#                                                               label_index={'depression':1, \"ptsd\":0}\n",
    "                                                                               )\n",
    "subjects_split_t2['test'] += subjects_split_t2['train']+subjects_split_t2['valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeddings_per_subject = {}\n",
    "for subject in subjects_split_t2['test']:\n",
    "    samplegenerator_t2 = DataGenerator(user_level_data_t2, {'test':[subject]}, \n",
    "                                     set_type='test', \n",
    "                                   hierarchical=True,\n",
    "                                seq_len=hyperparams['maxlen'], #batch_size=hyperparams['batch_size'],\n",
    "                                batch_size=1,\n",
    "                                max_posts_per_user=None,\n",
    "                                   pad_with_duplication=False,\n",
    "                                    posts_per_group=hyperparams['posts_per_group'],\n",
    "#                                     post_groups_per_user=None, \n",
    "                                    post_groups_per_user=None, \n",
    "                                     sample_seqs=False, shuffle=False)\n",
    "    print(subject, len(samplegenerator_t2))\n",
    "    user_embeddings_per_subject[subject] = user_representation_model.predict(samplegenerator_t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(user_embeddings_per_subject, open('user_embeddings_t2_2020_model101.pkl', 'wb+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_representation_model.predict(samplegenerator_t2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_words_positive = activation_model.predict(samplegenerator_positive)\n",
    "attention_words_negative = activation_model.predict(samplegenerator_negative)\n",
    "predictions_positive = original_model.predict(samplegenerator_positive)\n",
    "predictions_negative = original_model.predict(samplegenerator_negative)\n",
    "attention_positive_user = activation_model_user.predict(samplegenerator_positive)\n",
    "attention_negative_user = activation_model_user.predict(samplegenerator_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_negative.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(samplegenerator_positive), len(samplegenerator_negative), \n",
    "      predictions_positive.shape, predictions_negative.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df[writings_df['subject']==subjects_split_test_positive['test'][0]][['text']].head()\n",
    "# writings_df[writings_df['subject']==subjects_split_test_negative['test'][0]][['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions_negative.shape, attention_words_negative.shape, attention_negative_user.shape)\n",
    "print(predictions_negative.sum(), attention_words_negative.sum(axis=2), attention_negative_user.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_positive_user.shape\n",
    "# len(writings_df[writings_df['subject']==subjects_split_test_positive['test'][2]])\n",
    "len(predictions_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_scores(writings_df, subjects, generator, predictions, attention_user, attention_words, \n",
    "                      words_scores=None, nr_words=None, users=None):\n",
    "    unk = \" \"\n",
    "    if not words_scores:\n",
    "        words_scores = {t:0 for t in vocabulary_list}\n",
    "        words_scores_all = {t:[] for t in vocabulary_list}\n",
    "        nr_words = {t:0 for t in vocabulary_list}\n",
    "        words_scores[unk] = 0\n",
    "        words_scores_all[unk] = []\n",
    "        nr_words[unk] = 0\n",
    "    highlighted_texts = []\n",
    "    # NOTE: I am considering only first post group for each subject?\n",
    "    for i in range(len(predictions)):\n",
    "        if users and i not in users:\n",
    "            continue\n",
    "#         if predictions[i]<0.8:\n",
    "#             continue\n",
    "#         if i%57!=0:\n",
    "#             continue\n",
    "    #     if predictions_positive_user[i].mean() < 0.02:\n",
    "    #         continue\n",
    "#         sampletexts = writings_df[writings_df['subject']==subjects[i]][['tokenized_text']].values\n",
    "        post_lengths = []\n",
    "        for p in range(50):\n",
    "#             if  abs(attention_user[i][p]) < 0.05:\n",
    "#                 continue\n",
    "            print(i, p)\n",
    "            post_tokens = generator[i][0][0][0][p]\n",
    "            post_len = sum(post_tokens!=0)\n",
    "            post_lengths.append(post_len)\n",
    "            nr_posts = 50\n",
    "            if post_len == 0:\n",
    "                nr_posts -= 1\n",
    "                continue\n",
    "#             print(\"post len\", post_len, list(enumerate([vocabulary_list[j] if (j!=0 and j<len(vocabulary_list)) else \" \"\n",
    "#                           for j in post_tokens ])))\n",
    "            post_with_weights = [(k, tok, round(attention_words[i][p][k], 10)) \n",
    "                                 for (k, tok) in list(\n",
    "                enumerate([vocabulary_list[j] if (j!=0 and j<len(vocabulary_list)) else unk\n",
    "                          for j in post_tokens ]))]\n",
    "            highlighted_text = [(\">>>\", attention_user[i][p])]\n",
    "            for (pos, tok, att) in post_with_weights:\n",
    "                score = att * attention_user[i][p] * post_len\n",
    "                words_scores[tok] += score\n",
    "                words_scores_all[tok].append(score)\n",
    "                highlighted_text.append((tok,score))\n",
    "                nr_words[tok] += 1\n",
    "#             print(post_with_weights)\n",
    "            highlighted_texts.append(highlighted_text)\n",
    "            \n",
    "            print(\"post len\", post_len, \"nr posts\", nr_posts)\n",
    "        #     print([list(enumerate([t for t in sampletexts[-3:]])))\n",
    "            print(\"user attention\", attention_user[i][p], \"avg word attention\", attention_words[i][p].mean())\n",
    "            print(\"prediction\", predictions[i])\n",
    "            \n",
    "            pd.Series(attention_words[i][p]).plot(label=\"%d_%d\" %(i, p))\n",
    "#             pd.Series(attention_positive_user[i]).plot()\n",
    "#         plt.scatter(np.exp(attention_user[i]), post_lengths)\n",
    "#         plt.scatter(attention_words[i].mean(axis=1), post_lengths)\n",
    "    words_scores = {w: s/nr_words[tok] for (w,s) in words_scores.items()}\n",
    "    plt.legend()\n",
    "    return words_scores, words_scores_all, highlighted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "words_scores_positive, words_scores_positive_all, highlighted_texts = build_word_scores(writings_df, subjects_split_test_positive['test'], samplegenerator_positive,\n",
    "                                 predictions_positive, attention_positive_user, attention_words_positive,\n",
    "                                                            users=[2,3,5,7,88,99,44,66])\n",
    "\n",
    "words_scores_negative, words_scores_negative_all, highlighted_texts_negative = build_word_scores(writings_df, subjects_split_test_negative['test'], samplegenerator_negative,\n",
    "                                 predictions_negative, attention_negative_user, attention_words_negative,\n",
    "                                                                     users=[2,4,7])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_highlighted_html(texts):\n",
    "    htmltext = \"\"\"\"<html> \"\"\"\n",
    "    for text in texts:\n",
    "        for i, (tok, score) in enumerate(text):\n",
    "            if i==0:\n",
    "                htmltext += '<span style=\"background-color: rgba(200,0,0,%f)\"> %s </span>' % (20*abs(score), tok)\n",
    "            else:\n",
    "                htmltext += '<span style=\"background-color: rgba(0,0,200,%f)\"> %s </span>' % (10*abs(score), tok)\n",
    "        \n",
    "#         htmltext += \"</span>  <br> \\n\"\n",
    "        htmltext += \" <br> \\n\"\n",
    "    htmltext += \"\"\"</html>\"\"\"\n",
    "    return htmltext\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%html\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(generate_highlighted_html(highlighted_texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(words_scores_negative.items(), key=lambda i: -abs(i[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'i'\n",
    "print(pd.Series(words_scores_positive_all[word]).describe(), pd.Series(words_scores_negative_all[word]).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(words_scores_negative_all[word]).hist(bins=20, label=\"not self-harming\", alpha=0.7)\n",
    "pd.Series(words_scores_positive_all[word]).hist(bins=20, label=\"self-harming\", alpha=0.7)\n",
    "plt.xlabel('attention weights')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.concatenate([attention_positive_user,attention_negative_user]).mean(axis=0)).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.concatenate([attention_positive_user]).mean(axis=0)).plot(label=\"depressed\")\n",
    "pd.Series(np.concatenate([attention_negative_user]).mean(axis=0)).plot(label=\"not depressed\")\n",
    "plt.ylabel(\"attention weights for user encoder\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(attention_words_positive.mean(axis=(1,0))).plot(label=\"depressed\")\n",
    "pd.Series(attention_words_negative.mean(axis=(1,0))).plot(label=\"not depressed\")\n",
    "plt.ylabel(\"attention weights for post encoder\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in vocabulary_list:\n",
    "    if not words_scores_negative[word]:\n",
    "#         if words_scores_positive[word]:\n",
    "#             print(word)\n",
    "        continue\n",
    "    if not words_scores_positive[word]:\n",
    "        continue\n",
    "    if pd.Series(words_scores_positive_all[word]).mean()/pd.Series(words_scores_negative_all[word]).mean() > 5:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output layer analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_representations = user_representation_model.predict(samplegenerator_positive)\n",
    "negative_representations = user_representation_model.predict(samplegenerator_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(abs(positive_representations.mean(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "# pca.fit_transform(positive_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(positive_representations.shape, negative_representations.shape,\n",
    "      np.concatenate([positive_representations, negative_representations[:len(positive_representations)]]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_examples = np.concatenate([positive_representations, \n",
    "                                    negative_representations[:len(positive_representations)]])\n",
    "# making them balanced\n",
    "pca.fit_transform(training_examples)\n",
    "positive_reduced = pca.transform(positive_representations)\n",
    "negative_reduced = pca.transform(negative_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'axes.labelsize': 16,\n",
    "          'axes.titlesize': 16,\n",
    "          'legend.fontsize': 16,\n",
    "         'xtick.labelsize': 16,\n",
    "         'ytick.labelsize': 16}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter([x for x,y in negative_reduced], [y for x,y in negative_reduced], c='g', alpha=0.5, s=5, label='negative')\n",
    "\n",
    "plt.scatter([x for x,y in positive_reduced], [y for x,y in positive_reduced], c='r', alpha=0.5, s=5, label='positive')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_kde(positives, negatives):\n",
    "    graph = sns.jointplot([x for x,y in positives], [y for x,y in positives],\n",
    "                     cmap=\"Reds\", kind=\"kde\", marginal_kws={\"color\":\"r\", \"alpha\":.2}, shade=True, \n",
    "                          shade_lowest=False, alpha=.5)\n",
    "    graph.x = [x for x,y in negatives]\n",
    "    graph.y = [y for x,y in negatives]\n",
    "    graph.plot_joint(sns.kdeplot, cmap=\"Blues\", shade=True, shade_lowest=False, alpha=.5)\n",
    "    graph.plot_marginals(sns.kdeplot, color='b', shade=True, alpha=.2, legend=False)\n",
    "    ax = plt.gca()\n",
    "\n",
    "\n",
    "    ax.legend([\"positive\", \"negative\"])\n",
    "\n",
    "plot_kde(positive_reduced, negative_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html> <mark> text </mark> more\n",
    "\n",
    "<span style=\"background-color: rgba(0,0,200,0.2)\">This text is highlighted in yellow.</span>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting by LIWC features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_features_positive = []\n",
    "liwc_features_negative = []\n",
    "for i, subject in enumerate(subjects_split_test_positive['test']):\n",
    "    liwc_features_positive.extend(user_level_data[subject]['liwc'])\n",
    "for i, subject in enumerate(subjects_split_test_negative['test']):\n",
    "    liwc_features_negative.extend(user_level_data[subject]['liwc'])\n",
    "\n",
    "pcaliwc = PCA(n_components=2)\n",
    "pcaliwc.fit_transform(liwc_features_positive+liwc_features_negative)\n",
    "liwc_reduced_positive = pcaliwc.transform(liwc_features_positive)\n",
    "liwc_reduced_negative = pcaliwc.transform(liwc_features_negative)\n",
    "\n",
    "plot_kde(liwc_reduced_positive, liwc_reduced_negative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(training_examples)\n",
    "\n",
    "kmeans.predict(np.array(negative_representations[:10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(positive_representations)\n",
    "positive_clusters = kmeans.predict(np.array(positive_representations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=1\n",
    "hyperparams_loaded = json.load(open('%s.hp.json' % hyperparams_features['pretrained_model_path']))\n",
    "if 'optimizer' not in hyperparams_loaded:\n",
    "    hyperparams_loaded['optimizer'] = optimizers.Adam(lr=hyperparams_loaded['lr'], #beta_1=0.9, beta_2=0.999, epsilon=0.0001,\n",
    "                                   decay=hyperparams_loaded['decay'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams_loaded = hyperparams_collection[key]\n",
    "hyperparams_features['pretrained_model_path']\n",
    "# hyperparams_collection[key] = hyperparams_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_collection = {}\n",
    "key='lstm_seq'\n",
    "# hyperparams_loaded['trainable_embeddings']=True\n",
    "models_collection[key] = load_saved_model_weights(hyperparams_features['pretrained_model_path'], \n",
    "                                                      hyperparams_loaded, \n",
    "                                                      emotions, stopword_list, liwc_categories, classes=classes, \n",
    "                                                      h5=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_collection[key].get_output_at?\n",
    "# combined_model = models_collection[key]\n",
    "writings_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "all_predictions = {}\n",
    "# for subject in set(writings_df[writings_df['subset']=='test'].subject.values):\n",
    "for subject in set(subjects_split['test']):\n",
    "    all_predictions[subject] = {'true_label': None, 'predicted_label': None,\n",
    "                               'predicted_score': None, 'texts': None}\n",
    "    try:\n",
    "        user_level_data_subject = {subject: user_level_data[subject]}\n",
    "    except:\n",
    "        continue\n",
    "    if classes==1:\n",
    "        true_label = writings_df[writings_df['subject']==subject].label.values[0]\n",
    "    else:\n",
    "        true_label = user_level_data[subject]['label']\n",
    "#         true_label = label_index[user_level_data[subject]['condition']]\n",
    "    print(subject, \"Label\", true_label)\n",
    "    prediction = models_collection[key].predict(DataGenerator(user_level_data_subject, {'test':[subject]}, \n",
    "                                         set_type='test', \n",
    "                                    seq_len=hyperparams_loaded['maxlen'],   \n",
    "#                                         batch_size=len(subjects_split['test']), # on all data at once\n",
    "                                        batch_size=hyperparams_loaded['batch_size'], # on all data at once\n",
    "                                       hierarchical=hyperparams_loaded['hierarchical'],\n",
    "                                         max_posts_per_user=None,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams_loaded['posts_per_group'],\n",
    "                                        post_groups_per_user=1,  compute_liwc=True,\n",
    "                                        liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                         sample_seqs=False, shuffle=False), verbose=1)\n",
    "    if classes==1:\n",
    "        model_prediction = int(prediction[0][0]>=0.5)\n",
    "    else:\n",
    "        model_prediction = np.argmax(prediction[0])\n",
    "    print(\"Prediction: \", prediction[0], model_prediction)\n",
    "    # TODO: is this aggregation of texts correct?\n",
    "    if (true_label!=model_prediction):\n",
    "        print(\"Misclassification:\", len(\n",
    "            user_level_data_subject[subject]['raw']), \"words\")\n",
    "    all_predictions[subject]['true_label'] = true_label\n",
    "    all_predictions[subject]['predicted_label'] = model_prediction\n",
    "    all_predictions[subject]['predicted_score'] = prediction[0][0]\n",
    "    all_predictions[subject]['texts'] = \"\\n\".join(\n",
    "            user_level_data_subject[subject]['raw'][:hyperparams_loaded['posts_per_group']])\n",
    "    all_predictions[subject]['liwc'] = user_level_data_subject[subject]['liwc'][\n",
    "        :hyperparams_loaded['posts_per_group']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame.from_dict(all_predictions, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_df = pickle.load(open('predictions_df_depression_erisk_lstmhierarch56.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df['misclass'] = abs(predictions_df['true_label'] - predictions_df['predicted_label'])\n",
    "predictions_df['misclass'] = predictions_df['misclass'].apply(lambda x: 0 if x==0 else 1)\n",
    "predictions_df['misclass_signed'] = predictions_df['true_label'] - predictions_df['predicted_label']\n",
    "predictions_df[predictions_df['misclass']==1].predicted_score.describe()\n",
    "# predictions_df[predictions_df['misclass_signed']==-1].predicted_score.describe()\n",
    "# predictions_df[predictions_df['misclass']==0].predicted_score.describe()\n",
    "# predictions_df[predictions_df['misclass']==1].mean()\n",
    "# predictions_df[predictions_df['misclass']==0].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.groupby(['misclass_signed', 'true_label']).count().predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(Counter(tokenize(\"\\n\".join(predictions_df[predictions_df['misclass']==1].texts.values))).items(), key=lambda t: -t[1])[:50])\n",
    "print(sorted(Counter(tokenize(\"\\n\".join(predictions_df[predictions_df['misclass']==0].texts.values))).items(), key=lambda t: -t[1])[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "col = 'misclass'\n",
    "target_val = 1\n",
    "counter_val = None\n",
    "\n",
    "misclassified_texts = \"\\n\".join(predictions_df[predictions_df[col]==target_val].texts.values)\n",
    "if counter_val is None:\n",
    "    correct_texts = \"\\n\".join(predictions_df[predictions_df[col]!=target_val].texts.values)\n",
    "else:\n",
    "    correct_texts = \"\\n\".join(predictions_df[predictions_df[col]==counter_val].texts.values)\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize_tweets)\n",
    "term_doc=vectorizer.fit_transform([misclassified_texts, correct_texts])\n",
    "ch2 = SelectKBest(chi2, \"all\")\n",
    "X_train = ch2.fit_transform(term_doc, [1,0])\n",
    "\n",
    "scores = ch2.scores_\n",
    "pvalues = ch2.pvalues_\n",
    "words = vectorizer.get_feature_names()\n",
    "features_sorted = sorted([(words[i], scores[i]) for i in range(len(scores))], key=lambda t:t[1], reverse=True)\n",
    "\n",
    "print(features_sorted[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df['liwc_avg'] = predictions_df['liwc'].apply(lambda x: np.mean(x, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, c in enumerate(categories):\n",
    "    predictions_df[c] = predictions_df['liwc_avg'].apply(lambda x: x[i])\n",
    "# TODO: compute emotions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df=predictions_df[~predictions_df.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_df = pickle.load(open('predictions_df_depression_clpsych.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_words_for_categories = {}\n",
    "for categ in liwc_dict.keys():\n",
    "    liwc_words_for_categories[categ] = set()\n",
    "    for word in liwc_dict[categ]:\n",
    "        for t in vocabulary:\n",
    "            if t==word or (word[-1]=='*' and t.startswith(word[:-1])) \\\n",
    "                or (t==word.split(\"'\")[0]):\n",
    "                    liwc_words_for_categories[categ].add(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_words_for_categories.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df['all_tokens'] = predictions_df['texts'].apply(tokenize)\n",
    "\n",
    "# TODO: include the title\n",
    "def extract_emotions(tokens, emotion, relative=True):\n",
    "    if not tokens:\n",
    "        return None\n",
    "    emotion_words = [t for t in tokens \n",
    "                     if t in nrc_lexicon[emotion]]\n",
    "    if relative and len(tokens):\n",
    "        return len(emotion_words) / len(tokens)\n",
    "    else:\n",
    "        return len(emotion_words)\n",
    "    \n",
    "    return encoded_emotions\n",
    "\n",
    "from functools import partial\n",
    "for emotion in emotions:\n",
    "    predictions_df[emotion] = predictions_df['all_tokens'].apply(partial(extract_emotions, emotion=emotion, \n",
    "                                                                   relative=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_liwc_categories(tokens, category_words, relative=True):\n",
    "    category_cnt = 0\n",
    "    if not tokens:\n",
    "        return None\n",
    "    text_len = len(tokens)\n",
    "    for t in tokens:\n",
    "        for word in category_words:\n",
    "            if t==word or (word[-1]=='*' and t.startswith(word[:-1])) \\\n",
    "            or (t==word.split(\"'\")[0]):\n",
    "                category_cnt += 1\n",
    "                break # one token cannot belong to more than one word in the category\n",
    "    if relative:\n",
    "        return category_cnt/text_len\n",
    "    else:\n",
    "        return category_cnt\n",
    "\n",
    "for categ in liwc_dict.keys():\n",
    "    if categ in predictions_df.columns:\n",
    "        continue\n",
    "    print(\"Encoding for category %s...\" % categ)\n",
    "    predictions_df[categ] = predictions_df['all_tokens'].apply(partial(encode_liwc_categories, \n",
    "                                                                   category_words=liwc_dict[categ], \n",
    "                                                                   relative=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barplot_pvalues(categories_pscores, width=3, bar_width=None, yticks_freq=1, show_pval=False, \n",
    "                    yticks_start=1, angle=45, group_labels=None, logy=False, nr_bars=4):\n",
    "    \n",
    "    nr_bars = len(categories_pscores)\n",
    "    categories_pscores_aggregated = []\n",
    "    categories = set([t[0] for t in categories_pscores])\n",
    "#     for cat in categories:\n",
    "#         if aggreg=='mean':\n",
    "#             # Compute mean\n",
    "#             for c, v in categories_pscores:\n",
    "#                 s = 0\n",
    "#                 l = 0\n",
    "#                 if c==cat:\n",
    "#                     s += v\n",
    "#                     l += 1\n",
    "#             m = s/l\n",
    "#             categories_pscores_aggregated.append((cat, m))\n",
    "#         if aggreg=='group':\n",
    "#             for c, v in categories_pscores:\n",
    "#                 if c==cat:\n",
    "#                     categories_pscores_aggregated.append()\n",
    "    if type(categories_pscores) is list:\n",
    "        categories_pscores_aggregated = [categories_pscores]\n",
    "        labels = ['misclassif']\n",
    "    if type(categories_pscores) is dict:\n",
    "        labels = []\n",
    "        for group, scores in categories_pscores.items():\n",
    "            categories_pscores_aggregated.append(scores)\n",
    "            labels.append(group)\n",
    "#     if not bar_width:\n",
    "    bar_width = 1/(nr_bars+1) * width\n",
    "#         bar_width = 2/3 * width\n",
    " \n",
    "    positions = []\n",
    "    for i,scores in enumerate(categories_pscores_aggregated):\n",
    "        print(\"plotting\", labels[i], len(scores))\n",
    "        if len(positions)==0:\n",
    "            positions =  np.arange(0, len(scores) * width, width)\n",
    "        else:\n",
    "            positions = [x + bar_width for x in positions]\n",
    "        plt.bar(\n",
    "            positions,\n",
    "#            x = np.arange(0, len(scores) * width, width),\n",
    "            height=[s for (e,s) in scores], width=bar_width, align='edge', label=labels[i], edgecolor='white')\n",
    "    if show_pval:\n",
    "        plt.hlines(0.05, 0,width*nr_bars, linestyles=':', colors='red')\n",
    "    if logy:\n",
    "        plt.yscale('log')\n",
    "    plt.xticks(ticks=range(int(bar_width*2), width*len(scores), width), \n",
    "               labels=[e[:] for e,p in scores], rotation=angle)\n",
    "    if logy:\n",
    "#         plt.yticks(ticks=[10**i for i in range(-yticks_start,int(np.log10(min([p for (e,p) in scores])))\n",
    "#                                                ,-yticks_freq)] + [0.05],\n",
    "#             labels=[10**i for i in range(-yticks_start,int(np.log10(min([p for (e,p) in scores]))),\n",
    "#                                      -yticks_freq)],# + [0.05], \n",
    "#                )\n",
    "        plt.ylim(0,0.1)\n",
    "    else:\n",
    "#         plt.yticks(ticks=[10**i for i in range(-yticks_start,1,\n",
    "#                                                -yticks_freq)],\n",
    "#                    labels=[10**i for i in range(-yticks_start,1,\n",
    "#                                                -yticks_freq)]\n",
    "#                )\n",
    "        plt.ylim(0,0.1)\n",
    "#         plt.yticks(ticks=[10**i for i in range(-yticks_start,int(min([p for (e,p) in scores]))\n",
    "#                                                ,-yticks_freq)] + [0.05],\n",
    "#             labels=[10**i for i in range(-yticks_start,int(min([p for (e,p) in scores])),\n",
    "#                                      -yticks_freq)], # + [0.05], \n",
    "#                )\n",
    "    # Add xticks on the middle of the group bars\n",
    "#     plt.xlabel(group, fontweight='bold')\n",
    "#     plt.xticks([r + barWidth for r in range(len(bars1))], ['A', 'B', 'C', 'D', 'E'])\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot_pvalues([(\"first\", 0.001), (\"seco\", 0.01), (\"third\", 0.00001)], bar_width=0.5, logy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in categories:\n",
    "    print(c, 'misclass', predictions_df[predictions_df['misclass']==1][c].mean())\n",
    "    print(c, 'not misclass', predictions_df[predictions_df['misclass']==0][c].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in emotions:\n",
    "    print(c, 'misclass', predictions_df[predictions_df['misclass']==1][c].mean())\n",
    "    print(c, 'not misclass', predictions_df[predictions_df['misclass']==0][c].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, ttest_ind\n",
    "for c in categories:\n",
    "    print(c)\n",
    "    print('\\tmisclass', predictions_df[predictions_df['misclass']==1][c].mean())\n",
    "    print('\\tnot misclass', predictions_df[predictions_df['misclass']==0][c].mean())    \n",
    "    ttest = ttest_ind(predictions_df[predictions_df['misclass']==0][c],\n",
    "                                  predictions_df[predictions_df['misclass']==1][c])\n",
    "    print('\\tttest', ttest)\n",
    "    print('\\tpvalue', ttest[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'axes.labelsize': 25,\n",
    "          'axes.titlesize': 25,\n",
    "          'legend.fontsize': 25,\n",
    "         'xtick.labelsize': 25,\n",
    "         'ytick.labelsize': 25}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, ttest_ind\n",
    "\n",
    "def retrieve_differences_misclass_groups(predictions_df, categories, pval_thresh=0.05,\n",
    "                                        width=5, bar_width=1, angle=60, false_cat=None):\n",
    "    categories_pscores = {'FN': [], 'FP': []}\n",
    "#     categories_means = {'FN': [], 'FP': [], 'TN': [], 'TP': []}\n",
    "    if false_cat == 'FN':\n",
    "        categories_means = {'FN': [], 'TP': []}\n",
    "        cols_filters = {'misclass_signed': [('misclass_signed', 1), ('true_label', 1)]}\n",
    "    elif false_cat == 'FP':\n",
    "        categories_means = {'FP': [], 'TN': []}\n",
    "        cols_filters = {'misclass_signed': [('misclass_signed', -1), ('true_label', 0)]}\n",
    "\n",
    "    else:\n",
    "#         categories_means = {'FN': [], 'TN': [], 'TP': [], 'FP': []}\n",
    "        categories_means = {'FN': [], 'TN': [], 'TP': [], 'FP': []}\n",
    "        cols_filters = {'misclass': [('misclass', 1)]}\n",
    "\n",
    "    cols_values = {'FN': [('misclass_signed', 1)], 'FP': [('misclass_signed', -1)], \n",
    "                   'TN': [('misclass', 0), ('true_label', 0)], 'TP': [('misclass', 0), ('true_label', 1)]}\n",
    "    \n",
    "    for c in categories:\n",
    "        significant = True\n",
    "        for group in cols_filters:\n",
    "            cols_filter = cols_filters[group]\n",
    "            if len(cols_filter)==1:\n",
    "                col, val = cols_filter[0]\n",
    "                print(col,val)\n",
    "                groupdf = predictions_df.query('%s == %d' % (col, val))[c]\n",
    "                nongroupdf = predictions_df.query('%s != %d' % (col, val))[c]\n",
    "\n",
    "            if len(cols_filter)==2:\n",
    "                col1, val1 = cols_filter[0]\n",
    "                col2, val2 = cols_filter[1]\n",
    "                groupdf = predictions_df.query('%s == %d' % (col1, val1))[c]\n",
    "                nongroupdf = predictions_df.query('%s == %d' % (col2, val2))[c]\n",
    "\n",
    "            ttest = ttest_ind(groupdf, nongroupdf)\n",
    "            if ttest[1] >= pval_thresh:\n",
    "                print(\"Not significant\", c)\n",
    "                significant = False\n",
    "        if not significant:\n",
    "            continue\n",
    "#         for group in cols_values:\n",
    "        for group in categories_means:\n",
    "            print(group)\n",
    "            cols_value = cols_values[group]\n",
    "\n",
    "            print(c)\n",
    "            if len(cols_value)==1:\n",
    "                col, val = cols_value[0]\n",
    "                groupdf = predictions_df.query('%s == %d' % (col, val))[c]\n",
    "                nongroupdf = predictions_df.query('%s != %d' % (col, val))[c]\n",
    "\n",
    "            if len(cols_value)==2:\n",
    "                col1, val1 = cols_value[0]\n",
    "                col2, val2 = cols_value[1]\n",
    "                groupdf = predictions_df.query('%s == %d & %s == %d' % (col1, val1, col2, val2))[c]\n",
    "                nongroupdf = predictions_df.query('%s != %d | %s != %d' % (col1, val1, col2, val2))[c]\n",
    "\n",
    "            if len(groupdf)==0:\n",
    "                print(\"Group\", group, \"is empty. Skipping...\")\n",
    "                categories_pscores[group].append((c,0.99))\n",
    "                categories_means[group].append((c,0.0))\n",
    "                continue\n",
    "            print('\\t', col, val, groupdf.mean())\n",
    "            print('\\tnon', col, val, nongroupdf.mean())    \n",
    "            ttest = ttest_ind(groupdf, nongroupdf)\n",
    "            print('\\tttest', ttest)\n",
    "            print('\\tpvalue', ttest[1])\n",
    "#             if group in ('FN', 'FP'):\n",
    "#                 categories_pscores[group].append((c,ttest[1]))\n",
    "#             if ttest[1] < 0.05:\n",
    "#                 categories_means[group].append((c,groupdf.mean()))\n",
    "#             else:\n",
    "            categories_means[group].append((c,groupdf.mean()))\n",
    "\n",
    "    # emotion_pscores = sorted(emotion_pscores, key=lambda t:t[1])\n",
    "#     barplot_pvalues(categories_pscores, 3, yticks_start=2, bar_width=bar_width, show_pval=True)\n",
    "    print(categories_means)\n",
    "    barplot_pvalues(categories_means, bar_width=10, width=10, angle=angle, nr_bars=len(categories_means), logy=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[e for e in emotions if e in predictions_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_differences_misclass_groups(predictions_df, [e for e in emotions if e not in ['negative', 'positive']])\n",
    "#                                     false_cat='FN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_differences_misclass_groups(predictions_df, categories, \n",
    "#                                      pval_thresh=0.000001, \n",
    "#                                      pval_thresh=0.001, \n",
    "                                     width=5, bar_width=0.5, angle=90, false_cat='FP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, ttest_ind\n",
    "for c in categories:\n",
    "    print(c)\n",
    "    print('\\tpositive', predictions_df[predictions_df['true_label']==1][c].mean())\n",
    "    print('\\tnegative', predictions_df[predictions_df['true_label']==0][c].mean())    \n",
    "    ttest = ttest_ind(predictions_df[predictions_df['true_label']==0][c],\n",
    "                                  predictions_df[predictions_df['true_label']==1][c])\n",
    "    print('\\tttest', ttest)\n",
    "    print('\\tpvalue', ttest[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, ttest_ind\n",
    "for c in categories:\n",
    "    print(c)\n",
    "    print('\\tpredicted positive', predictions_df[predictions_df['predicted_label']==1][c].mean())\n",
    "    print('\\tpredicted_negative', predictions_df[predictions_df['predicted_label']==0][c].mean())    \n",
    "    ttest = ttest_ind(predictions_df[predictions_df['predicted_label']==0][c],\n",
    "                                  predictions_df[predictions_df['predicted_label']==1][c])\n",
    "    print('\\tttest', ttest)\n",
    "    print('\\tpvalue', ttest[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, ttest_ind\n",
    "categories_pscores = []\n",
    "for c in categories:\n",
    "    print(c)\n",
    "    print('\\tmisclassified', predictions_df[predictions_df['misclass']==1][c].mean())\n",
    "    print('\\tnot misclassified', predictions_df[predictions_df['misclass']==0][c].mean())    \n",
    "    ttest = ttest_ind(predictions_df[predictions_df['misclass']==0][c],\n",
    "                                  predictions_df[predictions_df['misclass']==1][c])\n",
    "    print('\\tttest', ttest)\n",
    "    print('\\tpvalue', ttest[1])\n",
    "    if ttest[1] < 0.05:\n",
    "        categories_pscores.append((c,ttest[1]))\n",
    "print(sorted(categories_pscores, key=lambda t:t[1]))        \n",
    "barplot_pvalues(categories_pscores, width=1, show_pval=False, yticks_start=1, yticks_freq=1, angle=90, \n",
    "               bar_width=0.5)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(predictions_df, open('predictions_df_depression_symanto_lstmhierarch64.pkl', 'wb+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_df = pickle.load(open('predictions_df_depression_erisk.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_predictions\n",
    "predictions_df.columns\n",
    "y_test = predictions_df.true_label.values\n",
    "y_score = predictions_df.predicted_score.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "# fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "# roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, \n",
    "#          label = dataset_type)\n",
    "         label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "# misclassified_users = set(predictions_df[predictions_df['misclass']==1].subject)\n",
    "text = \"\"\n",
    "for tokens in writings_df[writings_df['label']==1].tokenized_text.values:\n",
    "    text += \" \".join(tokens) + \" \"\n",
    "# text = \" \".join(predictions_df[predictions_df['misclass_signed']==-1].texts)\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud( background_color='white', max_words=1000, max_font_size=70).generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_saliency(loaded_model, pure_txt, text_class, pred_labels, text_sequence):\n",
    "    text_class = 'Positive' if text_class==1 else 'Negative'\n",
    "    pred_labels = 'Positive' if pred_labels==1 else 'Negative'\n",
    "\n",
    "    input_tensors = [loaded_model.input, K.learning_phase()]\n",
    "    model_input = loaded_model.layers[2].input # the input for convolution layer\n",
    "    model_output = loaded_model.output[0][1]\n",
    "    gradients = loaded_model.optimizer.get_gradients(model_output,model_input)\n",
    "    compute_gradients = K.function(inputs=input_tensors, outputs=gradients)\n",
    "    matrix = compute_gradients([text_sequence.reshape(1,30), text_class])[0][0]\n",
    "    matrix = matrix[:len(pure_txt),:]\n",
    "\n",
    "    matrix_magnify=np.zeros((matrix.shape[0]*10,matrix.shape[1]))\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(10):\n",
    "            matrix_magnify[i*10+j,:]=matrix[i,:]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.imshow(normalize_array(np.absolute(matrix_magnify)), interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.yticks(np.arange(5, matrix.shape[0]*10, 10), pure_txt, weight='bold',fontsize=24)\n",
    "    plt.xticks(np.arange(0, matrix.shape[1], 50), weight='bold',fontsize=24)\n",
    "    plt.title('True Label: \"{}\" Predicted Label: \"{}\"'.format(text_class,pred_labels), weight='bold',fontsize=24)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_saliency(models_collection[key], \"this is some text to classify\", 0, 0, [\"this\", \"is\", \"some\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_level_data3, subjects_split2, vocabulary = load_erisk_data(writings_df, \n",
    "                                                            voc_size=hyperparams_features['max_features'],\n",
    "                                                           emotion_lexicon=nrc_lexicon,\n",
    "                                                           emotions=[],\n",
    "                                                                liwc_categories=[],\n",
    "                                                           user_level=hyperparams_features['user_level'],\n",
    "                                                                                logger=logger,\n",
    "#                                                            vocabulary=pickle.load(open('vocabulary_40K_all.pkl', 'rb')),\n",
    "#                                                            vocabulary=pickle.load(open('vocab_clpsych_10000.pkl', 'rb')),\n",
    "                                                              vocabulary=vocabulary_dict,\n",
    "                                                              by_subset=False\n",
    "                                                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import lime.lime_text as lt\n",
    "import keras\n",
    "# my self written library\n",
    "import numpy as np\n",
    "\n",
    "# This is the text that I want to explain\n",
    "\n",
    "\n",
    "#\n",
    "# recodeText = pl.generate_one_hot(text=inputText, alphabet=alphabet, maxChars=maxChars)\n",
    "\n",
    "model = models_collection[key]\n",
    "\n",
    "\n",
    "\n",
    "# subjects = ['subject14410000', 'subject68010000', 'test_subject315', 'test_subject2270', \n",
    "#             'test_subject9306', 'test_subject9359']\n",
    "subjects = [s for s in set(writings_df.subject.values) if s in user_level_data3]\n",
    "labels = [writings_df[writings_df['subject']==subject].label.values[0] for subject in subjects]\n",
    "overall_importance_vocabulary = {}\n",
    "\n",
    "def textForSubject(subject):\n",
    "    user_level_data_subject = user_level_data3[subject]\n",
    "    text = \"\\n\".join([\" \".join(t) for t in user_level_data_subject['texts'][\n",
    "        :hyperparams_collection[key]['posts_per_group']]])\n",
    "    return text\n",
    "    \n",
    "# pipeline-like function\n",
    "# takes raw text as input, converts it to one-hot character vectors via the alphabet\n",
    "# feeds it into keras' model.predict() function to receive predictions\n",
    "# works for lists of text as well as for single strings\n",
    "# TODO: need to make this actually work on the text input, not dummy. it needs to feed new things\n",
    "def predictFromText(inputTexts):\n",
    "    if type(inputTexts) != list:\n",
    "        inputTexts = [inputTexts]\n",
    "    \n",
    "    # list for predictions\n",
    "    predStorage = []\n",
    "    # loop through input list and predict\n",
    "\n",
    "    predictions = models_collection[key].predict(DataGeneratorFromText(inputTexts,\n",
    "                                        seq_len=hyperparams_collection[key]['maxlen'],\n",
    "                                        liwc_words_for_categories=liwc_words_for_categories,\n",
    "#                                         emotions=[], liwc_categories=[]\n",
    "                                                             ), verbose=1)\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        pred = int(prediction>=0.5)\n",
    "\n",
    "#         recodeText = pl.generate_one_hot(text=textInput, alphabet=alphabet, maxChars=maxChars)\n",
    "#         pred = model.predict(recodeText.transpose())\n",
    "        # control output of function\n",
    "#         print(\"TEXT\", str(inputTexts[i][:100]), \"PRED\", prediction, \"\\n\")\n",
    "        predStorage.append((1-prediction[0], prediction[0]))\n",
    "    return(np.asarray(predStorage))\n",
    "\n",
    "inputTexts = [textForSubject(subject) for subject in subjects]\n",
    "\n",
    "print(len(inputTexts))\n",
    "print(labels)\n",
    "\n",
    "# print(inputTexts)\n",
    "# print(predictFromText(inputTexts))\n",
    "# Lime Explainer\n",
    "# bow controls if words are perturbed or overwritten with UNKWORDZ\n",
    "# False makes sense, if location of words is important as in this classifier\n",
    "explainer = lt.LimeTextExplainer(\n",
    "#     kernel_width=25, \n",
    "    verbose=False, class_names=['not depressed', 'depressed'],\n",
    "#                            feature_selection=\"auto\", split_expression=\" \", bow=False\n",
    ")\n",
    "for i, inputText in enumerate(inputTexts):\n",
    "    print(\"EXPLAINING...\", i)\n",
    "    exp = explainer.explain_instance(text_instance=inputText, labels=(0,1), \n",
    "#                                      labels=[labels[i]],\n",
    "                         classifier_fn=predictFromText, \n",
    "                                     num_features=100, num_samples=1000\n",
    "                                    )\n",
    "    for word, weight in exp.as_list():\n",
    "        if word not in overall_importance_vocabulary:\n",
    "            overall_importance_vocabulary[word] = []\n",
    "        overall_importance_vocabulary[word].append(weight)\n",
    "    # this works, yields an array with probabilities for both classes\n",
    "    # print(predictFromText(textInputList = listTexts))\n",
    "    # print(len(predictFromText(inputTexts[0])), len(inputTexts))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(user_level_data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.explain_instance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.explain_instance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_importance_vocabulary_averages = {word: np.mean(overall_importance_vocabulary[word]) \n",
    "                                          for word in overall_importance_vocabulary \n",
    "                                          if len(overall_importance_vocabulary[word])>=2}\n",
    "overall_importance_vocabulary_averages_abs = {word: np.abs(np.mean(overall_importance_vocabulary[word])) \n",
    "                                              for word in overall_importance_vocabulary\n",
    "                                             if len(overall_importance_vocabulary[word])>=2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(overall_importance_vocabulary_averages_abs.items(), key=lambda t: t[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(t[0], np.mean(t[1]), len(t[1])) for t in \n",
    " sorted(overall_importance_vocabulary.items(), key=lambda t: len(t[1]), reverse=True)[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, weight in sorted(overall_importance_vocabulary_averages_abs.items(), key=lambda t: t[1], reverse=True)[:20]:\n",
    "    print(word, overall_importance_vocabulary[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(overall_importance_vocabulary, open('overall_importance_vocabulary_depression.pkl', 'wb+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall_importance_vocabulary = pickle.load(open('overall_importance_vocabulary_selfharm.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([len(overall_importance_vocabulary[w]) for w in overall_importance_vocabulary]).hist(log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.explain_instance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in DataGenerator(user_level_data3, {'test':[subject]}, \n",
    "                                         set_type='test', \n",
    "                                    seq_len=hyperparams_collection[key]['maxlen'],   \n",
    "#                                         batch_size=len(subjects_split['test']), # on all data at once\n",
    "                                        batch_size=hyperparams_collection[key]['batch_size'], # on all data at once\n",
    "                                       hierarchical=hyperparams_collection[key]['hierarchical'],\n",
    "                                         max_posts_per_user=None,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams_collection[key]['posts_per_group'],\n",
    "                                        post_groups_per_user=1,  compute_liwc=True,\n",
    "                                        liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                         sample_seqs=False, shuffle=False,\n",
    "                                        emotions=[], liwc_categories=[]\n",
    "                                                             ):\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D visualization of emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def normalize_date(date):\n",
    "        return \" \".join(date.strip().split())\n",
    "writings_df['datetime'] = writings_df.date.apply(lambda d: datetime.datetime.strptime(\n",
    "            normalize_date(d), '%Y-%m-%d %H:%M:%S'))\n",
    "date_cutoff1 = datetime.datetime.strptime('2010','%Y').date()\n",
    "date_cutoff2 = datetime.datetime.strptime('2020','%Y').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_emotions = writings_df[\n",
    "    writings_df['datetime'].between(date_cutoff1, date_cutoff2)\n",
    "].groupby('subject').mean()[list(categories) + emotions + ['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "emotions_dict=users_emotions.dropna().to_dict(orient='index')\n",
    "subjects = list(emotions_dict.keys())\n",
    "emotions_matrix=[list(emotions_dict[s].values())[:-1] for s in subjects]\n",
    "emotions_labels=[emotions_dict[s]['label'] for s in subjects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(np.array(emotions_matrix).transpose())\n",
    "col = plt.scatter(pca.components_[0], pca.components_[1], c=['r' if l==1 else 'b' for l in emotions_labels],\n",
    "           s=[0.5 for s in emotions_labels])\n",
    "\n",
    "ax = col.axes\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "# manually define a new patch \n",
    "handles.append(mpatches.Patch(color='red', label='anorexic users'))\n",
    "handles.append(mpatches.Patch(color='blue', label='non anorexic users'))\n",
    "\n",
    "\n",
    "# plot the legend\n",
    "plt.legend(handles=handles, loc='upper center')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = initialize_experiment(hyperparams, nrc_lexicon_path, emotions, pretrained_embeddings_path, \n",
    "                          dataset_type, transfer_type)\n",
    "experiment.add_tag(\"cross-validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 5\n",
    "\n",
    "f1_scores = []\n",
    "for test_slice in range(folds): \n",
    "    logger.info(\"Testing on slice %d, training on the rest...\\n\" % test_slice)\n",
    "    user_level_data, subjects_split, vocabulary = load_erisk_data(writings_df, \n",
    "                                                            voc_size=hyperparams_features['max_features'],\n",
    "                                                           emotion_lexicon=nrc_lexicon,\n",
    "                                                           emotions=emotions,\n",
    "                                                           user_level=hyperparams_features['user_level'],\n",
    "                                                                                logger=logger,\n",
    "                                                           vocabulary=pickle.load(open('all_vocab_clpsych_erisk_2000_dict.pkl', 'rb')),\n",
    "                                                              by_subset=False,\n",
    "                                                              nr_slices=folds,\n",
    "                                                                valid_prop=0.1,\n",
    "                                                                  test_slice=test_slice\n",
    "                                                                 )\n",
    "    model, history = train(user_level_data, subjects_split, \n",
    "              hyperparams, hyperparams_features, \n",
    "              embedding_matrix, emotions, stopword_list, liwc_categories,\n",
    "                  experiment,\n",
    "                validation_set='test',\n",
    "              version=8, epochs=50,\n",
    "    )\n",
    "    f1_scores.append(sum(history.history['val_f1_m'][-6:-1])/5)\n",
    "experiment.add_metric(\"average_f1\", sum(f1_scores)/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on server data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize_vars(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"models/lstm_selfharm_seq16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams1, hyperparams_features1 = load_params(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams_features1['embeddings_path']=[\n",
    "#     '/home/anasab//eRisk/embeddings/finetuned_glove_clpsych_erisk_stop_20000_2.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_features1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_saved_model(model_path, hyperparams1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_list = pickle.load(open(hyperparams_features1['vocabulary_path'], 'rb'))\n",
    "vocabulary_dict={}\n",
    "for i,w in enumerate(vocabulary_list):\n",
    "    vocabulary_dict[w] = i\n",
    "embedding_matrix = load_embeddings2(hyperparams_features1['embeddings_path'], \n",
    "                                    hyperparams_features1['embedding_dim'], vocabulary_dict)\n",
    "liwc_words_for_categories = pickle.load(open(hyperparams_features1['liwc_words_cached'], 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'optimizer' not in hyperparams1:\n",
    "#         hyperparams1['optimizer'] = optimizers.Adam(lr=hyperparams1['lr'], #beta_1=0.9, beta_2=0.999, epsilon=0.0001,\n",
    "#                                        decay=hyperparams1['decay'])\n",
    "# loaded_model = initialize_model(hyperparams1, hyperparams_features1, embedding_matrix, emotions, stopword_list,\n",
    "#                     liwc_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_weights(model_path + \"_weights.h5\", by_name=True)\n",
    "# metrics_class = Metrics(threshold=0.5)\n",
    "# loaded_model.compile(hyperparams1['optimizer'], binary_crossentropy_custom,\n",
    "#                   metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_runs = {\n",
    "    'bert': 0,\n",
    "    'lstm_seq': 1,\n",
    "    'cnn_hierarch': 2,\n",
    "    'transfer': 3,\n",
    "    'ensemble': 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_key = 'lstm_seq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def load_erisk_server_data(datarounds_json, voc_size, emotion_lexicon, tokenizer,\n",
    "                           liwc_words_for_categories,\n",
    "                           emotions =  \n",
    "                    ['anger', 'anticipation', 'disgust', 'fear', 'joy', \n",
    "                     'negative', 'positive', 'sadness', 'surprise', 'trust'],\n",
    "                    liwc_categories = liwc_categories, \n",
    "                    pronouns = [\"i\", \"me\", \"my\", \"mine\", \"myself\"],\n",
    "                    user_level=True, vocabulary=vocabulary_dict,\n",
    "                   logger=logger):\n",
    "    def __encode_liwc_categories(tokens, liwc_words_for_categories, relative=True):\n",
    "        categories_cnt = [0 for c in liwc_categories]\n",
    "        if not tokens:\n",
    "            return categories_cnt\n",
    "        text_len = len(tokens)\n",
    "        for i, category in enumerate(liwc_categories):\n",
    "            for t in tokens:\n",
    "                if t in liwc_words_for_categories[category]:\n",
    "                    categories_cnt[i] += 1\n",
    "            if relative and text_len:\n",
    "                categories_cnt[i] = categories_cnt[i]/text_len\n",
    "        return categories_cnt\n",
    "    logger.debug(\"Loading data...\\n\")\n",
    "     \n",
    "    subjects_split = {'test': []}\n",
    " \n",
    "    user_level_texts = {}\n",
    "    for datapoints_json in datarounds_json:\n",
    "        for datapoint in datapoints_json:\n",
    "            words = []\n",
    "            raw_text = \"\"\n",
    "            if \"title\" in datapoint:\n",
    "                tokenized_title = tokenizer.tokenize(datapoint[\"title\"])\n",
    "                words.extend(tokenized_title)\n",
    "                raw_text += datapoint[\"title\"]\n",
    "            if \"content\" in datapoint:\n",
    "                tokenized_text = tokenizer.tokenize(datapoint[\"content\"])\n",
    "                words.extend(tokenized_text)\n",
    "                raw_text += datapoint[\"content\"]\n",
    "            \n",
    "            liwc_categs = __encode_liwc_categories(words, liwc_words_for_categories)\n",
    "            if datapoint[\"nick\"] not in user_level_texts.keys():\n",
    "                user_level_texts[datapoint[\"nick\"]] = {}\n",
    "                user_level_texts[datapoint[\"nick\"]]['texts'] = [words]\n",
    "                user_level_texts[datapoint[\"nick\"]]['liwc'] = [liwc_categs]\n",
    "                user_level_texts[datapoint[\"nick\"]]['raw'] = [raw_text]\n",
    "                subjects_split['test'].append(datapoint['nick'])\n",
    "            else:\n",
    "                user_level_texts[datapoint[\"nick\"]]['texts'].append(words)\n",
    "                user_level_texts[datapoint[\"nick\"]]['liwc'].append(liwc_categs)\n",
    "                user_level_texts[datapoint[\"nick\"]]['raw'].append(raw_text)\n",
    "            \n",
    "    return user_level_texts, subjects_split, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_datapoint(jlpath):\n",
    "    datapoints = []\n",
    "    with open(jlpath) as f:\n",
    "        for line in f:\n",
    "            datapoints.append(json.loads(line))\n",
    "    return datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# erisk_server_data, erisk_server_subjects_split, vocabulary = load_erisk_server_data(data,\n",
    "#                        tokenizer=regtokenizer,\n",
    "#                        liwc_words_for_categories=liwc_words_for_categories,\n",
    "#                     voc_size=hyperparams_features['max_features'],\n",
    "#                     emotion_lexicon=nrc_lexicon,\n",
    "#                     emotions=emotions,\n",
    "#                     user_level=hyperparams_features['user_level'],\n",
    "#                        vocabulary=vocabulary_dict,\n",
    "#     #                                                            vocabulary=pickle.load(open('vocabulary20K_selfharm.pkl', 'rb'))\n",
    "#                     logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erisk_server_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#TODO: predict only on last 50 posts\n",
    "\n",
    "with session_collection[model_key].as_default():\n",
    "    with session_collection[model_key].graph.as_default():\n",
    "        server_erisk_predictions = models_collection[model_key].predict(DataGenerator(erisk_server_data, erisk_server_subjects_split, \n",
    "                                             set_type='test', vocabulary=vocabulary_dict, \n",
    "                                           hierarchical=hyperparams_collection[model_key]['hierarchical'],\n",
    "                                        seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
    "                                             max_posts_per_user=None,\n",
    "                                           pad_with_duplication=False,\n",
    "                                            posts_per_group=hyperparams['posts_per_group'],\n",
    "                                            post_groups_per_user=None, \n",
    "                                             sample_seqs=False, shuffle=False,\n",
    "                                                    compute_liwc=False)\n",
    "                                                                       )\n",
    "        pd.Series(server_erisk_predictions.flatten()).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(server_erisk_predictions.flatten()).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seding results to server!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects = [d['nick'] for d in read_json_datapoint(\"client/data0.jl\")]\n",
    "# results = {s: 0 for s in subjects}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_next_data(rnd, results):\n",
    "#     # TODO: send results to get data\n",
    "#     response = build_response(results)\n",
    "#     # Send response\n",
    "#     data = {\"...\"}\n",
    "#     # Make sure it's the correct round\n",
    "#     assert data['number'] == rnd\n",
    "#     serialize_data(data)\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_next_data_dummy(rnd, results):\n",
    "#     return read_json_datapoint(\"client/data0.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for round and model\n",
    "results = {key: {} for key in models_runs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_chunk(rnds):\n",
    "    # Send same results for a chunk of rounds to get new posts\n",
    "    data = [read_json_datapoint(\"data_server/data%d.jl\" % i) for i in rnds]\n",
    "#     data_chunks = []\n",
    "#     for rnd in rnds:\n",
    "#         # TODO: REPLACE THIS WITH THE CORRECT ONE\n",
    "#         data = get_next_data_dummy(rnd, results)\n",
    "#         data_chunks.append(data)\n",
    "#         all_data[rnd] = data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def predict_for_round_chunk(model, hyperparams, hyperparams_features, vocabulary, data_chunk, subjects=[],\n",
    "                           model_key='', cache_round=None): \n",
    "    # preload for subjects not occurring in the round with results from previous round\n",
    "#     results = {s: 0 for s in subjects}\n",
    "    if cache_round:\n",
    "        results = load_results(model_key, cache_round)\n",
    "    else:\n",
    "        results = {s: 0 for s in subjects}\n",
    "        \n",
    "    \n",
    "    erisk_server_data, erisk_server_subjects_split, vocabulary = load_erisk_server_data(data_chunk,\n",
    "                       tokenizer=regtokenizer,\n",
    "                       liwc_words_for_categories=liwc_words_for_categories,\n",
    "                    voc_size=hyperparams_features['max_features'],\n",
    "                    emotion_lexicon=nrc_lexicon,\n",
    "                    emotions=emotions,\n",
    "                    user_level=1,\n",
    "                       vocabulary=vocabulary,\n",
    "                    logger=logger)\n",
    "\n",
    "    for features, subjects, _ in DataGenerator(erisk_server_data, erisk_server_subjects_split, \n",
    "                                         set_type='test', vocabulary=vocabulary, \n",
    "                                       hierarchical=hyperparams['hierarchical'],\n",
    "                                    seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
    "                                         max_posts_per_user=None,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams['posts_per_group'],\n",
    "                                        post_groups_per_user=None, \n",
    "                                         sample_seqs=False, shuffle=False,\n",
    "                                      return_subjects=True):\n",
    "        predictions = model.predict_on_batch(features)\n",
    "        print(len(features[0]), len(subjects), len(predictions), len(results))\n",
    "        for i,s in enumerate(subjects):\n",
    "            results[\"subject\" + str(s)] = predictions[i].item()\n",
    "    return(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_response(results, decision_thresh=0.5, model_name='', rnd=0):\n",
    "    response = []\n",
    "    for subject, score in results.items():\n",
    "        prediction = 1 if score >= decision_thresh else 0\n",
    "        response.append({'nick': subject, 'score': score, 'decision': prediction})\n",
    "    json.dump(response, open(\"data_server/response_run%s_rnd%d.json\" % (model_name, rnd), 'w+'))\n",
    "    return response\n",
    "# build_response(results, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_ensemble_results(rnd, all_results, model_keys_to_average=['lstm_seq', 'cnn_hierarch']):\n",
    "#     subjects = [s for s in all_results[model_keys_to_average[0]][rnd]]\n",
    "#     results_ensemble = {}\n",
    "#     for sub in subjects:\n",
    "#         s = 0\n",
    "#         for k in model_keys_to_average:\n",
    "#             s += all_results[k][rnd][sub]\n",
    "#         results_ensemble[sub] = s/len(model_keys_to_average)\n",
    "#     return results_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble_results(results_to_average):\n",
    "    subjects = [s for s in results_to_average[0]]\n",
    "    results_ensemble = {}\n",
    "    for sub in subjects:\n",
    "        s = 0\n",
    "        for res in results_to_average:\n",
    "            s += res[sub]\n",
    "        results_ensemble[sub] = s/len(results_to_average)\n",
    "    return results_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transfer_results(rnd, all_results, model_to_average='lstm_seq', rounds_back=100):\n",
    "    subjects = [s for s in all_results[model_to_average][rnd]]\n",
    "    results_ensemble = {}\n",
    "    for sub in subjects:\n",
    "        s = 0\n",
    "        existing_rounds = 0\n",
    "        for prev_rnd in range(rnd-rounds_back, rnd+1):\n",
    "#             print(\"rolling rnds\", prev_rnd)\n",
    "            if prev_rnd in all_results[model_to_average]:\n",
    "                s += all_results[model_to_average][prev_rnd][sub]\n",
    "                existing_rounds += 1\n",
    "        results_ensemble[sub] = s/existing_rounds\n",
    "#         print(\"Have found a rolling window of %d for the transfer model\" % existing_rounds)\n",
    "    return results_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(model_key, rnd):\n",
    "    results = {}\n",
    "    with open(\"data_server/response_run%s_rnd%d.json\" % (models_runs[model_key], rnd)) as f:\n",
    "        response = json.loads(f.read())\n",
    "        for line in response:\n",
    "            results[line['nick']] = line['score']\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results['lstm_seq'][20] = load_results('lstm_seq', 20)\n",
    "# results['lstm_seq'][40] = \n",
    "results# results['bert'][40] = load_results('bert', 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnds = range(500,600)\n",
    "decision_thresh = 0.5\n",
    "data_chunks = get_data_chunk(rnds)\n",
    "subjects = [d['nick'] for d in read_json_datapoint(\"data_server/data0.jl\")]\n",
    "# for key in ['transfer', 'ensemble']:\n",
    "for model_key in [\n",
    "                  'lstm_seq',\n",
    "                    'bert',\n",
    "                  'cnn_hierarch', \n",
    "                  'transfer', \n",
    "                  'ensemble',\n",
    "                ]:\n",
    "    print(model_key)\n",
    "    end_rnd = rnds[-1]\n",
    "#     if model_key=='lstm_seq':\n",
    "#         results[model_key][end_rnd]=load_results('lstm_seq', 20)\n",
    "    if model_key=='cnn_hierarch':\n",
    "        results[model_key][end_rnd]=load_results('cnn_hierarch', 40)\n",
    "    elif model_key=='bert':\n",
    "        results[model_key][end_rnd]=load_results('bert', end_rnd)\n",
    "    elif model_key=='ensemble':\n",
    "        model_keys_to_average=['bert', 'cnn_hierarch', 'lstm_seq']\n",
    "        missing_models = [m for m in model_keys_to_average if not results[m]]\n",
    "        if len(missing_models)!=0:\n",
    "            print(\"Missing models! cannot compute ensemble results\", missing_models)\n",
    "            continue\n",
    "        results_to_average = [results[m][end_rnd] for m in model_keys_to_average]\n",
    "#         results[model_key][end_rnd] = get_ensemble_results(rnd, results, \n",
    "#                                                 model_keys_to_average)\n",
    "        results[model_key][end_rnd] = get_ensemble_results(results_to_average)\n",
    "    ## For now\n",
    "    elif model_key=='transfer':\n",
    "        results[model_key][end_rnd]=get_transfer_results(\n",
    "            end_rnd, results, model_to_average='lstm_seq', rounds_back=60)\n",
    "#         results[model_key][end_rnd]=results['lstm_seq'][end_rnd]\n",
    "    ##\n",
    "    else:\n",
    "        with session_collection[model_key].as_default():\n",
    "            with session_collection[model_key].graph.as_default():\n",
    "                results[model_key][end_rnd] = predict_for_round_chunk(models_collection[model_key], \n",
    "                                              hyperparams_collection[model_key], hyperparams_features, \n",
    "                                              vocabulary_dict, \n",
    "                                          data_chunks, subjects=subjects, model_key=model_key, cache_round=499)\n",
    "\n",
    "    \n",
    "    print(len(results[model_key][end_rnd].values()), \"positive:\", \n",
    "      len([r for r in results[model_key][end_rnd].values() if r >=0.5]))\n",
    "    response1 = build_response(results[model_key][end_rnd], rnd=end_rnd, \n",
    "                               model_name=models_runs[model_key], decision_thresh=decision_thresh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['lstm_seq'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(list(results['transfer'][180].values())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "pearsonr(list(results['bert'][220].values()), list(results['bert'][180].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(list(results['lstm_seq'][160].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on eRisk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = {}\n",
    "# labels = {}\n",
    "# featuresall = {}\n",
    "# with session_collection['lstm_seq2'].as_default():\n",
    "#     with session_collection['lstm_seq2'].graph.as_default():\n",
    "        # for features, subjects, lbls in DataGenerator(user_level_data, subjects_split, \n",
    "        #                                          set_type='train', vocabulary=vocabulary_dict,\n",
    "        #                                        hierarchical=hyperparams1['hierarchical'],\n",
    "        #                                     seq_len=hyperparams1['maxlen'], batch_size=hyperparams1['batch_size'],\n",
    "        #                                          max_posts_per_user=None,\n",
    "        #                                        pad_with_duplication=False,\n",
    "        #                                         posts_per_group=hyperparams1['posts_per_group'],\n",
    "        #                                         post_groups_per_user=None, \n",
    "        #                                          sample_seqs=False, shuffle=False,\n",
    "        #                                                return_subjects=True):\n",
    "\n",
    "        #     predictions = loaded_model.predict_on_batch(features)\n",
    "        #     print(len(features[0]), len(subjects), len(predictions), len(labels), len(results))\n",
    "        #     for i,s in enumerate(subjects):\n",
    "        #         if s not in results:\n",
    "        #             results[s] = []\n",
    "        #             featuresall[s] = []\n",
    "        #         results[s].append(predictions[i].item())\n",
    "        #         featuresall[s].append([features[j][i] for j in range(len(features))])\n",
    "        #         labels[s] = lbls[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in results:\n",
    "    if not labels[subject]:\n",
    "        if np.std(results[subject])>0.0:\n",
    "            print(subject), print(results[subject][0], results[subject][-1]-results[subject][0])\n",
    "            pd.Series(results[subject]).rolling(window=5).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[featuresall[4278][i][0].sum() for i in range(len(featuresall[4278]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(user_level_data['subject4278']['raw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, s, y in DataGenerator(user_level_data, subjects_split, \n",
    "                                         set_type='test', vocabulary=vocabulary_dict,\n",
    "                                       hierarchical=hyperparams1['hierarchical'],\n",
    "                                    seq_len=hyperparams1['maxlen'], batch_size=hyperparams1['batch_size'],\n",
    "                                         max_posts_per_user=None,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams1['posts_per_group'],\n",
    "                                        post_groups_per_user=None, \n",
    "                                         sample_seqs=False, shuffle=False,\n",
    "                                               return_subjects=True):\n",
    "    print(\"subject\", s, \"features\", x[0].sum(axis=1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_key='lstm_seq'\n",
    "with session_collection[model_key].as_default():\n",
    "    with session_collection[model_key].graph.as_default():\n",
    "        res = models_collection[model_key].evaluate_generator(DataGenerator(user_level_data, subjects_split, \n",
    "                                              liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                         set_type='test', vocabulary=vocabulary_dict,\n",
    "                                       hierarchical=hyperparams_collection[model_key]['hierarchical'],\n",
    "                                    seq_len=hyperparams_collection[model_key]['maxlen'], \n",
    "                                    batch_size=hyperparams['batch_size'],\n",
    "                                         max_posts_per_user=None,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams_collection[model_key]['posts_per_group'],\n",
    "                                        post_groups_per_user=1,#None, \n",
    "                                         sample_seqs=False, shuffle=False,\n",
    "                                             compute_liwc=False))\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams_collection['cnn_bert'] = {'hierarchical': False, 'maxlen': 512, 'posts_per_group': 50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    (e, 'nrc') for e in emotions] + ['pers_pronouns'] + [\n",
    "    (c, 'liwc') for c in list(categories) if c in writings_df.columns] + [\n",
    "(st, 'stopword') for st in stopword_list]\n",
    "weights = model.get_layer('output_layer').get_weights()[0].tolist()[-(len(features)):]\n",
    "\n",
    "print(len(weights), len(features))\n",
    "feature_importance = {}\n",
    "for (i, f) in enumerate(features):\n",
    "    feature_importance[f] = weights[i][0]\n",
    "\n",
    "sorted(feature_importance.items(), key=lambda t: abs(t[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_point(subject, voc, hyperparams_features=hyperparams_features, nrc_lexicon=nrc_lexicon,\n",
    "                      emotions=emotions):\n",
    "    eval_writings_df = writings_df[writings_df['subject']==subject]\n",
    "    correct_label = eval_writings_df.label.values[0]\n",
    "    (x_train, y_train), (x_valid, y_valid), (x_test, y_test), voc = load_erisk_data(eval_writings_df,\n",
    "                        seq_len=hyperparams_features['maxlen'],\n",
    "                        voc_size=hyperparams_features['max_features'],\n",
    "                        emotion_lexicon=nrc_lexicon,\n",
    "                        emotions=emotions, user_level=False,\n",
    "                        train_prop=0.0, vocabulary=voc)\n",
    "    return x_test, y_test, correct_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_per_user(writings_df, majority_prop=0.2, train_prop=0.7, majority_nr=0, validate=False, voc=None,\n",
    "                    random=False, nr_slices=5, test_slice=2):\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    thresh=0.5\n",
    "    majority_proportion=majority_prop\n",
    "    valid_prop = 0.3\n",
    "    \n",
    "    if 'subset' in writings_df.columns:\n",
    "        training_subjects = list(set(writings_df[writings_df['subset']=='train'].subject))\n",
    "        test_subjects = list(set(writings_df[writings_df['subset']=='test'].subject))\n",
    "    else:\n",
    "        all_subjects = sorted(list(set(writings_df.subject)))\n",
    "        training_subjects_size = int(len(all_subjects) * train_prop)\n",
    "        test_subjects_size = len(all_subjects) - training_subjects_size\n",
    "        # Cross-validation, with fixed slice as input\n",
    "        test_prop = 1-train_prop\n",
    "        test_slice = min(test_slice, nr_slices)\n",
    "        logger.debug(\"start index: %f, from %f\\n\" % (\n",
    "            len(all_subjects)*(1/nr_slices)*test_slice, test_prop*test_slice))\n",
    "        start_slice = int(len(all_subjects)*(1/nr_slices)*test_slice)\n",
    "        test_subjects = all_subjects[start_slice: start_slice+test_subjects_size]\n",
    "        training_subjects = [s for s in all_subjects if s not in test_subjects]\n",
    "    training_subjects = sorted(training_subjects) # ensuring reproducibility\n",
    "    valid_subjects_size = int(len(training_subjects) * valid_prop)\n",
    "    valid_subjects = training_subjects[:valid_subjects_size]\n",
    "    training_subjects = training_subjects[valid_subjects_size:]\n",
    "    \n",
    "    if validate:\n",
    "        subjects = valid_subjects\n",
    "    else:\n",
    "        subjects = test_subjects\n",
    "    for subject in subjects:\n",
    "        x_test_user, y_test_user, label = get_data_for_point(subject, voc=voc)\n",
    "        outputs = model.predict(x_test_user)\n",
    "        if random:\n",
    "            sigma = np.std(outputs)\n",
    "            mu = np.mean(outputs)\n",
    "            print(\"generating random outputs with sigma\", sigma, \"and mu\", mu)\n",
    "            outputs = sigma*np.random.randn(len(outputs))+mu\n",
    "        positive_pred = sum(outputs>=thresh)\n",
    "        negative_pred = sum(outputs<thresh)\n",
    "        majority_pred = 0\n",
    "        if majority_proportion and positive_pred >= majority_proportion*negative_pred:\n",
    "            majority_pred = 1\n",
    "        if majority_nr and positive_pred>=majority_nr:\n",
    "            majority_pred = 1\n",
    "        if label == 1:\n",
    "            if majority_pred == 1:\n",
    "                tp+=1\n",
    "            else:\n",
    "                fn+=1\n",
    "        else:\n",
    "            if majority_pred == 0:\n",
    "                tn+=1\n",
    "            else:\n",
    "                fp+=1\n",
    "        print(negative_pred, positive_pred, majority_pred)\n",
    "        all_predictions.append(majority_pred)\n",
    "        all_labels.append(label)\n",
    "    def prec_recall_f1(tp, fp, tn, fn):\n",
    "        recall = tp/(tp+fn+0.0000001)\n",
    "        precision = tp/(tp+fp+0.0000001)\n",
    "        f1 = 2*precision*recall/(precision+recall+0.0000001)\n",
    "        print(\"Recall\", recall, \"Precision\", precision, \"F1\", f1)\n",
    "    if majority_prop:\n",
    "        print(\"Vote proportion\", majority_prop)\n",
    "    if majority_nr:\n",
    "        print(\"Vote points\", majority_nr)\n",
    "    prec_recall_f1(tp, fp, tn, fn)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_per_user(writings_df=writings_df, voc=voc, majority_prop=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_per_slice = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_slices=5\n",
    "logger.setLevel(logging.INFO)\n",
    "for tslice in range(nr_slices): \n",
    "    (x_train, y_train), (x_valid, y_valid), (x_test, y_test), voc = load_erisk_data(writings_df, \n",
    "                                                                seq_len=hyperparams_features['maxlen'],\n",
    "                                                                voc_size=hyperparams_features['max_features'],\n",
    "                                                               emotion_lexicon=nrc_lexicon,\n",
    "                                                               emotions=emotions,\n",
    "                                                               user_level=hyperparams_features['user_level'],\n",
    "                                                                                    test_slice=tslice,\n",
    "                                                                                    nr_slices=nr_slices,\n",
    "    #                                                            vocabulary=pickle.load(open('vocabulary20K_selfharm.pkl', 'rb'))\n",
    "                                                                                   logger=logger)\n",
    "    model, history = train_model(model, x_train, y_train, x_valid, y_valid,\n",
    "           epochs=200, batch_size=hyperparams['batch_size'],\n",
    "                      class_weight={0:0.5, 1:5}, start_epoch=0,\n",
    "                      callback_list = [freeze_layer, weights_history, reduce_lr],\n",
    "                      workers=2, verbose=0)\n",
    "    results_per_slice[tslice] = model.evaluate(x_test, y_test)\n",
    "    logger.info(\"Results for slice %d: %s\\n\" % (tslice, results_per_slice[tslice]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average F1 score: \", np.array([results_per_slice[s][1] for s in results_per_slice.keys()]).mean(),\n",
    "     \"all F1 scores: \", {s: v[1] for (s,v) in results_per_slice.items()} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tokens(row):\n",
    "    tokens = []\n",
    "    if row.tokenized_text:\n",
    "        tokens += row.tokenized_text\n",
    "    if row.tokenized_title:\n",
    "        tokens += row.tokenized_title\n",
    "    return tokens\n",
    "writings_df['all_tokens'] = writings_df.apply (lambda row: merge_tokens(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: include the title\n",
    "def extract_emotions(tokens, emotion, relative=True):\n",
    "    if not tokens:\n",
    "        return None\n",
    "    emotion_words = [t for t in tokens \n",
    "                     if t in nrc_lexicon[emotion]]\n",
    "    if relative and len(tokens):\n",
    "        return len(emotion_words) / len(tokens)\n",
    "    else:\n",
    "        return len(emotion_words)\n",
    "    \n",
    "    return encoded_emotions\n",
    "\n",
    "from functools import partial\n",
    "for emotion in emotions:\n",
    "    writings_df[emotion] = writings_df['all_tokens'].apply(partial(extract_emotions, emotion=emotion, \n",
    "                                                                   relative=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['pronouns'] = writings_df['all_tokens'].apply(partial(encode_pronouns, relative=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len'] + emotions].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len'] + emotions].groupby('label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentAnalyzer, SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid.polarity_scores(\"We are here today happiness is all around\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['neg_vader'] = writings_df.text.apply(lambda t: sid.polarity_scores(t)['neg']\n",
    "                                                 if type(t)==str else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['pos_vader'] = writings_df.text.apply(lambda t: sid.polarity_scores(t)['pos']\n",
    "                                                 if type(t)==str else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len', 'neg_vader', 'pos_vader'] + emotions].groupby('label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len', 'neg_vader', 'pos_vader'] + emotions].corr('spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from liwc_readDict import readDict\n",
    "\n",
    "liwc = readDict('/home/ana/resources/FakeOrFact/features/LIWC/LIWC/liwc.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [c for (w,c) in liwc]\n",
    "set(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_dict = {}\n",
    "for (w, c) in liwc:\n",
    "    if c not in liwc_dict:\n",
    "        liwc_dict[c] = []\n",
    "    liwc_dict[c].append(w)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_dict['pronoun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_liwc_categories(tokens, category_words, relative=True):\n",
    "    category_cnt = 0\n",
    "    if not tokens:\n",
    "        return None\n",
    "    text_len = len(tokens)\n",
    "    for t in tokens:\n",
    "        for word in category_words:\n",
    "            if t==word or (word[-1]=='*' and t.startswith(word[:-1])) \\\n",
    "            or (t==word.split(\"'\")[0]):\n",
    "                category_cnt += 1\n",
    "                break # one token cannot belong to more than one word in the category\n",
    "    if relative:\n",
    "        return category_cnt/text_len\n",
    "    else:\n",
    "        return category_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __encode_liwc_categories(self, tokens, relative=True):\n",
    "        categories_cnt = [0 for c in self.liwc_categories]\n",
    "        if not tokens:\n",
    "            return None\n",
    "        text_len = len(tokens)\n",
    "        for i, category in enumerate(self.liwc_categories):\n",
    "            for t in tokens:\n",
    "                print(\"token\", t)\n",
    "                if t in self.liwc_words_for_categories[category]:\n",
    "                    categories_cnt[i] += 1\n",
    "            if relative:\n",
    "                categories_cnt[i] = categories_cnt[i]/text_len\n",
    "        return categories_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from functools import partial\n",
    "# for categ in ['negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']:#liwc_dict.keys():\n",
    "for categ in liwc_dict.keys():\n",
    "    if categ in writings_df.columns:\n",
    "        continue\n",
    "    print(\"Encoding for category %s...\" % categ)\n",
    "    writings_df[categ] = writings_df['all_tokens'].apply(partial(encode_liwc_categories, \n",
    "                                                                   category_words=liwc_dict[categ], \n",
    "                                                                   relative=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# categories for all words in vocabulary\n",
    "# vocabulary = pickle.load(open(\"all_vocab_clpsych_erisk_50000.pkl\", \"rb\"))\n",
    "liwc_words_for_categories = {}\n",
    "for categ in liwc_dict.keys():\n",
    "    liwc_words_for_categories[categ] = set()\n",
    "    for word in liwc_dict[categ]:\n",
    "        for t in vocabulary:\n",
    "            if t==word or (word[-1]=='*' and t.startswith(word[:-1])) \\\n",
    "                or (t==word.split(\"'\")[0]):\n",
    "                    liwc_words_for_categories[categ].add(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(liwc_words_for_categories, open(\"data/liwc_categories_for_vocabulary_erisk_clpsych_stop_40K.pkl\", \"wb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df.groupby('subject').mean()[['label', 'negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df[['label', 'negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']].groupby('label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df.groupby('subject').mean()[['label'] + categories].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare your hyperparameters search:\n",
    "tune_epochs=150\n",
    "config = {\n",
    "      \"algorithm\": \"bayes\",\n",
    "      \"parameters\": {\n",
    "          \"lstm_units\": {\"type\": \"integer\", \"min\": 32, \"max\": 300},\n",
    "          \"lstm_units_user\": {\"type\": \"integer\", \"min\": 5, \"max\": 40},\n",
    "          \"dense_bow_units\": {\"type\": \"integer\", \"min\": 5, \"max\": 35},\n",
    "          \"filters\": {\"type\": \"integer\", \"min\": 30, \"max\": 250},\n",
    "          \"kernel_size\": {\"type\": \"integer\", \"min\": 3, \"max\": 7},\n",
    "          \"dense_sentence_units\": {\"type\": \"integer\", \"min\": 0, \"max\": 10},\n",
    "          \"dense_user_units\": {\"type\": \"integer\", \"min\": 0, \"max\": 0},\n",
    "          \"bert_dense_units\": {\"type\": \"integer\", \"min\": 10, \"max\": 200},\n",
    "          \"bert_finetune_layers\": {\"type\": \"integer\", \"min\": 0, \"max\": 2},\n",
    "          \"lr\": {\"type\": \"float\", \"min\": 0.0005, \"max\": 0.2, \"scalingType\": \"loguniform\"},\n",
    "          \"l2_dense\": {\"type\": \"float\", \"min\": 0.0000001, \"max\": 0.1, \"scalingType\": \"loguniform\"},\n",
    "          \"l2_embeddings\": {\"type\": \"float\", \"min\": 0.00000001, \"max\": 0.2, \"scalingType\": \"loguniform\"},\n",
    "          \"l2_bert\": {\"type\": \"float\", \"min\": 0.00000001, \"max\": 0.2, \"scalingType\": \"loguniform\"},\n",
    "          \"dropout\": {\"type\": \"float\", \"min\": 0, \"max\": 0.5, \"scalingType\": \"uniform\"},\n",
    "          \"norm_momentum\": {\"type\": \"float\", \"min\": 0, \"max\": 0.99, \"scalingType\": \"uniform\"},\n",
    "          \"optimizer\": {\"type\": \"categorical\", \"values\": [\"\"]},#\"adam\", \"adagrad\", \"\"]},\n",
    "          \"batch_size\": {\"type\": \"integer\", \"min\": 5, \"max\": 128, \"scalingType\": \"loguniform\"},\n",
    "          \"positive_class_weight\": {\"type\": \"integer\", \"min\": 3, \"max\": 10},\n",
    "          \"trainable_embeddings\": {\"type\": \"discrete\", \"values\": [True, False]},\n",
    "          \"sample_seqs\": {\"type\": \"discrete\", \"values\": [True, False]},\n",
    "          \"bert_trainable\": {\"type\": \"discrete\", \"values\": [True, False]},\n",
    "          \"bert_pooling\": {\"type\": \"categorical\", \"values\": ['first', 'mean']},\n",
    "#           \"hierarchical\": {\"type\": \"discrete\", \"values\": [True, False]},\n",
    "          \"freeze_patience\": {\"type\": \"integer\", \"min\": 2, \"max\": tune_epochs+1},\n",
    "          \"lr_reduce_factor\": {\"type\": \"float\", \"min\": 0.0001, \"max\": 0.8},\n",
    "          \"scheduled_lr_reduce_factor\": {\"type\": \"float\", \"min\": 0.0001, \"max\": 0.8},\n",
    "          \"lr_reduce_patience\": {\"type\": \"integer\", \"min\": 2, \"max\": tune_epochs+1},\n",
    "          \"scheduled_lr_reduce_patience\": {\"type\": \"integer\", \"min\": 2, \"max\": tune_epochs+1},\n",
    "          \"early_stopping_patience\": {\"type\": \"integer\", \"min\": 2, \"max\": tune_epochs+1},\n",
    "          \"decay\": {\"type\": \"float\", \"min\": 0.00000001, \"max\": 0.5, \"scalingType\": \"loguniform\"},\n",
    "#           \"ignore_layers_values\": {\"type\": \"categorical\", \"values\": [\"attention\", \"batchnorm\", \"bert_layer\"]},\n",
    "          \"sampling_distr\": {\"type\": \"categorical\", \"values\": [\"exp\", \"uniform\"]},\n",
    "          \"posts_per_group\": {\"type\": \"integer\", \"min\": 10, \"max\": 100},\n",
    "          \"post_groups_per_user\": {\"type\": \"integer\", \"min\": 1, \"max\": 50},\n",
    "#           \"posts_per_user\": {\"type\": \"integer\", \"min\": 0, \"max\": 1000},\n",
    "          \"maxlen\": {\"type\": \"integer\", \"min\":64, \"max\": 512},\n",
    "      },\n",
    "      \"spec\": {\n",
    "          \"metric\": \"loss\",\n",
    "          \"objective\": \"minimize\",\n",
    "      },\n",
    "  }\n",
    "optimizer = Optimizer(config, api_key=\"eoBdVyznAhfg3bK9pZ58ZSXfv\")\n",
    "\n",
    "hyperparams_config = hyperparams\n",
    "\n",
    "for experiment in optimizer.get_experiments(project_name=\"mental\"):\n",
    "    experiment.add_tag(\"tune\")\n",
    "    experiment.add_tag(dataset_type)\n",
    "    \n",
    "    print(hyperparams_config)\n",
    "    \n",
    "\n",
    "    data_generator_train = DataGenerator(user_level_data, subjects_split, set_type='train',\n",
    "                                         seq_len=hyperparams_config[\"maxlen\"],\n",
    "                                                     sample_seqs=hyperparams_config['sample_seqs'],\n",
    "                                                     sampling_distr=hyperparams_config['sampling_distr'],\n",
    "                                                    posts_per_group=hyperparams_config['posts_per_group'],\n",
    "                                                    post_groups_per_user=1,#hyperparams_config['post_groups_per_user'],\n",
    "                                                    max_posts_per_user=hyperparams_config['posts_per_user'],\n",
    "                                    hierarchical=hyperparams['hierarchical'])\n",
    "    data_generator_valid = DataGenerator(user_level_data, subjects_split, set_type='valid',\n",
    "                                         seq_len=hyperparams_config[\"maxlen\"],\n",
    "                                         \n",
    "                                                    posts_per_group=hyperparams_config['posts_per_group'],\n",
    "                                                    post_groups_per_user=hyperparams_config['post_groups_per_user'],\n",
    "                                                    max_posts_per_user=None,\n",
    "                                                    sample_seqs=False, \n",
    "                                                     shuffle=False,\n",
    "                                                hierarchical=hyperparams['hierarchical'])\n",
    "    try:\n",
    "        if hyperparams['hierarchical']:\n",
    "            model = build_hierarchical_model(hyperparams_config, hyperparams_features, embedding_matrix, emotions, stopword_list,\n",
    "                            liwc_categories=[c for c in categories if c in writings_df.columns]\n",
    "        ,\n",
    "                           ignore_layer=hyperparams_config['ignore_layer'])\n",
    "        else:\n",
    "            model = build_model(hyperparams_config, hyperparams_features, embedding_matrix, emotions, stopword_list,\n",
    "                            liwc_categories=[c for c in categories if c in writings_df.columns]\n",
    "        ,\n",
    "                           ignore_layer=hyperparams_config['ignore_layer'])\n",
    "        model.summary()\n",
    "\n",
    "        freeze_layer = FreezeLayer(model, patience=hyperparams['freeze_patience'], set_to=not hyperparams['trainable_embeddings'])\n",
    "        weights_history = WeightsHistory()\n",
    "        lr_history = LRHistory()\n",
    "        reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=hyperparams['reduce_lr_factor'],\n",
    "                                  patience=hyperparams['reduce_lr_patience'], min_lr=0.000001, verbose=1)\n",
    "        lr_schedule = callbacks.LearningRateScheduler(lambda epoch, lr: \n",
    "                                                      lr if (epoch+1)%hyperparams['scheduled_reduce_lr_freq']!=0 else\n",
    "                                                      lr*hyperparams['scheduled_reduce_lr_factor'], verbose=1)\n",
    "\n",
    "        model, history = train_model(model, hyperparams, data_generator_train, data_generator_valid,\n",
    "                           epochs=tune_epochs,\n",
    "                          class_weight={0:1, 1:experiment.get_parameter('positive_class_weight')}, \n",
    "                                     start_epoch=0,\n",
    "                          callback_list = [\n",
    "    #                                  weights_history, \n",
    "                                           reduce_lr, \n",
    "    #                                        lr_history, \n",
    "                                           lr_schedule\n",
    "                                          ],\n",
    "                          model_path='models/experiment', workers=4)\n",
    "    except tf.errors.ResourceExhaustedError as e:\n",
    "        print(e)\n",
    "        sess.close()\n",
    "        sess = tf.Session(config=sess_config)\n",
    "        initialize_vars(sess)\n",
    "\n",
    "    \n",
    "    loss = history.history['loss'][-1]\n",
    "    \n",
    "    \n",
    "    # Test the model\n",
    "    for param in config['parameters'].keys():    \n",
    "        hyperparams_config[param] = experiment.get_parameter(param)\n",
    "    if not hyperparams_config['optimizer']:\n",
    "        hyperparams_config['optimizer'] = optimizers.Adam(lr=hyperparams_config['lr'], \n",
    "                                   decay=hyperparams_config['decay'])\n",
    "#     hyperparams_config[\"ignore_layer\"] = []\n",
    "#     if hyperparams_config[\"ignore_layers_values\"]:\n",
    "#         hyperparams_config[\"ignore_layer\"] = [hyperparams_config[\"ignore_layers_values\"]]\n",
    "    hyperparams_config[\"ignore_layer\"] = [\"bert_layer\", \"batchnorm\"]\n",
    "        \n",
    "#     freeze_layer = FreezeLayer(model, patience=experiment.get_parameter('freeze_patience'),\n",
    "#                               set_to=not experiment.get_parameter('trainable_embeddings'))\n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            factor=experiment.get_parameter('lr_reduce_factor'),\n",
    "                                            patience=experiment.get_parameter('lr_reduce_patience'), \n",
    "                                            min_lr=0.00000001, verbose=1)\n",
    "    \n",
    "    # Report the loss, if not auto-logged:\n",
    "    experiment.log_metric(\"loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfconda",
   "language": "python",
   "name": "tfconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "362px",
    "width": "218px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
