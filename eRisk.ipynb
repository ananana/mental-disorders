{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob, os\n",
    "import numpy as np\n",
    "from comet_ml import Experiment, Optimizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_KERAS'] = '1'\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, CuDNNLSTM, Bidirectional, Input, concatenate\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import callbacks, optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# only reserve 1 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_subject_writings(subject_file):\n",
    "    writings = []\n",
    "    with open(subject_file) as sf:\n",
    "        contents = sf.read()\n",
    "        root = ET.fromstring(contents)\n",
    "        try:\n",
    "            subject = root.findall('ID')[0].text.strip()\n",
    "        except Exception:\n",
    "            print('Cannot extract ID', contents[:500], '\\n-------\\n')        \n",
    "        for w in root.iter('WRITING'):\n",
    "            subject_writings = {'subject': subject}\n",
    "            for title in w.findall('TITLE'):\n",
    "                subject_writings['title'] = title.text\n",
    "            for text in w.findall('TEXT'):\n",
    "                subject_writings['text'] = text.text\n",
    "            for date in w.findall('DATE'):\n",
    "                subject_writings['date'] = date.text\n",
    "            writings.append(subject_writings)\n",
    "    return writings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dir = '/home/bighanem/ana_data/' \n",
    "root_dir = '/home/ana/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eRisk 2020 T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir_T1 = root_dir + '/eRisk/data/eRisk2020_T1_train/eRISK2020_T1_training_data/eRISK2020_training_data/data/'\n",
    "labels_file_T1 = root_dir + '/eRisk/data//eRisk2020_T1_train/eRISK2020_T1_training_data/eRISK2020_training_data/golden_truth.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_2020(datadir_T1, labels_file_T1):\n",
    "    writings = []\n",
    "    for subject_file in os.listdir(datadir_T1):\n",
    "        print(subject_file)\n",
    "        writings.extend(read_subject_writings(os.path.join(datadir_T1, subject_file)))\n",
    "    writings_df = pd.DataFrame(writings)\n",
    "\n",
    "    labels_T1 = pd.read_csv(labels_file_T1, delimiter=' ', names=['subject', 'label'])\n",
    "    labels_T1 = labels_T1.set_index('subject')\n",
    "\n",
    "    writings_df['label'] = writings_df['subject'].apply(\n",
    "    lambda s: labels_T1.loc[s, 'label'])\n",
    "    \n",
    "    return writings_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eRisk 2019 T1 (Anorexia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadirs_T1_2019 = {\n",
    "    'train': ['2018 test/', '2018 train/positive_examples/', '2018 train/negative_examples/'],\n",
    "    'test': ['data/']\n",
    "}\n",
    "datadir_root_T1_2019 = {\n",
    "    'train': root_dir + '/eRisk/data/past/eRisk2019_T1/training data - t1/',\n",
    "    'test': root_dir + '/eRisk/data/past/eRisk2019_T1/test data - T1/'\n",
    "}\n",
    "    \n",
    "labels_files_T1_2019 = {\n",
    "    'train': ['2018 train/risk_golden_truth.txt', '2018 test/risk-golden-truth-test.txt'],\n",
    "    'test': ['T1_erisk_golden_truth.txt']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_2019(datadir_root_T1_2019,\n",
    "                   datadirs_T1_2019,\n",
    "                   labels_files_T1_2019,\n",
    "                   test_suffix='0000'):\n",
    "    writings = {'train': [], 'test': []}\n",
    "    writings_df = pd.DataFrame()\n",
    "    labels_df = pd.DataFrame()\n",
    "\n",
    "    for subset in ('train', 'test'):\n",
    "        for subdir in [os.path.join(datadir_root_T1_2019[subset], subp) for subp in datadirs_T1_2019[subset]]:\n",
    "            if subset=='train':\n",
    "                chunkdirs = [os.path.join(datadir_root_T1_2019[subset], subdir, chunkdir) \n",
    "                             for chunkdir in os.listdir(subdir)]\n",
    "            else:\n",
    "                chunkdirs = [os.path.join(datadir_root_T1_2019[subset], subdir)]\n",
    "                \n",
    "            for chunkdir in chunkdirs:\n",
    "                if not os.path.isdir(chunkdir):\n",
    "                    continue\n",
    "                for subject_file in os.listdir(chunkdir):\n",
    "                    writings[subset].extend(read_subject_writings(os.path.join(chunkdir, subject_file)))\n",
    "        writings_df_part = pd.DataFrame(writings[subset])\n",
    "        # add a suffix for users in the test -- the numbers are duplicated with the ones in train\n",
    "        if subset=='test':\n",
    "            writings_df_part['subject'] = writings_df_part['subject'].apply(lambda s: s+test_suffix)\n",
    "            print(subset, writings_df_part.subject)\n",
    "        writings_df_part['subset'] = subset\n",
    "        writings_df = pd.concat([writings_df, writings_df_part])\n",
    "        writings_df.reindex()\n",
    "\n",
    "        for label_file in labels_files_T1_2019[subset]:\n",
    "            labels = pd.read_csv(os.path.join(datadir_root_T1_2019[subset], label_file), \n",
    "                                 delimiter='\\s+', names=['subject', 'label'])\n",
    "            # add a suffix for users in the test -- the numbers are duplicated with the ones in train\n",
    "            if subset=='test':\n",
    "                labels['subject'] = labels['subject'].apply(lambda s: s+test_suffix)\n",
    "            labels_df = pd.concat([labels_df, labels])\n",
    "    labels_df = labels_df.drop_duplicates()\n",
    "    labels_df = labels_df.set_index('subject')\n",
    "\n",
    "    writings_df = writings_df.drop_duplicates()\n",
    "    \n",
    "    writings_df = writings_df.join(labels_df, on='subject')\n",
    "    \n",
    "    return writings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject8292.xml\n",
      "subject6644.xml\n",
      "subject7982.xml\n",
      "subject9260.xml\n",
      "subject9918.xml\n",
      "subject4284.xml\n",
      "subject9829.xml\n",
      "subject7661.xml\n",
      "subject8361.xml\n",
      "subject4831.xml\n",
      "subject2181.xml\n",
      "subject9077.xml\n",
      "subject2922.xml\n",
      "subject2238.xml\n",
      "subject4513.xml\n",
      "subject269.xml\n",
      "subject2678.xml\n",
      "subject9197.xml\n",
      "subject4143.xml\n",
      "subject2605.xml\n",
      "subject4226.xml\n",
      "subject7627.xml\n",
      "subject5150.xml\n",
      "subject4510.xml\n",
      "subject2182.xml\n",
      "subject280.xml\n",
      "subject1105.xml\n",
      "subject187.xml\n",
      "subject8001.xml\n",
      "subject9285.xml\n",
      "subject2621.xml\n",
      "subject4414.xml\n",
      "subject2685.xml\n",
      "subject9961.xml\n",
      "subject8065.xml\n",
      "subject8225.xml\n",
      "subject6866.xml\n",
      "subject9949.xml\n",
      "subject1507.xml\n",
      "subject8329.xml\n",
      "subject9411.xml\n",
      "subject7857.xml\n",
      "subject1545.xml\n",
      "subject9811.xml\n",
      "subject5000.xml\n",
      "subject4843.xml\n",
      "subject569.xml\n",
      "subject51.xml\n",
      "subject9156.xml\n",
      "subject6453.xml\n",
      "subject1210.xml\n",
      "subject5528.xml\n",
      "subject1485.xml\n",
      "subject5935.xml\n",
      "subject4527.xml\n",
      "subject3301.xml\n",
      "subject4074.xml\n",
      "subject6093.xml\n",
      "subject2088.xml\n",
      "subject8990.xml\n",
      "subject6459.xml\n",
      "subject7830.xml\n",
      "subject8395.xml\n",
      "subject4247.xml\n",
      "subject3667.xml\n",
      "subject5003.xml\n",
      "subject992.xml\n",
      "subject5644.xml\n",
      "subject242.xml\n",
      "subject7764.xml\n",
      "subject3283.xml\n",
      "subject6322.xml\n",
      "subject7678.xml\n",
      "subject6668.xml\n",
      "subject4333.xml\n",
      "subject1288.xml\n",
      "subject8200.xml\n",
      "subject5383.xml\n",
      "subject9039.xml\n",
      "subject7698.xml\n",
      "subject9652.xml\n",
      "subject5223.xml\n",
      "subject9725.xml\n",
      "subject1512.xml\n",
      "subject3994.xml\n",
      "subject7018.xml\n",
      "subject3644.xml\n",
      "subject1786.xml\n",
      "subject1027.xml\n",
      "subject8094.xml\n",
      "subject974.xml\n",
      "subject2947.xml\n",
      "subject9575.xml\n",
      "subject4570.xml\n",
      "subject5062.xml\n",
      "subject4729.xml\n",
      "subject5100.xml\n",
      "subject5177.xml\n",
      "subject505.xml\n",
      "subject5974.xml\n",
      "subject7499.xml\n",
      "subject1264.xml\n",
      "subject4071.xml\n",
      "subject7740.xml\n",
      "subject8721.xml\n",
      "subject9222.xml\n",
      "subject8432.xml\n",
      "subject2547.xml\n",
      "subject5995.xml\n",
      "subject6930.xml\n",
      "subject8472.xml\n",
      "subject6918.xml\n",
      "subject4198.xml\n",
      "subject501.xml\n",
      "subject7777.xml\n",
      "subject5375.xml\n",
      "subject7229.xml\n",
      "subject4762.xml\n",
      "subject5622.xml\n",
      "subject7637.xml\n",
      "subject47.xml\n",
      "subject1962.xml\n",
      "subject8795.xml\n",
      "subject4785.xml\n",
      "subject5840.xml\n",
      "subject3014.xml\n",
      "subject6464.xml\n",
      "subject522.xml\n",
      "subject5984.xml\n",
      "subject641.xml\n",
      "subject7326.xml\n",
      "subject4227.xml\n",
      "subject7428.xml\n",
      "subject203.xml\n",
      "subject6946.xml\n",
      "subject4563.xml\n",
      "subject682.xml\n",
      "subject9014.xml\n",
      "subject7435.xml\n",
      "subject8626.xml\n",
      "subject4459.xml\n",
      "subject733.xml\n",
      "subject7238.xml\n",
      "subject6428.xml\n",
      "subject7262.xml\n",
      "subject0.xml\n",
      "subject2269.xml\n",
      "subject8233.xml\n",
      "subject2522.xml\n",
      "subject5456.xml\n",
      "subject1064.xml\n",
      "subject8822.xml\n",
      "subject5033.xml\n",
      "subject1089.xml\n",
      "subject3277.xml\n",
      "subject5549.xml\n",
      "subject6352.xml\n",
      "subject6652.xml\n",
      "subject7669.xml\n",
      "subject5833.xml\n",
      "subject4795.xml\n",
      "subject4002.xml\n",
      "subject5878.xml\n",
      "subject1524.xml\n",
      "subject3928.xml\n",
      "subject9318.xml\n",
      "subject2935.xml\n",
      "subject1093.xml\n",
      "subject6786.xml\n",
      "subject3612.xml\n",
      "subject9114.xml\n",
      "subject4719.xml\n",
      "subject7439.xml\n",
      "subject1623.xml\n",
      "subject6290.xml\n",
      "subject8973.xml\n",
      "subject3844.xml\n",
      "subject7898.xml\n",
      "subject3605.xml\n",
      "subject2097.xml\n",
      "subject9381.xml\n",
      "subject3178.xml\n",
      "subject5908.xml\n",
      "subject3191.xml\n",
      "subject4196.xml\n",
      "subject8882.xml\n",
      "subject8845.xml\n",
      "subject5256.xml\n",
      "subject7318.xml\n",
      "subject4777.xml\n",
      "subject6309.xml\n",
      "subject4479.xml\n",
      "subject9393.xml\n",
      "subject4961.xml\n",
      "subject6247.xml\n",
      "subject1055.xml\n",
      "subject4644.xml\n",
      "subject7338.xml\n",
      "subject6284.xml\n",
      "subject5699.xml\n",
      "subject2580.xml\n",
      "subject2446.xml\n",
      "subject5409.xml\n",
      "subject1914.xml\n",
      "subject7263.xml\n",
      "subject5148.xml\n",
      "subject1793.xml\n",
      "subject9729.xml\n",
      "subject7952.xml\n",
      "subject9917.xml\n",
      "subject3868.xml\n",
      "subject5793.xml\n",
      "subject4934.xml\n",
      "subject3674.xml\n",
      "subject6019.xml\n",
      "subject2974.xml\n",
      "subject2857.xml\n",
      "subject855.xml\n",
      "subject5937.xml\n",
      "subject671.xml\n",
      "subject4318.xml\n",
      "subject5112.xml\n",
      "subject9249.xml\n",
      "subject7107.xml\n",
      "subject2996.xml\n",
      "subject5603.xml\n",
      "subject511.xml\n",
      "subject6518.xml\n",
      "subject5140.xml\n",
      "subject3737.xml\n",
      "subject9095.xml\n",
      "subject3227.xml\n",
      "subject7355.xml\n",
      "subject1617.xml\n",
      "subject6670.xml\n",
      "subject5387.xml\n",
      "subject3883.xml\n",
      "subject6146.xml\n",
      "subject2949.xml\n",
      "subject1763.xml\n",
      "subject2980.xml\n",
      "subject8933.xml\n",
      "subject6833.xml\n",
      "subject8802.xml\n",
      "subject8657.xml\n",
      "subject6259.xml\n",
      "subject1947.xml\n",
      "subject3635.xml\n",
      "subject8978.xml\n",
      "subject6423.xml\n",
      "subject1748.xml\n",
      "subject4702.xml\n",
      "subject8062.xml\n",
      "subject3555.xml\n",
      "subject2577.xml\n",
      "subject2475.xml\n",
      "subject8357.xml\n",
      "subject9492.xml\n",
      "subject3914.xml\n",
      "subject2495.xml\n",
      "subject7581.xml\n",
      "subject3725.xml\n",
      "subject6173.xml\n",
      "subject2247.xml\n",
      "subject8481.xml\n",
      "subject7946.xml\n",
      "subject7131.xml\n",
      "subject4848.xml\n",
      "subject747.xml\n",
      "subject5270.xml\n",
      "subject5979.xml\n",
      "subject6041.xml\n",
      "subject1950.xml\n",
      "subject7333.xml\n",
      "subject7247.xml\n",
      "subject814.xml\n",
      "subject5938.xml\n",
      "subject9160.xml\n",
      "subject6238.xml\n",
      "subject6957.xml\n",
      "subject8770.xml\n",
      "subject9497.xml\n",
      "subject807.xml\n",
      "subject6899.xml\n",
      "subject4014.xml\n",
      "subject2696.xml\n",
      "subject1885.xml\n",
      "subject8064.xml\n",
      "subject8081.xml\n",
      "subject2690.xml\n",
      "subject7462.xml\n",
      "subject8193.xml\n",
      "subject4526.xml\n",
      "subject7316.xml\n",
      "subject7290.xml\n",
      "subject463.xml\n",
      "subject4379.xml\n",
      "subject3181.xml\n",
      "subject5920.xml\n",
      "subject1728.xml\n",
      "subject2567.xml\n",
      "subject3904.xml\n",
      "subject4392.xml\n",
      "subject8581.xml\n",
      "subject9242.xml\n",
      "subject379.xml\n",
      "subject3881.xml\n",
      "subject8565.xml\n",
      "subject4505.xml\n",
      "subject3977.xml\n",
      "subject7489.xml\n",
      "subject2948.xml\n",
      "subject5342.xml\n",
      "subject8544.xml\n",
      "subject6903.xml\n",
      "subject7377.xml\n",
      "subject8769.xml\n",
      "subject3270.xml\n",
      "subject3224.xml\n",
      "subject2239.xml\n",
      "subject7801.xml\n",
      "subject3596.xml\n",
      "subject1469.xml\n",
      "subject4278.xml\n",
      "subject5282.xml\n",
      "subject3357.xml\n",
      "subject6013.xml\n",
      "subject5036.xml\n",
      "subject796.xml\n",
      "subject7692.xml\n",
      "subject7560.xml\n",
      "subject6035.xml\n",
      "subject1824.xml\n",
      "subject8726.xml\n",
      "subject6665.xml\n",
      "subject835.xml\n",
      "subject3117.xml\n",
      "subject519.xml\n",
      "subject1655.xml\n",
      "subject217.xml\n"
     ]
    }
   ],
   "source": [
    "writings_df = read_texts_2020(datadir_T1, labels_file_T1)\n",
    "# writings_df = read_texts_2019(datadir_root_T1_2019,\n",
    "#                    datadirs_T1_2019,\n",
    "#                    labels_files_T1_2019)\n",
    "# writings_df = pickle.load(open('writings_df_liwc3', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# writings_df[writings_df['subset']=='test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ana/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>If anyone could help with which sub to put thi...</td>\n",
       "      <td>2016-08-02 09:22:12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>I'm literally never gonna stop waiting...</td>\n",
       "      <td>2016-08-05 09:35:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>This is a really interesting study! Makes sens...</td>\n",
       "      <td>2016-08-05 21:36:24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>Mostly always me during this whole charade</td>\n",
       "      <td>2016-08-09 08:39:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>What is your best advice to a healthy, success...</td>\n",
       "      <td>2016-11-02 05:33:33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170664</th>\n",
       "      <td>subject217</td>\n",
       "      <td>scary</td>\n",
       "      <td>2018-06-24 14:26:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170668</th>\n",
       "      <td>subject217</td>\n",
       "      <td>rescuing man after his car got stuck on Rub' a...</td>\n",
       "      <td>2018-07-05 15:31:29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170680</th>\n",
       "      <td>subject217</td>\n",
       "      <td>Gorilla in streets of Riyadh,KSA</td>\n",
       "      <td>2018-07-24 22:22:48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170681</th>\n",
       "      <td>subject217</td>\n",
       "      <td>Gorilla in streets of Riyadh,KSA</td>\n",
       "      <td>2018-07-24 22:46:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170696</th>\n",
       "      <td>subject217</td>\n",
       "      <td>WHAT DID YOU SAY TO ME?</td>\n",
       "      <td>2018-08-20 10:54:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42757 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            subject                                              title  \\\n",
       "0       subject8292  If anyone could help with which sub to put thi...   \n",
       "1       subject8292          I'm literally never gonna stop waiting...   \n",
       "2       subject8292  This is a really interesting study! Makes sens...   \n",
       "4       subject8292         Mostly always me during this whole charade   \n",
       "7       subject8292  What is your best advice to a healthy, success...   \n",
       "...             ...                                                ...   \n",
       "170664   subject217                                              scary   \n",
       "170668   subject217  rescuing man after his car got stuck on Rub' a...   \n",
       "170680   subject217                   Gorilla in streets of Riyadh,KSA   \n",
       "170681   subject217                   Gorilla in streets of Riyadh,KSA   \n",
       "170696   subject217                            WHAT DID YOU SAY TO ME?   \n",
       "\n",
       "                       date text  label  \n",
       "0       2016-08-02 09:22:12  NaN      0  \n",
       "1       2016-08-05 09:35:55  NaN      0  \n",
       "2       2016-08-05 21:36:24  NaN      0  \n",
       "4       2016-08-09 08:39:41  NaN      0  \n",
       "7       2016-11-02 05:33:33  NaN      0  \n",
       "...                     ...  ...    ...  \n",
       "170664  2018-06-24 14:26:01  NaN      0  \n",
       "170668  2018-07-05 15:31:29  NaN      0  \n",
       "170680  2018-07-24 22:22:48  NaN      0  \n",
       "170681  2018-07-24 22:46:11  NaN      0  \n",
       "170696  2018-08-20 10:54:11  NaN      0  \n",
       "\n",
       "[42757 rows x 5 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[writings_df['text'].isna()][~writings_df['title'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f237f104d10>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "writings_df.label.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>If anyone could help with which sub to put thi...</td>\n",
       "      <td>2016-08-02 09:22:12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>I'm literally never gonna stop waiting...</td>\n",
       "      <td>2016-08-05 09:35:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>This is a really interesting study! Makes sens...</td>\n",
       "      <td>2016-08-05 21:36:24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>The only thing Frank is building ...</td>\n",
       "      <td>2016-08-07 23:35:23</td>\n",
       "      <td>... Is hype. Think about it, every time he wor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>Mostly always me during this whole charade</td>\n",
       "      <td>2016-08-09 08:39:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170693</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 11:29:21</td>\n",
       "      <td>this is my personal experience ,it may not ref...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170694</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 16:17:34</td>\n",
       "      <td>stop looking at 20 million saudis as one entit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170695</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 20:00:31</td>\n",
       "      <td>i am aware of stats now and then. i was just s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170696</th>\n",
       "      <td>subject217</td>\n",
       "      <td>WHAT DID YOU SAY TO ME?</td>\n",
       "      <td>2018-08-20 10:54:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170697</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-20 12:07:44</td>\n",
       "      <td>me smellz fish,me find no fish!...what the fuc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170698 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            subject                                              title  \\\n",
       "0       subject8292  If anyone could help with which sub to put thi...   \n",
       "1       subject8292          I'm literally never gonna stop waiting...   \n",
       "2       subject8292  This is a really interesting study! Makes sens...   \n",
       "3       subject8292               The only thing Frank is building ...   \n",
       "4       subject8292         Mostly always me during this whole charade   \n",
       "...             ...                                                ...   \n",
       "170693   subject217                                                NaN   \n",
       "170694   subject217                                                NaN   \n",
       "170695   subject217                                                NaN   \n",
       "170696   subject217                            WHAT DID YOU SAY TO ME?   \n",
       "170697   subject217                                                NaN   \n",
       "\n",
       "                       date  \\\n",
       "0       2016-08-02 09:22:12   \n",
       "1       2016-08-05 09:35:55   \n",
       "2       2016-08-05 21:36:24   \n",
       "3       2016-08-07 23:35:23   \n",
       "4       2016-08-09 08:39:41   \n",
       "...                     ...   \n",
       "170693  2018-08-19 11:29:21   \n",
       "170694  2018-08-19 16:17:34   \n",
       "170695  2018-08-19 20:00:31   \n",
       "170696  2018-08-20 10:54:11   \n",
       "170697  2018-08-20 12:07:44   \n",
       "\n",
       "                                                     text  label  \n",
       "0                                                     NaN      0  \n",
       "1                                                     NaN      0  \n",
       "2                                                     NaN      0  \n",
       "3       ... Is hype. Think about it, every time he wor...      0  \n",
       "4                                                     NaN      0  \n",
       "...                                                   ...    ...  \n",
       "170693  this is my personal experience ,it may not ref...      0  \n",
       "170694  stop looking at 20 million saudis as one entit...      0  \n",
       "170695  i am aware of stats now and then. i was just s...      0  \n",
       "170696                                                NaN      0  \n",
       "170697  me smellz fish,me find no fish!...what the fuc...      0  \n",
       "\n",
       "[170698 rows x 5 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def tokenize(t):\n",
    "    return tokenizer.tokenize(t.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'wasn', 't', 'ready', 'to', 'leave', 'buh', 'buw', 'dd', 'sasa']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"I wasn't ready to leave! buh-buw(dd). Sasa .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['tokenized_title'] = writings_df['title'].apply(lambda t: tokenize(t) if type(t)==str and t else None)\n",
    "writings_df['title_len'] = writings_df['tokenized_title'].apply(lambda t: len(t) if type(t)==list and t else None)\n",
    "writings_df['tokenized_text'] = writings_df['text'].apply(lambda t: tokenize(t) if type(t)==str and t else None)\n",
    "writings_df['text_len'] = writings_df['tokenized_text'].apply(lambda t: len(t) if type(t)==list and t else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    127604.00000\n",
       "mean         32.35415\n",
       "std          82.68303\n",
       "min           1.00000\n",
       "25%           6.00000\n",
       "50%          13.00000\n",
       "75%          31.00000\n",
       "max        7201.00000\n",
       "Name: text_len, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.text_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    49752.000000\n",
       "mean        10.701922\n",
       "std          9.282147\n",
       "min          1.000000\n",
       "25%          4.000000\n",
       "50%          8.000000\n",
       "75%         14.000000\n",
       "max        149.000000\n",
       "Name: title_len, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.title_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title_len</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subject0</th>\n",
       "      <td>0</td>\n",
       "      <td>20.285714</td>\n",
       "      <td>31.711712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1027</th>\n",
       "      <td>0</td>\n",
       "      <td>7.769231</td>\n",
       "      <td>1.190476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1055</th>\n",
       "      <td>0</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>79.983193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1064</th>\n",
       "      <td>1</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>68.410256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1089</th>\n",
       "      <td>0</td>\n",
       "      <td>9.823529</td>\n",
       "      <td>13.254902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9917</th>\n",
       "      <td>1</td>\n",
       "      <td>8.983607</td>\n",
       "      <td>95.806897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9918</th>\n",
       "      <td>0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.900901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject992</th>\n",
       "      <td>0</td>\n",
       "      <td>5.872928</td>\n",
       "      <td>19.914122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9949</th>\n",
       "      <td>0</td>\n",
       "      <td>10.609756</td>\n",
       "      <td>42.346979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9961</th>\n",
       "      <td>0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>26.389313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>340 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             label  title_len   text_len\n",
       "subject                                 \n",
       "subject0         0  20.285714  31.711712\n",
       "subject1027      0   7.769231   1.190476\n",
       "subject1055      0  16.666667  79.983193\n",
       "subject1064      1  13.000000  68.410256\n",
       "subject1089      0   9.823529  13.254902\n",
       "...            ...        ...        ...\n",
       "subject9917      1   8.983607  95.806897\n",
       "subject9918      0   5.000000  11.900901\n",
       "subject992       0   5.872928  19.914122\n",
       "subject9949      0  10.609756  42.346979\n",
       "subject9961      0   5.000000  26.389313\n",
       "\n",
       "[340 rows x 3 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.groupby('subject').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title_len</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>299</td>\n",
       "      <td>296</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>40</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date  title_len  text_len\n",
       "label                           \n",
       "0       299        296       299\n",
       "1        41         40        41"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.groupby('subject').max().groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of posts per user 146.35882352941175\n",
      "Average number of comments per user 376.2970588235294\n"
     ]
    }
   ],
   "source": [
    "print(\"Average number of posts per user\", writings_df.groupby('subject').count().title.mean())\n",
    "print(\"Average number of comments per user\", writings_df.groupby('subject').count().text.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>title_len</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>Okay friends so I messed up and posted to do a...</td>\n",
       "      <td>2017-04-25 22:37:57</td>\n",
       "      <td>Sorry for that, I truly didn't think it was go...</td>\n",
       "      <td>0</td>\n",
       "      <td>[okay, friends, so, i, messed, up, and, posted...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>[sorry, for, that, i, truly, didn, t, think, i...</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-09-16 06:29:13</td>\n",
       "      <td>You've got plenty of time to fix that. You can...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you, ve, got, plenty, of, time, to, fix, that...</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-11-24 01:33:22</td>\n",
       "      <td>LCD, Glass animals, Kendrick, The Weeknd, Jack...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[lcd, glass, animals, kendrick, the, weeknd, j...</td>\n",
       "      <td>127.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>Getting that coachella bod</td>\n",
       "      <td>2018-01-09 00:54:06</td>\n",
       "      <td>First I want to say whatever skin is your skin...</td>\n",
       "      <td>0</td>\n",
       "      <td>[getting, that, coachella, bod]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[first, i, want, to, say, whatever, skin, is, ...</td>\n",
       "      <td>149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-03-12 17:14:03</td>\n",
       "      <td>Not the same but me and my wife saw a man and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[not, the, same, but, me, and, my, wife, saw, ...</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170652</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-05-28 12:23:00</td>\n",
       "      <td>/r/keto /r/ketorecipes /r/ketodessert all are ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[r, keto, r, ketorecipes, r, ketodessert, all,...</td>\n",
       "      <td>197.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170653</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-05-28 12:32:36</td>\n",
       "      <td>its okay dont worry . as long as you don't exc...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[its, okay, dont, worry, as, long, as, you, do...</td>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170662</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-06-20 00:33:57</td>\n",
       "      <td>the national number is :1919 here are more com...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[the, national, number, is, 1919, here, are, m...</td>\n",
       "      <td>115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170693</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 11:29:21</td>\n",
       "      <td>this is my personal experience ,it may not ref...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[this, is, my, personal, experience, it, may, ...</td>\n",
       "      <td>153.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170695</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 20:00:31</td>\n",
       "      <td>i am aware of stats now and then. i was just s...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[i, am, aware, of, stats, now, and, then, i, w...</td>\n",
       "      <td>198.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7655 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            subject                                              title  \\\n",
       "122     subject8292  Okay friends so I messed up and posted to do a...   \n",
       "390     subject8292                                                NaN   \n",
       "498     subject8292                                                NaN   \n",
       "752     subject8292                         Getting that coachella bod   \n",
       "904     subject8292                                                NaN   \n",
       "...             ...                                                ...   \n",
       "170652   subject217                                                NaN   \n",
       "170653   subject217                                                NaN   \n",
       "170662   subject217                                                NaN   \n",
       "170693   subject217                                                NaN   \n",
       "170695   subject217                                                NaN   \n",
       "\n",
       "                       date  \\\n",
       "122     2017-04-25 22:37:57   \n",
       "390     2017-09-16 06:29:13   \n",
       "498     2017-11-24 01:33:22   \n",
       "752     2018-01-09 00:54:06   \n",
       "904     2018-03-12 17:14:03   \n",
       "...                     ...   \n",
       "170652  2018-05-28 12:23:00   \n",
       "170653  2018-05-28 12:32:36   \n",
       "170662  2018-06-20 00:33:57   \n",
       "170693  2018-08-19 11:29:21   \n",
       "170695  2018-08-19 20:00:31   \n",
       "\n",
       "                                                     text  label  \\\n",
       "122     Sorry for that, I truly didn't think it was go...      0   \n",
       "390     You've got plenty of time to fix that. You can...      0   \n",
       "498     LCD, Glass animals, Kendrick, The Weeknd, Jack...      0   \n",
       "752     First I want to say whatever skin is your skin...      0   \n",
       "904     Not the same but me and my wife saw a man and ...      0   \n",
       "...                                                   ...    ...   \n",
       "170652  /r/keto /r/ketorecipes /r/ketodessert all are ...      0   \n",
       "170653  its okay dont worry . as long as you don't exc...      0   \n",
       "170662  the national number is :1919 here are more com...      0   \n",
       "170693  this is my personal experience ,it may not ref...      0   \n",
       "170695  i am aware of stats now and then. i was just s...      0   \n",
       "\n",
       "                                          tokenized_title  title_len  \\\n",
       "122     [okay, friends, so, i, messed, up, and, posted...       34.0   \n",
       "390                                                  None        NaN   \n",
       "498                                                  None        NaN   \n",
       "752                       [getting, that, coachella, bod]        4.0   \n",
       "904                                                  None        NaN   \n",
       "...                                                   ...        ...   \n",
       "170652                                               None        NaN   \n",
       "170653                                               None        NaN   \n",
       "170662                                               None        NaN   \n",
       "170693                                               None        NaN   \n",
       "170695                                               None        NaN   \n",
       "\n",
       "                                           tokenized_text  text_len  \n",
       "122     [sorry, for, that, i, truly, didn, t, think, i...     120.0  \n",
       "390     [you, ve, got, plenty, of, time, to, fix, that...     104.0  \n",
       "498     [lcd, glass, animals, kendrick, the, weeknd, j...     127.0  \n",
       "752     [first, i, want, to, say, whatever, skin, is, ...     149.0  \n",
       "904     [not, the, same, but, me, and, my, wife, saw, ...     151.0  \n",
       "...                                                   ...       ...  \n",
       "170652  [r, keto, r, ketorecipes, r, ketodessert, all,...     197.0  \n",
       "170653  [its, okay, dont, worry, as, long, as, you, do...     109.0  \n",
       "170662  [the, national, number, is, 1919, here, are, m...     115.0  \n",
       "170693  [this, is, my, personal, experience, it, may, ...     153.0  \n",
       "170695  [i, am, aware, of, stats, now, and, then, i, w...     198.0  \n",
       "\n",
       "[7655 rows x 9 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[(~writings_df['text_len'].isnull()) & (writings_df['text_len'] > 100)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features and encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_features = {\n",
    "    \"max_features\": 20000,\n",
    "    # cut texts after this number of words\n",
    "    # (among top max_features most common words)\n",
    "    \"maxlen\": 100,\n",
    "    \"embedding_dim\": 100,\n",
    "    \"user_level\": True,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_NRC(nrc_path):\n",
    "    word_emotions = {}\n",
    "    emotion_words = {}\n",
    "    with open(nrc_path) as in_f:\n",
    "        for line in in_f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            word, emotion, label = line.split()\n",
    "            if word not in word_emotions:\n",
    "                word_emotions[word] = set()\n",
    "            if emotion not in emotion_words:\n",
    "                emotion_words[emotion] = set()\n",
    "            label = int(label)\n",
    "            if label:\n",
    "                word_emotions[word].add(emotion)\n",
    "                emotion_words[emotion].add(word)\n",
    "    return emotion_words\n",
    "\n",
    "nrc_lexicon_path = root_dir + '/resources/NRC-Sentiment-Emotion-Lexicons/NRC-Sentiment-Emotion-Lexicons/NRC-Emotion-Lexicon-v0.92/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'\n",
    "nrc_lexicon = load_NRC(nrc_lexicon_path)\n",
    "emotions = list(nrc_lexicon.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_emotions(tokens, emotion_lexicon, emotions, relative=True):\n",
    "    text_len = len(tokens)\n",
    "    encoded_emotions = [0 for e in emotions]\n",
    "    for i, emotion in enumerate(emotions):\n",
    "        try:\n",
    "            emotion_words = [t for t in tokens if t in emotion_lexicon[emotion]]\n",
    "            if relative:\n",
    "                encoded_emotions[i] = len(emotion_words) / len(tokens)\n",
    "            else:\n",
    "                encoded_emotions[i] = len(emotion_words)\n",
    "        except ValueError:\n",
    "            print(\"Emotion not found.\")\n",
    "    return encoded_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from liwc_readDict import readDict\n",
    "\n",
    "liwc = readDict(root_dir + '/resources/liwc.dic')\n",
    "\n",
    "categories = set([c for (w,c) in liwc])\n",
    "len(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Char n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(tokens):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Personal pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_person_pronouns = {\"i\", \"me\", \"my\", \"mine\", \"myself\"}\n",
    "def encode_pronouns(tokens, pronouns={\"i\", \"me\", \"my\", \"mine\", \"myself\"}, relative=True):\n",
    "    if not tokens:\n",
    "        return np.nan\n",
    "    text_len = len(tokens)\n",
    "    nr_pronouns = len([t for t in tokens if t in pronouns])\n",
    "    if relative:\n",
    "        return nr_pronouns/text_len\n",
    "    else:\n",
    "        return nr_pronouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words(\"english\")\n",
    "def encode_stopwords(tokens, stopwords=stopword_list):\n",
    "    encoded_stopwords = [0 for s in stopword_list]\n",
    "    if not tokens:\n",
    "        return encoded_stopwords\n",
    "    for i, stopword in enumerate(stopwords):\n",
    "        if stopword in tokens:\n",
    "            encoded_stopwords[i] += 1\n",
    "    return encoded_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def load_erisk_data(writings_df, voc_size, emotion_lexicon, seq_len, emotions =  \n",
    "                    ['anger', 'anticipation', 'disgust', 'fear', 'joy', \n",
    "                     'negative', 'positive', 'sadness', 'surprise', 'trust'],\n",
    "                    liwc_categories = categories,\n",
    "                    pronouns = [\"i\", \"me\", \"my\", \"mine\", \"myself\"],\n",
    "                    train_prop=0.7, valid_prop=0.3, min_post_len=3, min_word_len=1, \n",
    "                    user_level=True, vocabulary=None):\n",
    "    print(\"Loading data...\")\n",
    "    if not vocabulary:\n",
    "        vocabulary = {}\n",
    "        word_freqs = Counter()\n",
    "        for words in writings_df.tokenized_text:\n",
    "            word_freqs.update(words)\n",
    "        for words in writings_df.tokenized_title:\n",
    "            word_freqs.update(words)\n",
    "        i = 1\n",
    "        for w, f in word_freqs.most_common(voc_size-2): # keeping voc_size-1 for unk\n",
    "            if len(w) < min_word_len:\n",
    "                continue\n",
    "            vocabulary[w] = i\n",
    "            i += 1\n",
    "    tokens_data_train = []\n",
    "    categ_data_train = []\n",
    "    sparse_data_train = []\n",
    "    tokens_data_valid = []\n",
    "    categ_data_valid = []\n",
    "    sparse_data_valid = []\n",
    "    tokens_data_test = []\n",
    "    categ_data_test = []\n",
    "    sparse_data_test = []\n",
    "    labels_train = []\n",
    "    users_train = []\n",
    "    labels_valid = []\n",
    "    users_valid = []\n",
    "    users_test = []\n",
    "    labels_test = []\n",
    "    if 'subset' in writings_df.columns:\n",
    "        training_subjects = list(set(writings_df[writings_df['subset']=='train'].subject))\n",
    "        test_subjects = list(set(writings_df[writings_df['subset']=='test'].subject))\n",
    "    else:\n",
    "        all_subjects = sorted(list(set(writings_df.subject)))\n",
    "        training_subjects_size = int(len(all_subjects) * train_prop)\n",
    "        training_subjects = all_subjects[:training_subjects_size]\n",
    "        test_subjects = all_subjects[training_subjects_size:]\n",
    "    training_subjects = sorted(training_subjects) # ensuring reproducibility\n",
    "    valid_subjects_size = int(len(training_subjects) * valid_prop)\n",
    "    valid_subjects = training_subjects[:valid_subjects_size]\n",
    "    training_subjects = training_subjects[valid_subjects_size:]\n",
    "    categories = [c for c in liwc_categories if c in writings_df.columns]\n",
    "    print(len(training_subjects), \"training users, \", \n",
    "          len(valid_subjects), \"validation users, \",\n",
    "          len(test_subjects), \" test users.\")\n",
    "#     training_rows = writings_df[writings_df['subject'].isin(training_subjects)].sample(frac=1) # shuffling\n",
    "#     test_rows = writings_df[~writings_df['subject'].isin(training_subjects)].sample(frac=1)\n",
    "#     positive_training_users = training_rows.groupby('subject').max().groupby('label').count().date[1]\n",
    "#     positive_test_users = test_rows.groupby('subject').max().groupby('label').count().date[1]\n",
    "#     print(\"Positive training users: \", positive_training_users, \", positive test users: \", positive_test_users)\n",
    "    def encode_text(tokens):\n",
    "        # Using voc_size-1 value for OOV token\n",
    "        encoded_tokens = [vocabulary.get(w, voc_size-1) for w in tokens]\n",
    "        encoded_emotions = encode_emotions(tokens, emotion_lexicon, emotions)\n",
    "        encoded_pronouns = encode_pronouns(tokens, pronouns)\n",
    "        encoded_stopwords = encode_stopwords(tokens)\n",
    "        return (encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords)\n",
    "    user_level_texts = {}\n",
    "    for row in writings_df.itertuples():\n",
    "        words = []\n",
    "        if row.tokenized_title:\n",
    "            words.extend(row.tokenized_title)\n",
    "        if row.tokenized_text:\n",
    "            words.extend(row.tokenized_text)\n",
    "        if not words or len(words)<min_post_len:\n",
    "            continue\n",
    "        label = row.label\n",
    "        liwc_categs = [getattr(row, categ) for categ in categories]\n",
    "        if row.subject not in user_level_texts.keys():\n",
    "            user_level_texts[row.subject] = {}\n",
    "            user_level_texts[row.subject]['texts'] = [words]\n",
    "            user_level_texts[row.subject]['label'] = label\n",
    "            user_level_texts[row.subject]['liwc'] = [liwc_categs]\n",
    "        else:\n",
    "            user_level_texts[row.subject]['texts'].append(words) # TODO: sort datapoints chronologically\n",
    "            user_level_texts[row.subject]['liwc'].append(liwc_categs)\n",
    "\n",
    "    for subject in user_level_texts.keys():\n",
    "        texts = user_level_texts[subject]['texts']\n",
    "        label = user_level_texts[subject]['label']\n",
    "        if user_level:\n",
    "            all_words = [sum(texts, [])] # merge all texts in one list\n",
    "            liwc_aggreg = [np.array(user_level_texts[subject]['liwc']).mean(axis=0).tolist()]\n",
    "        else:\n",
    "            all_words = texts\n",
    "            liwc_aggreg = user_level_texts[subject]['liwc']\n",
    "        for i, words in enumerate(all_words):\n",
    "            encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords = encode_text(words)\n",
    "            subject_id = int(subject.split('t')[1])\n",
    "            if subject in training_subjects:\n",
    "                tokens_data_train.append(encoded_tokens)\n",
    "                categ_data_train.append(encoded_emotions + [encoded_pronouns] + liwc_aggreg[i])\n",
    "                sparse_data_train.append(encoded_stopwords)\n",
    "                labels_train.append(label)\n",
    "                users_train.append(subject_id)\n",
    "            elif subject in valid_subjects:\n",
    "                tokens_data_valid.append(encoded_tokens)\n",
    "                categ_data_valid.append(encoded_emotions + [encoded_pronouns]  + liwc_aggreg[i])\n",
    "                sparse_data_valid.append(encoded_stopwords)\n",
    "                labels_valid.append(label)\n",
    "                users_valid.append(subject_id)\n",
    "            else:\n",
    "                tokens_data_test.append(encoded_tokens)\n",
    "                categ_data_test.append(encoded_emotions + [encoded_pronouns] + liwc_aggreg[i])\n",
    "                sparse_data_test.append(encoded_stopwords)\n",
    "                labels_test.append(label)\n",
    "                users_test.append(subject_id)\n",
    "        \n",
    "    # using zeros for padding\n",
    "    tokens_data_train_padded = sequence.pad_sequences(tokens_data_train, maxlen=seq_len)\n",
    "    tokens_data_valid_padded = sequence.pad_sequences(tokens_data_valid, maxlen=seq_len)\n",
    "    tokens_data_test_padded = sequence.pad_sequences(tokens_data_test, maxlen=seq_len)\n",
    "        \n",
    "    return ([np.array(tokens_data_train_padded), np.array(categ_data_train), np.array(sparse_data_train),\n",
    "            np.array(users_train)],\n",
    "            np.array(labels_train)), \\\n",
    "            ([np.array(tokens_data_valid_padded), np.array(categ_data_valid), np.array(sparse_data_valid),\n",
    "            np.array(users_valid)],\n",
    "            np.array(labels_valid)), \\\n",
    "            ([np.array(tokens_data_test_padded), np.array(categ_data_test), np.array(sparse_data_test),\n",
    "             np.array(users_test)],\n",
    "             np.array(labels_test)), vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "166 training users,  71 validation users,  103  test users.\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_valid, y_valid), (x_test, y_test), voc = load_erisk_data(writings_df, \n",
    "                                                            seq_len=hyperparams_features['maxlen'],\n",
    "                                                            voc_size=hyperparams_features['max_features'],\n",
    "                                                           emotion_lexicon=nrc_lexicon,\n",
    "                                                           emotions=emotions,\n",
    "                                                           user_level=hyperparams_features['user_level'],\n",
    "#                                                            vocabulary=pickle.load(open('vocabulary20K_selfharm.pkl', 'rb'))\n",
    "                                                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166 train sequences\n",
      "71 train sequences\n",
      "103 test sequences\n"
     ]
    }
   ],
   "source": [
    "x_train_seq, x_train_categ, x_train_sparse, x_train_users = x_train\n",
    "x_valid_seq, x_valid_categ, x_valid_sparse, x_valid_users = x_valid\n",
    "x_test_seq, x_test_categ, x_test_sparse, x_test_users = x_test\n",
    "print(len(x_train_seq), 'train sequences')\n",
    "print(len(x_valid_seq), 'train sequences')\n",
    "print(len(x_test_seq), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 positive training examples\n",
      "5 positive validation examples\n",
      "17 positive test examples\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(y_train).sum(), \"positive training examples\")\n",
    "print(pd.Series(y_valid).sum(), \"positive validation examples\")\n",
    "\n",
    "print(pd.Series(y_test).sum(), \"positive test examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 5537,   185,     7, ...,    64,    11, 11012],\n",
       "        [    2,   121,     4, ...,  1495,   436, 19999],\n",
       "        [  250,     3,    89, ..., 19999,   829,   207],\n",
       "        ...,\n",
       "        [   27,    32,    21, ...,   240,  1425,  2193],\n",
       "        [  230,     7,     8, ...,     3,   340,    10],\n",
       "        [    2,    32,    15, ...,     4,   324, 19999]], dtype=int32),\n",
       " array([[0.01368948, 0.02103506, 0.00834725, ..., 0.01068447, 0.02838063,\n",
       "         0.02337229],\n",
       "        [0.01486989, 0.04089219, 0.01115242, ..., 0.01115242, 0.04460967,\n",
       "         0.05576208],\n",
       "        [0.01768878, 0.02106219, 0.01037972, ..., 0.01851051, 0.0350532 ,\n",
       "         0.01448837],\n",
       "        ...,\n",
       "        [0.01091779, 0.019756  , 0.00713989, ..., 0.00849161, 0.02526688,\n",
       "         0.05580202],\n",
       "        [0.01725149, 0.01764255, 0.00809181, ..., 0.00976131, 0.0382932 ,\n",
       "         0.00753531],\n",
       "        [0.01476301, 0.02797203, 0.01087801, ..., 0.01709402, 0.03807304,\n",
       "         0.05749806]]),\n",
       " array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 0, 1, ..., 0, 1, 0],\n",
       "        [1, 1, 1, ..., 0, 1, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 0, 1, 0],\n",
       "        [1, 1, 1, ..., 0, 1, 0],\n",
       "        [1, 1, 1, ..., 0, 1, 0]]),\n",
       " array([6644, 4284, 4831, 4513, 4143, 4226, 5150, 4510, 4414, 6866, 5000,\n",
       "        4843,  569,   51, 6453, 5528, 5935, 4527, 3301, 4074, 6093, 6459,\n",
       "        4247, 3667, 5003, 5644, 3283, 6322, 6668, 4333, 5383, 5223, 3994,\n",
       "        7018, 3644, 4570, 5062, 4729, 5100, 5177,  505, 5974, 4071, 5995,\n",
       "        6930, 6918, 4198,  501, 5375, 7229, 4762, 5622,   47, 4785, 5840,\n",
       "        6464,  522, 5984,  641, 7326, 4227, 6946, 4563,  682, 4459, 7238,\n",
       "        6428, 7262, 5456, 5033, 3277, 5549, 6352, 6652, 5833, 4795, 4002,\n",
       "        5878, 3928, 6786, 3612, 4719, 6290, 3844, 3605, 5908, 3191, 4196,\n",
       "        5256, 7318, 4777, 6309, 4479, 4961, 6247, 4644, 6284, 5699, 5409,\n",
       "        7263, 5148, 3868, 5793, 4934, 3674, 6019, 5937,  671, 4318, 5112,\n",
       "        7107, 5603,  511, 6518, 5140, 3737, 3227, 6670, 5387, 3883, 6146,\n",
       "        6833, 6259, 3635, 6423, 4702, 3555, 3914, 3725, 6173, 7131, 4848,\n",
       "        5270, 5979, 6041, 7247, 5938, 6238, 6957, 6899, 4014, 4526, 7316,\n",
       "        7290,  463, 4379, 5920, 3904, 4392,  379, 3881, 4505, 3977, 5342,\n",
       "        6903, 3270, 3224, 3596, 4278, 5282, 3357, 6013, 5036, 6035, 6665,\n",
       "         519])]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.56462585, 4.36842105])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 20000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "def load_embeddings(path, embedding_dim, voc):\n",
    "    # random matrix with mean value = 0\n",
    "    embedding_matrix = np.random.random((len(voc)+2, embedding_dim)) - 0.5 # voc + unk + pad value(0)\n",
    "#     embedding_matrix = np.zeros((len(voc)+1, embedding_dim))\n",
    "\n",
    "    f = open(path)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_i = voc.get(word)\n",
    "        if word_i is not None:\n",
    "            embedding_matrix[word_i] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print('Total %s word vectors.' % len(embedding_matrix))\n",
    "\n",
    " \n",
    "    return embedding_matrix\n",
    "\n",
    "# pretrained_embeddings_path = '/home/ana/resources/glove.6B/glove.6B.%dd.txt' % hyperparams_features['embedding_dim']\n",
    "pretrained_embeddings_path = root_dir + '/resources/glove.twitter.27B/glove.twitter.27B.%dd.txt' % hyperparams_features['embedding_dim']\n",
    "embedding_matrix = load_embeddings(pretrained_embeddings_path, hyperparams_features['embedding_dim'], voc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166, 179)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'lstm_units': 100,\n",
    "    'dense_bow_units': 5,\n",
    "    'dropout': 0.14,\n",
    "    'l2_dense': 0.00011,\n",
    "    'l2_embeddings': 0.00001,\n",
    "    'optimizer': 'adam',\n",
    "    'decay': 0.0001,\n",
    "    'lr': 0.001,\n",
    "    \"batch_size\": 32,\n",
    "    \"trainable_embeddings\": False,\n",
    "    \"reduce_lr_factor\": 0.02,\n",
    "    \"reduce_lr_patience\": 50,\n",
    "    \"freeze_patience\": 50,\n",
    "    'threshold': 0.5,\n",
    "\n",
    "\n",
    "}\n",
    "if not hyperparams['optimizer']:\n",
    "    hyperparams['optimizer'] = optimizers.Adam(lr=hyperparams['lr'], #beta_1=0.9, beta_2=0.999, epsilon=0.0001,\n",
    "                                   decay=hyperparams['decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics():\n",
    "    def __init__(self, threshold=0.5):\n",
    "        self.threshold=threshold\n",
    "        \n",
    "    def recall_m(self, y_true, y_pred):\n",
    "            y_labels = y_true#tf.reshape(y_true[:], (-1,1))\n",
    "    #         y_pred = tf.reshape(y_pred, (-1,1))\n",
    "            y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), self.threshold), K.floatx())        \n",
    "            possible_positives = K.sum(K.round(K.clip(y_labels, 0, 1)))\n",
    "            true_positives = K.sum(K.round(K.clip(y_labels * y_pred, 0, 1)))\n",
    "            recall = true_positives / (possible_positives + K.epsilon())\n",
    "            return recall\n",
    "\n",
    "    def precision_m(self, y_true, y_pred):\n",
    "            y_labels = y_true#tf.reshape(y_true[:], (-1,1))\n",
    "    #         y_pred = tf.reshape(y_pred, (-1,1))\n",
    "            y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), self.threshold), K.floatx())        \n",
    "            true_positives = K.sum(K.round(K.clip(y_labels * y_pred, 0, 1)))\n",
    "            predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "            precision = true_positives / (predicted_positives + K.epsilon())\n",
    "            return precision\n",
    "\n",
    "    def f1_m(self, y_true, y_pred):\n",
    "        precision = self.precision_m(y_true, y_pred)\n",
    "        recall = self.recall_m(y_true, y_pred)\n",
    "        return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def binary_crossentropy_custom(y_true, y_pred):\n",
    "    y_labels = y_true#tf.reshape(y_true[0],(1,-1))\n",
    "    return K.binary_crossentropy(y_labels, \n",
    "                                 y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopwords_list,\n",
    "                liwc_categories,\n",
    "               ignore_layer=[]):\n",
    "\n",
    "    tokens_features = Input(shape=(hyperparams_features['maxlen'],), name='word_seq')\n",
    "    embedding_layer = Embedding(hyperparams_features['max_features'], \n",
    "                                hyperparams_features['embedding_dim'], \n",
    "                                input_length=hyperparams_features['maxlen'],\n",
    "                                mask_zero=True,\n",
    "                                embeddings_regularizer=regularizers.l2(hyperparams['l2_embeddings']),\n",
    "                                weights=[embedding_matrix], \n",
    "                                trainable=hyperparams['trainable_embeddings'],\n",
    "                               name='embeddings_layer')(\n",
    "        tokens_features)\n",
    "#     lstm_layers = Bidirectional(LSTM(hyperparams['lstm_units']))(embedding_layer)\n",
    "    lstm_layers = CuDNNLSTM(hyperparams['lstm_units'], \n",
    "#                             dropout=hyperparams['dropout'],\n",
    "#                       recurrent_dropout=hyperparams['dropout'],\n",
    "                      name='LSTM_layer')(embedding_layer)\n",
    "    dropout_layer = Dropout(hyperparams['dropout'], name='lstm_wdropout')(lstm_layers)\n",
    "    numerical_features = Input(shape=(len(emotions) + 1 + len(liwc_categories),), name='numeric_input') # emotions and pronouns\n",
    "    dense_layer = Dense(units=1,\n",
    "                        kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                        name='numerical_dense_layer',\n",
    "                       )(numerical_features)\n",
    "    sparse_features = Input(shape=(len(stopwords_list),), name='sparse_input') # stopwords\n",
    "    subjects = Input(shape=(1,), name='subjects')\n",
    "    dense_layer_sparse = Dense(units=hyperparams['dense_bow_units'],\n",
    "                              name='sparse_feat_dense_layer',\n",
    "                                kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                              )(sparse_features)\n",
    "#     # TODO: this is getting out of hand. Refactor this ablation part.\n",
    "    all_layers = {\n",
    "        'lstm_layers': dropout_layer,\n",
    "        'numerical_dense_layer': numerical_features,# dense_layer,\n",
    "        'sparse_feat_dense_layer': sparse_features#dense_layer_sparse,\n",
    "    }\n",
    "    layers_to_merge = []\n",
    "    for n, l in all_layers.items():\n",
    "        if n in ignore_layer:\n",
    "            continue\n",
    "        layers_to_merge.append(l)\n",
    "        \n",
    "    if len(layers_to_merge) == 1:\n",
    "        merged_layers = layers_to_merge[0]\n",
    "    else:\n",
    "        merged_layers = concatenate(layers_to_merge)\n",
    "    output_layer = Dense(1, activation='sigmoid',\n",
    "                         name='output_layer',\n",
    "                        kernel_regularizer=regularizers.l2(hyperparams['l2_dense']))(merged_layers)\n",
    "\n",
    "    # Compile model\n",
    "    model = Model(inputs=[tokens_features, numerical_features, sparse_features, subjects], \n",
    "                  outputs=output_layer)\n",
    "    metrics_class = Metrics(threshold=hyperparams['threshold'])\n",
    "    model.compile(hyperparams['optimizer'], binary_crossentropy_custom,\n",
    "                  metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "numeric_input (InputLayer)      (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sparse_input (InputLayer)       (None, 179)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 190)          0           numeric_input[0][0]              \n",
      "                                                                 sparse_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 1)            191         concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 191\n",
      "Trainable params: 191\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopword_list,\n",
    "                    liwc_categories=[c for c in categories if c in writings_df.columns]\n",
    ",\n",
    "                   ignore_layer=['lstm_layers'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, 'models/lstm_plus_ablated3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary:\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     url: https://www.comet.ml/ananana/mental/be350999b7a24760a3cfdad7240a257f\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     batch_f1_m [501]        : (0.0, 0.6666666269302368)\n",
      "COMET INFO:     batch_loss [501]        : (0.31647342443466187, 0.9732389450073242)\n",
      "COMET INFO:     batch_precision_m [501] : (0.0, 0.5)\n",
      "COMET INFO:     batch_recall_m [501]    : (0.0, 1.0)\n",
      "COMET INFO:     epoch_duration [501]    : (0.03274282399797812, 1.448123376001604)\n",
      "COMET INFO:     f1_m [501]              : (0.0, 0.42467594146728516)\n",
      "COMET INFO:     loss [501]              : (0.4481326693511871, 1.1047792305429298)\n",
      "COMET INFO:     lr [501]                : (9.999999974752427e-07, 0.0010000000474974513)\n",
      "COMET INFO:     precision_m [501]       : (0.0, 0.320507675409317)\n",
      "COMET INFO:     recall_m [501]          : (0.0, 1.0)\n",
      "COMET INFO:     step                    : 3006\n",
      "COMET INFO:     sys.cpu.percent.01 [40] : (7.9, 51.2)\n",
      "COMET INFO:     sys.cpu.percent.02 [40] : (7.8, 32.5)\n",
      "COMET INFO:     sys.cpu.percent.03 [40] : (8.8, 33.2)\n",
      "COMET INFO:     sys.cpu.percent.04 [40] : (8.8, 33.3)\n",
      "COMET INFO:     sys.cpu.percent.avg [40]: (9.5, 36.825)\n",
      "COMET INFO:     sys.gpu.0.total_memory  : (1073414144.0, 1073414144.0)\n",
      "COMET INFO:     sys.load.avg [40]       : (0.27, 1.33)\n",
      "COMET INFO:     sys.ram.total [40]      : (8277307392.0, 8277307392.0)\n",
      "COMET INFO:     sys.ram.used [40]       : (3960983552.0, 4544270336.0)\n",
      "COMET INFO:     val_f1_m [501]          : (0.0, 0.42727264761924744)\n",
      "COMET INFO:     val_loss [501]          : (0.4405082724463772, 1.1506741147645763)\n",
      "COMET INFO:     val_precision_m [501]   : (0.0, 0.38627445697784424)\n",
      "COMET INFO:     val_recall_m [501]      : (0.0, 1.0)\n",
      "COMET INFO:   Other [count]:\n",
      "COMET INFO:     trainable_params: 191\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     git-patch  : 1\n",
      "COMET INFO:     histogram3d: 502\n",
      "COMET INFO: ----------------------------\n",
      "COMET INFO: old comet version (3.0.2) detected. current: 3.0.3 please update your comet lib with command: `pip install --no-cache-dir --upgrade comet_ml`\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/ananana/mental/0f17afe011eb4bdaa42517a37012cc20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(api_key=\"eoBdVyznAhfg3bK9pZ58ZSXfv\",\n",
    "                        project_name=\"mental\", workspace=\"ananana\")\n",
    "\n",
    "experiment.log_parameters(hyperparams_features)\n",
    "\n",
    "experiment.log_parameter('emotion_lexicon', nrc_lexicon_path)\n",
    "experiment.log_parameter('emotions', emotions)\n",
    "experiment.log_parameter('embeddings_path', pretrained_embeddings_path)\n",
    "\n",
    "experiment.log_parameters(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'i': 2,\n",
       " 'to': 3,\n",
       " 'a': 4,\n",
       " 'and': 5,\n",
       " 'of': 6,\n",
       " 'it': 7,\n",
       " 'in': 8,\n",
       " 'you': 9,\n",
       " 'that': 10,\n",
       " 'is': 11,\n",
       " 's': 12,\n",
       " 'for': 13,\n",
       " 'this': 14,\n",
       " 't': 15,\n",
       " 'on': 16,\n",
       " 'with': 17,\n",
       " 'but': 18,\n",
       " 'my': 19,\n",
       " '8217': 20,\n",
       " 'be': 21,\n",
       " 'was': 22,\n",
       " 'have': 23,\n",
       " 'are': 24,\n",
       " 'not': 25,\n",
       " 'they': 26,\n",
       " 'as': 27,\n",
       " 'if': 28,\n",
       " 'so': 29,\n",
       " 'just': 30,\n",
       " 'what': 31,\n",
       " 'can': 32,\n",
       " 'like': 33,\n",
       " 'he': 34,\n",
       " 'or': 35,\n",
       " 'at': 36,\n",
       " 'we': 37,\n",
       " 'me': 38,\n",
       " 'from': 39,\n",
       " 'your': 40,\n",
       " 'm': 41,\n",
       " 'do': 42,\n",
       " 'com': 43,\n",
       " 'all': 44,\n",
       " 'about': 45,\n",
       " 'an': 46,\n",
       " 'one': 47,\n",
       " 'there': 48,\n",
       " 'would': 49,\n",
       " 'out': 50,\n",
       " 'up': 51,\n",
       " 'when': 52,\n",
       " 'more': 53,\n",
       " 'get': 54,\n",
       " 'don': 55,\n",
       " 'people': 56,\n",
       " 'by': 57,\n",
       " 'will': 58,\n",
       " 'no': 59,\n",
       " 'how': 60,\n",
       " 'https': 61,\n",
       " 'gt': 62,\n",
       " 'has': 63,\n",
       " 'them': 64,\n",
       " 'his': 65,\n",
       " 'time': 66,\n",
       " 'some': 67,\n",
       " 're': 68,\n",
       " 'know': 69,\n",
       " 'think': 70,\n",
       " 'who': 71,\n",
       " 'their': 72,\n",
       " 'because': 73,\n",
       " 'had': 74,\n",
       " 'she': 75,\n",
       " 'here': 76,\n",
       " 'good': 77,\n",
       " 'really': 78,\n",
       " 'www': 79,\n",
       " 'r': 80,\n",
       " 'now': 81,\n",
       " 've': 82,\n",
       " 'been': 83,\n",
       " 'only': 84,\n",
       " 'her': 85,\n",
       " 'also': 86,\n",
       " 'were': 87,\n",
       " 'than': 88,\n",
       " 'see': 89,\n",
       " 'any': 90,\n",
       " 'http': 91,\n",
       " 'even': 92,\n",
       " 'make': 93,\n",
       " 'other': 94,\n",
       " 'then': 95,\n",
       " '128056': 96,\n",
       " 'much': 97,\n",
       " '1': 98,\n",
       " 'which': 99,\n",
       " 'him': 100,\n",
       " 'could': 101,\n",
       " 'go': 102,\n",
       " '2': 103,\n",
       " 'first': 104,\n",
       " 'd': 105,\n",
       " 'want': 106,\n",
       " 'new': 107,\n",
       " 'why': 108,\n",
       " 'well': 109,\n",
       " 'did': 110,\n",
       " 'too': 111,\n",
       " 'right': 112,\n",
       " 'way': 113,\n",
       " 'into': 114,\n",
       " 'very': 115,\n",
       " 'after': 116,\n",
       " 'being': 117,\n",
       " 'over': 118,\n",
       " 'back': 119,\n",
       " 'still': 120,\n",
       " 'got': 121,\n",
       " 'most': 122,\n",
       " 'reddit': 123,\n",
       " 'should': 124,\n",
       " 'something': 125,\n",
       " 'post': 126,\n",
       " 'going': 127,\n",
       " 'll': 128,\n",
       " '3': 129,\n",
       " 'our': 130,\n",
       " 'these': 131,\n",
       " 'thanks': 132,\n",
       " 'never': 133,\n",
       " '8220': 134,\n",
       " 'need': 135,\n",
       " '8221': 136,\n",
       " 'say': 137,\n",
       " 'where': 138,\n",
       " 'us': 139,\n",
       " 'use': 140,\n",
       " 'am': 141,\n",
       " 'love': 142,\n",
       " 'work': 143,\n",
       " 'day': 144,\n",
       " 'same': 145,\n",
       " 'off': 146,\n",
       " 'sure': 147,\n",
       " 'game': 148,\n",
       " 'before': 149,\n",
       " 'said': 150,\n",
       " 'didn': 151,\n",
       " '10': 152,\n",
       " 'lot': 153,\n",
       " 'those': 154,\n",
       " 'thing': 155,\n",
       " 'years': 156,\n",
       " 'someone': 157,\n",
       " 'does': 158,\n",
       " 'take': 159,\n",
       " 'best': 160,\n",
       " 'made': 161,\n",
       " 'feel': 162,\n",
       " 'great': 163,\n",
       " 'two': 164,\n",
       " 'actually': 165,\n",
       " 'better': 166,\n",
       " 'look': 167,\n",
       " '_': 168,\n",
       " 'its': 169,\n",
       " '5': 170,\n",
       " 'every': 171,\n",
       " 'down': 172,\n",
       " 'year': 173,\n",
       " 'pretty': 174,\n",
       " 'life': 175,\n",
       " 'doesn': 176,\n",
       " 'amp': 177,\n",
       " 'things': 178,\n",
       " 'many': 179,\n",
       " 'though': 180,\n",
       " '4': 181,\n",
       " 'while': 182,\n",
       " 'last': 183,\n",
       " 'always': 184,\n",
       " 'around': 185,\n",
       " 'man': 186,\n",
       " 'imgur': 187,\n",
       " 'thank': 188,\n",
       " 'world': 189,\n",
       " 'give': 190,\n",
       " 'find': 191,\n",
       " 'little': 192,\n",
       " 'help': 193,\n",
       " 'anything': 194,\n",
       " 'u': 195,\n",
       " 'through': 196,\n",
       " 'trump': 197,\n",
       " 'long': 198,\n",
       " 'used': 199,\n",
       " 'o': 200,\n",
       " 'yeah': 201,\n",
       " 'may': 202,\n",
       " 'few': 203,\n",
       " 'ever': 204,\n",
       " 'thought': 205,\n",
       " 'since': 206,\n",
       " 'yes': 207,\n",
       " 'doing': 208,\n",
       " 'getting': 209,\n",
       " 'shit': 210,\n",
       " 'anyone': 211,\n",
       " 'comments': 212,\n",
       " 'probably': 213,\n",
       " 'let': 214,\n",
       " 'point': 215,\n",
       " 'person': 216,\n",
       " 'bad': 217,\n",
       " 'both': 218,\n",
       " 'watch': 219,\n",
       " 'another': 220,\n",
       " 'again': 221,\n",
       " 'own': 222,\n",
       " 'might': 223,\n",
       " 'isn': 224,\n",
       " 'looking': 225,\n",
       " 'maybe': 226,\n",
       " 'everyone': 227,\n",
       " 'old': 228,\n",
       " 'come': 229,\n",
       " 'put': 230,\n",
       " 'mean': 231,\n",
       " 'without': 232,\n",
       " 'try': 233,\n",
       " 'please': 234,\n",
       " 'looks': 235,\n",
       " 'keep': 236,\n",
       " 'trying': 237,\n",
       " 'big': 238,\n",
       " 'video': 239,\n",
       " 'different': 240,\n",
       " 'show': 241,\n",
       " 'guy': 242,\n",
       " 'next': 243,\n",
       " 'e': 244,\n",
       " 'part': 245,\n",
       " 'play': 246,\n",
       " 'using': 247,\n",
       " 'makes': 248,\n",
       " 'already': 249,\n",
       " 'enough': 250,\n",
       " '2018': 251,\n",
       " 'found': 252,\n",
       " 'money': 253,\n",
       " 'oh': 254,\n",
       " 'real': 255,\n",
       " 'such': 256,\n",
       " '9685': 257,\n",
       " 'high': 258,\n",
       " 'end': 259,\n",
       " 'between': 260,\n",
       " '12388': 261,\n",
       " '6': 262,\n",
       " 'done': 263,\n",
       " 'read': 264,\n",
       " 'link': 265,\n",
       " 'lol': 266,\n",
       " 'nice': 267,\n",
       " 'having': 268,\n",
       " 'away': 269,\n",
       " 'tell': 270,\n",
       " 'nothing': 271,\n",
       " 'kind': 272,\n",
       " 'making': 273,\n",
       " 'question': 274,\n",
       " 'live': 275,\n",
       " 'fuck': 276,\n",
       " 'v': 277,\n",
       " 'start': 278,\n",
       " 'else': 279,\n",
       " 'team': 280,\n",
       " 'today': 281,\n",
       " 'hard': 282,\n",
       " '0': 283,\n",
       " 'against': 284,\n",
       " 'once': 285,\n",
       " 'days': 286,\n",
       " 'seen': 287,\n",
       " 'far': 288,\n",
       " 'bit': 289,\n",
       " 'able': 290,\n",
       " 'org': 291,\n",
       " 'place': 292,\n",
       " 'week': 293,\n",
       " 'everything': 294,\n",
       " 'each': 295,\n",
       " 'free': 296,\n",
       " 'wrong': 297,\n",
       " 'school': 298,\n",
       " 'least': 299,\n",
       " 'times': 300,\n",
       " 'less': 301,\n",
       " 'edit': 302,\n",
       " 'source': 303,\n",
       " 'idea': 304,\n",
       " 'top': 305,\n",
       " 'went': 306,\n",
       " 'during': 307,\n",
       " 'name': 308,\n",
       " 'believe': 309,\n",
       " 'until': 310,\n",
       " 'won': 311,\n",
       " 'state': 312,\n",
       " 'definitely': 313,\n",
       " 'seems': 314,\n",
       " 'guys': 315,\n",
       " 'saying': 316,\n",
       " 'home': 317,\n",
       " 'hope': 318,\n",
       " 'title': 319,\n",
       " 'story': 320,\n",
       " '7': 321,\n",
       " 'youtube': 322,\n",
       " 'fucking': 323,\n",
       " 'small': 324,\n",
       " 'says': 325,\n",
       " 'full': 326,\n",
       " 'rep': 327,\n",
       " 'n': 328,\n",
       " 'yet': 329,\n",
       " 'case': 330,\n",
       " 'sorry': 331,\n",
       " 'stop': 332,\n",
       " 'twitter': 333,\n",
       " 'friend': 334,\n",
       " 'under': 335,\n",
       " 'stuff': 336,\n",
       " 'reason': 337,\n",
       " 'ago': 338,\n",
       " 'support': 339,\n",
       " 'change': 340,\n",
       " '8': 341,\n",
       " 'job': 342,\n",
       " 'however': 343,\n",
       " 'house': 344,\n",
       " 'wasn': 345,\n",
       " 'family': 346,\n",
       " 'left': 347,\n",
       " 'second': 348,\n",
       " 'friends': 349,\n",
       " 'etc': 350,\n",
       " 'guess': 351,\n",
       " 'comment': 352,\n",
       " 'true': 353,\n",
       " 'problem': 354,\n",
       " 'power': 355,\n",
       " 'gets': 356,\n",
       " 'myself': 357,\n",
       " 'called': 358,\n",
       " 'news': 359,\n",
       " 'buy': 360,\n",
       " 'either': 361,\n",
       " 'rules': 362,\n",
       " 'sub': 363,\n",
       " 'set': 364,\n",
       " 'wanted': 365,\n",
       " 'happy': 366,\n",
       " 'w': 367,\n",
       " 'water': 368,\n",
       " 'fun': 369,\n",
       " 'white': 370,\n",
       " 'almost': 371,\n",
       " 'fact': 372,\n",
       " 'wiki': 373,\n",
       " 'experience': 374,\n",
       " 'open': 375,\n",
       " 'call': 376,\n",
       " 'whole': 377,\n",
       " 'started': 378,\n",
       " 'understand': 379,\n",
       " 'wouldn': 380,\n",
       " '20': 381,\n",
       " 'night': 382,\n",
       " '9': 383,\n",
       " 'article': 384,\n",
       " 'h': 385,\n",
       " 'side': 386,\n",
       " '100': 387,\n",
       " 'check': 388,\n",
       " 'came': 389,\n",
       " 'dog': 390,\n",
       " 'took': 391,\n",
       " 'net': 392,\n",
       " 'possible': 393,\n",
       " 'months': 394,\n",
       " 'jpg': 395,\n",
       " 'hours': 396,\n",
       " 'face': 397,\n",
       " 'food': 398,\n",
       " 'saw': 399,\n",
       " 'playing': 400,\n",
       " 'doi': 401,\n",
       " 'care': 402,\n",
       " '12': 403,\n",
       " 'internet': 404,\n",
       " 'study': 405,\n",
       " 'talking': 406,\n",
       " 'remember': 407,\n",
       " 'content': 408,\n",
       " 'mind': 409,\n",
       " 'de': 410,\n",
       " 'ask': 411,\n",
       " 'level': 412,\n",
       " 'told': 413,\n",
       " 'instead': 414,\n",
       " 'season': 415,\n",
       " 'god': 416,\n",
       " 'cool': 417,\n",
       " 'thinking': 418,\n",
       " 'working': 419,\n",
       " 'must': 420,\n",
       " 'human': 421,\n",
       " 'based': 422,\n",
       " 'country': 423,\n",
       " 'talk': 424,\n",
       " '225': 425,\n",
       " 'p': 426,\n",
       " 'movie': 427,\n",
       " 'aren': 428,\n",
       " 'run': 429,\n",
       " 'body': 430,\n",
       " '233': 431,\n",
       " 'agree': 432,\n",
       " 'posted': 433,\n",
       " '227': 434,\n",
       " 'black': 435,\n",
       " '65039': 436,\n",
       " 'awesome': 437,\n",
       " 'party': 438,\n",
       " 'wait': 439,\n",
       " 'games': 440,\n",
       " 'journal': 441,\n",
       " 'others': 442,\n",
       " 'group': 443,\n",
       " 'likely': 444,\n",
       " 'exactly': 445,\n",
       " 'worth': 446,\n",
       " 'government': 447,\n",
       " 'gonna': 448,\n",
       " 'que': 449,\n",
       " 'coming': 450,\n",
       " 'women': 451,\n",
       " 'means': 452,\n",
       " 'tried': 453,\n",
       " 'information': 454,\n",
       " '30': 455,\n",
       " 'pay': 456,\n",
       " 'haven': 457,\n",
       " 'head': 458,\n",
       " 'future': 459,\n",
       " '2017': 460,\n",
       " 'hey': 461,\n",
       " 'c': 462,\n",
       " 'taking': 463,\n",
       " 'single': 464,\n",
       " 'literally': 465,\n",
       " 'hear': 466,\n",
       " 'hate': 467,\n",
       " 'super': 468,\n",
       " 'self': 469,\n",
       " 'goes': 470,\n",
       " 'health': 471,\n",
       " 'amazing': 472,\n",
       " 'hand': 473,\n",
       " 'message': 474,\n",
       " 'within': 475,\n",
       " 'issue': 476,\n",
       " 'comes': 477,\n",
       " 'happened': 478,\n",
       " 'sounds': 479,\n",
       " 'system': 480,\n",
       " 'sense': 481,\n",
       " 'car': 482,\n",
       " 'couple': 483,\n",
       " 'type': 484,\n",
       " 'half': 485,\n",
       " 'social': 486,\n",
       " 'usually': 487,\n",
       " 'facebook': 488,\n",
       " 'order': 489,\n",
       " '3901': 490,\n",
       " '8211': 491,\n",
       " 'close': 492,\n",
       " '3900': 493,\n",
       " 'comic': 494,\n",
       " 'book': 495,\n",
       " 'three': 496,\n",
       " 'children': 497,\n",
       " 'needs': 498,\n",
       " 'interesting': 499,\n",
       " 'futurology': 500,\n",
       " 'number': 501,\n",
       " 'past': 502,\n",
       " 'data': 503,\n",
       " 'girl': 504,\n",
       " 'quite': 505,\n",
       " 'low': 506,\n",
       " 'kids': 507,\n",
       " '128514': 508,\n",
       " 'reference': 509,\n",
       " 'course': 510,\n",
       " 'subreddit': 511,\n",
       " 'dont': 512,\n",
       " 'ok': 513,\n",
       " 'heard': 514,\n",
       " 'weeks': 515,\n",
       " 'yourself': 516,\n",
       " '000': 517,\n",
       " 'together': 518,\n",
       " 'front': 519,\n",
       " 'especially': 520,\n",
       " 'war': 521,\n",
       " 'important': 522,\n",
       " 'control': 523,\n",
       " 'picture': 524,\n",
       " 'happen': 525,\n",
       " 'sometimes': 526,\n",
       " 'fine': 527,\n",
       " 'eat': 528,\n",
       " 'parents': 529,\n",
       " 'list': 530,\n",
       " 'rather': 531,\n",
       " 'line': 532,\n",
       " 'law': 533,\n",
       " 'thread': 534,\n",
       " 'opinion': 535,\n",
       " 'release': 536,\n",
       " 'often': 537,\n",
       " 'public': 538,\n",
       " 'works': 539,\n",
       " 'seem': 540,\n",
       " '8216': 541,\n",
       " '50': 542,\n",
       " 'later': 543,\n",
       " 'vote': 544,\n",
       " 'non': 545,\n",
       " 'american': 546,\n",
       " 'matter': 547,\n",
       " 'favorite': 548,\n",
       " 'posts': 549,\n",
       " 'phone': 550,\n",
       " '15': 551,\n",
       " 'wish': 552,\n",
       " 'original': 553,\n",
       " 'deal': 554,\n",
       " 'b': 555,\n",
       " 'hit': 556,\n",
       " 'google': 557,\n",
       " '8212': 558,\n",
       " 'leave': 559,\n",
       " 'win': 560,\n",
       " 'completely': 561,\n",
       " 'im': 562,\n",
       " 'add': 563,\n",
       " 'damn': 564,\n",
       " 'ones': 565,\n",
       " 'example': 566,\n",
       " 'space': 567,\n",
       " 'due': 568,\n",
       " 'cat': 569,\n",
       " 'lost': 570,\n",
       " 'entire': 571,\n",
       " 'fight': 572,\n",
       " 'absolutely': 573,\n",
       " 'kill': 574,\n",
       " 'basically': 575,\n",
       " 'whether': 576,\n",
       " '11': 577,\n",
       " 'image': 578,\n",
       " 'haha': 579,\n",
       " 'history': 580,\n",
       " 'removed': 581,\n",
       " 'early': 582,\n",
       " 'cause': 583,\n",
       " 'easy': 584,\n",
       " 'die': 585,\n",
       " 'room': 586,\n",
       " 'op': 587,\n",
       " 'couldn': 588,\n",
       " 'media': 589,\n",
       " 'become': 590,\n",
       " 'neutrality': 591,\n",
       " 'players': 592,\n",
       " 'death': 593,\n",
       " 'baby': 594,\n",
       " 'men': 595,\n",
       " 'okay': 596,\n",
       " 'minutes': 597,\n",
       " 'similar': 598,\n",
       " 'answer': 599,\n",
       " 'age': 600,\n",
       " 'company': 601,\n",
       " 'child': 602,\n",
       " 'sound': 603,\n",
       " 'gif': 604,\n",
       " 'music': 605,\n",
       " 'huge': 606,\n",
       " 'move': 607,\n",
       " 'whatever': 608,\n",
       " 'results': 609,\n",
       " 'weight': 610,\n",
       " 'dude': 611,\n",
       " 'page': 612,\n",
       " 'city': 613,\n",
       " 'questions': 614,\n",
       " 'pain': 615,\n",
       " 'hell': 616,\n",
       " 'online': 617,\n",
       " 'ban': 618,\n",
       " 'woman': 619,\n",
       " 'community': 620,\n",
       " 'status': 621,\n",
       " 'soon': 622,\n",
       " 'fire': 623,\n",
       " 'month': 624,\n",
       " 'turn': 625,\n",
       " 'class': 626,\n",
       " 'dead': 627,\n",
       " 'red': 628,\n",
       " 'stay': 629,\n",
       " 'quality': 630,\n",
       " 'watching': 631,\n",
       " 'police': 632,\n",
       " 'simply': 633,\n",
       " 'per': 634,\n",
       " 'business': 635,\n",
       " 'president': 636,\n",
       " 'asked': 637,\n",
       " 'general': 638,\n",
       " 'en': 639,\n",
       " 'weird': 640,\n",
       " 'x': 641,\n",
       " 'chance': 642,\n",
       " 'wants': 643,\n",
       " 'research': 644,\n",
       " 'act': 645,\n",
       " 'seeing': 646,\n",
       " 'area': 647,\n",
       " 'higher': 648,\n",
       " 'court': 649,\n",
       " 'song': 650,\n",
       " 'linked': 651,\n",
       " 'reading': 652,\n",
       " 'press': 653,\n",
       " 'dad': 654,\n",
       " 'light': 655,\n",
       " 'rest': 656,\n",
       " 'advice': 657,\n",
       " 'price': 658,\n",
       " 'wife': 659,\n",
       " 'photo': 660,\n",
       " 'shot': 661,\n",
       " 'mine': 662,\n",
       " 'episode': 663,\n",
       " 'project': 664,\n",
       " 'amount': 665,\n",
       " 'sex': 666,\n",
       " 'finally': 667,\n",
       " 'large': 668,\n",
       " 'mods': 669,\n",
       " 'outside': 670,\n",
       " 'shows': 671,\n",
       " 'honestly': 672,\n",
       " 'series': 673,\n",
       " 'funny': 674,\n",
       " 'account': 675,\n",
       " 'taken': 676,\n",
       " 'risk': 677,\n",
       " 'save': 678,\n",
       " 'states': 679,\n",
       " 'given': 680,\n",
       " 'knew': 681,\n",
       " 'mom': 682,\n",
       " 'felt': 683,\n",
       " 'sleep': 684,\n",
       " 'perfect': 685,\n",
       " 'word': 686,\n",
       " 'player': 687,\n",
       " 'issues': 688,\n",
       " 'behind': 689,\n",
       " 'j': 690,\n",
       " 'hot': 691,\n",
       " 'played': 692,\n",
       " 'fan': 693,\n",
       " 'across': 694,\n",
       " 'wow': 695,\n",
       " 'running': 696,\n",
       " 'copy': 697,\n",
       " 'blue': 698,\n",
       " 'cut': 699,\n",
       " 'anyway': 700,\n",
       " 'normal': 701,\n",
       " '2016': 702,\n",
       " 'share': 703,\n",
       " 'uk': 704,\n",
       " 'store': 705,\n",
       " 'feeling': 706,\n",
       " 'current': 707,\n",
       " 'young': 708,\n",
       " 'points': 709,\n",
       " 'abstract': 710,\n",
       " 'kid': 711,\n",
       " 'takes': 712,\n",
       " 'vs': 713,\n",
       " 'short': 714,\n",
       " 'actual': 715,\n",
       " 'g': 716,\n",
       " 'rule': 717,\n",
       " 'although': 718,\n",
       " 'looked': 719,\n",
       " 'recently': 720,\n",
       " 'site': 721,\n",
       " 'xbox': 722,\n",
       " 'bring': 723,\n",
       " 'moment': 724,\n",
       " 'enjoy': 725,\n",
       " 'late': 726,\n",
       " 'f': 727,\n",
       " 'specific': 728,\n",
       " '18': 729,\n",
       " 'happens': 730,\n",
       " 'related': 731,\n",
       " 'stupid': 732,\n",
       " 'version': 733,\n",
       " 'words': 734,\n",
       " 'middle': 735,\n",
       " 'imagine': 736,\n",
       " 'sign': 737,\n",
       " 'bill': 738,\n",
       " 'test': 739,\n",
       " 'along': 740,\n",
       " 'longer': 741,\n",
       " 'poor': 742,\n",
       " 'break': 743,\n",
       " '25': 744,\n",
       " 'tv': 745,\n",
       " 'garfield': 746,\n",
       " 'giving': 747,\n",
       " 'crazy': 748,\n",
       " 'college': 749,\n",
       " 'near': 750,\n",
       " 'unless': 751,\n",
       " 'situation': 752,\n",
       " 'eating': 753,\n",
       " 'million': 754,\n",
       " 'needed': 755,\n",
       " 'learn': 756,\n",
       " 'joke': 757,\n",
       " 'follow': 758,\n",
       " 'living': 759,\n",
       " 'themselves': 760,\n",
       " 'known': 761,\n",
       " 'instagram': 762,\n",
       " 'blood': 763,\n",
       " 'l': 764,\n",
       " 'pick': 765,\n",
       " 'inside': 766,\n",
       " 'totally': 767,\n",
       " 'star': 768,\n",
       " 'evidence': 769,\n",
       " 'common': 770,\n",
       " 'discussion': 771,\n",
       " 'beautiful': 772,\n",
       " 'png': 773,\n",
       " 'album': 774,\n",
       " 'personal': 775,\n",
       " 'serious': 776,\n",
       " 'bought': 777,\n",
       " 'interested': 778,\n",
       " '16': 779,\n",
       " 'currently': 780,\n",
       " 'build': 781,\n",
       " 'plan': 782,\n",
       " 'gave': 783,\n",
       " 'effect': 784,\n",
       " 'killed': 785,\n",
       " 'clear': 786,\n",
       " 'gay': 787,\n",
       " 'pkk': 788,\n",
       " 'character': 789,\n",
       " 'asking': 790,\n",
       " 'worked': 791,\n",
       " 'fast': 792,\n",
       " 'pass': 793,\n",
       " 'relationship': 794,\n",
       " 'local': 795,\n",
       " 'office': 796,\n",
       " 'air': 797,\n",
       " 'league': 798,\n",
       " 'national': 799,\n",
       " 'political': 800,\n",
       " 'several': 801,\n",
       " 'size': 802,\n",
       " 'main': 803,\n",
       " 'gun': 804,\n",
       " 'process': 805,\n",
       " 'brain': 806,\n",
       " 'report': 807,\n",
       " 'difference': 808,\n",
       " 'above': 809,\n",
       " 'sort': 810,\n",
       " 'explain': 811,\n",
       " 'fair': 812,\n",
       " 'major': 813,\n",
       " 'science': 814,\n",
       " 'term': 815,\n",
       " 'certain': 816,\n",
       " 'lose': 817,\n",
       " 'film': 818,\n",
       " 'html': 819,\n",
       " 'boy': 820,\n",
       " '11088': 821,\n",
       " 'eyes': 822,\n",
       " 'strong': 823,\n",
       " 'response': 824,\n",
       " 'anymore': 825,\n",
       " 'himself': 826,\n",
       " 'glad': 827,\n",
       " 'problems': 828,\n",
       " 'obviously': 829,\n",
       " 'associated': 830,\n",
       " 'worst': 831,\n",
       " 'morning': 832,\n",
       " 'knows': 833,\n",
       " 'allowed': 834,\n",
       " 'sell': 835,\n",
       " 'trade': 836,\n",
       " 'congress': 837,\n",
       " 'positive': 838,\n",
       " 'alone': 839,\n",
       " 'fat': 840,\n",
       " 'hold': 841,\n",
       " 'attack': 842,\n",
       " 'card': 843,\n",
       " 'action': 844,\n",
       " 'form': 845,\n",
       " 'credit': 846,\n",
       " 'kinda': 847,\n",
       " 'hour': 848,\n",
       " 'recommend': 849,\n",
       " 'mod': 850,\n",
       " 'starting': 851,\n",
       " 'service': 852,\n",
       " 'decided': 853,\n",
       " '99': 854,\n",
       " 'ass': 855,\n",
       " 'piece': 856,\n",
       " 'special': 857,\n",
       " 'personally': 858,\n",
       " 'effects': 859,\n",
       " 'worse': 860,\n",
       " 'paste': 861,\n",
       " 'average': 862,\n",
       " 'market': 863,\n",
       " 'seriously': 864,\n",
       " 'appreciate': 865,\n",
       " 'heart': 866,\n",
       " 'y': 867,\n",
       " 'dark': 868,\n",
       " 'earth': 869,\n",
       " 'key': 870,\n",
       " 'drop': 871,\n",
       " '160': 872,\n",
       " 'til': 873,\n",
       " 'hi': 874,\n",
       " 'skin': 875,\n",
       " 'event': 876,\n",
       " 'including': 877,\n",
       " 'product': 878,\n",
       " 'wikipedia': 879,\n",
       " 'thoughts': 880,\n",
       " 'simple': 881,\n",
       " 'review': 882,\n",
       " 'nature': 883,\n",
       " 'united': 884,\n",
       " 'luck': 885,\n",
       " 'hands': 886,\n",
       " 'bed': 887,\n",
       " '40': 888,\n",
       " 'ideas': 889,\n",
       " 'send': 890,\n",
       " 'gone': 891,\n",
       " 'value': 892,\n",
       " 'view': 893,\n",
       " 'posting': 894,\n",
       " 'safe': 895,\n",
       " 'extra': 896,\n",
       " 'gives': 897,\n",
       " 'hair': 898,\n",
       " 'attention': 899,\n",
       " 'according': 900,\n",
       " 'drive': 901,\n",
       " 'meme': 902,\n",
       " 'gold': 903,\n",
       " '13': 904,\n",
       " 'info': 905,\n",
       " 'drug': 906,\n",
       " 'mother': 907,\n",
       " 'green': 908,\n",
       " 'except': 909,\n",
       " 'itself': 910,\n",
       " 'k': 911,\n",
       " 'rights': 912,\n",
       " 'race': 913,\n",
       " 'spend': 914,\n",
       " 'ball': 915,\n",
       " 'provide': 916,\n",
       " 'option': 917,\n",
       " 'america': 918,\n",
       " 'iamhoneydill': 919,\n",
       " 'cost': 920,\n",
       " 'app': 921,\n",
       " 'son': 922,\n",
       " 'academic': 923,\n",
       " 'mostly': 924,\n",
       " 'stand': 925,\n",
       " 'force': 926,\n",
       " 'decision': 927,\n",
       " 'avoid': 928,\n",
       " 'available': 929,\n",
       " 'potential': 930,\n",
       " 'create': 931,\n",
       " 'final': 932,\n",
       " 'members': 933,\n",
       " 'energy': 934,\n",
       " 'meant': 935,\n",
       " 'website': 936,\n",
       " '14': 937,\n",
       " 'videos': 938,\n",
       " 'shouldn': 939,\n",
       " 'sad': 940,\n",
       " 'walk': 941,\n",
       " 'fake': 942,\n",
       " 'quick': 943,\n",
       " 'total': 944,\n",
       " 'fit': 945,\n",
       " 'proof': 946,\n",
       " 'lower': 947,\n",
       " 'straight': 948,\n",
       " '24': 949,\n",
       " 'wonder': 950,\n",
       " 'holy': 951,\n",
       " 'ready': 952,\n",
       " 'offers': 953,\n",
       " 'feels': 954,\n",
       " 'king': 955,\n",
       " 'art': 956,\n",
       " 'campaign': 957,\n",
       " 'user': 958,\n",
       " '17': 959,\n",
       " 'countries': 960,\n",
       " 'users': 961,\n",
       " 'compared': 962,\n",
       " 'majority': 963,\n",
       " 'recent': 964,\n",
       " 'park': 965,\n",
       " 'access': 966,\n",
       " 'allow': 967,\n",
       " 'note': 968,\n",
       " 'language': 969,\n",
       " 'clinton': 970,\n",
       " 'correct': 971,\n",
       " 'search': 972,\n",
       " 'south': 973,\n",
       " 'spent': 974,\n",
       " 'changed': 975,\n",
       " 'choice': 976,\n",
       " 'pro': 977,\n",
       " 'damage': 978,\n",
       " 'lead': 979,\n",
       " 'figure': 980,\n",
       " 'paper': 981,\n",
       " 'following': 982,\n",
       " 'plus': 983,\n",
       " 'cannot': 984,\n",
       " 'co': 985,\n",
       " 'turned': 986,\n",
       " '8201': 987,\n",
       " 'multiple': 988,\n",
       " 'four': 989,\n",
       " 'welcome': 990,\n",
       " 'further': 991,\n",
       " 'ability': 992,\n",
       " 'legal': 993,\n",
       " 'official': 994,\n",
       " 'popular': 995,\n",
       " 'expect': 996,\n",
       " 'offer': 997,\n",
       " 'lives': 998,\n",
       " 'upvote': 999,\n",
       " 'added': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightsHistory(callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.log_weights(0)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.log_weights(epoch)\n",
    "        \n",
    "    def log_weights(self, step):\n",
    "        for layer_name in ['LSTM_layer', 'embeddings_layer', 'sparse_feat_dense_layer', 'output_layer']:\n",
    "            try:\n",
    "                layer = self.model.get_layer(layer_name)\n",
    "                experiment.log_histogram_3d(layer.get_weights()[0], \n",
    "                                            name=layer_name,\n",
    "                                   step=step)  \n",
    "            except Exception as e:\n",
    "                print(\"Logging weights\", e)\n",
    "                # layer probably does not exist\n",
    "                pass\n",
    "        \n",
    "class FreezeLayer(callbacks.Callback):\n",
    "    def __init__(self, logs={}, patience=5, layer='embeddings_layer', verbose=1, set_to=False):\n",
    "        super(FreezeLayer, self).__init__()\n",
    "        self.freeze_epoch = patience\n",
    "        self.freeze_layer = layer\n",
    "        self.verbose = verbose\n",
    "        self.set_to = set_to\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        if epoch == self.freeze_epoch:\n",
    "            try:\n",
    "                layer = model.get_layer(self.freeze_layer)\n",
    "                old_value = layer.trainable\n",
    "                layer.trainable = self.set_to\n",
    "                model.compile()\n",
    "                if self.verbose:\n",
    "                    print(\"Setting %s layer from %s to trainable=%s...\" % (layer.name, old_value,\n",
    "                                                                   model.get_layer(self.freeze_layer).trainable))\n",
    "            except Exception as e:\n",
    "                # layer probably does not exist\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, \n",
    "                x_train, y_train, x_test, y_test, \n",
    "                batch_size, epochs, class_weight, start_epoch=0, workers=4,\n",
    "                callback_list = [],\n",
    "                model_path='/tmp/model'):\n",
    "    print('Train...')\n",
    "    experiment.log_parameter('class_weight', class_weight.values())\n",
    "    experiment.log_parameter('callbacks', callbacks)\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs, initial_epoch=start_epoch, \n",
    "              class_weight=class_weight,\n",
    "              validation_data=[x_test, y_test],\n",
    "#               validation_split=0.3,\n",
    "                       workers=workers,\n",
    "            callbacks = [\n",
    "#                 callbacks.ModelCheckpoint(filepath='%s_best' % model_path, verbose=1, \n",
    "#                                           save_best_only=True, save_weights_only=True),\n",
    "                callbacks.EarlyStopping(patience=500), *callback_list\n",
    "            ])\n",
    "    model.save(model_path)#, save_weights_only=True)\n",
    "    experiment.log_parameter('model_path', model_path)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 166 samples, validate on 71 samples\n",
      "Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "Epoch 1/1000\n",
      " 32/166 [====>.........................] - ETA: 2s - loss: 0.6045 - f1_m: 0.1818 - precision_m: 0.1000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 1s 6ms/sample - loss: 0.6082 - f1_m: 0.2447 - precision_m: 0.1665 - recall_m: 0.9111 - val_loss: 1.0046 - val_f1_m: 0.1656 - val_precision_m: 0.0914 - val_recall_m: 1.0000\n",
      "Epoch 2/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5928 - f1_m: 0.2941 - precision_m: 0.1786 - recall_m: 0.8333Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 262us/sample - loss: 0.6290 - f1_m: 0.1773 - precision_m: 0.1032 - recall_m: 0.8056 - val_loss: 0.9629 - val_f1_m: 0.1891 - val_precision_m: 0.1087 - val_recall_m: 1.0000\n",
      "Epoch 3/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5242 - f1_m: 0.2941 - precision_m: 0.1724 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 329us/sample - loss: 0.6499 - f1_m: 0.2092 - precision_m: 0.1232 - recall_m: 0.7111 - val_loss: 0.8378 - val_f1_m: 0.2196 - val_precision_m: 0.1332 - val_recall_m: 1.0000\n",
      "Epoch 4/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5315 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 369us/sample - loss: 0.5960 - f1_m: 0.1854 - precision_m: 0.1067 - recall_m: 0.7083 - val_loss: 0.8240 - val_f1_m: 0.1200 - val_precision_m: 0.0660 - val_recall_m: 0.6667\n",
      "Epoch 5/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5728 - f1_m: 0.2963 - precision_m: 0.1818 - recall_m: 0.8000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 373us/sample - loss: 0.5950 - f1_m: 0.2302 - precision_m: 0.1356 - recall_m: 0.8417 - val_loss: 0.8144 - val_f1_m: 0.1944 - val_precision_m: 0.1100 - val_recall_m: 1.0000\n",
      "Epoch 6/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6271 - f1_m: 0.1600 - precision_m: 0.0909 - recall_m: 0.6667Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 386us/sample - loss: 0.5918 - f1_m: 0.2085 - precision_m: 0.1220 - recall_m: 0.7778 - val_loss: 0.8335 - val_f1_m: 0.1111 - val_precision_m: 0.0621 - val_recall_m: 0.6667\n",
      "Epoch 7/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4925 - f1_m: 0.3871 - precision_m: 0.2400 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 391us/sample - loss: 0.5775 - f1_m: 0.2490 - precision_m: 0.1456 - recall_m: 0.9583 - val_loss: 0.8334 - val_f1_m: 0.1218 - val_precision_m: 0.0699 - val_recall_m: 0.6667\n",
      "Epoch 8/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5190 - f1_m: 0.3226 - precision_m: 0.1923 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 419us/sample - loss: 0.6241 - f1_m: 0.1997 - precision_m: 0.1188 - recall_m: 0.6333 - val_loss: 0.8328 - val_f1_m: 0.1176 - val_precision_m: 0.0667 - val_recall_m: 0.6667\n",
      "Epoch 9/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6851 - f1_m: 0.1538 - precision_m: 0.0833 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 339us/sample - loss: 0.5779 - f1_m: 0.2500 - precision_m: 0.1463 - recall_m: 0.9667 - val_loss: 0.7930 - val_f1_m: 0.1111 - val_precision_m: 0.0628 - val_recall_m: 0.5000\n",
      "Epoch 10/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5483 - f1_m: 0.3571 - precision_m: 0.2273 - recall_m: 0.8333Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 396us/sample - loss: 0.5608 - f1_m: 0.2682 - precision_m: 0.1583 - recall_m: 0.9722 - val_loss: 0.8128 - val_f1_m: 0.1010 - val_precision_m: 0.0559 - val_recall_m: 0.5556\n",
      "Epoch 11/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4966 - f1_m: 0.3077 - precision_m: 0.1905 - recall_m: 0.8000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 371us/sample - loss: 0.5650 - f1_m: 0.2091 - precision_m: 0.1238 - recall_m: 0.8000 - val_loss: 0.8552 - val_f1_m: 0.1940 - val_precision_m: 0.1089 - val_recall_m: 1.0000\n",
      "Epoch 12/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5141 - f1_m: 0.2581 - precision_m: 0.1481 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 385us/sample - loss: 0.5424 - f1_m: 0.2613 - precision_m: 0.1512 - recall_m: 1.0000 - val_loss: 0.8766 - val_f1_m: 0.1824 - val_precision_m: 0.1029 - val_recall_m: 1.0000\n",
      "Epoch 13/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5909 - f1_m: 0.1875 - precision_m: 0.1034 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 323us/sample - loss: 0.5394 - f1_m: 0.2449 - precision_m: 0.1412 - recall_m: 1.0000 - val_loss: 0.9234 - val_f1_m: 0.1090 - val_precision_m: 0.0595 - val_recall_m: 0.6667\n",
      "Epoch 14/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5182 - f1_m: 0.2581 - precision_m: 0.1481 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 248us/sample - loss: 0.5406 - f1_m: 0.1968 - precision_m: 0.1119 - recall_m: 0.8333 - val_loss: 0.9538 - val_f1_m: 0.1071 - val_precision_m: 0.0585 - val_recall_m: 0.6667\n",
      "Epoch 15/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5283 - f1_m: 0.2353 - precision_m: 0.1333 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 338us/sample - loss: 0.5476 - f1_m: 0.1933 - precision_m: 0.1108 - recall_m: 0.8333 - val_loss: 0.8984 - val_f1_m: 0.1873 - val_precision_m: 0.1050 - val_recall_m: 1.0000\n",
      "Epoch 16/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4597 - f1_m: 0.2069 - precision_m: 0.1154 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 364us/sample - loss: 0.5481 - f1_m: 0.2854 - precision_m: 0.1785 - recall_m: 1.0000 - val_loss: 0.7962 - val_f1_m: 0.1624 - val_precision_m: 0.0957 - val_recall_m: 0.5556\n",
      "Epoch 17/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5040 - f1_m: 0.2000 - precision_m: 0.1111 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 361us/sample - loss: 0.5351 - f1_m: 0.3190 - precision_m: 0.1971 - recall_m: 1.0000 - val_loss: 0.7929 - val_f1_m: 0.0870 - val_precision_m: 0.0502 - val_recall_m: 0.5000\n",
      "Epoch 18/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5503 - f1_m: 0.3200 - precision_m: 0.1905 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 369us/sample - loss: 0.5322 - f1_m: 0.2418 - precision_m: 0.1420 - recall_m: 0.8333 - val_loss: 0.8362 - val_f1_m: 0.1017 - val_precision_m: 0.0575 - val_recall_m: 0.5833\n",
      "Epoch 19/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6072 - f1_m: 0.1481 - precision_m: 0.0800 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 370us/sample - loss: 0.5303 - f1_m: 0.3230 - precision_m: 0.2078 - recall_m: 1.0000 - val_loss: 0.8144 - val_f1_m: 0.0867 - val_precision_m: 0.0488 - val_recall_m: 0.3889\n",
      "Epoch 20/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5015 - f1_m: 0.3704 - precision_m: 0.2273 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 267us/sample - loss: 0.5345 - f1_m: 0.2255 - precision_m: 0.1356 - recall_m: 0.8333 - val_loss: 0.8118 - val_f1_m: 0.0882 - val_precision_m: 0.0504 - val_recall_m: 0.3889\n",
      "Epoch 21/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.7925 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 417us/sample - loss: 0.5620 - f1_m: 0.2370 - precision_m: 0.1455 - recall_m: 0.6667 - val_loss: 0.7323 - val_f1_m: 0.1052 - val_precision_m: 0.0608 - val_recall_m: 0.3889\n",
      "Epoch 22/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4991 - f1_m: 0.4286 - precision_m: 0.2727 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 440us/sample - loss: 0.5317 - f1_m: 0.2709 - precision_m: 0.1650 - recall_m: 0.8333 - val_loss: 0.6895 - val_f1_m: 0.1225 - val_precision_m: 0.0714 - val_recall_m: 0.4444\n",
      "Epoch 23/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4658 - f1_m: 0.4545 - precision_m: 0.2941 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 311us/sample - loss: 0.5297 - f1_m: 0.3143 - precision_m: 0.1967 - recall_m: 0.8333 - val_loss: 0.6659 - val_f1_m: 0.1217 - val_precision_m: 0.0722 - val_recall_m: 0.3889\n",
      "Epoch 24/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5548 - f1_m: 0.3158 - precision_m: 0.1875 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 250us/sample - loss: 0.5308 - f1_m: 0.4073 - precision_m: 0.2827 - recall_m: 0.9444 - val_loss: 0.6754 - val_f1_m: 0.1000 - val_precision_m: 0.0625 - val_recall_m: 0.2500\n",
      "Epoch 25/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4788 - f1_m: 0.2105 - precision_m: 0.1176 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 347us/sample - loss: 0.5170 - f1_m: 0.4067 - precision_m: 0.2812 - recall_m: 1.0000 - val_loss: 0.7141 - val_f1_m: 0.1111 - val_precision_m: 0.0649 - val_recall_m: 0.3889\n",
      "Epoch 26/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4882 - f1_m: 0.3636 - precision_m: 0.2222 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 223us/sample - loss: 0.5051 - f1_m: 0.3346 - precision_m: 0.2087 - recall_m: 1.0000 - val_loss: 0.8071 - val_f1_m: 0.1532 - val_precision_m: 0.1000 - val_recall_m: 0.3889\n",
      "Epoch 27/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6918 - f1_m: 0.0909 - precision_m: 0.0476 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 413us/sample - loss: 0.5297 - f1_m: 0.2285 - precision_m: 0.1384 - recall_m: 0.8333 - val_loss: 0.8834 - val_f1_m: 0.1059 - val_precision_m: 0.0594 - val_recall_m: 0.5000\n",
      "Epoch 28/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4862 - f1_m: 0.2222 - precision_m: 0.1250 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 379us/sample - loss: 0.4837 - f1_m: 0.2704 - precision_m: 0.1592 - recall_m: 1.0000 - val_loss: 0.8855 - val_f1_m: 0.0967 - val_precision_m: 0.0545 - val_recall_m: 0.5833\n",
      "Epoch 29/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6073 - f1_m: 0.1481 - precision_m: 0.0800 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 379us/sample - loss: 0.4950 - f1_m: 0.3127 - precision_m: 0.1996 - recall_m: 1.0000 - val_loss: 0.8767 - val_f1_m: 0.0791 - val_precision_m: 0.0437 - val_recall_m: 0.4444\n",
      "Epoch 30/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6370 - f1_m: 0.0833 - precision_m: 0.0435 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 352us/sample - loss: 0.5101 - f1_m: 0.3066 - precision_m: 0.1989 - recall_m: 1.0000 - val_loss: 0.8650 - val_f1_m: 0.1439 - val_precision_m: 0.0827 - val_recall_m: 0.5556\n",
      "Epoch 31/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4620 - f1_m: 0.2143 - precision_m: 0.1200 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 383us/sample - loss: 0.5027 - f1_m: 0.2264 - precision_m: 0.1345 - recall_m: 0.8333 - val_loss: 0.8642 - val_f1_m: 0.0857 - val_precision_m: 0.0485 - val_recall_m: 0.3889\n",
      "Epoch 32/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4445 - f1_m: 0.3571 - precision_m: 0.2174 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 402us/sample - loss: 0.4865 - f1_m: 0.2820 - precision_m: 0.1671 - recall_m: 1.0000 - val_loss: 0.8110 - val_f1_m: 0.0957 - val_precision_m: 0.0547 - val_recall_m: 0.3889\n",
      "Epoch 33/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5528 - f1_m: 0.1176 - precision_m: 0.0625 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166/166 [==============================] - 0s 420us/sample - loss: 0.4936 - f1_m: 0.3286 - precision_m: 0.2110 - recall_m: 1.0000 - val_loss: 0.7610 - val_f1_m: 0.1587 - val_precision_m: 0.0948 - val_recall_m: 0.5000\n",
      "Epoch 34/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5120 - f1_m: 0.1818 - precision_m: 0.1000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 358us/sample - loss: 0.4792 - f1_m: 0.3233 - precision_m: 0.1962 - recall_m: 1.0000 - val_loss: 0.7590 - val_f1_m: 0.2297 - val_precision_m: 0.1556 - val_recall_m: 0.5000\n",
      "Epoch 35/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4527 - f1_m: 0.3200 - precision_m: 0.1905 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 427us/sample - loss: 0.4854 - f1_m: 0.3219 - precision_m: 0.1979 - recall_m: 1.0000 - val_loss: 0.7836 - val_f1_m: 0.1206 - val_precision_m: 0.0698 - val_recall_m: 0.4444\n",
      "Epoch 36/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5548 - f1_m: 0.2222 - precision_m: 0.1250 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 291us/sample - loss: 0.4773 - f1_m: 0.2566 - precision_m: 0.1532 - recall_m: 0.8333 - val_loss: 0.8036 - val_f1_m: 0.0972 - val_precision_m: 0.0559 - val_recall_m: 0.5000\n",
      "Epoch 37/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3339 - f1_m: 0.5455 - precision_m: 0.3750 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 267us/sample - loss: 0.4851 - f1_m: 0.2636 - precision_m: 0.1622 - recall_m: 0.8333 - val_loss: 0.7808 - val_f1_m: 0.1023 - val_precision_m: 0.0590 - val_recall_m: 0.4444\n",
      "Epoch 38/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5599 - f1_m: 0.2400 - precision_m: 0.1364 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 244us/sample - loss: 0.4790 - f1_m: 0.2565 - precision_m: 0.1566 - recall_m: 0.8333 - val_loss: 0.7216 - val_f1_m: 0.1270 - val_precision_m: 0.0781 - val_recall_m: 0.3889\n",
      "Epoch 39/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4478 - f1_m: 0.3636 - precision_m: 0.2222 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 396us/sample - loss: 0.4663 - f1_m: 0.3369 - precision_m: 0.2042 - recall_m: 1.0000 - val_loss: 0.6824 - val_f1_m: 0.1157 - val_precision_m: 0.0683 - val_recall_m: 0.3889\n",
      "Epoch 40/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4428 - f1_m: 0.2609 - precision_m: 0.1500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 336us/sample - loss: 0.4684 - f1_m: 0.2824 - precision_m: 0.1767 - recall_m: 0.8000 - val_loss: 0.6937 - val_f1_m: 0.0952 - val_precision_m: 0.0556 - val_recall_m: 0.3333\n",
      "Epoch 41/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4395 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 391us/sample - loss: 0.4743 - f1_m: 0.2723 - precision_m: 0.1692 - recall_m: 0.8000 - val_loss: 0.6884 - val_f1_m: 0.1944 - val_precision_m: 0.1145 - val_recall_m: 0.7778\n",
      "Epoch 42/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4854 - f1_m: 0.2500 - precision_m: 0.1500 - recall_m: 0.7500Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 356us/sample - loss: 0.4651 - f1_m: 0.3500 - precision_m: 0.2179 - recall_m: 0.9250 - val_loss: 0.6852 - val_f1_m: 0.1052 - val_precision_m: 0.0608 - val_recall_m: 0.3889\n",
      "Epoch 43/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4291 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 418us/sample - loss: 0.4539 - f1_m: 0.3537 - precision_m: 0.2184 - recall_m: 1.0000 - val_loss: 0.7175 - val_f1_m: 0.1133 - val_precision_m: 0.0655 - val_recall_m: 0.4444\n",
      "Epoch 44/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3885 - f1_m: 0.3636 - precision_m: 0.2222 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 353us/sample - loss: 0.4734 - f1_m: 0.3643 - precision_m: 0.2371 - recall_m: 1.0000 - val_loss: 0.7796 - val_f1_m: 0.1027 - val_precision_m: 0.0593 - val_recall_m: 0.3889\n",
      "Epoch 45/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5836 - f1_m: 0.0952 - precision_m: 0.0500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 319us/sample - loss: 0.4656 - f1_m: 0.2462 - precision_m: 0.1488 - recall_m: 0.8333 - val_loss: 0.8047 - val_f1_m: 0.2273 - val_precision_m: 0.1462 - val_recall_m: 0.5556\n",
      "Epoch 46/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6192 - f1_m: 0.1667 - precision_m: 0.0909 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 361us/sample - loss: 0.4601 - f1_m: 0.3046 - precision_m: 0.1818 - recall_m: 1.0000 - val_loss: 0.7904 - val_f1_m: 0.1092 - val_precision_m: 0.0625 - val_recall_m: 0.4444\n",
      "Epoch 47/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4559 - f1_m: 0.2727 - precision_m: 0.1579 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 383us/sample - loss: 0.4453 - f1_m: 0.3170 - precision_m: 0.1903 - recall_m: 1.0000 - val_loss: 0.8107 - val_f1_m: 0.2573 - val_precision_m: 0.1863 - val_recall_m: 0.5000\n",
      "Epoch 48/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5778 - f1_m: 0.1053 - precision_m: 0.0556 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 283us/sample - loss: 0.5260 - f1_m: 0.3202 - precision_m: 0.2133 - recall_m: 0.8333 - val_loss: 0.7889 - val_f1_m: 0.1000 - val_precision_m: 0.0625 - val_recall_m: 0.2500\n",
      "Epoch 49/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4343 - f1_m: 0.3846 - precision_m: 0.2381 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 382us/sample - loss: 0.4434 - f1_m: 0.3324 - precision_m: 0.2063 - recall_m: 1.0000 - val_loss: 0.7909 - val_f1_m: 0.1250 - val_precision_m: 0.0833 - val_recall_m: 0.2500\n",
      "Epoch 50/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4227 - f1_m: 0.3478 - precision_m: 0.2105 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 403us/sample - loss: 0.4367 - f1_m: 0.3145 - precision_m: 0.1882 - recall_m: 1.0000 - val_loss: 0.8138 - val_f1_m: 0.1087 - val_precision_m: 0.0643 - val_recall_m: 0.3889\n",
      "Epoch 51/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4952 - f1_m: 0.2308 - precision_m: 0.1304 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 389us/sample - loss: 0.4434 - f1_m: 0.3338 - precision_m: 0.2072 - recall_m: 1.0000 - val_loss: 0.8282 - val_f1_m: 0.0800 - val_precision_m: 0.0476 - val_recall_m: 0.2500\n",
      "Epoch 52/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5721 - f1_m: 0.1818 - precision_m: 0.1000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 348us/sample - loss: 0.4467 - f1_m: 0.2847 - precision_m: 0.1691 - recall_m: 1.0000 - val_loss: 0.8494 - val_f1_m: 0.0924 - val_precision_m: 0.0537 - val_recall_m: 0.5000\n",
      "Epoch 53/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6305 - f1_m: 0.1739 - precision_m: 0.0952 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 398us/sample - loss: 0.4518 - f1_m: 0.3487 - precision_m: 0.2367 - recall_m: 1.0000 - val_loss: 0.8441 - val_f1_m: 0.0833 - val_precision_m: 0.0500 - val_recall_m: 0.2500\n",
      "Epoch 54/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3867 - f1_m: 0.3571 - precision_m: 0.2174 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 348us/sample - loss: 0.4432 - f1_m: 0.3149 - precision_m: 0.1926 - recall_m: 1.0000 - val_loss: 0.8518 - val_f1_m: 0.0925 - val_precision_m: 0.0526 - val_recall_m: 0.5000\n",
      "Epoch 55/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5514 - f1_m: 0.1905 - precision_m: 0.1053 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 353us/sample - loss: 0.4461 - f1_m: 0.3358 - precision_m: 0.2229 - recall_m: 1.0000 - val_loss: 0.8680 - val_f1_m: 0.0873 - val_precision_m: 0.0493 - val_recall_m: 0.3889\n",
      "Epoch 56/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6846 - f1_m: 0.0833 - precision_m: 0.0435 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 398us/sample - loss: 0.4751 - f1_m: 0.3288 - precision_m: 0.2155 - recall_m: 1.0000 - val_loss: 0.8411 - val_f1_m: 0.2327 - val_precision_m: 0.1478 - val_recall_m: 0.6667\n",
      "Epoch 57/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3440 - f1_m: 0.3810 - precision_m: 0.2353 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 439us/sample - loss: 0.4438 - f1_m: 0.2582 - precision_m: 0.1551 - recall_m: 0.8333 - val_loss: 0.8184 - val_f1_m: 0.1035 - val_precision_m: 0.0602 - val_recall_m: 0.3889\n",
      "Epoch 58/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3873 - f1_m: 0.3077 - precision_m: 0.1818 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 378us/sample - loss: 0.4485 - f1_m: 0.2573 - precision_m: 0.1567 - recall_m: 0.8333 - val_loss: 0.7417 - val_f1_m: 0.1157 - val_precision_m: 0.0683 - val_recall_m: 0.3889\n",
      "Epoch 59/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5180 - f1_m: 0.2609 - precision_m: 0.1500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 355us/sample - loss: 0.4991 - f1_m: 0.2504 - precision_m: 0.1727 - recall_m: 0.6667 - val_loss: 0.6402 - val_f1_m: 0.1261 - val_precision_m: 0.0754 - val_recall_m: 0.3889\n",
      "Epoch 60/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4411 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 393us/sample - loss: 0.4503 - f1_m: 0.3417 - precision_m: 0.2167 - recall_m: 0.8917 - val_loss: 0.5743 - val_f1_m: 0.2322 - val_precision_m: 0.1667 - val_recall_m: 0.6667\n",
      "Epoch 61/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4180 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 381us/sample - loss: 0.4583 - f1_m: 0.3450 - precision_m: 0.2315 - recall_m: 0.7500 - val_loss: 0.5816 - val_f1_m: 0.2500 - val_precision_m: 0.1624 - val_recall_m: 0.5556\n",
      "Epoch 62/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4582 - f1_m: 0.4706 - precision_m: 0.3333 - recall_m: 0.8000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 389us/sample - loss: 0.4664 - f1_m: 0.2988 - precision_m: 0.2076 - recall_m: 0.6929 - val_loss: 0.5902 - val_f1_m: 0.2624 - val_precision_m: 0.1667 - val_recall_m: 0.7778\n",
      "Epoch 63/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4581 - f1_m: 0.3810 - precision_m: 0.2500 - recall_m: 0.8000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 400us/sample - loss: 0.4542 - f1_m: 0.3203 - precision_m: 0.2112 - recall_m: 0.7167 - val_loss: 0.6127 - val_f1_m: 0.1250 - val_precision_m: 0.0909 - val_recall_m: 0.2000\n",
      "Epoch 64/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6033 - f1_m: 0.1429 - precision_m: 0.0833 - recall_m: 0.5000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 409us/sample - loss: 0.4628 - f1_m: 0.3978 - precision_m: 0.2706 - recall_m: 0.8929 - val_loss: 0.6317 - val_f1_m: 0.1346 - val_precision_m: 0.0833 - val_recall_m: 0.5000\n",
      "Epoch 65/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3741 - f1_m: 0.4706 - precision_m: 0.3077 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 399us/sample - loss: 0.4387 - f1_m: 0.2857 - precision_m: 0.1847 - recall_m: 0.7857 - val_loss: 0.6610 - val_f1_m: 0.2407 - val_precision_m: 0.1528 - val_recall_m: 0.6667\n",
      "Epoch 66/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4173 - f1_m: 0.2609 - precision_m: 0.1500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 426us/sample - loss: 0.4340 - f1_m: 0.2705 - precision_m: 0.1692 - recall_m: 0.7500 - val_loss: 0.6715 - val_f1_m: 0.2333 - val_precision_m: 0.1528 - val_recall_m: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4820 - f1_m: 0.2500 - precision_m: 0.1429 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 366us/sample - loss: 0.4288 - f1_m: 0.3307 - precision_m: 0.2327 - recall_m: 0.8929 - val_loss: 0.6628 - val_f1_m: 0.1429 - val_precision_m: 0.0907 - val_recall_m: 0.5556\n",
      "Epoch 68/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4105 - f1_m: 0.3478 - precision_m: 0.2105 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 334us/sample - loss: 0.4368 - f1_m: 0.2737 - precision_m: 0.1766 - recall_m: 0.8125 - val_loss: 0.6827 - val_f1_m: 0.1345 - val_precision_m: 0.0828 - val_recall_m: 0.3889\n",
      "Epoch 69/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4763 - f1_m: 0.1905 - precision_m: 0.1053 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 391us/sample - loss: 0.4275 - f1_m: 0.2734 - precision_m: 0.1683 - recall_m: 0.8056 - val_loss: 0.6568 - val_f1_m: 0.1000 - val_precision_m: 0.0588 - val_recall_m: 0.3333\n",
      "Epoch 70/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3342 - f1_m: 0.5600 - precision_m: 0.3889 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 368us/sample - loss: 0.4394 - f1_m: 0.3214 - precision_m: 0.2024 - recall_m: 0.8611 - val_loss: 0.6439 - val_f1_m: 0.1254 - val_precision_m: 0.0747 - val_recall_m: 0.3889\n",
      "Epoch 71/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3862 - f1_m: 0.4545 - precision_m: 0.3125 - recall_m: 0.8333Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 402us/sample - loss: 0.4298 - f1_m: 0.3428 - precision_m: 0.2170 - recall_m: 0.9167 - val_loss: 0.6747 - val_f1_m: 0.1111 - val_precision_m: 0.0667 - val_recall_m: 0.3333\n",
      "Epoch 72/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3904 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 361us/sample - loss: 0.4139 - f1_m: 0.3846 - precision_m: 0.2488 - recall_m: 1.0000 - val_loss: 0.7117 - val_f1_m: 0.1155 - val_precision_m: 0.0709 - val_recall_m: 0.5000\n",
      "Epoch 73/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4472 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 460us/sample - loss: 0.4372 - f1_m: 0.2827 - precision_m: 0.1769 - recall_m: 0.8333 - val_loss: 0.7388 - val_f1_m: 0.1204 - val_precision_m: 0.0698 - val_recall_m: 0.4444\n",
      "Epoch 74/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4784 - f1_m: 0.1818 - precision_m: 0.1000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 359us/sample - loss: 0.4167 - f1_m: 0.3690 - precision_m: 0.2387 - recall_m: 1.0000 - val_loss: 0.7199 - val_f1_m: 0.1176 - val_precision_m: 0.0769 - val_recall_m: 0.2500\n",
      "Epoch 75/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3352 - f1_m: 0.4211 - precision_m: 0.2667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 394us/sample - loss: 0.4276 - f1_m: 0.3506 - precision_m: 0.2213 - recall_m: 1.0000 - val_loss: 0.7255 - val_f1_m: 0.2148 - val_precision_m: 0.1298 - val_recall_m: 0.6667\n",
      "Epoch 76/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3884 - f1_m: 0.4348 - precision_m: 0.2778 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 353us/sample - loss: 0.4184 - f1_m: 0.3233 - precision_m: 0.1977 - recall_m: 1.0000 - val_loss: 0.7457 - val_f1_m: 0.1083 - val_precision_m: 0.0630 - val_recall_m: 0.3889\n",
      "Epoch 77/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3389 - f1_m: 0.4800 - precision_m: 0.3158 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 254us/sample - loss: 0.4177 - f1_m: 0.2753 - precision_m: 0.1676 - recall_m: 0.8333 - val_loss: 0.7327 - val_f1_m: 0.1176 - val_precision_m: 0.0698 - val_recall_m: 0.3889\n",
      "Epoch 78/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4104 - f1_m: 0.2353 - precision_m: 0.1333 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 335us/sample - loss: 0.4234 - f1_m: 0.2702 - precision_m: 0.1732 - recall_m: 0.8095 - val_loss: 0.6530 - val_f1_m: 0.2560 - val_precision_m: 0.1624 - val_recall_m: 0.7778\n",
      "Epoch 79/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4025 - f1_m: 0.4000 - precision_m: 0.2632 - recall_m: 0.8333Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 400us/sample - loss: 0.4107 - f1_m: 0.3609 - precision_m: 0.2290 - recall_m: 0.9167 - val_loss: 0.6147 - val_f1_m: 0.1306 - val_precision_m: 0.0794 - val_recall_m: 0.3889\n",
      "Epoch 80/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4101 - f1_m: 0.3846 - precision_m: 0.2500 - recall_m: 0.8333Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 398us/sample - loss: 0.4190 - f1_m: 0.2957 - precision_m: 0.1880 - recall_m: 0.7639 - val_loss: 0.6272 - val_f1_m: 0.1345 - val_precision_m: 0.0875 - val_recall_m: 0.5000\n",
      "Epoch 81/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3380 - f1_m: 0.4706 - precision_m: 0.3077 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 404us/sample - loss: 0.4242 - f1_m: 0.4102 - precision_m: 0.2964 - recall_m: 0.9111 - val_loss: 0.6155 - val_f1_m: 0.3115 - val_precision_m: 0.2192 - val_recall_m: 0.7778\n",
      "Epoch 82/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3689 - f1_m: 0.5714 - precision_m: 0.4444 - recall_m: 0.8000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 377us/sample - loss: 0.4314 - f1_m: 0.3044 - precision_m: 0.2058 - recall_m: 0.7583 - val_loss: 0.6497 - val_f1_m: 0.2118 - val_precision_m: 0.1587 - val_recall_m: 0.3889\n",
      "Epoch 83/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4098 - f1_m: 0.2105 - precision_m: 0.1176 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166/166 [==============================] - 0s 383us/sample - loss: 0.4581 - f1_m: 0.4047 - precision_m: 0.2939 - recall_m: 0.7639 - val_loss: 0.6114 - val_f1_m: 0.2100 - val_precision_m: 0.1389 - val_recall_m: 0.6667\n",
      "Epoch 84/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3942 - f1_m: 0.2500 - precision_m: 0.1429 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 340us/sample - loss: 0.4228 - f1_m: 0.3000 - precision_m: 0.1931 - recall_m: 0.7583 - val_loss: 0.6188 - val_f1_m: 0.1241 - val_precision_m: 0.0741 - val_recall_m: 0.3889\n",
      "Epoch 85/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3259 - f1_m: 0.5833 - precision_m: 0.4118 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 390us/sample - loss: 0.4430 - f1_m: 0.3694 - precision_m: 0.2512 - recall_m: 0.7778 - val_loss: 0.6374 - val_f1_m: 0.1278 - val_precision_m: 0.0769 - val_recall_m: 0.3889\n",
      "Epoch 86/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3548 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 350us/sample - loss: 0.4095 - f1_m: 0.3799 - precision_m: 0.2479 - recall_m: 0.9167 - val_loss: 0.6789 - val_f1_m: 0.1503 - val_precision_m: 0.0905 - val_recall_m: 0.4444\n",
      "Epoch 87/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3846 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 355us/sample - loss: 0.4028 - f1_m: 0.3831 - precision_m: 0.2508 - recall_m: 1.0000 - val_loss: 0.7155 - val_f1_m: 0.1083 - val_precision_m: 0.0630 - val_recall_m: 0.3889\n",
      "Epoch 88/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5720 - f1_m: 0.1000 - precision_m: 0.0526 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 337us/sample - loss: 0.4028 - f1_m: 0.3296 - precision_m: 0.2035 - recall_m: 1.0000 - val_loss: 0.7311 - val_f1_m: 0.1053 - val_precision_m: 0.0714 - val_recall_m: 0.2000\n",
      "Epoch 89/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4159 - f1_m: 0.2222 - precision_m: 0.1250 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 353us/sample - loss: 0.4113 - f1_m: 0.2771 - precision_m: 0.1722 - recall_m: 0.8333 - val_loss: 0.7381 - val_f1_m: 0.1133 - val_precision_m: 0.0685 - val_recall_m: 0.5000\n",
      "Epoch 90/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4477 - f1_m: 0.2400 - precision_m: 0.1364 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 383us/sample - loss: 0.4067 - f1_m: 0.2903 - precision_m: 0.1777 - recall_m: 0.8333 - val_loss: 0.7035 - val_f1_m: 0.1157 - val_precision_m: 0.0698 - val_recall_m: 0.5000\n",
      "Epoch 91/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4208 - f1_m: 0.3158 - precision_m: 0.1875 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 418us/sample - loss: 0.4036 - f1_m: 0.3399 - precision_m: 0.2085 - recall_m: 1.0000 - val_loss: 0.6681 - val_f1_m: 0.1217 - val_precision_m: 0.0720 - val_recall_m: 0.4444\n",
      "Epoch 92/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3573 - f1_m: 0.5714 - precision_m: 0.4000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 367us/sample - loss: 0.3976 - f1_m: 0.3365 - precision_m: 0.2099 - recall_m: 1.0000 - val_loss: 0.6610 - val_f1_m: 0.2451 - val_precision_m: 0.1587 - val_recall_m: 0.5556\n",
      "Epoch 93/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3591 - f1_m: 0.3200 - precision_m: 0.1905 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 385us/sample - loss: 0.4123 - f1_m: 0.2855 - precision_m: 0.1788 - recall_m: 0.7500 - val_loss: 0.6689 - val_f1_m: 0.1217 - val_precision_m: 0.0733 - val_recall_m: 0.5000\n",
      "Epoch 94/1000\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 0.3971 - f1_m: 0.3482 - precision_m: 0.2177 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 509us/sample - loss: 0.4014 - f1_m: 0.2901 - precision_m: 0.1814 - recall_m: 0.8333 - val_loss: 0.6458 - val_f1_m: 0.1111 - val_precision_m: 0.0714 - val_recall_m: 0.2500\n",
      "Epoch 95/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4574 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 297us/sample - loss: 0.3916 - f1_m: 0.2996 - precision_m: 0.1864 - recall_m: 0.8333 - val_loss: 0.6268 - val_f1_m: 0.1053 - val_precision_m: 0.0714 - val_recall_m: 0.2000\n",
      "Epoch 96/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4404 - f1_m: 0.1538 - precision_m: 0.0833 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 297us/sample - loss: 0.3970 - f1_m: 0.2867 - precision_m: 0.1874 - recall_m: 0.8125 - val_loss: 0.6446 - val_f1_m: 0.1333 - val_precision_m: 0.0794 - val_recall_m: 0.5556\n",
      "Epoch 97/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6265 - f1_m: 0.1905 - precision_m: 0.1053 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 394us/sample - loss: 0.4204 - f1_m: 0.2929 - precision_m: 0.1894 - recall_m: 0.7917 - val_loss: 0.6355 - val_f1_m: 0.1146 - val_precision_m: 0.0673 - val_recall_m: 0.3889\n",
      "Epoch 98/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4070 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 450us/sample - loss: 0.4058 - f1_m: 0.3822 - precision_m: 0.2562 - recall_m: 0.9167 - val_loss: 0.6022 - val_f1_m: 0.3248 - val_precision_m: 0.2273 - val_recall_m: 0.6667\n",
      "Epoch 99/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3568 - f1_m: 0.6000 - precision_m: 0.4615 - recall_m: 0.8571Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 328us/sample - loss: 0.4036 - f1_m: 0.3077 - precision_m: 0.2052 - recall_m: 0.7540 - val_loss: 0.6145 - val_f1_m: 0.4180 - val_precision_m: 0.3819 - val_recall_m: 0.6667\n",
      "Epoch 100/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3697 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 423us/sample - loss: 0.3914 - f1_m: 0.3173 - precision_m: 0.2011 - recall_m: 0.7917 - val_loss: 0.6027 - val_f1_m: 0.1250 - val_precision_m: 0.0769 - val_recall_m: 0.3333\n",
      "Epoch 101/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4162 - f1_m: 0.4286 - precision_m: 0.2727 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 326us/sample - loss: 0.3897 - f1_m: 0.3882 - precision_m: 0.2437 - recall_m: 0.9583 - val_loss: 0.5974 - val_f1_m: 0.1306 - val_precision_m: 0.0778 - val_recall_m: 0.5556\n",
      "Epoch 102/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3010 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 255us/sample - loss: 0.3849 - f1_m: 0.3768 - precision_m: 0.2389 - recall_m: 0.9583 - val_loss: 0.6189 - val_f1_m: 0.1333 - val_precision_m: 0.0791 - val_recall_m: 0.4444\n",
      "Epoch 103/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4182 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 382us/sample - loss: 0.4115 - f1_m: 0.3174 - precision_m: 0.2020 - recall_m: 0.8333 - val_loss: 0.6497 - val_f1_m: 0.1278 - val_precision_m: 0.0769 - val_recall_m: 0.3889\n",
      "Epoch 104/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.2540 - f1_m: 0.5217 - precision_m: 0.3529 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 415us/sample - loss: 0.3916 - f1_m: 0.3735 - precision_m: 0.2373 - recall_m: 1.0000 - val_loss: 0.6979 - val_f1_m: 0.1176 - val_precision_m: 0.0769 - val_recall_m: 0.2500\n",
      "Epoch 105/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3449 - f1_m: 0.4545 - precision_m: 0.2941 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 243us/sample - loss: 0.4161 - f1_m: 0.3700 - precision_m: 0.2461 - recall_m: 0.8333 - val_loss: 0.7150 - val_f1_m: 0.2513 - val_precision_m: 0.1590 - val_recall_m: 0.7778\n",
      "Epoch 106/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3695 - f1_m: 0.2353 - precision_m: 0.1333 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 377us/sample - loss: 0.4132 - f1_m: 0.2705 - precision_m: 0.1733 - recall_m: 0.8333 - val_loss: 0.6885 - val_f1_m: 0.1278 - val_precision_m: 0.0754 - val_recall_m: 0.4444\n",
      "Epoch 107/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5403 - f1_m: 0.2222 - precision_m: 0.1250 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 418us/sample - loss: 0.3912 - f1_m: 0.3910 - precision_m: 0.2579 - recall_m: 1.0000 - val_loss: 0.6296 - val_f1_m: 0.1278 - val_precision_m: 0.0769 - val_recall_m: 0.3889\n",
      "Epoch 108/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3756 - f1_m: 0.3000 - precision_m: 0.1765 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 291us/sample - loss: 0.3838 - f1_m: 0.3415 - precision_m: 0.2184 - recall_m: 0.9792 - val_loss: 0.6033 - val_f1_m: 0.2302 - val_precision_m: 0.1481 - val_recall_m: 0.5556\n",
      "Epoch 109/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4472 - f1_m: 0.1538 - precision_m: 0.0833 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 186us/sample - loss: 0.3814 - f1_m: 0.3713 - precision_m: 0.2399 - recall_m: 0.9722 - val_loss: 0.6012 - val_f1_m: 0.1397 - val_precision_m: 0.0844 - val_recall_m: 0.5556\n",
      "Epoch 110/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3169 - f1_m: 0.3636 - precision_m: 0.2222 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "\n",
      "Epoch 00110: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
      "166/166 [==============================] - 0s 346us/sample - loss: 0.3931 - f1_m: 0.3101 - precision_m: 0.2000 - recall_m: 0.7778 - val_loss: 0.6392 - val_f1_m: 0.1229 - val_precision_m: 0.0733 - val_recall_m: 0.3889\n",
      "Epoch 111/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4494 - f1_m: 0.3636 - precision_m: 0.2222 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 272us/sample - loss: 0.4000 - f1_m: 0.2933 - precision_m: 0.1870 - recall_m: 0.8333 - val_loss: 0.6388 - val_f1_m: 0.1148 - val_precision_m: 0.0673 - val_recall_m: 0.3889\n",
      "Epoch 112/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3932 - f1_m: 0.2609 - precision_m: 0.1500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 282us/sample - loss: 0.3674 - f1_m: 0.3087 - precision_m: 0.1923 - recall_m: 0.8333 - val_loss: 0.6381 - val_f1_m: 0.1444 - val_precision_m: 0.0852 - val_recall_m: 0.5000\n",
      "Epoch 113/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3162 - f1_m: 0.3810 - precision_m: 0.2353 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 568us/sample - loss: 0.3792 - f1_m: 0.2954 - precision_m: 0.1845 - recall_m: 0.8333 - val_loss: 0.6373 - val_f1_m: 0.1222 - val_precision_m: 0.0725 - val_recall_m: 0.3889\n",
      "Epoch 114/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3486 - f1_m: 0.3750 - precision_m: 0.2308 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 239us/sample - loss: 0.3728 - f1_m: 0.3636 - precision_m: 0.2241 - recall_m: 1.0000 - val_loss: 0.6366 - val_f1_m: 0.2254 - val_precision_m: 0.1627 - val_recall_m: 0.6667\n",
      "Epoch 115/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4297 - f1_m: 0.1818 - precision_m: 0.1000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 344us/sample - loss: 0.4043 - f1_m: 0.3819 - precision_m: 0.2490 - recall_m: 1.0000 - val_loss: 0.6365 - val_f1_m: 0.1333 - val_precision_m: 0.0769 - val_recall_m: 0.5000\n",
      "Epoch 116/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4208 - f1_m: 0.2727 - precision_m: 0.1579 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 321us/sample - loss: 0.3636 - f1_m: 0.4131 - precision_m: 0.2689 - recall_m: 1.0000 - val_loss: 0.6370 - val_f1_m: 0.2083 - val_precision_m: 0.1226 - val_recall_m: 0.7778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3743 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 368us/sample - loss: 0.3749 - f1_m: 0.3552 - precision_m: 0.2239 - recall_m: 1.0000 - val_loss: 0.6381 - val_f1_m: 0.1000 - val_precision_m: 0.0588 - val_recall_m: 0.3333\n",
      "Epoch 118/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.2918 - f1_m: 0.4706 - precision_m: 0.3077 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 365us/sample - loss: 0.3873 - f1_m: 0.2990 - precision_m: 0.1908 - recall_m: 0.8333 - val_loss: 0.6385 - val_f1_m: 0.1215 - val_precision_m: 0.0722 - val_recall_m: 0.5000\n",
      "Epoch 119/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.2871 - f1_m: 0.5714 - precision_m: 0.4000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 355us/sample - loss: 0.3816 - f1_m: 0.3114 - precision_m: 0.2015 - recall_m: 0.8333 - val_loss: 0.6381 - val_f1_m: 0.1278 - val_precision_m: 0.0769 - val_recall_m: 0.3889\n",
      "Epoch 120/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3266 - f1_m: 0.3636 - precision_m: 0.2222 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 242us/sample - loss: 0.3601 - f1_m: 0.4089 - precision_m: 0.2653 - recall_m: 1.0000 - val_loss: 0.6375 - val_f1_m: 0.1000 - val_precision_m: 0.0625 - val_recall_m: 0.2500\n",
      "Epoch 121/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3699 - f1_m: 0.3750 - precision_m: 0.2308 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 240us/sample - loss: 0.3970 - f1_m: 0.3025 - precision_m: 0.1912 - recall_m: 0.8333 - val_loss: 0.6378 - val_f1_m: 0.1250 - val_precision_m: 0.0909 - val_recall_m: 0.2000\n",
      "Epoch 122/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3169 - f1_m: 0.5455 - precision_m: 0.3750 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 294us/sample - loss: 0.3830 - f1_m: 0.2960 - precision_m: 0.1857 - recall_m: 0.8333 - val_loss: 0.6374 - val_f1_m: 0.1261 - val_precision_m: 0.0733 - val_recall_m: 0.5556\n",
      "Epoch 123/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.2687 - f1_m: 0.4706 - precision_m: 0.3077 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 297us/sample - loss: 0.4003 - f1_m: 0.4167 - precision_m: 0.3000 - recall_m: 1.0000 - val_loss: 0.6362 - val_f1_m: 0.1365 - val_precision_m: 0.0791 - val_recall_m: 0.5000\n",
      "Epoch 124/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3171 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 234us/sample - loss: 0.3882 - f1_m: 0.3668 - precision_m: 0.2296 - recall_m: 1.0000 - val_loss: 0.6363 - val_f1_m: 0.1396 - val_precision_m: 0.0828 - val_recall_m: 0.4444\n",
      "Epoch 125/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3370 - f1_m: 0.5455 - precision_m: 0.3750 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 305us/sample - loss: 0.3692 - f1_m: 0.3704 - precision_m: 0.2324 - recall_m: 1.0000 - val_loss: 0.6367 - val_f1_m: 0.1308 - val_precision_m: 0.0787 - val_recall_m: 0.3889\n",
      "Epoch 126/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.2713 - f1_m: 0.4762 - precision_m: 0.3125 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 404us/sample - loss: 0.3915 - f1_m: 0.3128 - precision_m: 0.1957 - recall_m: 0.8333 - val_loss: 0.6376 - val_f1_m: 0.1281 - val_precision_m: 0.0814 - val_recall_m: 0.5000\n",
      "Epoch 127/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4658 - f1_m: 0.3000 - precision_m: 0.1765 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 352us/sample - loss: 0.3769 - f1_m: 0.2996 - precision_m: 0.1874 - recall_m: 0.8333 - val_loss: 0.6377 - val_f1_m: 0.1310 - val_precision_m: 0.0791 - val_recall_m: 0.3889\n",
      "Epoch 128/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3924 - f1_m: 0.4545 - precision_m: 0.2941 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 445us/sample - loss: 0.3836 - f1_m: 0.3006 - precision_m: 0.1881 - recall_m: 0.8333 - val_loss: 0.6372 - val_f1_m: 0.1053 - val_precision_m: 0.0714 - val_recall_m: 0.2000\n",
      "Epoch 129/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3492 - f1_m: 0.3529 - precision_m: 0.2143 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 332us/sample - loss: 0.3759 - f1_m: 0.3137 - precision_m: 0.1941 - recall_m: 0.8333 - val_loss: 0.6364 - val_f1_m: 0.1240 - val_precision_m: 0.0791 - val_recall_m: 0.5000\n",
      "Epoch 130/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3766 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 435us/sample - loss: 0.3656 - f1_m: 0.4527 - precision_m: 0.3413 - recall_m: 1.0000 - val_loss: 0.6358 - val_f1_m: 0.1254 - val_precision_m: 0.0720 - val_recall_m: 0.5000\n",
      "Epoch 131/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3699 - f1_m: 0.2727 - precision_m: 0.1579 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 456us/sample - loss: 0.3702 - f1_m: 0.3827 - precision_m: 0.2418 - recall_m: 1.0000 - val_loss: 0.6362 - val_f1_m: 0.1261 - val_precision_m: 0.0769 - val_recall_m: 0.5000\n",
      "Epoch 132/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3918 - f1_m: 0.5833 - precision_m: 0.4118 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 373us/sample - loss: 0.3727 - f1_m: 0.2956 - precision_m: 0.1880 - recall_m: 0.8333 - val_loss: 0.6369 - val_f1_m: 0.1310 - val_precision_m: 0.0812 - val_recall_m: 0.5000\n",
      "Epoch 133/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3811 - f1_m: 0.3636 - precision_m: 0.2222 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 391us/sample - loss: 0.3836 - f1_m: 0.3177 - precision_m: 0.1990 - recall_m: 0.8333 - val_loss: 0.6367 - val_f1_m: 0.2281 - val_precision_m: 0.1374 - val_recall_m: 0.7778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3013 - f1_m: 0.4762 - precision_m: 0.3125 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 359us/sample - loss: 0.3689 - f1_m: 0.4068 - precision_m: 0.2684 - recall_m: 1.0000 - val_loss: 0.6363 - val_f1_m: 0.2254 - val_precision_m: 0.1624 - val_recall_m: 0.6667\n",
      "Epoch 135/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3486 - f1_m: 0.4615 - precision_m: 0.3000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 376us/sample - loss: 0.3841 - f1_m: 0.4128 - precision_m: 0.2769 - recall_m: 1.0000 - val_loss: 0.6363 - val_f1_m: 0.1254 - val_precision_m: 0.0754 - val_recall_m: 0.5000\n",
      "Epoch 136/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4069 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 331us/sample - loss: 0.3780 - f1_m: 0.3273 - precision_m: 0.2137 - recall_m: 0.8333 - val_loss: 0.6368 - val_f1_m: 0.1347 - val_precision_m: 0.0778 - val_recall_m: 0.5556\n",
      "Epoch 137/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4914 - f1_m: 0.0952 - precision_m: 0.0500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 305us/sample - loss: 0.3906 - f1_m: 0.3208 - precision_m: 0.2069 - recall_m: 0.8333 - val_loss: 0.6364 - val_f1_m: 0.1185 - val_precision_m: 0.0701 - val_recall_m: 0.3889\n",
      "Epoch 138/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4657 - f1_m: 0.1818 - precision_m: 0.1000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 342us/sample - loss: 0.3777 - f1_m: 0.3549 - precision_m: 0.2212 - recall_m: 1.0000 - val_loss: 0.6356 - val_f1_m: 0.1333 - val_precision_m: 0.0769 - val_recall_m: 0.5000\n",
      "Epoch 139/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3433 - f1_m: 0.3750 - precision_m: 0.2308 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 355us/sample - loss: 0.3784 - f1_m: 0.3013 - precision_m: 0.1855 - recall_m: 0.8333 - val_loss: 0.6354 - val_f1_m: 0.1111 - val_precision_m: 0.0667 - val_recall_m: 0.3333\n",
      "Epoch 140/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3312 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 403us/sample - loss: 0.3689 - f1_m: 0.3715 - precision_m: 0.2338 - recall_m: 1.0000 - val_loss: 0.6345 - val_f1_m: 0.1952 - val_precision_m: 0.1136 - val_recall_m: 0.7778\n",
      "Epoch 141/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.2908 - f1_m: 0.4348 - precision_m: 0.2778 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 431us/sample - loss: 0.3758 - f1_m: 0.3153 - precision_m: 0.1956 - recall_m: 0.8333 - val_loss: 0.6343 - val_f1_m: 0.1278 - val_precision_m: 0.0769 - val_recall_m: 0.3889\n",
      "Epoch 142/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.2763 - f1_m: 0.6400 - precision_m: 0.4706 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 361us/sample - loss: 0.4037 - f1_m: 0.2960 - precision_m: 0.1924 - recall_m: 0.8333 - val_loss: 0.6337 - val_f1_m: 0.1053 - val_precision_m: 0.0667 - val_recall_m: 0.2500\n",
      "Epoch 143/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3505 - f1_m: 0.3000 - precision_m: 0.1765 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 400us/sample - loss: 0.3765 - f1_m: 0.3041 - precision_m: 0.1897 - recall_m: 0.8333 - val_loss: 0.6327 - val_f1_m: 0.1201 - val_precision_m: 0.0714 - val_recall_m: 0.3889\n",
      "Epoch 144/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3343 - f1_m: 0.5600 - precision_m: 0.3889 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 328us/sample - loss: 0.3895 - f1_m: 0.3614 - precision_m: 0.2331 - recall_m: 1.0000 - val_loss: 0.6318 - val_f1_m: 0.1418 - val_precision_m: 0.0949 - val_recall_m: 0.5000\n",
      "Epoch 145/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3407 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 416us/sample - loss: 0.3707 - f1_m: 0.3162 - precision_m: 0.1970 - recall_m: 0.8333 - val_loss: 0.6313 - val_f1_m: 0.1297 - val_precision_m: 0.0791 - val_recall_m: 0.5000\n",
      "Epoch 146/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5255 - f1_m: 0.2105 - precision_m: 0.1176 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 433us/sample - loss: 0.3882 - f1_m: 0.3224 - precision_m: 0.2069 - recall_m: 0.8333 - val_loss: 0.6308 - val_f1_m: 0.1217 - val_precision_m: 0.0701 - val_recall_m: 0.5556\n",
      "Epoch 147/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3724 - f1_m: 0.4000 - precision_m: 0.2667 - recall_m: 0.8000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 391us/sample - loss: 0.3636 - f1_m: 0.3951 - precision_m: 0.2586 - recall_m: 0.9667 - val_loss: 0.6301 - val_f1_m: 0.1278 - val_precision_m: 0.0769 - val_recall_m: 0.3889\n",
      "Epoch 148/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5060 - f1_m: 0.2222 - precision_m: 0.1333 - recall_m: 0.6667Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 358us/sample - loss: 0.3788 - f1_m: 0.3927 - precision_m: 0.2566 - recall_m: 0.9444 - val_loss: 0.6302 - val_f1_m: 0.1261 - val_precision_m: 0.0747 - val_recall_m: 0.4444\n",
      "Epoch 149/1000\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 0.3757 - f1_m: 0.3412 - precision_m: 0.2114 - recall_m: 0.9333Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 543us/sample - loss: 0.3722 - f1_m: 0.3677 - precision_m: 0.2318 - recall_m: 0.9444 - val_loss: 0.6307 - val_f1_m: 0.1377 - val_precision_m: 0.0863 - val_recall_m: 0.3889\n",
      "Epoch 150/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.2797 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 392us/sample - loss: 0.3758 - f1_m: 0.3170 - precision_m: 0.1997 - recall_m: 0.8333 - val_loss: 0.6312 - val_f1_m: 0.1179 - val_precision_m: 0.0695 - val_recall_m: 0.3889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4423 - f1_m: 0.3478 - precision_m: 0.2105 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 383us/sample - loss: 0.3754 - f1_m: 0.4179 - precision_m: 0.2820 - recall_m: 1.0000 - val_loss: 0.6312 - val_f1_m: 0.0909 - val_precision_m: 0.0556 - val_recall_m: 0.2500\n",
      "Epoch 152/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3668 - f1_m: 0.2667 - precision_m: 0.1538 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 257us/sample - loss: 0.3674 - f1_m: 0.4218 - precision_m: 0.2878 - recall_m: 1.0000 - val_loss: 0.6315 - val_f1_m: 0.1000 - val_precision_m: 0.0625 - val_recall_m: 0.2500\n",
      "Epoch 153/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3733 - f1_m: 0.2667 - precision_m: 0.1538 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 289us/sample - loss: 0.3857 - f1_m: 0.3006 - precision_m: 0.1930 - recall_m: 0.8333 - val_loss: 0.6321 - val_f1_m: 0.1254 - val_precision_m: 0.0747 - val_recall_m: 0.3889\n",
      "Epoch 154/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3421 - f1_m: 0.4444 - precision_m: 0.2857 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 290us/sample - loss: 0.3690 - f1_m: 0.3902 - precision_m: 0.2476 - recall_m: 1.0000 - val_loss: 0.6323 - val_f1_m: 0.3083 - val_precision_m: 0.2167 - val_recall_m: 0.7778\n",
      "Epoch 155/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4450 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 336us/sample - loss: 0.3787 - f1_m: 0.3071 - precision_m: 0.1904 - recall_m: 0.8333 - val_loss: 0.6325 - val_f1_m: 0.1250 - val_precision_m: 0.0909 - val_recall_m: 0.2000\n",
      "Epoch 156/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3645 - f1_m: 0.3158 - precision_m: 0.1875 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 340us/sample - loss: 0.3616 - f1_m: 0.4159 - precision_m: 0.2711 - recall_m: 1.0000 - val_loss: 0.6324 - val_f1_m: 0.2556 - val_precision_m: 0.1667 - val_recall_m: 0.5556\n",
      "Epoch 157/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4904 - f1_m: 0.4167 - precision_m: 0.2632 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 309us/sample - loss: 0.3633 - f1_m: 0.3775 - precision_m: 0.2362 - recall_m: 1.0000 - val_loss: 0.6329 - val_f1_m: 0.1229 - val_precision_m: 0.0722 - val_recall_m: 0.4444\n",
      "Epoch 158/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3801 - f1_m: 0.5385 - precision_m: 0.3684 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 300us/sample - loss: 0.3904 - f1_m: 0.3640 - precision_m: 0.2356 - recall_m: 1.0000 - val_loss: 0.6330 - val_f1_m: 0.1310 - val_precision_m: 0.0791 - val_recall_m: 0.3889\n",
      "Epoch 159/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4292 - f1_m: 0.4211 - precision_m: 0.2667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 383us/sample - loss: 0.3747 - f1_m: 0.4363 - precision_m: 0.3000 - recall_m: 1.0000 - val_loss: 0.6331 - val_f1_m: 0.2281 - val_precision_m: 0.1374 - val_recall_m: 0.7778\n",
      "Epoch 160/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3030 - f1_m: 0.4545 - precision_m: 0.2941 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "\n",
      "Epoch 00160: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "166/166 [==============================] - 0s 363us/sample - loss: 0.3703 - f1_m: 0.4085 - precision_m: 0.2656 - recall_m: 1.0000 - val_loss: 0.6338 - val_f1_m: 0.2118 - val_precision_m: 0.1310 - val_recall_m: 0.5556\n",
      "Epoch 161/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3145 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 336us/sample - loss: 0.3921 - f1_m: 0.3437 - precision_m: 0.2199 - recall_m: 1.0000 - val_loss: 0.6338 - val_f1_m: 0.1261 - val_precision_m: 0.0769 - val_recall_m: 0.5000\n",
      "Epoch 162/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3842 - f1_m: 0.2105 - precision_m: 0.1176 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 387us/sample - loss: 0.3736 - f1_m: 0.3986 - precision_m: 0.2638 - recall_m: 1.0000 - val_loss: 0.6338 - val_f1_m: 0.2254 - val_precision_m: 0.1368 - val_recall_m: 0.7778\n",
      "Epoch 163/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3366 - f1_m: 0.5385 - precision_m: 0.3684 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 345us/sample - loss: 0.3806 - f1_m: 0.3022 - precision_m: 0.1884 - recall_m: 0.8333 - val_loss: 0.6339 - val_f1_m: 0.1281 - val_precision_m: 0.0778 - val_recall_m: 0.3889\n",
      "Epoch 164/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3393 - f1_m: 0.3158 - precision_m: 0.1875 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 272us/sample - loss: 0.3746 - f1_m: 0.3655 - precision_m: 0.2276 - recall_m: 1.0000 - val_loss: 0.6338 - val_f1_m: 0.1217 - val_precision_m: 0.0720 - val_recall_m: 0.4444\n",
      "Epoch 165/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5318 - f1_m: 0.1429 - precision_m: 0.0769 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 367us/sample - loss: 0.4026 - f1_m: 0.3959 - precision_m: 0.2772 - recall_m: 1.0000 - val_loss: 0.6338 - val_f1_m: 0.2263 - val_precision_m: 0.1374 - val_recall_m: 0.6667\n",
      "Epoch 166/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.7140 - f1_m: 0.1250 - precision_m: 0.0667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 357us/sample - loss: 0.4146 - f1_m: 0.3885 - precision_m: 0.2606 - recall_m: 1.0000 - val_loss: 0.6339 - val_f1_m: 0.1000 - val_precision_m: 0.0625 - val_recall_m: 0.2500\n",
      "Epoch 167/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3191 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166/166 [==============================] - 0s 439us/sample - loss: 0.4260 - f1_m: 0.2942 - precision_m: 0.1951 - recall_m: 0.6667 - val_loss: 0.6339 - val_f1_m: 0.1345 - val_precision_m: 0.0778 - val_recall_m: 0.5000\n",
      "Epoch 168/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5474 - f1_m: 0.2000 - precision_m: 0.1111 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 317us/sample - loss: 0.4088 - f1_m: 0.2988 - precision_m: 0.2017 - recall_m: 0.8333 - val_loss: 0.6339 - val_f1_m: 0.3111 - val_precision_m: 0.2222 - val_recall_m: 0.5556\n",
      "Epoch 169/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3603 - f1_m: 0.4348 - precision_m: 0.2778 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 235us/sample - loss: 0.3646 - f1_m: 0.3747 - precision_m: 0.2324 - recall_m: 1.0000 - val_loss: 0.6338 - val_f1_m: 0.1185 - val_precision_m: 0.0701 - val_recall_m: 0.3889\n",
      "Epoch 170/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4318 - f1_m: 0.3000 - precision_m: 0.1765 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 449us/sample - loss: 0.4120 - f1_m: 0.2828 - precision_m: 0.1840 - recall_m: 0.6667 - val_loss: 0.6338 - val_f1_m: 0.1323 - val_precision_m: 0.0863 - val_recall_m: 0.5000\n",
      "Epoch 171/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4280 - f1_m: 0.4800 - precision_m: 0.3158 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 360us/sample - loss: 0.3631 - f1_m: 0.3060 - precision_m: 0.1917 - recall_m: 0.8333 - val_loss: 0.6338 - val_f1_m: 0.1250 - val_precision_m: 0.0714 - val_recall_m: 0.5000\n",
      "Epoch 172/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6750 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 368us/sample - loss: 0.4093 - f1_m: 0.3705 - precision_m: 0.2417 - recall_m: 0.8333 - val_loss: 0.6337 - val_f1_m: 0.1000 - val_precision_m: 0.0625 - val_recall_m: 0.2500\n",
      "Epoch 173/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3971 - f1_m: 0.2105 - precision_m: 0.1176 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 323us/sample - loss: 0.3810 - f1_m: 0.3927 - precision_m: 0.2594 - recall_m: 1.0000 - val_loss: 0.6337 - val_f1_m: 0.1257 - val_precision_m: 0.0747 - val_recall_m: 0.5000\n",
      "Epoch 174/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3428 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 309us/sample - loss: 0.3884 - f1_m: 0.3701 - precision_m: 0.2329 - recall_m: 1.0000 - val_loss: 0.6337 - val_f1_m: 0.1225 - val_precision_m: 0.0764 - val_recall_m: 0.5000\n",
      "Epoch 175/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.6853 - f1_m: 0.1111 - precision_m: 0.0588 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 389us/sample - loss: 0.4482 - f1_m: 0.3566 - precision_m: 0.2368 - recall_m: 0.8333 - val_loss: 0.6337 - val_f1_m: 0.2000 - val_precision_m: 0.1225 - val_recall_m: 0.5556\n",
      "Epoch 176/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3067 - f1_m: 0.4800 - precision_m: 0.3158 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 398us/sample - loss: 0.3858 - f1_m: 0.3015 - precision_m: 0.1901 - recall_m: 0.8333 - val_loss: 0.6338 - val_f1_m: 0.1215 - val_precision_m: 0.0695 - val_recall_m: 0.5000\n",
      "Epoch 177/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.2745 - f1_m: 0.4545 - precision_m: 0.2941 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 395us/sample - loss: 0.3839 - f1_m: 0.3503 - precision_m: 0.2205 - recall_m: 1.0000 - val_loss: 0.6337 - val_f1_m: 0.1345 - val_precision_m: 0.0778 - val_recall_m: 0.5000\n",
      "Epoch 178/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4375 - f1_m: 0.3478 - precision_m: 0.2105 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 421us/sample - loss: 0.3635 - f1_m: 0.3699 - precision_m: 0.2295 - recall_m: 1.0000 - val_loss: 0.6338 - val_f1_m: 0.3083 - val_precision_m: 0.2161 - val_recall_m: 0.6667\n",
      "Epoch 179/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4288 - f1_m: 0.1905 - precision_m: 0.1053 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 439us/sample - loss: 0.3708 - f1_m: 0.3696 - precision_m: 0.2321 - recall_m: 1.0000 - val_loss: 0.6338 - val_f1_m: 0.3111 - val_precision_m: 0.2479 - val_recall_m: 0.5000\n",
      "Epoch 180/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3752 - f1_m: 0.3000 - precision_m: 0.1765 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 332us/sample - loss: 0.3851 - f1_m: 0.3727 - precision_m: 0.2370 - recall_m: 1.0000 - val_loss: 0.6338 - val_f1_m: 0.2063 - val_precision_m: 0.1226 - val_recall_m: 0.7778\n",
      "Epoch 181/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.2918 - f1_m: 0.4167 - precision_m: 0.2632 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 297us/sample - loss: 0.3992 - f1_m: 0.3127 - precision_m: 0.2040 - recall_m: 0.8333 - val_loss: 0.6338 - val_f1_m: 0.1176 - val_precision_m: 0.0833 - val_recall_m: 0.2000\n",
      "Epoch 182/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3298 - f1_m: 0.4800 - precision_m: 0.3158 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 395us/sample - loss: 0.4110 - f1_m: 0.3846 - precision_m: 0.2730 - recall_m: 0.8333 - val_loss: 0.6338 - val_f1_m: 0.1254 - val_precision_m: 0.0747 - val_recall_m: 0.3889\n",
      "Epoch 183/1000\n",
      "128/166 [======================>.......] - ETA: 0s - loss: 0.3731 - f1_m: 0.3554 - precision_m: 0.2212 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 507us/sample - loss: 0.3745 - f1_m: 0.3917 - precision_m: 0.2485 - recall_m: 1.0000 - val_loss: 0.6338 - val_f1_m: 0.1944 - val_precision_m: 0.1346 - val_recall_m: 0.3889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4742 - f1_m: 0.1333 - precision_m: 0.0714 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 384us/sample - loss: 0.3934 - f1_m: 0.3795 - precision_m: 0.2532 - recall_m: 1.0000 - val_loss: 0.6338 - val_f1_m: 0.3333 - val_precision_m: 0.2333 - val_recall_m: 0.6667\n",
      "Epoch 185/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4003 - f1_m: 0.2400 - precision_m: 0.1364 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 311us/sample - loss: 0.3948 - f1_m: 0.4546 - precision_m: 0.3462 - recall_m: 1.0000 - val_loss: 0.6338 - val_f1_m: 0.1254 - val_precision_m: 0.0754 - val_recall_m: 0.5000\n",
      "Epoch 186/1000\n",
      " 96/166 [================>.............] - ETA: 0s - loss: 0.4284 - f1_m: 0.3013 - precision_m: 0.1861 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 730us/sample - loss: 0.3988 - f1_m: 0.3470 - precision_m: 0.2157 - recall_m: 1.0000 - val_loss: 0.6339 - val_f1_m: 0.3143 - val_precision_m: 0.2525 - val_recall_m: 0.4444\n",
      "Epoch 187/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4582 - f1_m: 0.3000 - precision_m: 0.1765 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 222us/sample - loss: 0.3711 - f1_m: 0.3670 - precision_m: 0.2298 - recall_m: 1.0000 - val_loss: 0.6339 - val_f1_m: 0.1201 - val_precision_m: 0.0735 - val_recall_m: 0.5000\n",
      "Epoch 188/1000\n",
      "128/166 [======================>.......] - ETA: 0s - loss: 0.3531 - f1_m: 0.4074 - precision_m: 0.2645 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 791us/sample - loss: 0.3873 - f1_m: 0.3216 - precision_m: 0.2057 - recall_m: 0.8333 - val_loss: 0.6339 - val_f1_m: 0.1053 - val_precision_m: 0.0667 - val_recall_m: 0.2500\n",
      "Epoch 189/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4171 - f1_m: 0.4444 - precision_m: 0.2857 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 275us/sample - loss: 0.3947 - f1_m: 0.3106 - precision_m: 0.1971 - recall_m: 0.8333 - val_loss: 0.6339 - val_f1_m: 0.1278 - val_precision_m: 0.0794 - val_recall_m: 0.5000\n",
      "Epoch 190/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3858 - f1_m: 0.2609 - precision_m: 0.1500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 433us/sample - loss: 0.3659 - f1_m: 0.3160 - precision_m: 0.1966 - recall_m: 0.8333 - val_loss: 0.6339 - val_f1_m: 0.1176 - val_precision_m: 0.0714 - val_recall_m: 0.3333\n",
      "Epoch 191/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3163 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 444us/sample - loss: 0.3687 - f1_m: 0.4107 - precision_m: 0.2683 - recall_m: 1.0000 - val_loss: 0.6338 - val_f1_m: 0.1254 - val_precision_m: 0.0722 - val_recall_m: 0.5556\n",
      "Epoch 192/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4261 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 373us/sample - loss: 0.3730 - f1_m: 0.3126 - precision_m: 0.1956 - recall_m: 0.8333 - val_loss: 0.6339 - val_f1_m: 0.1396 - val_precision_m: 0.0814 - val_recall_m: 0.5000\n",
      "Epoch 193/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3502 - f1_m: 0.5217 - precision_m: 0.3529 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 362us/sample - loss: 0.3871 - f1_m: 0.3501 - precision_m: 0.2201 - recall_m: 1.0000 - val_loss: 0.6338 - val_f1_m: 0.1333 - val_precision_m: 0.0812 - val_recall_m: 0.3889\n",
      "Epoch 194/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.2755 - f1_m: 0.5217 - precision_m: 0.3529 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 426us/sample - loss: 0.3699 - f1_m: 0.3659 - precision_m: 0.2278 - recall_m: 1.0000 - val_loss: 0.6338 - val_f1_m: 0.1377 - val_precision_m: 0.0863 - val_recall_m: 0.3889\n",
      "Epoch 195/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4628 - f1_m: 0.2000 - precision_m: 0.1111 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 438us/sample - loss: 0.3775 - f1_m: 0.3063 - precision_m: 0.1921 - recall_m: 0.8333 - val_loss: 0.6339 - val_f1_m: 0.1369 - val_precision_m: 0.0794 - val_recall_m: 0.5000\n",
      "Epoch 196/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.2715 - f1_m: 0.6316 - precision_m: 0.4615 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 401us/sample - loss: 0.3899 - f1_m: 0.3636 - precision_m: 0.2331 - recall_m: 1.0000 - val_loss: 0.6338 - val_f1_m: 0.1278 - val_precision_m: 0.0754 - val_recall_m: 0.4444\n",
      "Epoch 197/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3839 - f1_m: 0.5000 - precision_m: 0.3333 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 320us/sample - loss: 0.3791 - f1_m: 0.3859 - precision_m: 0.2462 - recall_m: 1.0000 - val_loss: 0.6339 - val_f1_m: 0.1323 - val_precision_m: 0.0863 - val_recall_m: 0.5000\n",
      "Epoch 198/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4474 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 446us/sample - loss: 0.3837 - f1_m: 0.3116 - precision_m: 0.1929 - recall_m: 0.8333 - val_loss: 0.6339 - val_f1_m: 0.0952 - val_precision_m: 0.0588 - val_recall_m: 0.2500\n",
      "Epoch 199/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3429 - f1_m: 0.4706 - precision_m: 0.3077 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 369us/sample - loss: 0.3690 - f1_m: 0.3867 - precision_m: 0.2439 - recall_m: 1.0000 - val_loss: 0.6339 - val_f1_m: 0.1281 - val_precision_m: 0.0778 - val_recall_m: 0.3889\n",
      "Epoch 200/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3549 - f1_m: 0.4211 - precision_m: 0.2667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 392us/sample - loss: 0.4036 - f1_m: 0.3637 - precision_m: 0.2331 - recall_m: 0.8333 - val_loss: 0.6339 - val_f1_m: 0.2254 - val_precision_m: 0.1368 - val_recall_m: 0.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4410 - f1_m: 0.2667 - precision_m: 0.1538 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 355us/sample - loss: 0.3773 - f1_m: 0.3163 - precision_m: 0.2001 - recall_m: 0.8333 - val_loss: 0.6339 - val_f1_m: 0.1111 - val_precision_m: 0.0769 - val_recall_m: 0.2000\n",
      "Epoch 202/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3929 - f1_m: 0.3478 - precision_m: 0.2105 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 370us/sample - loss: 0.3664 - f1_m: 0.3882 - precision_m: 0.2466 - recall_m: 1.0000 - val_loss: 0.6338 - val_f1_m: 0.1306 - val_precision_m: 0.0778 - val_recall_m: 0.5556\n",
      "Epoch 203/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.2758 - f1_m: 0.4706 - precision_m: 0.3077 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 340us/sample - loss: 0.4117 - f1_m: 0.2883 - precision_m: 0.1868 - recall_m: 0.6667 - val_loss: 0.6338 - val_f1_m: 0.1254 - val_precision_m: 0.0747 - val_recall_m: 0.3889\n",
      "Epoch 204/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3976 - f1_m: 0.4444 - precision_m: 0.2857 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 296us/sample - loss: 0.3889 - f1_m: 0.3227 - precision_m: 0.2046 - recall_m: 0.8333 - val_loss: 0.6337 - val_f1_m: 0.1333 - val_precision_m: 0.0833 - val_recall_m: 0.3333\n",
      "Epoch 205/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4443 - f1_m: 0.3000 - precision_m: 0.1765 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 209us/sample - loss: 0.3712 - f1_m: 0.3810 - precision_m: 0.2376 - recall_m: 1.0000 - val_loss: 0.6337 - val_f1_m: 0.1250 - val_precision_m: 0.0751 - val_recall_m: 0.3889\n",
      "Epoch 206/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3206 - f1_m: 0.3333 - precision_m: 0.2000 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 333us/sample - loss: 0.3775 - f1_m: 0.3211 - precision_m: 0.2015 - recall_m: 0.8333 - val_loss: 0.6337 - val_f1_m: 0.3018 - val_precision_m: 0.2407 - val_recall_m: 0.6667\n",
      "Epoch 207/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3940 - f1_m: 0.2500 - precision_m: 0.1429 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 478us/sample - loss: 0.3660 - f1_m: 0.4125 - precision_m: 0.2817 - recall_m: 1.0000 - val_loss: 0.6336 - val_f1_m: 0.1229 - val_precision_m: 0.0751 - val_recall_m: 0.5000\n",
      "Epoch 208/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.2843 - f1_m: 0.6087 - precision_m: 0.4375 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 355us/sample - loss: 0.3981 - f1_m: 0.3811 - precision_m: 0.2476 - recall_m: 1.0000 - val_loss: 0.6337 - val_f1_m: 0.1281 - val_precision_m: 0.0735 - val_recall_m: 0.5000\n",
      "Epoch 209/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.2746 - f1_m: 0.5882 - precision_m: 0.4167 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 327us/sample - loss: 0.3827 - f1_m: 0.3855 - precision_m: 0.2513 - recall_m: 1.0000 - val_loss: 0.6337 - val_f1_m: 0.0909 - val_precision_m: 0.0556 - val_recall_m: 0.2500\n",
      "Epoch 210/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4108 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 401us/sample - loss: 0.3679 - f1_m: 0.3904 - precision_m: 0.2500 - recall_m: 1.0000 - val_loss: 0.6337 - val_f1_m: 0.1323 - val_precision_m: 0.0814 - val_recall_m: 0.3889\n",
      "Epoch 211/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4582 - f1_m: 0.1111 - precision_m: 0.0588 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 358us/sample - loss: 0.3798 - f1_m: 0.3812 - precision_m: 0.2437 - recall_m: 1.0000 - val_loss: 0.6337 - val_f1_m: 0.0952 - val_precision_m: 0.0588 - val_recall_m: 0.2500\n",
      "Epoch 212/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3405 - f1_m: 0.4444 - precision_m: 0.2857 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 374us/sample - loss: 0.3785 - f1_m: 0.4045 - precision_m: 0.2643 - recall_m: 1.0000 - val_loss: 0.6338 - val_f1_m: 0.1176 - val_precision_m: 0.0833 - val_recall_m: 0.2000\n",
      "Epoch 213/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3577 - f1_m: 0.3529 - precision_m: 0.2143 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 336us/sample - loss: 0.3645 - f1_m: 0.3699 - precision_m: 0.2314 - recall_m: 1.0000 - val_loss: 0.6338 - val_f1_m: 0.1176 - val_precision_m: 0.0769 - val_recall_m: 0.2500\n",
      "Epoch 214/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3850 - f1_m: 0.2857 - precision_m: 0.1667 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 230us/sample - loss: 0.4008 - f1_m: 0.3130 - precision_m: 0.2051 - recall_m: 0.8333 - val_loss: 0.6339 - val_f1_m: 0.2286 - val_precision_m: 0.1667 - val_recall_m: 0.5000\n",
      "Epoch 215/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.3212 - f1_m: 0.4000 - precision_m: 0.2500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 319us/sample - loss: 0.3906 - f1_m: 0.3196 - precision_m: 0.2012 - recall_m: 0.8333 - val_loss: 0.6339 - val_f1_m: 0.1310 - val_precision_m: 0.0791 - val_recall_m: 0.3889\n",
      "Epoch 216/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4828 - f1_m: 0.2609 - precision_m: 0.1500 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 375us/sample - loss: 0.3872 - f1_m: 0.3177 - precision_m: 0.2011 - recall_m: 0.8333 - val_loss: 0.6338 - val_f1_m: 0.2698 - val_precision_m: 0.1923 - val_recall_m: 0.6667\n",
      "Epoch 217/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4218 - f1_m: 0.2667 - precision_m: 0.1538 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166/166 [==============================] - 0s 350us/sample - loss: 0.3788 - f1_m: 0.2882 - precision_m: 0.1826 - recall_m: 0.8333 - val_loss: 0.6338 - val_f1_m: 0.1397 - val_precision_m: 0.0862 - val_recall_m: 0.3889\n",
      "Epoch 218/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.4888 - f1_m: 0.1111 - precision_m: 0.0588 - recall_m: 1.0000Logging weights No such layer: LSTM_layer\n",
      "Logging weights No such layer: embeddings_layer\n",
      "Logging weights No such layer: sparse_feat_dense_layer\n",
      "166/166 [==============================] - 0s 384us/sample - loss: 0.3629 - f1_m: 0.3969 - precision_m: 0.2688 - recall_m: 1.0000 - val_loss: 0.6337 - val_f1_m: 0.1229 - val_precision_m: 0.0722 - val_recall_m: 0.4444\n",
      "Epoch 219/1000\n",
      " 32/166 [====>.........................] - ETA: 0s - loss: 0.5065 - f1_m: 0.3529 - precision_m: 0.2143 - recall_m: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-117-b29c6b424842>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, x_train, y_train, x_test, y_test, batch_size, epochs, class_weight, start_epoch, workers, callback_list, model_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#                 callbacks.ModelCheckpoint(filepath='%s_best' % model_path, verbose=1,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#                                           save_best_only=True, save_weights_only=True),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcallback_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             ])\n\u001b[1;32m     22\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, save_weights_only=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/comet_ml/monkey_patching.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m                     )\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Call after callbacks once we have the return value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3164\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3165\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3166\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3167\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n\u001b[1;32m   3168\u001b[0m                                  [x.numpy() for x in outputs])\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    366\u001b[0m           \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Got two values for keyword '{}'.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munused_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Keyword arguments {} unknown.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    267\u001b[0m           \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m           \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction_call_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_proto_serialized\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m           executor_type=function_call_options.executor_type)\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/functional_ops.py\u001b[0m in \u001b[0;36mpartitioned_call\u001b[0;34m(args, f, tout, executing_eagerly, config, executor_type)\u001b[0m\n\u001b[1;32m   1081\u001b[0m       outputs = gen_functional_ops.stateful_partitioned_call(\n\u001b[1;32m   1082\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_proto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1083\u001b[0;31m           executor_type=executor_type)\n\u001b[0m\u001b[1;32m   1084\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m       outputs = gen_functional_ops.partitioned_call(\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/gen_functional_ops.py\u001b[0m in \u001b[0;36mstateful_partitioned_call\u001b[0;34m(args, Tout, f, config, config_proto, executor_type, name)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;34m\"StatefulPartitionedCall\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;34m\"Tout\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"f\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_proto\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         \"executor_type\", executor_type)\n\u001b[0m\u001b[1;32m    484\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "freeze_layer = FreezeLayer(hyperparams['freeze_patience'], set_to=not hyperparams['trainable_embeddings'])\n",
    "weights_history = WeightsHistory()\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=hyperparams['reduce_lr_factor'],\n",
    "                          patience=hyperparams['reduce_lr_patience'], min_lr=0.000001, verbose=1)\n",
    "\n",
    "history = train_model(model, x_train, y_train, x_valid, y_valid,\n",
    "           epochs=1000, batch_size=hyperparams['batch_size'],\n",
    "                      class_weight={0:0.5, 1:10}, start_epoch=0,\n",
    "                      callback_list = [freeze_layer, weights_history, reduce_lr],\n",
    "                      model_path='models/lstm_plus_ablated_user3', workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.44971931e+00],\n",
       "        [ 5.99275768e-01],\n",
       "        [ 4.79707479e-01],\n",
       "        [-1.02073348e+00],\n",
       "        [ 7.06191480e-01],\n",
       "        [-5.70192754e-01],\n",
       "        [-6.15594923e-01],\n",
       "        [-8.78930449e-01],\n",
       "        [-3.44653964e+00],\n",
       "        [-4.17300969e-01],\n",
       "        [-3.47588360e-01],\n",
       "        [ 4.52555120e-01],\n",
       "        [ 3.45672297e+00],\n",
       "        [-4.62037057e-01],\n",
       "        [-2.84551525e+00],\n",
       "        [-3.03839874e+00],\n",
       "        [ 1.36626410e+00],\n",
       "        [-1.01885557e+00],\n",
       "        [-2.80554235e-01],\n",
       "        [-1.33873188e+00],\n",
       "        [ 6.81673288e-01],\n",
       "        [-1.62495267e+00],\n",
       "        [-5.50505295e+01],\n",
       "        [-1.37413883e+00],\n",
       "        [-1.92312765e+00],\n",
       "        [ 6.35300815e-01],\n",
       "        [ 1.14118254e+00],\n",
       "        [-2.15535760e+00],\n",
       "        [-2.97780067e-01],\n",
       "        [-3.38145828e+00],\n",
       "        [ 1.78429782e-01],\n",
       "        [ 1.10952210e+00],\n",
       "        [-1.82261813e+00],\n",
       "        [ 1.32531798e+00],\n",
       "        [-1.74499857e+00],\n",
       "        [ 1.44931817e+00],\n",
       "        [-7.66425788e-01],\n",
       "        [-4.54369038e-01],\n",
       "        [-1.16801679e+00],\n",
       "        [-3.87844086e-01],\n",
       "        [ 2.28751719e-01],\n",
       "        [ 1.56348526e+00],\n",
       "        [-1.63736224e+00],\n",
       "        [ 3.82462770e-01],\n",
       "        [-1.03432298e+00],\n",
       "        [-7.87146270e-01],\n",
       "        [-2.62502146e+00],\n",
       "        [-2.20739746e+00],\n",
       "        [ 5.63182294e-01],\n",
       "        [ 8.16648304e-01],\n",
       "        [-3.21735024e+00],\n",
       "        [ 5.76238155e-01],\n",
       "        [-1.11849129e+00],\n",
       "        [ 2.86429214e+00],\n",
       "        [ 3.59776092e+00],\n",
       "        [-2.37842631e+00],\n",
       "        [-1.04187226e+00],\n",
       "        [-1.62664819e+00],\n",
       "        [ 1.39853597e+00],\n",
       "        [-1.47555101e+00],\n",
       "        [-1.28982377e+00],\n",
       "        [-6.34847343e-01],\n",
       "        [ 3.06845516e-01],\n",
       "        [ 5.43143213e-01],\n",
       "        [-6.39110357e-02],\n",
       "        [-1.80369878e+00],\n",
       "        [ 9.95237753e-03],\n",
       "        [ 2.02436543e+00],\n",
       "        [ 1.20272577e+00],\n",
       "        [-1.74875998e+00],\n",
       "        [-1.44120014e+00],\n",
       "        [ 1.42027450e+00],\n",
       "        [-2.24666548e+00],\n",
       "        [-1.64019156e+00],\n",
       "        [-8.06684971e-01],\n",
       "        [ 3.39318424e-01],\n",
       "        [ 5.30882537e-01],\n",
       "        [ 3.53614151e-01],\n",
       "        [ 8.44027996e-01],\n",
       "        [-1.70989111e-01],\n",
       "        [-4.57769662e-01],\n",
       "        [-1.28381786e+01],\n",
       "        [-1.62698019e+00],\n",
       "        [-1.24002673e-01],\n",
       "        [ 7.90004879e-02],\n",
       "        [-9.80859548e-02],\n",
       "        [ 1.06734931e-02],\n",
       "        [ 2.51013041e-02],\n",
       "        [ 8.20404068e-02],\n",
       "        [-1.49290776e+00],\n",
       "        [ 4.26178515e-01],\n",
       "        [-1.25629730e+01],\n",
       "        [-2.04875559e-01],\n",
       "        [ 3.00567418e-01],\n",
       "        [ 1.92661826e-02],\n",
       "        [-2.04317880e+00],\n",
       "        [ 5.75706512e-02],\n",
       "        [-8.21892470e-02],\n",
       "        [ 2.18561254e-02],\n",
       "        [-1.32251549e+01],\n",
       "        [-1.49451628e+01],\n",
       "        [ 1.48946047e-01],\n",
       "        [ 1.05145842e-01],\n",
       "        [-1.57194976e-02],\n",
       "        [-5.76147199e-01],\n",
       "        [-2.20573284e-02],\n",
       "        [ 2.15103514e-02],\n",
       "        [-2.55923092e-01],\n",
       "        [-1.34906569e+01],\n",
       "        [-1.16044474e+00],\n",
       "        [-3.41843143e-02],\n",
       "        [-5.29423356e-01],\n",
       "        [-3.16753566e-01],\n",
       "        [-1.27255523e+00],\n",
       "        [-2.60655850e-01],\n",
       "        [-1.36507243e-01],\n",
       "        [ 9.97354984e-02],\n",
       "        [-5.90253389e-03],\n",
       "        [-1.29536971e-01],\n",
       "        [ 6.17593706e-01],\n",
       "        [ 3.97334769e-02],\n",
       "        [-2.15299815e-01],\n",
       "        [-3.79910017e-03],\n",
       "        [ 5.38597628e-02],\n",
       "        [-5.39592616e-02],\n",
       "        [ 1.89929649e-01],\n",
       "        [ 2.10691109e-01],\n",
       "        [ 1.98218390e-01],\n",
       "        [-1.59804478e-01],\n",
       "        [ 1.97658598e-01],\n",
       "        [ 3.67126703e-01],\n",
       "        [ 3.40350628e-01],\n",
       "        [ 1.99894771e-01],\n",
       "        [ 1.08632468e-01],\n",
       "        [ 6.80857003e-02],\n",
       "        [-6.66646147e-03],\n",
       "        [-1.93662360e-01],\n",
       "        [ 8.96340832e-02],\n",
       "        [ 2.06507415e-01],\n",
       "        [ 1.14517771e-01],\n",
       "        [ 6.46155775e-02],\n",
       "        [ 3.34020197e-01],\n",
       "        [ 1.48022309e-01],\n",
       "        [-5.26544213e-01],\n",
       "        [ 1.24531411e-01],\n",
       "        [ 2.31826201e-01],\n",
       "        [-2.64728051e-02],\n",
       "        [-1.15541674e-01],\n",
       "        [-3.53352100e-01],\n",
       "        [ 9.27269757e-02],\n",
       "        [ 4.04484347e-02],\n",
       "        [ 7.84482136e-02],\n",
       "        [-9.58437979e-01],\n",
       "        [-1.13082059e-01],\n",
       "        [ 1.05072230e-01],\n",
       "        [ 9.86793712e-02],\n",
       "        [-4.13428456e-01],\n",
       "        [ 2.00053096e-01],\n",
       "        [ 1.62010953e-01],\n",
       "        [-3.74420464e-01],\n",
       "        [-1.11065435e+00],\n",
       "        [-1.62434071e-01],\n",
       "        [-4.86821681e-01],\n",
       "        [ 4.73745689e-02],\n",
       "        [ 1.42917767e-01],\n",
       "        [-1.22786678e-01],\n",
       "        [ 2.19687462e-01],\n",
       "        [-3.08683276e-01],\n",
       "        [-5.51062971e-02],\n",
       "        [-2.35218778e-01],\n",
       "        [-4.18748823e-04],\n",
       "        [ 9.82765406e-02],\n",
       "        [ 5.87903103e-03],\n",
       "        [ 3.29366252e-02],\n",
       "        [-4.08651441e-01],\n",
       "        [-3.66301030e-01],\n",
       "        [-9.19910297e-02],\n",
       "        [ 2.35519499e-01],\n",
       "        [-5.75208664e-01],\n",
       "        [-9.35269403e-04],\n",
       "        [-1.35722116e-01],\n",
       "        [-9.92173776e-02],\n",
       "        [ 3.49313989e-02],\n",
       "        [-6.61788834e-03],\n",
       "        [-8.25603828e-02],\n",
       "        [ 1.03399612e-01],\n",
       "        [-1.35449484e-01],\n",
       "        [-5.48128271e-04],\n",
       "        [ 3.13403785e-01],\n",
       "        [ 1.12053223e-01],\n",
       "        [ 3.47759694e-01],\n",
       "        [ 4.18254435e-01],\n",
       "        [-1.00189358e-01],\n",
       "        [-2.95435280e-01],\n",
       "        [ 1.95707887e-01],\n",
       "        [-1.11491054e-01],\n",
       "        [ 2.44915307e-01],\n",
       "        [ 1.23568386e-01],\n",
       "        [ 2.61244118e-01],\n",
       "        [ 1.29018068e-01],\n",
       "        [ 1.69486225e-01],\n",
       "        [-1.74785897e-01],\n",
       "        [-1.74565185e-02],\n",
       "        [ 1.11848325e-01],\n",
       "        [-2.41175935e-01],\n",
       "        [ 2.72021383e-01],\n",
       "        [ 4.54719841e-01],\n",
       "        [ 9.27790403e-02],\n",
       "        [ 2.20335931e-01],\n",
       "        [-7.48416185e-02],\n",
       "        [ 1.30733252e-01],\n",
       "        [ 1.73391536e-01],\n",
       "        [ 1.07428297e-01],\n",
       "        [ 1.45580709e-01],\n",
       "        [ 1.55159131e-01],\n",
       "        [-2.72214651e-01],\n",
       "        [ 5.57822101e-02],\n",
       "        [-1.17972374e+00],\n",
       "        [ 7.60140896e-01],\n",
       "        [ 2.39980593e-01],\n",
       "        [-1.28228098e-01],\n",
       "        [-3.51427436e-01],\n",
       "        [ 1.51922911e-01],\n",
       "        [-1.28607094e-01],\n",
       "        [-2.24804729e-02],\n",
       "        [-3.93109739e-01],\n",
       "        [ 1.36323422e-01],\n",
       "        [ 4.40599136e-02],\n",
       "        [ 1.96252763e-02],\n",
       "        [-8.49482596e-01],\n",
       "        [-1.38452023e-01],\n",
       "        [ 4.48073834e-01],\n",
       "        [ 3.29953879e-02],\n",
       "        [ 2.69325614e-01],\n",
       "        [ 1.42175704e-01],\n",
       "        [-1.47752924e+01],\n",
       "        [ 1.22107178e-01],\n",
       "        [ 8.40429217e-02],\n",
       "        [ 5.07928878e-02],\n",
       "        [-2.02921927e-02],\n",
       "        [-9.49106312e+00],\n",
       "        [ 1.46319211e-01],\n",
       "        [-9.42043686e+00],\n",
       "        [-1.01587743e-01],\n",
       "        [-1.07210374e+00],\n",
       "        [-2.76902318e-03],\n",
       "        [ 4.68347594e-02],\n",
       "        [ 4.88146842e-02],\n",
       "        [-4.99129117e-01],\n",
       "        [ 1.51265264e-01],\n",
       "        [ 4.41928774e-01],\n",
       "        [ 2.87823230e-02],\n",
       "        [-3.41137230e-01],\n",
       "        [ 8.50599110e-02]], dtype=float32), array([-1.2966485], dtype=float32)]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='models/lstm_plus4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels Tensor(\"dense_1_target_1:0\", shape=(?, ?), dtype=float32)\n",
      "Predictions Tensor(\"Sigmoid_2:0\", shape=(?, 1), dtype=float32)\n",
      "True positives Tensor(\"metrics_1/f1_m/Sum_2:0\", shape=(), dtype=float32)\n",
      "Labels Tensor(\"dense_1_target_1:0\", shape=(?, ?), dtype=float32)\n",
      "Predictions Tensor(\"Sigmoid_2:0\", shape=(?, 1), dtype=float32)\n",
      "True positives Tensor(\"metrics_1/f1_m/Sum_7:0\", shape=(), dtype=float32)\n",
      "Labels Tensor(\"dense_1_target_1:0\", shape=(?, ?), dtype=float32)\n",
      "Predictions Tensor(\"Sigmoid_2:0\", shape=(?, 1), dtype=float32)\n",
      "True positives Tensor(\"metrics_1/recall_m/Sum:0\", shape=(), dtype=float32)\n",
      "Labels Tensor(\"dense_1_target_1:0\", shape=(?, ?), dtype=float32)\n",
      "Predictions Tensor(\"Sigmoid_2:0\", shape=(?, 1), dtype=float32)\n",
      "True positives Tensor(\"metrics_1/recall_m/Sum_3:0\", shape=(), dtype=float32)\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "dependencies = {\n",
    "    'f1_m': f1_m,\n",
    "    'precision_m': precision_m,\n",
    "    'recall_m': recall_m,\n",
    "    'binary_crossentropy_custom': binary_crossentropy_custom\n",
    "}\n",
    "model = load_model('models/lstm_plus8_remote', custom_objects=dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 0s 62us/sample - loss: 0.7235 - f1_m: 0.3611 - precision_m: 0.2788 - recall_m: 0.7054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7234657777165904, 0.36112642, 0.2787594, 0.70535713]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254 254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('herself', 'stopword'), -1.3356316089630127),\n",
       " (('ma', 'stopword'), -1.2046297788619995),\n",
       " (('theirs', 'stopword'), -1.053725242614746),\n",
       " (('work', 'liwc'), -1.0329701900482178),\n",
       " ('pers_pronouns', 0.9913920164108276),\n",
       " (('hers', 'stopword'), -0.9762506484985352),\n",
       " (('ours', 'stopword'), -0.8895556330680847),\n",
       " (('i', 'liwc'), 0.8726097941398621),\n",
       " (('himself', 'stopword'), -0.8083417415618896),\n",
       " (('yourselves', 'stopword'), -0.7712955474853516),\n",
       " (('tentat', 'liwc'), 0.7579227089881897),\n",
       " (('until', 'stopword'), 0.7409663200378418),\n",
       " (('conj', 'liwc'), 0.7305939793586731),\n",
       " (('myself', 'stopword'), 0.7131538391113281),\n",
       " (('feel', 'liwc'), 0.7049807906150818),\n",
       " (('o', 'stopword'), -0.678808867931366),\n",
       " (('few', 'stopword'), 0.6215648055076599),\n",
       " (('we', 'liwc'), -0.6206851601600647),\n",
       " (('his', 'stopword'), -0.6190535426139832),\n",
       " (('excl', 'liwc'), 0.6161367297172546),\n",
       " (('anger', 'liwc'), -0.5914202332496643),\n",
       " (('below', 'stopword'), -0.5909287929534912),\n",
       " (('ppron', 'liwc'), 0.5817850828170776),\n",
       " (('swear', 'liwc'), -0.581218421459198),\n",
       " (('yours', 'stopword'), -0.5725081562995911),\n",
       " (('article', 'liwc'), -0.5439066290855408),\n",
       " (('hasn', 'stopword'), -0.5247345566749573),\n",
       " (('very', 'stopword'), 0.5000014305114746),\n",
       " (('relig', 'liwc'), -0.4997096061706543),\n",
       " (('anx', 'liwc'), 0.4947203993797302),\n",
       " (('ingest', 'liwc'), 0.4922645092010498),\n",
       " (('hadn', 'stopword'), 0.4752688705921173),\n",
       " (('present', 'liwc'), 0.4713248610496521),\n",
       " (('verb', 'liwc'), 0.4595204293727875),\n",
       " (('by', 'stopword'), -0.45368775725364685),\n",
       " (('shan', 'stopword'), -0.44859275221824646),\n",
       " (('he', 'stopword'), -0.4307499825954437),\n",
       " (('time', 'liwc'), -0.418591171503067),\n",
       " (('aren', 'stopword'), 0.4138379693031311),\n",
       " (('insight', 'liwc'), 0.40919163823127747),\n",
       " (('future', 'liwc'), 0.4080585241317749),\n",
       " (('leisure', 'liwc'), -0.4071531891822815),\n",
       " (('under', 'stopword'), 0.4022611975669861),\n",
       " (('discrep', 'liwc'), 0.39239001274108887),\n",
       " (('him', 'stopword'), -0.38899335265159607),\n",
       " (('needn', 'stopword'), -0.38787218928337097),\n",
       " (('achieve', 'liwc'), -0.3876109719276428),\n",
       " (('bio', 'liwc'), 0.3789690434932709),\n",
       " (('cogmech', 'liwc'), 0.3768191337585449),\n",
       " (('motion', 'liwc'), -0.3728558421134949),\n",
       " (('weren', 'stopword'), 0.365419864654541),\n",
       " (('body', 'liwc'), -0.3645724058151245),\n",
       " (('family', 'liwc'), -0.36287128925323486),\n",
       " (('adverb', 'liwc'), 0.36157554388046265),\n",
       " (('home', 'liwc'), -0.35912832617759705),\n",
       " (('pronoun', 'liwc'), 0.3581582307815552),\n",
       " (('out', 'stopword'), 0.3579675257205963),\n",
       " (('shehe', 'liwc'), -0.3519456386566162),\n",
       " (('after', 'stopword'), -0.3496268391609192),\n",
       " (('sadness', 'nrc'), 0.34892940521240234),\n",
       " (('auxverb', 'liwc'), 0.33512982726097107),\n",
       " (('themselves', 'stopword'), -0.33341631293296814),\n",
       " (('both', 'stopword'), -0.3298112154006958),\n",
       " (('health', 'liwc'), 0.32835784554481506),\n",
       " (('money', 'liwc'), -0.31899940967559814),\n",
       " (('they', 'liwc'), 0.3179054856300354),\n",
       " (('positive', 'nrc'), -0.31289586424827576),\n",
       " (('funct', 'liwc'), 0.3119775652885437),\n",
       " (('trust', 'nrc'), -0.3093487322330475),\n",
       " (('shouldn', 'stopword'), -0.30607879161834717),\n",
       " (('disgust', 'nrc'), 0.3033227324485779),\n",
       " (('ll', 'stopword'), 0.293954074382782),\n",
       " (('space', 'liwc'), -0.29085439443588257),\n",
       " (('further', 'stopword'), -0.2798490524291992),\n",
       " (('percept', 'liwc'), 0.27881792187690735),\n",
       " (('been', 'stopword'), 0.2608383595943451),\n",
       " (('yourself', 'stopword'), 0.2543991208076477),\n",
       " (('them', 'stopword'), 0.25240030884742737),\n",
       " (('surprise', 'nrc'), -0.2503567934036255),\n",
       " (('ve', 'stopword'), 0.2496464103460312),\n",
       " (('as', 'stopword'), -0.24870102107524872),\n",
       " (('each', 'stopword'), -0.245611310005188),\n",
       " (('number', 'liwc'), -0.24530421197414398),\n",
       " (('haven', 'stopword'), 0.24318109452724457),\n",
       " (('and', 'stopword'), -0.24257555603981018),\n",
       " (('itself', 'stopword'), -0.23581476509571075),\n",
       " (('same', 'stopword'), -0.2342565357685089),\n",
       " (('incl', 'liwc'), 0.23325571417808533),\n",
       " (('relativ', 'liwc'), -0.23283173143863678),\n",
       " (('negative', 'nrc'), 0.22808456420898438),\n",
       " (('quant', 'liwc'), 0.22544842958450317),\n",
       " (('at', 'stopword'), 0.22377219796180725),\n",
       " (('the', 'stopword'), -0.22198012471199036),\n",
       " (('from', 'stopword'), -0.21687014400959015),\n",
       " (('we', 'stopword'), -0.21655981242656708),\n",
       " (('such', 'stopword'), -0.21212950348854065),\n",
       " (('up', 'stopword'), -0.2112637609243393),\n",
       " (('above', 'stopword'), -0.20800115168094635),\n",
       " (('re', 'stopword'), 0.20762237906455994),\n",
       " (('was', 'stopword'), 0.20656119287014008),\n",
       " (('of', 'stopword'), -0.20248201489448547),\n",
       " (('ipron', 'liwc'), 0.19887207448482513),\n",
       " (('couldn', 'stopword'), 0.1955803781747818),\n",
       " (('who', 'stopword'), -0.19025352597236633),\n",
       " (('once', 'stopword'), 0.18295541405677795),\n",
       " (('isn', 'stopword'), 0.18227005004882812),\n",
       " (('y', 'stopword'), -0.18139927089214325),\n",
       " (('affect', 'liwc'), -0.1781630963087082),\n",
       " (('be', 'stopword'), -0.17665843665599823),\n",
       " (('over', 'stopword'), 0.17379377782344818),\n",
       " (('past', 'liwc'), -0.1721743494272232),\n",
       " (('again', 'stopword'), -0.17194461822509766),\n",
       " (('have', 'stopword'), -0.16945166885852814),\n",
       " (('whom', 'stopword'), 0.16306211054325104),\n",
       " (('ain', 'stopword'), 0.16090939939022064),\n",
       " (('having', 'stopword'), 0.15988227725028992),\n",
       " (('nor', 'stopword'), 0.15981262922286987),\n",
       " (('being', 'stopword'), 0.15936267375946045),\n",
       " (('death', 'liwc'), -0.15907643735408783),\n",
       " (('they', 'stopword'), -0.1587887406349182),\n",
       " (('you', 'liwc'), 0.15767675638198853),\n",
       " (('or', 'stopword'), 0.1569456160068512),\n",
       " (('now', 'stopword'), -0.15690122544765472),\n",
       " (('our', 'stopword'), 0.15653271973133087),\n",
       " (('did', 'stopword'), -0.15523745119571686),\n",
       " (('against', 'stopword'), 0.1523963212966919),\n",
       " (('than', 'stopword'), 0.15210117399692535),\n",
       " (('certain', 'liwc'), -0.1505623608827591),\n",
       " (('any', 'stopword'), 0.1498701274394989),\n",
       " (('no', 'stopword'), -0.14858011901378632),\n",
       " (('when', 'stopword'), 0.14639966189861298),\n",
       " (('through', 'stopword'), -0.1461431086063385),\n",
       " (('see', 'liwc'), -0.14061015844345093),\n",
       " (('cause', 'liwc'), 0.1402399241924286),\n",
       " (('negemo', 'liwc'), -0.1371796876192093),\n",
       " (('so', 'stopword'), -0.13152635097503662),\n",
       " (('during', 'stopword'), -0.13076840341091156),\n",
       " (('negate', 'liwc'), 0.12986278533935547),\n",
       " (('down', 'stopword'), -0.12894216179847717),\n",
       " (('to', 'stopword'), -0.1277695596218109),\n",
       " (('her', 'stopword'), -0.12695199251174927),\n",
       " (('preps', 'liwc'), -0.12617912888526917),\n",
       " (('do', 'stopword'), 0.12519802153110504),\n",
       " (('just', 'stopword'), 0.12452486157417297),\n",
       " (('your', 'stopword'), 0.12339791655540466),\n",
       " (('between', 'stopword'), -0.12084139138460159),\n",
       " (('does', 'stopword'), 0.12059500813484192),\n",
       " (('where', 'stopword'), 0.11789218336343765),\n",
       " (('had', 'stopword'), -0.11755301058292389),\n",
       " (('why', 'stopword'), 0.11627567559480667),\n",
       " (('that', 'stopword'), -0.11624356359243393),\n",
       " (('most', 'stopword'), 0.11621569097042084),\n",
       " (('d', 'stopword'), -0.11580691486597061),\n",
       " (('it', 'stopword'), 0.11469321697950363),\n",
       " (('here', 'stopword'), -0.11285648494958878),\n",
       " (('this', 'stopword'), -0.11177773028612137),\n",
       " (('if', 'stopword'), 0.1091422438621521),\n",
       " (('into', 'stopword'), -0.10838837921619415),\n",
       " (('while', 'stopword'), 0.10702334344387054),\n",
       " (('hear', 'liwc'), 0.1011635959148407),\n",
       " (('own', 'stopword'), 0.10094280540943146),\n",
       " (('its', 'stopword'), -0.10046254843473434),\n",
       " (('my', 'stopword'), -0.09742218255996704),\n",
       " (('off', 'stopword'), 0.09663952142000198),\n",
       " (('some', 'stopword'), 0.09639038890600204),\n",
       " (('which', 'stopword'), 0.09573265165090561),\n",
       " (('ourselves', 'stopword'), 0.09565218538045883),\n",
       " (('sexual', 'liwc'), 0.09513385593891144),\n",
       " (('these', 'stopword'), 0.09491167962551117),\n",
       " (('should', 'stopword'), 0.09465561807155609),\n",
       " (('doing', 'stopword'), 0.09269241988658905),\n",
       " (('those', 'stopword'), -0.09187839925289154),\n",
       " (('sad', 'liwc'), 0.09161501377820969),\n",
       " (('wouldn', 'stopword'), -0.08971942216157913),\n",
       " (('anger', 'nrc'), -0.08304160833358765),\n",
       " (('other', 'stopword'), -0.08298277109861374),\n",
       " (('their', 'stopword'), 0.0821615532040596),\n",
       " (('has', 'stopword'), -0.08057548850774765),\n",
       " (('for', 'stopword'), -0.07765636593103409),\n",
       " (('in', 'stopword'), -0.0775950700044632),\n",
       " (('m', 'stopword'), 0.07438976317644119),\n",
       " (('more', 'stopword'), -0.07392022013664246),\n",
       " (('will', 'stopword'), -0.06547122448682785),\n",
       " (('social', 'liwc'), 0.06494510173797607),\n",
       " (('too', 'stopword'), 0.06468281894922256),\n",
       " (('what', 'stopword'), -0.06412648409605026),\n",
       " (('posemo', 'liwc'), 0.06336244195699692),\n",
       " (('can', 'stopword'), -0.06320313364267349),\n",
       " (('doesn', 'stopword'), 0.061901796609163284),\n",
       " (('with', 'stopword'), -0.06097278743982315),\n",
       " (('how', 'stopword'), -0.06076586991548538),\n",
       " (('an', 'stopword'), 0.06059075519442558),\n",
       " (('were', 'stopword'), -0.05412108823657036),\n",
       " (('there', 'stopword'), -0.04987156763672829),\n",
       " (('am', 'stopword'), 0.04812067747116089),\n",
       " (('wasn', 'stopword'), -0.04781721904873848),\n",
       " (('assent', 'liwc'), -0.04703705012798309),\n",
       " (('won', 'stopword'), 0.044944435358047485),\n",
       " (('friend', 'liwc'), -0.044939104467630386),\n",
       " (('only', 'stopword'), 0.044179730117321014),\n",
       " (('t', 'stopword'), 0.044149454683065414),\n",
       " (('a', 'stopword'), 0.04115057736635208),\n",
       " (('but', 'stopword'), 0.03832564502954483),\n",
       " (('joy', 'nrc'), -0.03815344721078873),\n",
       " (('i', 'stopword'), -0.03788602352142334),\n",
       " (('s', 'stopword'), -0.03562389686703682),\n",
       " (('didn', 'stopword'), 0.033468108624219894),\n",
       " (('me', 'stopword'), 0.0321679450571537),\n",
       " (('fear', 'nrc'), 0.03204983472824097),\n",
       " (('she', 'stopword'), 0.03191811591386795),\n",
       " (('humans', 'liwc'), -0.03117934986948967),\n",
       " (('you', 'stopword'), 0.028208190575242043),\n",
       " (('on', 'stopword'), -0.028110895305871964),\n",
       " (('because', 'stopword'), 0.02758379653096199),\n",
       " (('about', 'stopword'), 0.02348855882883072),\n",
       " (('is', 'stopword'), -0.02236570231616497),\n",
       " (('anticipation', 'nrc'), 0.015547719784080982),\n",
       " (('before', 'stopword'), 0.015047989785671234),\n",
       " (('don', 'stopword'), -0.01425930019468069),\n",
       " (('nonfl', 'liwc'), 0.012189127504825592),\n",
       " (('all', 'stopword'), -0.010963043197989464),\n",
       " (('inhib', 'liwc'), 0.010879608802497387),\n",
       " (('then', 'stopword'), 0.010646040551364422),\n",
       " (('not', 'stopword'), -0.008417755365371704),\n",
       " (('are', 'stopword'), 0.005582477897405624),\n",
       " (('filler', 'liwc'), -0.003179051447659731),\n",
       " ((\"wouldn't\", 'stopword'), -5.453806545406998e-32),\n",
       " ((\"mightn't\", 'stopword'), -5.257430810147328e-32),\n",
       " ((\"she's\", 'stopword'), -5.103391091679699e-32),\n",
       " ((\"you'd\", 'stopword'), 4.962030255280038e-32),\n",
       " ((\"hasn't\", 'stopword'), 4.9146654736551806e-32),\n",
       " ((\"doesn't\", 'stopword'), -4.845891707292384e-32),\n",
       " ((\"mustn't\", 'stopword'), -4.448507907410042e-32),\n",
       " ((\"shouldn't\", 'stopword'), 3.925458471700218e-32),\n",
       " ((\"you're\", 'stopword'), 3.396222115348429e-32),\n",
       " ((\"hadn't\", 'stopword'), 3.229437981002772e-32),\n",
       " ((\"shan't\", 'stopword'), 3.145352519099752e-32),\n",
       " ((\"don't\", 'stopword'), 3.0496899072090714e-32),\n",
       " ((\"isn't\", 'stopword'), -2.9659917706946474e-32),\n",
       " (('mustn', 'stopword'), -2.8515056738906616e-32),\n",
       " (('mightn', 'stopword'), -2.440059582026226e-32),\n",
       " ((\"weren't\", 'stopword'), -2.0919948231269836e-32),\n",
       " ((\"should've\", 'stopword'), 1.7811875046138474e-32),\n",
       " ((\"couldn't\", 'stopword'), 1.446776994220169e-32),\n",
       " ((\"you'll\", 'stopword'), 1.3835706629764546e-32),\n",
       " ((\"haven't\", 'stopword'), -1.221902280045576e-32),\n",
       " ((\"won't\", 'stopword'), -1.134160883774705e-32),\n",
       " ((\"it's\", 'stopword'), -1.1217034354550719e-32),\n",
       " ((\"needn't\", 'stopword'), -1.0666406352002666e-32),\n",
       " ((\"aren't\", 'stopword'), 9.390903145864326e-33),\n",
       " ((\"you've\", 'stopword'), 7.764519284109349e-33),\n",
       " ((\"that'll\", 'stopword'), 5.1391261199446964e-33),\n",
       " ((\"wasn't\", 'stopword'), -4.209051479687086e-33),\n",
       " ((\"didn't\", 'stopword'), -3.883667296539784e-33)]"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.get_weights()[0].tolist()\n",
    "features = [(e, 'nrc') for e in emotions] + ['pers_pronouns'] + [\n",
    "    (c, 'liwc') for c in list(categories)] + [(st, 'stopword') for st in stopword_list]\n",
    "print(len(weights), len(features))\n",
    "feature_importance = {}\n",
    "for (i, f) in enumerate(features):\n",
    "    feature_importance[f] = weights[i][0]\n",
    "\n",
    "sorted(feature_importance.items(), key=lambda t: abs(t[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "save() got an unexpected keyword argument 'save_weights_only'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-324-694b85eaee79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/lstm_plus_ablated_user'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_weights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: save() got an unexpected keyword argument 'save_weights_only'"
     ]
    }
   ],
   "source": [
    "model.save('models/lstm_plus_ablated_user', save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5631"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5644"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "voc2=pickle.load(open('vocabulary20K_selfharm.pkl', 'rb'))\n",
    "voc2['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(predictions.flatten()).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([84])"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(predictions>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19])"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(predictions<0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_point(subject, voc, hyperparams_features=hyperparams_features, nrc_lexicon=nrc_lexicon,\n",
    "                      emotions=emotions):\n",
    "    eval_writings_df = writings_df[writings_df['subject']==subject]\n",
    "    correct_label = eval_writings_df.label.values[0]\n",
    "    (x_train, y_train), (x_valid, y_valid), (x_test, y_test), voc = load_erisk_data(eval_writings_df,\n",
    "                        seq_len=hyperparams_features['maxlen'],\n",
    "                        voc_size=hyperparams_features['max_features'],\n",
    "                        emotion_lexicon=nrc_lexicon,\n",
    "                        emotions=emotions, user_level=False,\n",
    "                        train_prop=0.0, vocabulary=voc)\n",
    "    return x_test, y_test, correct_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_per_user(writings_df, majority_prop=0.2, train_prop=0.7, majority_nr=0, validate=False, voc=None):\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    thresh=0.5\n",
    "    majority_proportion=majority_prop\n",
    "    valid_prop = 0.3\n",
    "    \n",
    "    if 'subset' in writings_df.columns:\n",
    "        training_subjects = list(set(writings_df[writings_df['subset']=='train'].subject))\n",
    "        test_subjects = list(set(writings_df[writings_df['subset']=='test'].subject))\n",
    "    else:\n",
    "        all_subjects = sorted(list(set(writings_df.subject)))\n",
    "        training_subjects_size = int(len(all_subjects) * train_prop)\n",
    "        training_subjects = all_subjects[:training_subjects_size]\n",
    "        test_subjects = all_subjects[training_subjects_size:]\n",
    "    training_subjects = sorted(training_subjects) # ensuring reproducibility\n",
    "    valid_subjects_size = int(len(training_subjects) * valid_prop)\n",
    "    valid_subjects = training_subjects[:valid_subjects_size]\n",
    "    training_subjects = training_subjects[valid_subjects_size:]\n",
    "    \n",
    "    if validate:\n",
    "        subjects = valid_subjects\n",
    "    else:\n",
    "        subjects = test_subjects\n",
    "    for subject in subjects:\n",
    "        x_test_user, y_test_user, label = get_data_for_point(subject, voc=voc)\n",
    "        outputs = model.predict(x_test_user)\n",
    "        positive_pred = sum(outputs>=thresh)\n",
    "        negative_pred = sum(outputs<thresh)\n",
    "        majority_pred = 0\n",
    "        if majority_proportion and positive_pred >= majority_proportion*negative_pred:\n",
    "            majority_pred = 1\n",
    "        if majority_nr and positive_pred>=majority_nr:\n",
    "            majority_pred = 1\n",
    "        if label == 1:\n",
    "            if majority_pred == 1:\n",
    "                tp+=1\n",
    "            else:\n",
    "                fn+=1\n",
    "        else:\n",
    "            if majority_pred == 0:\n",
    "                tn+=1\n",
    "            else:\n",
    "                fp+=1\n",
    "        print(negative_pred, positive_pred, majority_pred)\n",
    "        all_predictions.append(majority_pred)\n",
    "        all_labels.append(label)\n",
    "    def prec_recall_f1(tp, fp, tn, fn):\n",
    "        recall = tp/(tp+fn+0.0000001)\n",
    "        precision = tp/(tp+fp+0.0000001)\n",
    "        f1 = 2*precision*recall/(precision+recall+0.0000001)\n",
    "        print(\"Recall\", recall, \"Precision\", precision, \"F1\", f1)\n",
    "    if majority_prop:\n",
    "        print(\"Vote proportion\", majority_prop)\n",
    "    if majority_nr:\n",
    "        print(\"Vote points\", majority_nr)\n",
    "    prec_recall_f1(tp, fp, tn, fn)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[339] [100] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[52] [33] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[451] [52] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1046] [301] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[147] [39] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[104] [8] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1076] [182] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1813] [63] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[227] [65] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[106] [8] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[437] [43] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[7] [2] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[145] [13] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[178] [21] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[822] [200] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[786] [98] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[89] [18] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[61] [9] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[35] [8] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[543] [182] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[886] [93] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[141] [31] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[265] [63] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[93] [8] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[291] [109] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[22] [10] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1013] [102] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[332] [34] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[63] [33] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[17] [2] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[155] [5] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[52] [10] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1109] [77] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[21] [6] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[122] [44] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[480] [105] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[997] [241] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[260] [26] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[14] [2] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[836] [134] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[818] [130] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[364] [30] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[22] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[698] [192] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1193] [187] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1104] [68] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[16] [13] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[18] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[37] [9] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[12] [4] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[38] [18] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1018] [135] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[214] [95] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[102] [17] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[177] [101] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[232] [36] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[30] [25] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[21] [7] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1031] [117] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[465] [117] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[339] [132] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[46] [3] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[46] [6] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[95] [2] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[988] [103] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[201] [43] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[31] [3] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[301] [15] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[873] [52] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1202] [204] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[290] [41] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1455] [144] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[19] [10] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[31] [74] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[365] [55] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[39] [13] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[417] [72] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[35] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[110] [25] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[120] [24] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[28] [7] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[31] [8] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[372] [158] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1404] [158] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[28] [5] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[15] [7] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[82] [13] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[21] [25] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[125] [16] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[10] [2] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[46] [9] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[31] [4] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[103] [24] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[713] [105] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[89] [9] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[217] [33] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[112] [10] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[77] [17] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[31] [117] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[241] [48] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[903] [160] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[527] [64] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[82] [18] 1\n",
      "Vote proportion 0.2\n",
      "Recall 0.9411764650519031 Precision 0.3478260862003781 F1 0.5079364669186223\n"
     ]
    }
   ],
   "source": [
    "predict_per_user(writings_df=writings_df, voc=voc, majority_prop=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tokens(row):\n",
    "    tokens = []\n",
    "    if row.tokenized_text:\n",
    "        tokens += row.tokenized_text\n",
    "    if row.tokenized_title:\n",
    "        tokens += row.tokenized_title\n",
    "    return tokens\n",
    "writings_df['all_tokens'] = writings_df.apply (lambda row: merge_tokens(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: include the title\n",
    "def extract_emotions(tokens, emotion, relative=True):\n",
    "    if not tokens:\n",
    "        return None\n",
    "    emotion_words = [t for t in tokens \n",
    "                     if t in nrc_lexicon[emotion]]\n",
    "    if relative:\n",
    "        return len(emotion_words) / len(tokens)\n",
    "    else:\n",
    "        return len(emotion_words)\n",
    "    \n",
    "    return encoded_emotions\n",
    "\n",
    "from functools import partial\n",
    "for emotion in emotions:\n",
    "    writings_df[emotion] = writings_df['all_tokens'].apply(partial(extract_emotions, emotion=emotion, \n",
    "                                                                   relative=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['pronouns'] = writings_df['all_tokens'].apply(partial(encode_pronouns, relative=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>text_len</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.104269</td>\n",
       "      <td>0.011986</td>\n",
       "      <td>0.020197</td>\n",
       "      <td>0.031982</td>\n",
       "      <td>0.031271</td>\n",
       "      <td>0.019335</td>\n",
       "      <td>0.040782</td>\n",
       "      <td>0.023853</td>\n",
       "      <td>0.023621</td>\n",
       "      <td>0.032969</td>\n",
       "      <td>0.020421</td>\n",
       "      <td>0.023590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pronouns</th>\n",
       "      <td>0.104269</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.636745</td>\n",
       "      <td>0.449384</td>\n",
       "      <td>0.567496</td>\n",
       "      <td>0.452098</td>\n",
       "      <td>0.464899</td>\n",
       "      <td>0.548570</td>\n",
       "      <td>0.513029</td>\n",
       "      <td>0.571303</td>\n",
       "      <td>0.524614</td>\n",
       "      <td>0.461328</td>\n",
       "      <td>0.538335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_len</th>\n",
       "      <td>0.011986</td>\n",
       "      <td>0.636745</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.708853</td>\n",
       "      <td>0.791715</td>\n",
       "      <td>0.642980</td>\n",
       "      <td>0.738146</td>\n",
       "      <td>0.728836</td>\n",
       "      <td>0.823974</td>\n",
       "      <td>0.867609</td>\n",
       "      <td>0.723653</td>\n",
       "      <td>0.650420</td>\n",
       "      <td>0.834939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.020197</td>\n",
       "      <td>0.449384</td>\n",
       "      <td>0.708853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.643459</td>\n",
       "      <td>0.762591</td>\n",
       "      <td>0.858442</td>\n",
       "      <td>0.564162</td>\n",
       "      <td>0.835345</td>\n",
       "      <td>0.681573</td>\n",
       "      <td>0.774846</td>\n",
       "      <td>0.583704</td>\n",
       "      <td>0.671042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anticipation</th>\n",
       "      <td>0.031982</td>\n",
       "      <td>0.567496</td>\n",
       "      <td>0.791715</td>\n",
       "      <td>0.643459</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.573916</td>\n",
       "      <td>0.668326</td>\n",
       "      <td>0.834784</td>\n",
       "      <td>0.684882</td>\n",
       "      <td>0.849864</td>\n",
       "      <td>0.668269</td>\n",
       "      <td>0.727331</td>\n",
       "      <td>0.818885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.031271</td>\n",
       "      <td>0.452098</td>\n",
       "      <td>0.642980</td>\n",
       "      <td>0.762591</td>\n",
       "      <td>0.573916</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.729799</td>\n",
       "      <td>0.526733</td>\n",
       "      <td>0.765865</td>\n",
       "      <td>0.603013</td>\n",
       "      <td>0.737717</td>\n",
       "      <td>0.540439</td>\n",
       "      <td>0.589641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.019335</td>\n",
       "      <td>0.464899</td>\n",
       "      <td>0.738146</td>\n",
       "      <td>0.858442</td>\n",
       "      <td>0.668326</td>\n",
       "      <td>0.729799</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.570632</td>\n",
       "      <td>0.862778</td>\n",
       "      <td>0.706676</td>\n",
       "      <td>0.824782</td>\n",
       "      <td>0.569688</td>\n",
       "      <td>0.687232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.040782</td>\n",
       "      <td>0.548570</td>\n",
       "      <td>0.728836</td>\n",
       "      <td>0.564162</td>\n",
       "      <td>0.834784</td>\n",
       "      <td>0.526733</td>\n",
       "      <td>0.570632</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.604964</td>\n",
       "      <td>0.850961</td>\n",
       "      <td>0.603296</td>\n",
       "      <td>0.722710</td>\n",
       "      <td>0.811529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0.023853</td>\n",
       "      <td>0.513029</td>\n",
       "      <td>0.823974</td>\n",
       "      <td>0.835345</td>\n",
       "      <td>0.684882</td>\n",
       "      <td>0.765865</td>\n",
       "      <td>0.862778</td>\n",
       "      <td>0.604964</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.735431</td>\n",
       "      <td>0.840379</td>\n",
       "      <td>0.597634</td>\n",
       "      <td>0.706808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>0.023621</td>\n",
       "      <td>0.571303</td>\n",
       "      <td>0.867609</td>\n",
       "      <td>0.681573</td>\n",
       "      <td>0.849864</td>\n",
       "      <td>0.603013</td>\n",
       "      <td>0.706676</td>\n",
       "      <td>0.850961</td>\n",
       "      <td>0.735431</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.702751</td>\n",
       "      <td>0.678778</td>\n",
       "      <td>0.916526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.032969</td>\n",
       "      <td>0.524614</td>\n",
       "      <td>0.723653</td>\n",
       "      <td>0.774846</td>\n",
       "      <td>0.668269</td>\n",
       "      <td>0.737717</td>\n",
       "      <td>0.824782</td>\n",
       "      <td>0.603296</td>\n",
       "      <td>0.840379</td>\n",
       "      <td>0.702751</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.584816</td>\n",
       "      <td>0.665750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.020421</td>\n",
       "      <td>0.461328</td>\n",
       "      <td>0.650420</td>\n",
       "      <td>0.583704</td>\n",
       "      <td>0.727331</td>\n",
       "      <td>0.540439</td>\n",
       "      <td>0.569688</td>\n",
       "      <td>0.722710</td>\n",
       "      <td>0.597634</td>\n",
       "      <td>0.678778</td>\n",
       "      <td>0.584816</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.660681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust</th>\n",
       "      <td>0.023590</td>\n",
       "      <td>0.538335</td>\n",
       "      <td>0.834939</td>\n",
       "      <td>0.671042</td>\n",
       "      <td>0.818885</td>\n",
       "      <td>0.589641</td>\n",
       "      <td>0.687232</td>\n",
       "      <td>0.811529</td>\n",
       "      <td>0.706808</td>\n",
       "      <td>0.916526</td>\n",
       "      <td>0.665750</td>\n",
       "      <td>0.660681</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 label  pronouns  text_len     anger  anticipation   disgust  \\\n",
       "label         1.000000  0.104269  0.011986  0.020197      0.031982  0.031271   \n",
       "pronouns      0.104269  1.000000  0.636745  0.449384      0.567496  0.452098   \n",
       "text_len      0.011986  0.636745  1.000000  0.708853      0.791715  0.642980   \n",
       "anger         0.020197  0.449384  0.708853  1.000000      0.643459  0.762591   \n",
       "anticipation  0.031982  0.567496  0.791715  0.643459      1.000000  0.573916   \n",
       "disgust       0.031271  0.452098  0.642980  0.762591      0.573916  1.000000   \n",
       "fear          0.019335  0.464899  0.738146  0.858442      0.668326  0.729799   \n",
       "joy           0.040782  0.548570  0.728836  0.564162      0.834784  0.526733   \n",
       "negative      0.023853  0.513029  0.823974  0.835345      0.684882  0.765865   \n",
       "positive      0.023621  0.571303  0.867609  0.681573      0.849864  0.603013   \n",
       "sadness       0.032969  0.524614  0.723653  0.774846      0.668269  0.737717   \n",
       "surprise      0.020421  0.461328  0.650420  0.583704      0.727331  0.540439   \n",
       "trust         0.023590  0.538335  0.834939  0.671042      0.818885  0.589641   \n",
       "\n",
       "                  fear       joy  negative  positive   sadness  surprise  \\\n",
       "label         0.019335  0.040782  0.023853  0.023621  0.032969  0.020421   \n",
       "pronouns      0.464899  0.548570  0.513029  0.571303  0.524614  0.461328   \n",
       "text_len      0.738146  0.728836  0.823974  0.867609  0.723653  0.650420   \n",
       "anger         0.858442  0.564162  0.835345  0.681573  0.774846  0.583704   \n",
       "anticipation  0.668326  0.834784  0.684882  0.849864  0.668269  0.727331   \n",
       "disgust       0.729799  0.526733  0.765865  0.603013  0.737717  0.540439   \n",
       "fear          1.000000  0.570632  0.862778  0.706676  0.824782  0.569688   \n",
       "joy           0.570632  1.000000  0.604964  0.850961  0.603296  0.722710   \n",
       "negative      0.862778  0.604964  1.000000  0.735431  0.840379  0.597634   \n",
       "positive      0.706676  0.850961  0.735431  1.000000  0.702751  0.678778   \n",
       "sadness       0.824782  0.603296  0.840379  0.702751  1.000000  0.584816   \n",
       "surprise      0.569688  0.722710  0.597634  0.678778  0.584816  1.000000   \n",
       "trust         0.687232  0.811529  0.706808  0.916526  0.665750  0.660681   \n",
       "\n",
       "                 trust  \n",
       "label         0.023590  \n",
       "pronouns      0.538335  \n",
       "text_len      0.834939  \n",
       "anger         0.671042  \n",
       "anticipation  0.818885  \n",
       "disgust       0.589641  \n",
       "fear          0.687232  \n",
       "joy           0.811529  \n",
       "negative      0.706808  \n",
       "positive      0.916526  \n",
       "sadness       0.665750  \n",
       "surprise      0.660681  \n",
       "trust         1.000000  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len'] + emotions].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pronouns</th>\n",
       "      <th>text_len</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.868213</td>\n",
       "      <td>32.031615</td>\n",
       "      <td>0.386069</td>\n",
       "      <td>0.58984</td>\n",
       "      <td>0.263683</td>\n",
       "      <td>0.478014</td>\n",
       "      <td>0.479908</td>\n",
       "      <td>0.818800</td>\n",
       "      <td>1.280788</td>\n",
       "      <td>0.385315</td>\n",
       "      <td>0.284790</td>\n",
       "      <td>0.830560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.484271</td>\n",
       "      <td>36.398389</td>\n",
       "      <td>0.529232</td>\n",
       "      <td>0.86985</td>\n",
       "      <td>0.416203</td>\n",
       "      <td>0.654371</td>\n",
       "      <td>0.769766</td>\n",
       "      <td>1.152422</td>\n",
       "      <td>1.717428</td>\n",
       "      <td>0.627088</td>\n",
       "      <td>0.375418</td>\n",
       "      <td>1.128341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pronouns   text_len     anger  anticipation   disgust      fear  \\\n",
       "label                                                                    \n",
       "0      0.868213  32.031615  0.386069       0.58984  0.263683  0.478014   \n",
       "1      2.484271  36.398389  0.529232       0.86985  0.416203  0.654371   \n",
       "\n",
       "            joy  negative  positive   sadness  surprise     trust  \n",
       "label                                                              \n",
       "0      0.479908  0.818800  1.280788  0.385315  0.284790  0.830560  \n",
       "1      0.769766  1.152422  1.717428  0.627088  0.375418  1.128341  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len'] + emotions].groupby('label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentAnalyzer, SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.66, 'pos': 0.34, 'compound': 0.5574}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid.polarity_scores(\"We are here today happiness is all around\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['neg_vader'] = writings_df.text.apply(lambda t: sid.polarity_scores(t)['neg']\n",
    "                                                 if type(t)==str else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>title_len</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>text_len</th>\n",
       "      <th>emotions</th>\n",
       "      <th>...</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>all_tokens</th>\n",
       "      <th>neg_vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>If anyone could help with which sub to put thi...</td>\n",
       "      <td>2016-08-02 09:22:12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[if, anyone, could, help, with, which, sub, to...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[if, anyone, could, help, with, which, sub, to...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>I'm literally never gonna stop waiting...</td>\n",
       "      <td>2016-08-05 09:35:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, m, literally, never, gonna, stop, waiting]</td>\n",
       "      <td>7.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[i, m, literally, never, gonna, stop, waiting]</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>This is a really interesting study! Makes sens...</td>\n",
       "      <td>2016-08-05 21:36:24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[this, is, a, really, interesting, study, make...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[this, is, a, really, interesting, study, make...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>The only thing Frank is building ...</td>\n",
       "      <td>2016-08-07 23:35:23</td>\n",
       "      <td>... Is hype. Think about it, every time he wor...</td>\n",
       "      <td>0</td>\n",
       "      <td>[the, only, thing, frank, is, building]</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[is, hype, think, about, it, every, time, he, ...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[is, hype, think, about, it, every, time, he, ...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>Mostly always me during this whole charade</td>\n",
       "      <td>2016-08-09 08:39:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[mostly, always, me, during, this, whole, char...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[mostly, always, me, during, this, whole, char...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170693</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 11:29:21</td>\n",
       "      <td>this is my personal experience ,it may not ref...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[this, is, my, personal, experience, it, may, ...</td>\n",
       "      <td>153.0</td>\n",
       "      <td>0.026144</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[this, is, my, personal, experience, it, may, ...</td>\n",
       "      <td>0.089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170694</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 16:17:34</td>\n",
       "      <td>stop looking at 20 million saudis as one entit...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[stop, looking, at, 20, million, saudis, as, o...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[stop, looking, at, 20, million, saudis, as, o...</td>\n",
       "      <td>0.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170695</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 20:00:31</td>\n",
       "      <td>i am aware of stats now and then. i was just s...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[i, am, aware, of, stats, now, and, then, i, w...</td>\n",
       "      <td>198.0</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>[i, am, aware, of, stats, now, and, then, i, w...</td>\n",
       "      <td>0.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170696</th>\n",
       "      <td>subject217</td>\n",
       "      <td>WHAT DID YOU SAY TO ME?</td>\n",
       "      <td>2018-08-20 10:54:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[what, did, you, say, to, me]</td>\n",
       "      <td>6.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[what, did, you, say, to, me]</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170697</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-20 12:07:44</td>\n",
       "      <td>me smellz fish,me find no fish!...what the fuc...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[me, smellz, fish, me, find, no, fish, what, t...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[me, smellz, fish, me, find, no, fish, what, t...</td>\n",
       "      <td>0.484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170698 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            subject                                              title  \\\n",
       "0       subject8292  If anyone could help with which sub to put thi...   \n",
       "1       subject8292          I'm literally never gonna stop waiting...   \n",
       "2       subject8292  This is a really interesting study! Makes sens...   \n",
       "3       subject8292               The only thing Frank is building ...   \n",
       "4       subject8292         Mostly always me during this whole charade   \n",
       "...             ...                                                ...   \n",
       "170693   subject217                                                NaN   \n",
       "170694   subject217                                                NaN   \n",
       "170695   subject217                                                NaN   \n",
       "170696   subject217                            WHAT DID YOU SAY TO ME?   \n",
       "170697   subject217                                                NaN   \n",
       "\n",
       "                       date  \\\n",
       "0       2016-08-02 09:22:12   \n",
       "1       2016-08-05 09:35:55   \n",
       "2       2016-08-05 21:36:24   \n",
       "3       2016-08-07 23:35:23   \n",
       "4       2016-08-09 08:39:41   \n",
       "...                     ...   \n",
       "170693  2018-08-19 11:29:21   \n",
       "170694  2018-08-19 16:17:34   \n",
       "170695  2018-08-19 20:00:31   \n",
       "170696  2018-08-20 10:54:11   \n",
       "170697  2018-08-20 12:07:44   \n",
       "\n",
       "                                                     text  label  \\\n",
       "0                                                     NaN      0   \n",
       "1                                                     NaN      0   \n",
       "2                                                     NaN      0   \n",
       "3       ... Is hype. Think about it, every time he wor...      0   \n",
       "4                                                     NaN      0   \n",
       "...                                                   ...    ...   \n",
       "170693  this is my personal experience ,it may not ref...      0   \n",
       "170694  stop looking at 20 million saudis as one entit...      0   \n",
       "170695  i am aware of stats now and then. i was just s...      0   \n",
       "170696                                                NaN      0   \n",
       "170697  me smellz fish,me find no fish!...what the fuc...      0   \n",
       "\n",
       "                                          tokenized_title  title_len  \\\n",
       "0       [if, anyone, could, help, with, which, sub, to...       11.0   \n",
       "1          [i, m, literally, never, gonna, stop, waiting]        7.0   \n",
       "2       [this, is, a, really, interesting, study, make...        9.0   \n",
       "3                 [the, only, thing, frank, is, building]        6.0   \n",
       "4       [mostly, always, me, during, this, whole, char...        7.0   \n",
       "...                                                   ...        ...   \n",
       "170693                                               None        NaN   \n",
       "170694                                               None        NaN   \n",
       "170695                                               None        NaN   \n",
       "170696                      [what, did, you, say, to, me]        6.0   \n",
       "170697                                               None        NaN   \n",
       "\n",
       "                                           tokenized_text  text_len  emotions  \\\n",
       "0                                                    None       NaN       NaN   \n",
       "1                                                    None       NaN       NaN   \n",
       "2                                                    None       NaN       NaN   \n",
       "3       [is, hype, think, about, it, every, time, he, ...      26.0  0.000000   \n",
       "4                                                    None       NaN       NaN   \n",
       "...                                                   ...       ...       ...   \n",
       "170693  [this, is, my, personal, experience, it, may, ...     153.0  0.026144   \n",
       "170694  [stop, looking, at, 20, million, saudis, as, o...      15.0  0.000000   \n",
       "170695  [i, am, aware, of, stats, now, and, then, i, w...     198.0  0.030303   \n",
       "170696                                               None       NaN       NaN   \n",
       "170697  [me, smellz, fish, me, find, no, fish, what, t...      11.0  0.000000   \n",
       "\n",
       "        ...  fear  joy  negative  positive  sadness  surprise  trust  \\\n",
       "0       ...   0.0  0.0       0.0       0.0      0.0       0.0    0.0   \n",
       "1       ...   0.0  0.0       0.0       0.0      0.0       0.0    0.0   \n",
       "2       ...   0.0  0.0       0.0       3.0      0.0       0.0    0.0   \n",
       "3       ...   0.0  0.0       3.0       3.0      0.0       0.0    1.0   \n",
       "4       ...   0.0  0.0       1.0       0.0      0.0       0.0    0.0   \n",
       "...     ...   ...  ...       ...       ...      ...       ...    ...   \n",
       "170693  ...   1.0  1.0       1.0       7.0      0.0       1.0    4.0   \n",
       "170694  ...   1.0  0.0       1.0       0.0      0.0       0.0    0.0   \n",
       "170695  ...   2.0  3.0       4.0      11.0      3.0       0.0    6.0   \n",
       "170696  ...   0.0  0.0       0.0       0.0      0.0       0.0    0.0   \n",
       "170697  ...   0.0  0.0       0.0       0.0      0.0       0.0    0.0   \n",
       "\n",
       "        pronouns                                         all_tokens  neg_vader  \n",
       "0            0.0  [if, anyone, could, help, with, which, sub, to...      0.000  \n",
       "1            1.0     [i, m, literally, never, gonna, stop, waiting]      0.000  \n",
       "2            0.0  [this, is, a, really, interesting, study, make...      0.000  \n",
       "3            0.0  [is, hype, think, about, it, every, time, he, ...      0.000  \n",
       "4            1.0  [mostly, always, me, during, this, whole, char...      0.000  \n",
       "...          ...                                                ...        ...  \n",
       "170693       4.0  [this, is, my, personal, experience, it, may, ...      0.089  \n",
       "170694       0.0  [stop, looking, at, 20, million, saudis, as, o...      0.145  \n",
       "170695      16.0  [i, am, aware, of, stats, now, and, then, i, w...      0.070  \n",
       "170696       1.0                      [what, did, you, say, to, me]      0.000  \n",
       "170697       2.0  [me, smellz, fish, me, find, no, fish, what, t...      0.484  \n",
       "\n",
       "[170698 rows x 23 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['pos_vader'] = writings_df.text.apply(lambda t: sid.polarity_scores(t)['pos']\n",
    "                                                 if type(t)==str else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pronouns</th>\n",
       "      <th>text_len</th>\n",
       "      <th>neg_vader</th>\n",
       "      <th>pos_vader</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.868213</td>\n",
       "      <td>32.031615</td>\n",
       "      <td>0.054259</td>\n",
       "      <td>0.109981</td>\n",
       "      <td>0.386069</td>\n",
       "      <td>0.58984</td>\n",
       "      <td>0.263683</td>\n",
       "      <td>0.478014</td>\n",
       "      <td>0.479908</td>\n",
       "      <td>0.818800</td>\n",
       "      <td>1.280788</td>\n",
       "      <td>0.385315</td>\n",
       "      <td>0.284790</td>\n",
       "      <td>0.830560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.484271</td>\n",
       "      <td>36.398389</td>\n",
       "      <td>0.079191</td>\n",
       "      <td>0.148154</td>\n",
       "      <td>0.529232</td>\n",
       "      <td>0.86985</td>\n",
       "      <td>0.416203</td>\n",
       "      <td>0.654371</td>\n",
       "      <td>0.769766</td>\n",
       "      <td>1.152422</td>\n",
       "      <td>1.717428</td>\n",
       "      <td>0.627088</td>\n",
       "      <td>0.375418</td>\n",
       "      <td>1.128341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pronouns   text_len  neg_vader  pos_vader     anger  anticipation  \\\n",
       "label                                                                      \n",
       "0      0.868213  32.031615   0.054259   0.109981  0.386069       0.58984   \n",
       "1      2.484271  36.398389   0.079191   0.148154  0.529232       0.86985   \n",
       "\n",
       "        disgust      fear       joy  negative  positive   sadness  surprise  \\\n",
       "label                                                                         \n",
       "0      0.263683  0.478014  0.479908  0.818800  1.280788  0.385315  0.284790   \n",
       "1      0.416203  0.654371  0.769766  1.152422  1.717428  0.627088  0.375418   \n",
       "\n",
       "          trust  \n",
       "label            \n",
       "0      0.830560  \n",
       "1      1.128341  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len', 'neg_vader', 'pos_vader'] + emotions].groupby('label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>text_len</th>\n",
       "      <th>neg_vader</th>\n",
       "      <th>pos_vader</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.097800</td>\n",
       "      <td>0.033477</td>\n",
       "      <td>0.067170</td>\n",
       "      <td>0.065211</td>\n",
       "      <td>0.022057</td>\n",
       "      <td>0.025666</td>\n",
       "      <td>0.030664</td>\n",
       "      <td>0.019114</td>\n",
       "      <td>0.033977</td>\n",
       "      <td>0.022934</td>\n",
       "      <td>0.019590</td>\n",
       "      <td>0.032641</td>\n",
       "      <td>0.018109</td>\n",
       "      <td>0.024014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pronouns</th>\n",
       "      <td>0.097800</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.332071</td>\n",
       "      <td>0.193938</td>\n",
       "      <td>0.221419</td>\n",
       "      <td>0.076345</td>\n",
       "      <td>0.128030</td>\n",
       "      <td>0.094069</td>\n",
       "      <td>0.063176</td>\n",
       "      <td>0.144011</td>\n",
       "      <td>0.076670</td>\n",
       "      <td>0.106055</td>\n",
       "      <td>0.100827</td>\n",
       "      <td>0.106790</td>\n",
       "      <td>0.122914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_len</th>\n",
       "      <td>0.033477</td>\n",
       "      <td>0.332071</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.343154</td>\n",
       "      <td>0.159673</td>\n",
       "      <td>0.360460</td>\n",
       "      <td>0.386351</td>\n",
       "      <td>0.312393</td>\n",
       "      <td>0.381410</td>\n",
       "      <td>0.339398</td>\n",
       "      <td>0.370250</td>\n",
       "      <td>0.330075</td>\n",
       "      <td>0.384031</td>\n",
       "      <td>0.349498</td>\n",
       "      <td>0.389620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_vader</th>\n",
       "      <td>0.067170</td>\n",
       "      <td>0.193938</td>\n",
       "      <td>0.343154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.169624</td>\n",
       "      <td>0.384510</td>\n",
       "      <td>0.141868</td>\n",
       "      <td>0.362582</td>\n",
       "      <td>0.339245</td>\n",
       "      <td>0.126042</td>\n",
       "      <td>0.431111</td>\n",
       "      <td>0.099767</td>\n",
       "      <td>0.374256</td>\n",
       "      <td>0.159302</td>\n",
       "      <td>0.143060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_vader</th>\n",
       "      <td>0.065211</td>\n",
       "      <td>0.221419</td>\n",
       "      <td>0.159673</td>\n",
       "      <td>0.169624</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.079693</td>\n",
       "      <td>0.225925</td>\n",
       "      <td>0.087309</td>\n",
       "      <td>0.071450</td>\n",
       "      <td>0.323148</td>\n",
       "      <td>0.058266</td>\n",
       "      <td>0.270687</td>\n",
       "      <td>0.095040</td>\n",
       "      <td>0.186243</td>\n",
       "      <td>0.231954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.022057</td>\n",
       "      <td>0.076345</td>\n",
       "      <td>0.360460</td>\n",
       "      <td>0.384510</td>\n",
       "      <td>0.079693</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.196795</td>\n",
       "      <td>0.583864</td>\n",
       "      <td>0.587460</td>\n",
       "      <td>0.157202</td>\n",
       "      <td>0.631708</td>\n",
       "      <td>0.128169</td>\n",
       "      <td>0.528980</td>\n",
       "      <td>0.273195</td>\n",
       "      <td>0.169261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anticipation</th>\n",
       "      <td>0.025666</td>\n",
       "      <td>0.128030</td>\n",
       "      <td>0.386351</td>\n",
       "      <td>0.141868</td>\n",
       "      <td>0.225925</td>\n",
       "      <td>0.196795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.164649</td>\n",
       "      <td>0.241958</td>\n",
       "      <td>0.583107</td>\n",
       "      <td>0.178827</td>\n",
       "      <td>0.452457</td>\n",
       "      <td>0.198972</td>\n",
       "      <td>0.460851</td>\n",
       "      <td>0.469028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.030664</td>\n",
       "      <td>0.094069</td>\n",
       "      <td>0.312393</td>\n",
       "      <td>0.362582</td>\n",
       "      <td>0.087309</td>\n",
       "      <td>0.583864</td>\n",
       "      <td>0.164649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.440376</td>\n",
       "      <td>0.152731</td>\n",
       "      <td>0.552021</td>\n",
       "      <td>0.116588</td>\n",
       "      <td>0.490181</td>\n",
       "      <td>0.232166</td>\n",
       "      <td>0.153723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.019114</td>\n",
       "      <td>0.063176</td>\n",
       "      <td>0.381410</td>\n",
       "      <td>0.339245</td>\n",
       "      <td>0.071450</td>\n",
       "      <td>0.587460</td>\n",
       "      <td>0.241958</td>\n",
       "      <td>0.440376</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.159907</td>\n",
       "      <td>0.576962</td>\n",
       "      <td>0.141985</td>\n",
       "      <td>0.583703</td>\n",
       "      <td>0.248160</td>\n",
       "      <td>0.184240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.033977</td>\n",
       "      <td>0.144011</td>\n",
       "      <td>0.339398</td>\n",
       "      <td>0.126042</td>\n",
       "      <td>0.323148</td>\n",
       "      <td>0.157202</td>\n",
       "      <td>0.583107</td>\n",
       "      <td>0.152731</td>\n",
       "      <td>0.159907</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.113400</td>\n",
       "      <td>0.645827</td>\n",
       "      <td>0.176440</td>\n",
       "      <td>0.477317</td>\n",
       "      <td>0.582920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0.022934</td>\n",
       "      <td>0.076670</td>\n",
       "      <td>0.370250</td>\n",
       "      <td>0.431111</td>\n",
       "      <td>0.058266</td>\n",
       "      <td>0.631708</td>\n",
       "      <td>0.178827</td>\n",
       "      <td>0.552021</td>\n",
       "      <td>0.576962</td>\n",
       "      <td>0.113400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.105821</td>\n",
       "      <td>0.612781</td>\n",
       "      <td>0.226230</td>\n",
       "      <td>0.145220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>0.019590</td>\n",
       "      <td>0.106055</td>\n",
       "      <td>0.330075</td>\n",
       "      <td>0.099767</td>\n",
       "      <td>0.270687</td>\n",
       "      <td>0.128169</td>\n",
       "      <td>0.452457</td>\n",
       "      <td>0.116588</td>\n",
       "      <td>0.141985</td>\n",
       "      <td>0.645827</td>\n",
       "      <td>0.105821</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.139827</td>\n",
       "      <td>0.333998</td>\n",
       "      <td>0.648163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.032641</td>\n",
       "      <td>0.100827</td>\n",
       "      <td>0.384031</td>\n",
       "      <td>0.374256</td>\n",
       "      <td>0.095040</td>\n",
       "      <td>0.528980</td>\n",
       "      <td>0.198972</td>\n",
       "      <td>0.490181</td>\n",
       "      <td>0.583703</td>\n",
       "      <td>0.176440</td>\n",
       "      <td>0.612781</td>\n",
       "      <td>0.139827</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.265026</td>\n",
       "      <td>0.171245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.018109</td>\n",
       "      <td>0.106790</td>\n",
       "      <td>0.349498</td>\n",
       "      <td>0.159302</td>\n",
       "      <td>0.186243</td>\n",
       "      <td>0.273195</td>\n",
       "      <td>0.460851</td>\n",
       "      <td>0.232166</td>\n",
       "      <td>0.248160</td>\n",
       "      <td>0.477317</td>\n",
       "      <td>0.226230</td>\n",
       "      <td>0.333998</td>\n",
       "      <td>0.265026</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.354746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust</th>\n",
       "      <td>0.024014</td>\n",
       "      <td>0.122914</td>\n",
       "      <td>0.389620</td>\n",
       "      <td>0.143060</td>\n",
       "      <td>0.231954</td>\n",
       "      <td>0.169261</td>\n",
       "      <td>0.469028</td>\n",
       "      <td>0.153723</td>\n",
       "      <td>0.184240</td>\n",
       "      <td>0.582920</td>\n",
       "      <td>0.145220</td>\n",
       "      <td>0.648163</td>\n",
       "      <td>0.171245</td>\n",
       "      <td>0.354746</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 label  pronouns  text_len  neg_vader  pos_vader     anger  \\\n",
       "label         1.000000  0.097800  0.033477   0.067170   0.065211  0.022057   \n",
       "pronouns      0.097800  1.000000  0.332071   0.193938   0.221419  0.076345   \n",
       "text_len      0.033477  0.332071  1.000000   0.343154   0.159673  0.360460   \n",
       "neg_vader     0.067170  0.193938  0.343154   1.000000   0.169624  0.384510   \n",
       "pos_vader     0.065211  0.221419  0.159673   0.169624   1.000000  0.079693   \n",
       "anger         0.022057  0.076345  0.360460   0.384510   0.079693  1.000000   \n",
       "anticipation  0.025666  0.128030  0.386351   0.141868   0.225925  0.196795   \n",
       "disgust       0.030664  0.094069  0.312393   0.362582   0.087309  0.583864   \n",
       "fear          0.019114  0.063176  0.381410   0.339245   0.071450  0.587460   \n",
       "joy           0.033977  0.144011  0.339398   0.126042   0.323148  0.157202   \n",
       "negative      0.022934  0.076670  0.370250   0.431111   0.058266  0.631708   \n",
       "positive      0.019590  0.106055  0.330075   0.099767   0.270687  0.128169   \n",
       "sadness       0.032641  0.100827  0.384031   0.374256   0.095040  0.528980   \n",
       "surprise      0.018109  0.106790  0.349498   0.159302   0.186243  0.273195   \n",
       "trust         0.024014  0.122914  0.389620   0.143060   0.231954  0.169261   \n",
       "\n",
       "              anticipation   disgust      fear       joy  negative  positive  \\\n",
       "label             0.025666  0.030664  0.019114  0.033977  0.022934  0.019590   \n",
       "pronouns          0.128030  0.094069  0.063176  0.144011  0.076670  0.106055   \n",
       "text_len          0.386351  0.312393  0.381410  0.339398  0.370250  0.330075   \n",
       "neg_vader         0.141868  0.362582  0.339245  0.126042  0.431111  0.099767   \n",
       "pos_vader         0.225925  0.087309  0.071450  0.323148  0.058266  0.270687   \n",
       "anger             0.196795  0.583864  0.587460  0.157202  0.631708  0.128169   \n",
       "anticipation      1.000000  0.164649  0.241958  0.583107  0.178827  0.452457   \n",
       "disgust           0.164649  1.000000  0.440376  0.152731  0.552021  0.116588   \n",
       "fear              0.241958  0.440376  1.000000  0.159907  0.576962  0.141985   \n",
       "joy               0.583107  0.152731  0.159907  1.000000  0.113400  0.645827   \n",
       "negative          0.178827  0.552021  0.576962  0.113400  1.000000  0.105821   \n",
       "positive          0.452457  0.116588  0.141985  0.645827  0.105821  1.000000   \n",
       "sadness           0.198972  0.490181  0.583703  0.176440  0.612781  0.139827   \n",
       "surprise          0.460851  0.232166  0.248160  0.477317  0.226230  0.333998   \n",
       "trust             0.469028  0.153723  0.184240  0.582920  0.145220  0.648163   \n",
       "\n",
       "               sadness  surprise     trust  \n",
       "label         0.032641  0.018109  0.024014  \n",
       "pronouns      0.100827  0.106790  0.122914  \n",
       "text_len      0.384031  0.349498  0.389620  \n",
       "neg_vader     0.374256  0.159302  0.143060  \n",
       "pos_vader     0.095040  0.186243  0.231954  \n",
       "anger         0.528980  0.273195  0.169261  \n",
       "anticipation  0.198972  0.460851  0.469028  \n",
       "disgust       0.490181  0.232166  0.153723  \n",
       "fear          0.583703  0.248160  0.184240  \n",
       "joy           0.176440  0.477317  0.582920  \n",
       "negative      0.612781  0.226230  0.145220  \n",
       "positive      0.139827  0.333998  0.648163  \n",
       "sadness       1.000000  0.265026  0.171245  \n",
       "surprise      0.265026  1.000000  0.354746  \n",
       "trust         0.171245  0.354746  1.000000  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len', 'neg_vader', 'pos_vader'] + emotions].corr('spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from liwc_readDict import readDict\n",
    "\n",
    "liwc = readDict('/home/ana/resources/FakeOrFact/features/LIWC/LIWC/liwc.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'achieve',\n",
       " 'adverb',\n",
       " 'affect',\n",
       " 'anger',\n",
       " 'anx',\n",
       " 'article',\n",
       " 'assent',\n",
       " 'auxverb',\n",
       " 'bio',\n",
       " 'body',\n",
       " 'cause',\n",
       " 'certain',\n",
       " 'cogmech',\n",
       " 'conj',\n",
       " 'death',\n",
       " 'discrep',\n",
       " 'excl',\n",
       " 'family',\n",
       " 'feel',\n",
       " 'filler',\n",
       " 'friend',\n",
       " 'funct',\n",
       " 'future',\n",
       " 'health',\n",
       " 'hear',\n",
       " 'home',\n",
       " 'humans',\n",
       " 'i',\n",
       " 'incl',\n",
       " 'ingest',\n",
       " 'inhib',\n",
       " 'insight',\n",
       " 'ipron',\n",
       " 'leisure',\n",
       " 'money',\n",
       " 'motion',\n",
       " 'negate',\n",
       " 'negemo',\n",
       " 'nonfl',\n",
       " 'number',\n",
       " 'past',\n",
       " 'percept',\n",
       " 'posemo',\n",
       " 'ppron',\n",
       " 'preps',\n",
       " 'present',\n",
       " 'pronoun',\n",
       " 'quant',\n",
       " 'relativ',\n",
       " 'relig',\n",
       " 'sad',\n",
       " 'see',\n",
       " 'sexual',\n",
       " 'shehe',\n",
       " 'social',\n",
       " 'space',\n",
       " 'swear',\n",
       " 'tentat',\n",
       " 'they',\n",
       " 'time',\n",
       " 'verb',\n",
       " 'we',\n",
       " 'work',\n",
       " 'you'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = [c for (w,c) in liwc]\n",
    "set(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 'funct'],\n",
       " ['a', 'article'],\n",
       " ['abandon*', 'affect'],\n",
       " ['abandon*', 'negemo'],\n",
       " ['abandon*', 'sad'],\n",
       " ['abandon*', 'cogmech'],\n",
       " ['abandon*', 'inhib'],\n",
       " ['abdomen*', 'bio'],\n",
       " ['abdomen*', 'body'],\n",
       " ['abilit*', 'achieve'],\n",
       " ['able*', 'achieve'],\n",
       " ['abortion*', 'bio'],\n",
       " ['abortion*', 'health'],\n",
       " ['abortion*', 'sexual'],\n",
       " ['about', 'funct'],\n",
       " ['about', 'adverb'],\n",
       " ['about', 'preps'],\n",
       " ['above', 'funct'],\n",
       " ['above', 'preps'],\n",
       " ['above', 'space'],\n",
       " ['above', 'relativ'],\n",
       " ['abrupt*', 'time'],\n",
       " ['abrupt*', 'relativ'],\n",
       " ['abs', 'bio'],\n",
       " ['abs', 'body'],\n",
       " ['absent*', 'work'],\n",
       " ['absolute', 'cogmech'],\n",
       " ['absolute', 'certain'],\n",
       " ['absolutely', 'funct'],\n",
       " ['absolutely', 'adverb'],\n",
       " ['absolutely', 'cogmech'],\n",
       " ['absolutely', 'certain'],\n",
       " ['absolutely', 'assent'],\n",
       " ['abstain*', 'cogmech'],\n",
       " ['abstain*', 'inhib'],\n",
       " ['abuse*', 'affect'],\n",
       " ['abuse*', 'negemo'],\n",
       " ['abuse*', 'anger'],\n",
       " ['abusi*', 'affect'],\n",
       " ['abusi*', 'negemo'],\n",
       " ['abusi*', 'anger'],\n",
       " ['academ*', 'work'],\n",
       " ['accept', 'affect'],\n",
       " ['accept', 'posemo'],\n",
       " ['accept', 'cogmech'],\n",
       " ['accept', 'insight'],\n",
       " ['accepta*', 'affect'],\n",
       " ['accepta*', 'posemo'],\n",
       " ['accepta*', 'cogmech'],\n",
       " ['accepta*', 'insight'],\n",
       " ['accepted', 'verb'],\n",
       " ['accepted', 'past'],\n",
       " ['accepted', 'affect'],\n",
       " ['accepted', 'posemo'],\n",
       " ['accepted', 'cogmech'],\n",
       " ['accepted', 'insight'],\n",
       " ['accepting', 'affect'],\n",
       " ['accepting', 'posemo'],\n",
       " ['accepting', 'cogmech'],\n",
       " ['accepting', 'insight'],\n",
       " ['accepts', 'affect'],\n",
       " ['accepts', 'posemo'],\n",
       " ['accepts', 'cogmech'],\n",
       " ['accepts', 'insight'],\n",
       " ['accomplish*', 'work'],\n",
       " ['accomplish*', 'achieve'],\n",
       " ['account*', 'money'],\n",
       " ['accura*', 'cogmech'],\n",
       " ['accura*', 'certain'],\n",
       " ['ace', 'achieve'],\n",
       " ['ache*', 'affect'],\n",
       " ['ache*', 'negemo'],\n",
       " ['ache*', 'sad'],\n",
       " ['ache*', 'bio'],\n",
       " ['ache*', 'health'],\n",
       " ['achiev*', 'work'],\n",
       " ['achiev*', 'achieve'],\n",
       " ['aching', 'affect'],\n",
       " ['aching', 'negemo'],\n",
       " ['aching', 'sad'],\n",
       " ['aching', 'bio'],\n",
       " ['aching', 'health'],\n",
       " ['acid*', 'percept'],\n",
       " ['acknowledg*', 'cogmech'],\n",
       " ['acknowledg*', 'insight'],\n",
       " ['acne', 'bio'],\n",
       " ['acne', 'health'],\n",
       " ['acquainta*', 'social'],\n",
       " ['acquainta*', 'friend'],\n",
       " ['acquir*', 'achieve'],\n",
       " ['acquisition*', 'achieve'],\n",
       " ['acrid*', 'percept'],\n",
       " ['across', 'funct'],\n",
       " ['across', 'preps'],\n",
       " ['across', 'space'],\n",
       " ['across', 'relativ'],\n",
       " ['act', 'relativ'],\n",
       " ['act', 'motion'],\n",
       " ['action*', 'motion'],\n",
       " ['action*', 'relativ'],\n",
       " ['activat*', 'cogmech'],\n",
       " ['activat*', 'cause'],\n",
       " ['active*', 'affect'],\n",
       " ['active*', 'posemo'],\n",
       " ['actor*', 'leisure'],\n",
       " ['actress*', 'leisure'],\n",
       " ['actually', 'funct'],\n",
       " ['actually', 'adverb'],\n",
       " ['add', 'cogmech'],\n",
       " ['add', 'incl'],\n",
       " ['addict*', 'bio'],\n",
       " ['addict*', 'health'],\n",
       " ['addit*', 'cogmech'],\n",
       " ['addit*', 'incl'],\n",
       " ['address', 'home'],\n",
       " ['adequa*', 'achieve'],\n",
       " ['adjust*', 'cogmech'],\n",
       " ['adjust*', 'insight'],\n",
       " ['administrat*', 'work'],\n",
       " ['admir*', 'affect'],\n",
       " ['admir*', 'posemo'],\n",
       " ['admit', 'verb'],\n",
       " ['admit', 'present'],\n",
       " ['admit', 'social'],\n",
       " ['admit', 'cogmech'],\n",
       " ['admit', 'insight'],\n",
       " ['admits', 'verb'],\n",
       " ['admits', 'present'],\n",
       " ['admits', 'social'],\n",
       " ['admits', 'cogmech'],\n",
       " ['admits', 'insight'],\n",
       " ['admitted', 'verb'],\n",
       " ['admitted', 'past'],\n",
       " ['admitted', 'social'],\n",
       " ['admitted', 'cogmech'],\n",
       " ['admitted', 'insight'],\n",
       " ['admitting', 'social'],\n",
       " ['admitting', 'cogmech'],\n",
       " ['admitting', 'insight'],\n",
       " ['ador*', 'affect'],\n",
       " ['ador*', 'posemo'],\n",
       " ['adult', 'social'],\n",
       " ['adult', 'humans'],\n",
       " ['adults', 'social'],\n",
       " ['adults', 'humans'],\n",
       " ['advanc*', 'motion'],\n",
       " ['advanc*', 'relativ'],\n",
       " ['advanc*', 'achieve'],\n",
       " ['advantag*', 'affect'],\n",
       " ['advantag*', 'posemo'],\n",
       " ['advantag*', 'achieve'],\n",
       " ['adventur*', 'affect'],\n",
       " ['adventur*', 'posemo'],\n",
       " ['advers*', 'affect'],\n",
       " ['advers*', 'negemo'],\n",
       " ['advertising', 'work'],\n",
       " ['advice', 'social'],\n",
       " ['advil', 'bio'],\n",
       " ['advil', 'health'],\n",
       " ['advis*', 'social'],\n",
       " ['advis*', 'work'],\n",
       " ['aerobic*', 'leisure'],\n",
       " ['affair*', 'social'],\n",
       " ['affect', 'cogmech'],\n",
       " ['affect', 'cause'],\n",
       " ['affected', 'verb'],\n",
       " ['affected', 'past'],\n",
       " ['affected', 'cogmech'],\n",
       " ['affected', 'cause'],\n",
       " ['affecting', 'cogmech'],\n",
       " ['affecting', 'cause'],\n",
       " ['affection*', 'affect'],\n",
       " ['affection*', 'posemo'],\n",
       " ['affects', 'cogmech'],\n",
       " ['affects', 'cause'],\n",
       " ['afraid', 'affect'],\n",
       " ['afraid', 'negemo'],\n",
       " ['afraid', 'anx'],\n",
       " ['after', 'funct'],\n",
       " ['after', 'preps'],\n",
       " ['after', 'time'],\n",
       " ['after', 'relativ'],\n",
       " ['afterlife*', 'time'],\n",
       " ['afterlife*', 'relativ'],\n",
       " ['afterlife*', 'relig'],\n",
       " ['aftermath*', 'time'],\n",
       " ['aftermath*', 'relativ'],\n",
       " ['afternoon*', 'time'],\n",
       " ['afternoon*', 'relativ'],\n",
       " ['afterthought*', 'cogmech'],\n",
       " ['afterthought*', 'insight'],\n",
       " ['afterthought*', 'time'],\n",
       " ['afterthought*', 'relativ'],\n",
       " ['afterward*', 'time'],\n",
       " ['afterward*', 'relativ'],\n",
       " ['again', 'funct'],\n",
       " ['again', 'adverb'],\n",
       " ['again', 'time'],\n",
       " ['again', 'relativ'],\n",
       " ['against', 'funct'],\n",
       " ['against', 'preps'],\n",
       " ['age', 'time'],\n",
       " ['age', 'relativ'],\n",
       " ['aged', 'time'],\n",
       " ['aged', 'relativ'],\n",
       " ['agent', 'work'],\n",
       " ['agents', 'work'],\n",
       " ['ages', 'time'],\n",
       " ['ages', 'relativ'],\n",
       " ['aggravat*', 'affect'],\n",
       " ['aggravat*', 'negemo'],\n",
       " ['aggravat*', 'anger'],\n",
       " ['aggravat*', 'cogmech'],\n",
       " ['aggravat*', 'cause'],\n",
       " ['aggress*', 'affect'],\n",
       " ['aggress*', 'negemo'],\n",
       " ['aggress*', 'anger'],\n",
       " ['aging', 'time'],\n",
       " ['aging', 'relativ'],\n",
       " ['agitat*', 'affect'],\n",
       " ['agitat*', 'negemo'],\n",
       " ['agitat*', 'anger'],\n",
       " ['agnost*', 'relig'],\n",
       " ['ago', 'time'],\n",
       " ['ago', 'relativ'],\n",
       " ['agoniz*', 'affect'],\n",
       " ['agoniz*', 'negemo'],\n",
       " ['agoniz*', 'sad'],\n",
       " ['agony', 'affect'],\n",
       " ['agony', 'negemo'],\n",
       " ['agony', 'sad'],\n",
       " ['agree', 'affect'],\n",
       " ['agree', 'posemo'],\n",
       " ['agree', 'assent'],\n",
       " ['agreeab*', 'affect'],\n",
       " ['agreeab*', 'posemo'],\n",
       " ['agreed', 'affect'],\n",
       " ['agreed', 'posemo'],\n",
       " ['agreeing', 'affect'],\n",
       " ['agreeing', 'posemo'],\n",
       " ['agreement*', 'affect'],\n",
       " ['agreement*', 'posemo'],\n",
       " ['agrees', 'affect'],\n",
       " ['agrees', 'posemo'],\n",
       " ['ah', 'assent'],\n",
       " ['ahead', 'funct'],\n",
       " ['ahead', 'preps'],\n",
       " ['ahead', 'time'],\n",
       " ['ahead', 'relativ'],\n",
       " ['ahead', 'achieve'],\n",
       " ['aids', 'bio'],\n",
       " ['aids', 'health'],\n",
       " ['aids', 'sexual'],\n",
       " [\"ain't\", 'verb'],\n",
       " [\"ain't\", 'funct'],\n",
       " [\"ain't\", 'auxverb'],\n",
       " [\"ain't\", 'present'],\n",
       " [\"ain't\", 'negate'],\n",
       " ['aint', 'verb'],\n",
       " ['aint', 'funct'],\n",
       " ['aint', 'auxverb'],\n",
       " ['aint', 'present'],\n",
       " ['aint', 'negate'],\n",
       " ['air', 'relativ'],\n",
       " ['air', 'space'],\n",
       " ['alarm*', 'affect'],\n",
       " ['alarm*', 'negemo'],\n",
       " ['alarm*', 'anx'],\n",
       " ['alcohol*', 'bio'],\n",
       " ['alcohol*', 'health'],\n",
       " ['alcohol*', 'ingest'],\n",
       " ['alive', 'bio'],\n",
       " ['alive', 'health'],\n",
       " ['alive', 'death'],\n",
       " ['all', 'funct'],\n",
       " ['all', 'quant'],\n",
       " ['all', 'cogmech'],\n",
       " ['all', 'certain'],\n",
       " ['alla', 'relig'],\n",
       " ['allah*', 'relig'],\n",
       " ['allerg*', 'bio'],\n",
       " ['allerg*', 'health'],\n",
       " ['allot', 'funct'],\n",
       " ['allot', 'quant'],\n",
       " ['allot', 'cogmech'],\n",
       " ['allot', 'tentat'],\n",
       " ['allow*', 'cogmech'],\n",
       " ['allow*', 'cause'],\n",
       " ['almost', 'cogmech'],\n",
       " ['almost', 'tentat'],\n",
       " ['alone', 'affect'],\n",
       " ['alone', 'negemo'],\n",
       " ['alone', 'sad'],\n",
       " ['along', 'funct'],\n",
       " ['along', 'preps'],\n",
       " ['along', 'cogmech'],\n",
       " ['along', 'incl'],\n",
       " ['alot', 'funct'],\n",
       " ['alot', 'article'],\n",
       " ['alot', 'quant'],\n",
       " ['alot', 'cogmech'],\n",
       " ['alot', 'tentat'],\n",
       " ['already', 'time'],\n",
       " ['already', 'relativ'],\n",
       " ['alright*', 'affect'],\n",
       " ['alright*', 'posemo'],\n",
       " ['alright*', 'assent'],\n",
       " ['also', 'funct'],\n",
       " ['also', 'adverb'],\n",
       " ['also', 'conj'],\n",
       " ['altar*', 'relig'],\n",
       " ['although', 'funct'],\n",
       " ['although', 'conj'],\n",
       " ['altogether', 'cogmech'],\n",
       " ['altogether', 'certain'],\n",
       " ['always', 'cogmech'],\n",
       " ['always', 'certain'],\n",
       " ['always', 'time'],\n",
       " ['always', 'relativ'],\n",
       " ['am', 'verb'],\n",
       " ['am', 'funct'],\n",
       " ['am', 'auxverb'],\n",
       " ['am', 'present'],\n",
       " ['amaz*', 'affect'],\n",
       " ['amaz*', 'posemo'],\n",
       " ['ambigu*', 'cogmech'],\n",
       " ['ambigu*', 'tentat'],\n",
       " ['ambiti*', 'work'],\n",
       " ['ambiti*', 'achieve'],\n",
       " ['amen', 'relig'],\n",
       " ['amigo*', 'social'],\n",
       " ['amigo*', 'friend'],\n",
       " ['amish', 'relig'],\n",
       " ['among*', 'funct'],\n",
       " ['among*', 'preps'],\n",
       " ['among*', 'space'],\n",
       " ['among*', 'relativ'],\n",
       " ['amor*', 'affect'],\n",
       " ['amor*', 'posemo'],\n",
       " ['amount*', 'quant'],\n",
       " ['amput*', 'bio'],\n",
       " ['amput*', 'health'],\n",
       " ['amus*', 'affect'],\n",
       " ['amus*', 'posemo'],\n",
       " ['amus*', 'leisure'],\n",
       " ['an', 'funct'],\n",
       " ['an', 'article'],\n",
       " ['anal', 'cogmech'],\n",
       " ['anal', 'inhib'],\n",
       " ['anal', 'bio'],\n",
       " ['anal', 'body'],\n",
       " ['analy*', 'cogmech'],\n",
       " ['analy*', 'insight'],\n",
       " ['ancient*', 'time'],\n",
       " ['ancient*', 'relativ'],\n",
       " ['and', 'funct'],\n",
       " ['and', 'conj'],\n",
       " ['and', 'cogmech'],\n",
       " ['and', 'incl'],\n",
       " ['angel', 'relig'],\n",
       " ['angelic*', 'relig'],\n",
       " ['angels', 'relig'],\n",
       " ['anger*', 'affect'],\n",
       " ['anger*', 'negemo'],\n",
       " ['anger*', 'anger'],\n",
       " ['angr*', 'affect'],\n",
       " ['angr*', 'negemo'],\n",
       " ['angr*', 'anger'],\n",
       " ['anguish*', 'affect'],\n",
       " ['anguish*', 'negemo'],\n",
       " ['anguish*', 'anx'],\n",
       " ['ankle*', 'bio'],\n",
       " ['ankle*', 'body'],\n",
       " ['annoy*', 'affect'],\n",
       " ['annoy*', 'negemo'],\n",
       " ['annoy*', 'anger'],\n",
       " ['annual*', 'time'],\n",
       " ['annual*', 'relativ'],\n",
       " ['anorexi*', 'bio'],\n",
       " ['anorexi*', 'health'],\n",
       " ['anorexi*', 'ingest'],\n",
       " ['another', 'funct'],\n",
       " ['another', 'quant'],\n",
       " ['answer*', 'cogmech'],\n",
       " ['answer*', 'insight'],\n",
       " ['antacid*', 'bio'],\n",
       " ['antacid*', 'health'],\n",
       " ['antagoni*', 'affect'],\n",
       " ['antagoni*', 'negemo'],\n",
       " ['antagoni*', 'anger'],\n",
       " ['antidepressant*', 'bio'],\n",
       " ['antidepressant*', 'health'],\n",
       " ['anus*', 'bio'],\n",
       " ['anus*', 'body'],\n",
       " ['anxi*', 'affect'],\n",
       " ['anxi*', 'negemo'],\n",
       " ['anxi*', 'anx'],\n",
       " ['any', 'funct'],\n",
       " ['any', 'quant'],\n",
       " ['any', 'cogmech'],\n",
       " ['any', 'tentat'],\n",
       " ['anybod*', 'funct'],\n",
       " ['anybod*', 'pronoun'],\n",
       " ['anybod*', 'ipron'],\n",
       " ['anybod*', 'social'],\n",
       " ['anybod*', 'cogmech'],\n",
       " ['anybod*', 'tentat'],\n",
       " ['anyhow', 'cogmech'],\n",
       " ['anyhow', 'tentat'],\n",
       " ['anymore', 'funct'],\n",
       " ['anymore', 'quant'],\n",
       " ['anymore', 'relativ'],\n",
       " ['anymore', 'time'],\n",
       " ['anyone*', 'funct'],\n",
       " ['anyone*', 'pronoun'],\n",
       " ['anyone*', 'ipron'],\n",
       " ['anyone*', 'social'],\n",
       " ['anyone*', 'cogmech'],\n",
       " ['anyone*', 'tentat'],\n",
       " ['anything', 'funct'],\n",
       " ['anything', 'pronoun'],\n",
       " ['anything', 'ipron'],\n",
       " ['anything', 'cogmech'],\n",
       " ['anything', 'tentat'],\n",
       " ['anytime', 'cogmech'],\n",
       " ['anytime', 'tentat'],\n",
       " ['anytime', 'time'],\n",
       " ['anytime', 'relativ'],\n",
       " ['anyway*', 'funct'],\n",
       " ['anyway*', 'adverb'],\n",
       " ['anywhere', 'funct'],\n",
       " ['anywhere', 'adverb'],\n",
       " ['anywhere', 'cogmech'],\n",
       " ['anywhere', 'tentat'],\n",
       " ['anywhere', 'space'],\n",
       " ['anywhere', 'relativ'],\n",
       " ['aok', 'affect'],\n",
       " ['aok', 'posemo'],\n",
       " ['aok', 'assent'],\n",
       " ['apart', 'space'],\n",
       " ['apart', 'relativ'],\n",
       " ['apartment*', 'leisure'],\n",
       " ['apartment*', 'home'],\n",
       " ['apath*', 'affect'],\n",
       " ['apath*', 'negemo'],\n",
       " ['apolog*', 'social'],\n",
       " ['appall*', 'affect'],\n",
       " ['appall*', 'negemo'],\n",
       " ['apparent', 'cogmech'],\n",
       " ['apparent', 'certain'],\n",
       " ['apparently', 'funct'],\n",
       " ['apparently', 'adverb'],\n",
       " ['apparently', 'cogmech'],\n",
       " ['apparently', 'tentat'],\n",
       " ['appear', 'verb'],\n",
       " ['appear', 'present'],\n",
       " ['appear', 'cogmech'],\n",
       " ['appear', 'tentat'],\n",
       " ['appear', 'motion'],\n",
       " ['appear', 'relativ'],\n",
       " ['appeared', 'verb'],\n",
       " ['appeared', 'past'],\n",
       " ['appeared', 'cogmech'],\n",
       " ['appeared', 'tentat'],\n",
       " ['appeared', 'motion'],\n",
       " ['appeared', 'relativ'],\n",
       " ['appearing', 'cogmech'],\n",
       " ['appearing', 'tentat'],\n",
       " ['appearing', 'motion'],\n",
       " ['appearing', 'relativ'],\n",
       " ['appears', 'verb'],\n",
       " ['appears', 'present'],\n",
       " ['appears', 'cogmech'],\n",
       " ['appears', 'tentat'],\n",
       " ['appears', 'motion'],\n",
       " ['appears', 'relativ'],\n",
       " ['appendic*', 'bio'],\n",
       " ['appendic*', 'health'],\n",
       " ['appendix', 'bio'],\n",
       " ['appendix', 'body'],\n",
       " ['appeti*', 'bio'],\n",
       " ['appeti*', 'ingest'],\n",
       " ['applicant*', 'work'],\n",
       " ['applicat*', 'work'],\n",
       " ['appreciat*', 'affect'],\n",
       " ['appreciat*', 'posemo'],\n",
       " ['appreciat*', 'cogmech'],\n",
       " ['appreciat*', 'insight'],\n",
       " ['apprehens*', 'affect'],\n",
       " ['apprehens*', 'negemo'],\n",
       " ['apprehens*', 'anx'],\n",
       " ['apprentic*', 'work'],\n",
       " ['approach*', 'motion'],\n",
       " ['approach*', 'relativ'],\n",
       " ['approv*', 'achieve'],\n",
       " ['approximat*', 'cogmech'],\n",
       " ['approximat*', 'tentat'],\n",
       " ['april', 'time'],\n",
       " ['april', 'relativ'],\n",
       " ['arbitrar*', 'cogmech'],\n",
       " ['arbitrar*', 'tentat'],\n",
       " ['arch', 'bio'],\n",
       " ['arch', 'body'],\n",
       " ['are', 'verb'],\n",
       " ['are', 'funct'],\n",
       " ['are', 'auxverb'],\n",
       " ['are', 'present'],\n",
       " ['area*', 'space'],\n",
       " ['area*', 'relativ'],\n",
       " [\"aren't\", 'verb'],\n",
       " [\"aren't\", 'funct'],\n",
       " [\"aren't\", 'auxverb'],\n",
       " [\"aren't\", 'present'],\n",
       " [\"aren't\", 'negate'],\n",
       " ['arent', 'verb'],\n",
       " ['arent', 'funct'],\n",
       " ['arent', 'auxverb'],\n",
       " ['arent', 'present'],\n",
       " ['arent', 'negate'],\n",
       " ['argh*', 'affect'],\n",
       " ['argh*', 'negemo'],\n",
       " ['argh*', 'anger'],\n",
       " ['argu*', 'social'],\n",
       " ['argu*', 'affect'],\n",
       " ['argu*', 'negemo'],\n",
       " ['argu*', 'anger'],\n",
       " ['arm', 'bio'],\n",
       " ['arm', 'body'],\n",
       " ['armies', 'social'],\n",
       " ['armpit*', 'bio'],\n",
       " ['armpit*', 'body'],\n",
       " ['arms*', 'bio'],\n",
       " ['arms*', 'body'],\n",
       " ['army', 'social'],\n",
       " ['aroma*', 'percept'],\n",
       " ['around', 'funct'],\n",
       " ['around', 'adverb'],\n",
       " ['around', 'preps'],\n",
       " ['around', 'cogmech'],\n",
       " ['around', 'incl'],\n",
       " ['around', 'space'],\n",
       " ['around', 'relativ'],\n",
       " ['arous*', 'bio'],\n",
       " ['arous*', 'body'],\n",
       " ['arous*', 'sexual'],\n",
       " ['arrival*', 'motion'],\n",
       " ['arrival*', 'relativ'],\n",
       " ['arrive', 'verb'],\n",
       " ['arrive', 'present'],\n",
       " ['arrive', 'motion'],\n",
       " ['arrive', 'relativ'],\n",
       " ['arrived', 'verb'],\n",
       " ['arrived', 'past'],\n",
       " ['arrived', 'motion'],\n",
       " ['arrived', 'relativ'],\n",
       " ['arrives', 'verb'],\n",
       " ['arrives', 'present'],\n",
       " ['arrives', 'motion'],\n",
       " ['arrives', 'relativ'],\n",
       " ['arriving', 'motion'],\n",
       " ['arriving', 'relativ'],\n",
       " ['arrogan*', 'affect'],\n",
       " ['arrogan*', 'negemo'],\n",
       " ['arrogan*', 'anger'],\n",
       " ['arse', 'bio'],\n",
       " ['arse', 'body'],\n",
       " ['arse', 'swear'],\n",
       " ['arsehole*', 'swear'],\n",
       " ['arses', 'bio'],\n",
       " ['arses', 'body'],\n",
       " ['arses', 'swear'],\n",
       " ['art', 'leisure'],\n",
       " ['arter*', 'bio'],\n",
       " ['arter*', 'body'],\n",
       " ['arthr*', 'bio'],\n",
       " ['arthr*', 'health'],\n",
       " ['artist*', 'leisure'],\n",
       " ['arts', 'leisure'],\n",
       " ['as', 'funct'],\n",
       " ['as', 'preps'],\n",
       " ['as', 'conj'],\n",
       " ['asham*', 'affect'],\n",
       " ['asham*', 'negemo'],\n",
       " ['asham*', 'anx'],\n",
       " ['ask', 'verb'],\n",
       " ['ask', 'present'],\n",
       " ['ask', 'social'],\n",
       " ['asked', 'verb'],\n",
       " ['asked', 'past'],\n",
       " ['asked', 'social'],\n",
       " ['asking', 'social'],\n",
       " ['asks', 'verb'],\n",
       " ['asks', 'present'],\n",
       " ['asks', 'social'],\n",
       " ['asleep', 'bio'],\n",
       " ['asleep', 'body'],\n",
       " ['aspirin*', 'bio'],\n",
       " ['aspirin*', 'health'],\n",
       " ['ass', 'bio'],\n",
       " ['ass', 'body'],\n",
       " ['ass', 'sexual'],\n",
       " ['ass', 'swear'],\n",
       " ['assault*', 'affect'],\n",
       " ['assault*', 'negemo'],\n",
       " ['assault*', 'anger'],\n",
       " ['assembl*', 'social'],\n",
       " ['asses', 'bio'],\n",
       " ['asses', 'body'],\n",
       " ['asses', 'sexual'],\n",
       " ['asses', 'swear'],\n",
       " ['asshole*', 'affect'],\n",
       " ['asshole*', 'negemo'],\n",
       " ['asshole*', 'anger'],\n",
       " ['asshole*', 'swear'],\n",
       " ['assign*', 'work'],\n",
       " ['assistan*', 'work'],\n",
       " ['associat*', 'work'],\n",
       " ['assum*', 'cogmech'],\n",
       " ['assum*', 'insight'],\n",
       " ['assum*', 'tentat'],\n",
       " ['assur*', 'affect'],\n",
       " ['assur*', 'posemo'],\n",
       " ['assur*', 'cogmech'],\n",
       " ['assur*', 'certain'],\n",
       " ['asthma*', 'bio'],\n",
       " ['asthma*', 'health'],\n",
       " ['at', 'funct'],\n",
       " ['at', 'preps'],\n",
       " ['at', 'space'],\n",
       " ['at', 'relativ'],\n",
       " ['ate', 'verb'],\n",
       " ['ate', 'past'],\n",
       " ['ate', 'bio'],\n",
       " ['ate', 'ingest'],\n",
       " ['athletic*', 'leisure'],\n",
       " ['atho', 'funct'],\n",
       " ['atho', 'conj'],\n",
       " ['atm', 'money'],\n",
       " ['atms', 'money'],\n",
       " ['atop', 'funct'],\n",
       " ['atop', 'preps'],\n",
       " ['atop', 'space'],\n",
       " ['atop', 'relativ'],\n",
       " ['attachment*', 'affect'],\n",
       " ['attachment*', 'posemo'],\n",
       " ['attack*', 'affect'],\n",
       " ['attack*', 'negemo'],\n",
       " ['attack*', 'anger'],\n",
       " ['attain*', 'achieve'],\n",
       " ['attempt*', 'achieve'],\n",
       " ['attend', 'motion'],\n",
       " ['attend', 'relativ'],\n",
       " ['attended', 'motion'],\n",
       " ['attended', 'relativ'],\n",
       " ['attending', 'motion'],\n",
       " ['attending', 'relativ'],\n",
       " ['attends', 'motion'],\n",
       " ['attends', 'relativ'],\n",
       " ['attent*', 'cogmech'],\n",
       " ['attent*', 'insight'],\n",
       " ['attract*', 'affect'],\n",
       " ['attract*', 'posemo'],\n",
       " ['attribut*', 'cogmech'],\n",
       " ['attribut*', 'cause'],\n",
       " ['auction*', 'money'],\n",
       " ['audibl*', 'percept'],\n",
       " ['audibl*', 'hear'],\n",
       " ['audio*', 'percept'],\n",
       " ['audio*', 'hear'],\n",
       " ['audit', 'money'],\n",
       " ['audited', 'money'],\n",
       " ['auditing', 'money'],\n",
       " ['auditor', 'money'],\n",
       " ['auditorium*', 'work'],\n",
       " ['auditors', 'money'],\n",
       " ['audits', 'money'],\n",
       " ['august', 'time'],\n",
       " ['august', 'relativ'],\n",
       " ['aunt*', 'social'],\n",
       " ['aunt*', 'family'],\n",
       " ['authorit*', 'achieve'],\n",
       " ['autops*', 'death'],\n",
       " ['autumn', 'time'],\n",
       " ['autumn', 'relativ'],\n",
       " ['aversi*', 'affect'],\n",
       " ['aversi*', 'negemo'],\n",
       " ['aversi*', 'anx'],\n",
       " ['avert*', 'cogmech'],\n",
       " ['avert*', 'inhib'],\n",
       " ['avoid*', 'affect'],\n",
       " ['avoid*', 'negemo'],\n",
       " ['avoid*', 'anx'],\n",
       " ['avoid*', 'cogmech'],\n",
       " ['avoid*', 'inhib'],\n",
       " ['aw', 'assent'],\n",
       " ['award*', 'affect'],\n",
       " ['award*', 'posemo'],\n",
       " ['award*', 'work'],\n",
       " ['award*', 'achieve'],\n",
       " ['aware*', 'cogmech'],\n",
       " ['aware*', 'insight'],\n",
       " ['away', 'funct'],\n",
       " ['away', 'preps'],\n",
       " ['away', 'space'],\n",
       " ['away', 'relativ'],\n",
       " ['awesome', 'affect'],\n",
       " ['awesome', 'posemo'],\n",
       " ['awesome', 'assent'],\n",
       " ['awful', 'affect'],\n",
       " ['awful', 'negemo'],\n",
       " ['awhile', 'time'],\n",
       " ['awhile', 'relativ'],\n",
       " ['awkward*', 'affect'],\n",
       " ['awkward*', 'negemo'],\n",
       " ['awkward*', 'anx'],\n",
       " ['babe*', 'social'],\n",
       " ['babe*', 'humans'],\n",
       " ['babies', 'social'],\n",
       " ['babies', 'humans'],\n",
       " ['baby*', 'social'],\n",
       " ['baby*', 'humans'],\n",
       " ['back', 'funct'],\n",
       " ['back', 'adverb'],\n",
       " ['back', 'time'],\n",
       " ['back', 'relativ'],\n",
       " ['backward*', 'space'],\n",
       " ['backward*', 'relativ'],\n",
       " ['backyard', 'home'],\n",
       " ['bad', 'affect'],\n",
       " ['bad', 'negemo'],\n",
       " ['bake*', 'bio'],\n",
       " ['bake*', 'ingest'],\n",
       " ['bake*', 'home'],\n",
       " ['baking', 'bio'],\n",
       " ['baking', 'ingest'],\n",
       " ['baking', 'home'],\n",
       " ['balcon*', 'home'],\n",
       " ['bald', 'bio'],\n",
       " ['bald', 'body'],\n",
       " ['ball', 'leisure'],\n",
       " ['ballet*', 'leisure'],\n",
       " ['bambino*', 'social'],\n",
       " ['bambino*', 'humans'],\n",
       " ['ban', 'cogmech'],\n",
       " ['ban', 'inhib'],\n",
       " ['band', 'social'],\n",
       " ['band', 'leisure'],\n",
       " ['bandage*', 'bio'],\n",
       " ['bandage*', 'health'],\n",
       " ['bandaid', 'bio'],\n",
       " ['bandaid', 'health'],\n",
       " ['bands', 'social'],\n",
       " ['bands', 'leisure'],\n",
       " ['bank*', 'money'],\n",
       " ['banned', 'cogmech'],\n",
       " ['banned', 'inhib'],\n",
       " ['banning', 'cogmech'],\n",
       " ['banning', 'inhib'],\n",
       " ['bans', 'cogmech'],\n",
       " ['bans', 'inhib'],\n",
       " ['baptis*', 'relig'],\n",
       " ['baptiz*', 'relig'],\n",
       " ['bar', 'bio'],\n",
       " ['bar', 'ingest'],\n",
       " ['bar', 'leisure'],\n",
       " ['barely', 'cogmech'],\n",
       " ['barely', 'tentat'],\n",
       " ['bargain*', 'money'],\n",
       " ['barrier*', 'cogmech'],\n",
       " ['barrier*', 'inhib'],\n",
       " ['bars', 'bio'],\n",
       " ['bars', 'ingest'],\n",
       " ['bars', 'leisure'],\n",
       " ['baseball*', 'leisure'],\n",
       " ['based', 'cogmech'],\n",
       " ['based', 'cause'],\n",
       " ['bases', 'cogmech'],\n",
       " ['bases', 'cause'],\n",
       " ['bashful*', 'affect'],\n",
       " ['bashful*', 'negemo'],\n",
       " ['basically', 'funct'],\n",
       " ['basically', 'adverb'],\n",
       " ['basis', 'cogmech'],\n",
       " ['basis', 'cause'],\n",
       " ['basketball*', 'leisure'],\n",
       " ['bastard*', 'affect'],\n",
       " ['bastard*', 'negemo'],\n",
       " ['bastard*', 'anger'],\n",
       " ['bastard*', 'swear'],\n",
       " ['bath*', 'leisure'],\n",
       " ['bath*', 'home'],\n",
       " ['battl*', 'affect'],\n",
       " ['battl*', 'negemo'],\n",
       " ['battl*', 'anger'],\n",
       " ['be', 'verb'],\n",
       " ['be', 'funct'],\n",
       " ['be', 'auxverb'],\n",
       " ['beach*', 'leisure'],\n",
       " ['beat', 'achieve'],\n",
       " ['beaten', 'affect'],\n",
       " ['beaten', 'negemo'],\n",
       " ['beaten', 'anger'],\n",
       " ['beaten', 'work'],\n",
       " ['beaten', 'achieve'],\n",
       " ['beaut*', 'affect'],\n",
       " ['beaut*', 'posemo'],\n",
       " ['beaut*', 'percept'],\n",
       " ['beaut*', 'see'],\n",
       " ['became', 'verb'],\n",
       " ['became', 'funct'],\n",
       " ['became', 'auxverb'],\n",
       " ['became', 'past'],\n",
       " ['became', 'cogmech'],\n",
       " ['became', 'insight'],\n",
       " ['because', 'funct'],\n",
       " ['because', 'conj'],\n",
       " ['because', 'cogmech'],\n",
       " ['because', 'cause'],\n",
       " ['become', 'verb'],\n",
       " ['become', 'funct'],\n",
       " ['become', 'auxverb'],\n",
       " ['become', 'present'],\n",
       " ['become', 'cogmech'],\n",
       " ['become', 'insight'],\n",
       " ['becomes', 'verb'],\n",
       " ['becomes', 'funct'],\n",
       " ['becomes', 'auxverb'],\n",
       " ['becomes', 'present'],\n",
       " ['becomes', 'cogmech'],\n",
       " ['becomes', 'insight'],\n",
       " ['becoming', 'verb'],\n",
       " ['becoming', 'funct'],\n",
       " ['becoming', 'auxverb'],\n",
       " ['becoming', 'cogmech'],\n",
       " ['becoming', 'insight'],\n",
       " ['bed', 'home'],\n",
       " ['bedding', 'home'],\n",
       " ['bedroom*', 'home'],\n",
       " ['beds', 'home'],\n",
       " ['been', 'verb'],\n",
       " ['been', 'funct'],\n",
       " ['been', 'auxverb'],\n",
       " ['been', 'past'],\n",
       " ['beer*', 'bio'],\n",
       " ['beer*', 'ingest'],\n",
       " ['beer*', 'leisure'],\n",
       " ['before', 'funct'],\n",
       " ['before', 'preps'],\n",
       " ['before', 'time'],\n",
       " ['before', 'relativ'],\n",
       " ['began', 'verb'],\n",
       " ['began', 'past'],\n",
       " ['began', 'time'],\n",
       " ['began', 'relativ'],\n",
       " ['beggar*', 'money'],\n",
       " ['begging', 'money'],\n",
       " ['begin', 'verb'],\n",
       " ['begin', 'present'],\n",
       " ['begin', 'time'],\n",
       " ['begin', 'relativ'],\n",
       " ['beginn*', 'time'],\n",
       " ['beginn*', 'relativ'],\n",
       " ['begins', 'verb'],\n",
       " ['begins', 'present'],\n",
       " ['begins', 'time'],\n",
       " ['begins', 'relativ'],\n",
       " ['begun', 'time'],\n",
       " ['begun', 'relativ'],\n",
       " ['behavio*', 'relativ'],\n",
       " ['behavio*', 'motion'],\n",
       " ['behind', 'funct'],\n",
       " ['behind', 'preps'],\n",
       " ['being', 'verb'],\n",
       " ['being', 'funct'],\n",
       " ['being', 'auxverb'],\n",
       " ['belief*', 'cogmech'],\n",
       " ['belief*', 'insight'],\n",
       " ['belief*', 'relig'],\n",
       " ['believe', 'verb'],\n",
       " ['believe', 'present'],\n",
       " ['believe', 'cogmech'],\n",
       " ['believe', 'insight'],\n",
       " ['believed', 'verb'],\n",
       " ['believed', 'past'],\n",
       " ['believed', 'cogmech'],\n",
       " ['believed', 'insight'],\n",
       " ['believes', 'verb'],\n",
       " ['believes', 'present'],\n",
       " ['believes', 'cogmech'],\n",
       " ['believes', 'insight'],\n",
       " ['believing', 'cogmech'],\n",
       " ['believing', 'insight'],\n",
       " ['bellies', 'bio'],\n",
       " ['bellies', 'body'],\n",
       " ['belly', 'bio'],\n",
       " ['belly', 'body'],\n",
       " ['beloved', 'affect'],\n",
       " ['beloved', 'posemo'],\n",
       " ['below', 'funct'],\n",
       " ['below', 'preps'],\n",
       " ['below', 'space'],\n",
       " ['below', 'relativ'],\n",
       " ['bend', 'space'],\n",
       " ['bend', 'relativ'],\n",
       " ['bending', 'space'],\n",
       " ['bending', 'relativ'],\n",
       " ['bends', 'space'],\n",
       " ['bends', 'relativ'],\n",
       " ['beneath', 'funct'],\n",
       " ['beneath', 'preps'],\n",
       " ['beneath', 'space'],\n",
       " ['beneath', 'relativ'],\n",
       " ['benefic*', 'affect'],\n",
       " ['benefic*', 'posemo'],\n",
       " ['benefit', 'affect'],\n",
       " ['benefit', 'posemo'],\n",
       " ['benefits', 'affect'],\n",
       " ['benefits', 'posemo'],\n",
       " ['benefits', 'work'],\n",
       " ['benefitt*', 'affect'],\n",
       " ['benefitt*', 'posemo'],\n",
       " ['benevolen*', 'affect'],\n",
       " ['benevolen*', 'posemo'],\n",
       " ['benign*', 'affect'],\n",
       " ['benign*', 'posemo'],\n",
       " ['bent', 'space'],\n",
       " ['bent', 'relativ'],\n",
       " ['bereave*', 'death'],\n",
       " ['beside', 'funct'],\n",
       " ['beside', 'preps'],\n",
       " ['beside', 'space'],\n",
       " ['beside', 'relativ'],\n",
       " ['besides', 'funct'],\n",
       " ['besides', 'preps'],\n",
       " ['besides', 'quant'],\n",
       " ['besides', 'cogmech'],\n",
       " ['besides', 'discrep'],\n",
       " ['best', 'affect'],\n",
       " ['best', 'posemo'],\n",
       " ['best', 'achieve'],\n",
       " ['best', 'funct'],\n",
       " ['best', 'quant'],\n",
       " ['bet', 'cogmech'],\n",
       " ['bet', 'tentat'],\n",
       " ['bet', 'money'],\n",
       " ['bets', 'cogmech'],\n",
       " ['bets', 'tentat'],\n",
       " ['bets', 'money'],\n",
       " ['better', 'affect'],\n",
       " ['better', 'posemo'],\n",
       " ['better', 'achieve'],\n",
       " ['betting', 'cogmech'],\n",
       " ['betting', 'tentat'],\n",
       " ['betting', 'money'],\n",
       " ['between', 'funct'],\n",
       " ['between', 'preps'],\n",
       " ['beyond', 'funct'],\n",
       " ['beyond', 'adverb'],\n",
       " ['beyond', 'preps'],\n",
       " ['beyond', 'space'],\n",
       " ['beyond', 'relativ'],\n",
       " ['bf*', 'social'],\n",
       " ['bf*', 'friend'],\n",
       " ['bi', 'bio'],\n",
       " ['bi', 'sexual'],\n",
       " ['biannu*', 'time'],\n",
       " ['biannu*', 'relativ'],\n",
       " ['bible*', 'relig'],\n",
       " ['biblic*', 'relig'],\n",
       " ['bicep*', 'bio'],\n",
       " ['bicep*', 'body'],\n",
       " ['bicyc*', 'leisure'],\n",
       " ['big', 'space'],\n",
       " ['big', 'relativ'],\n",
       " ['bigger', 'space'],\n",
       " ['bigger', 'relativ'],\n",
       " ['biggest', 'space'],\n",
       " ['biggest', 'relativ'],\n",
       " ['bike*', 'leisure'],\n",
       " ['bill', 'money'],\n",
       " ['billed', 'money'],\n",
       " ['billing*', 'money'],\n",
       " ['billion*', 'funct'],\n",
       " ['billion*', 'number'],\n",
       " ['bills', 'money'],\n",
       " ['bimonth*', 'time'],\n",
       " ['bimonth*', 'relativ'],\n",
       " ['binding', 'cogmech'],\n",
       " ['binding', 'inhib'],\n",
       " ['binge*', 'bio'],\n",
       " ['binge*', 'health'],\n",
       " ['binge*', 'ingest'],\n",
       " ['binging', 'bio'],\n",
       " ['binging', 'health'],\n",
       " ['binging', 'ingest'],\n",
       " ['biolog*', 'work'],\n",
       " ['bipolar', 'bio'],\n",
       " ['bipolar', 'health'],\n",
       " ['birdie*', 'leisure'],\n",
       " ['birth*', 'time'],\n",
       " ...]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liwc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_dict = {}\n",
    "for (w, c) in liwc:\n",
    "    if c not in liwc_dict:\n",
    "        liwc_dict[c] = []\n",
    "    liwc_dict[c].append(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anybod*',\n",
       " 'anyone*',\n",
       " 'anything',\n",
       " 'everybod*',\n",
       " 'everyone*',\n",
       " 'everything*',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he's\",\n",
       " 'hed',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'hes',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'id',\n",
       " 'im',\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'itd',\n",
       " 'itll',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'ive',\n",
       " \"let's\",\n",
       " 'lets',\n",
       " 'me',\n",
       " 'mine',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'nobod*',\n",
       " 'oneself',\n",
       " 'other',\n",
       " 'others',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'shes',\n",
       " 'somebod*',\n",
       " 'someone*',\n",
       " 'something*',\n",
       " 'somewhere',\n",
       " 'stuff',\n",
       " 'that',\n",
       " \"that'd\",\n",
       " \"that'll\",\n",
       " \"that's\",\n",
       " 'thatd',\n",
       " 'thatll',\n",
       " 'thats',\n",
       " 'thee',\n",
       " 'their*',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they've\",\n",
       " 'theyd',\n",
       " 'theyll',\n",
       " 'theyve',\n",
       " 'thine',\n",
       " 'thing*',\n",
       " 'this',\n",
       " 'those',\n",
       " 'thou',\n",
       " 'thoust',\n",
       " 'thy',\n",
       " 'us',\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'weve',\n",
       " 'what',\n",
       " \"what's\",\n",
       " 'whatever',\n",
       " 'whats',\n",
       " 'which',\n",
       " 'whichever',\n",
       " 'who',\n",
       " \"who'd\",\n",
       " \"who'll\",\n",
       " 'whod',\n",
       " 'wholl',\n",
       " 'whom',\n",
       " 'whose',\n",
       " \"y'all\",\n",
       " 'ya',\n",
       " 'yall',\n",
       " 'ye',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'youd',\n",
       " 'youll',\n",
       " 'your',\n",
       " 'youre',\n",
       " 'yours',\n",
       " 'youve']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liwc_dict['pronoun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_liwc_categories(tokens, category_words, relative=True):\n",
    "    category_cnt = 0\n",
    "    if not tokens:\n",
    "        return None\n",
    "    text_len = len(tokens)\n",
    "    for t in tokens:\n",
    "        for word in category_words:\n",
    "            if t==word or (word[-1]=='*' and t.startswith(word[:-1])) \\\n",
    "            or (t==word.split(\"'\")[0]):\n",
    "                category_cnt += 1\n",
    "                break # one token cannot belong to more than one word in the category\n",
    "    if relative:\n",
    "        return category_cnt/text_len\n",
    "    else:\n",
    "        return category_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing for category pronoun...\n",
      "CPU times: user 1min 13s, sys: 37.9 ms, total: 1min 13s\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from functools import partial\n",
    "# for categ in ['negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']:#liwc_dict.keys():\n",
    "for categ in liwc_dict.keys():\n",
    "    if categ in writings_df.columns:\n",
    "        continue\n",
    "    print(\"Encoding for category %s...\" % categ)\n",
    "    writings_df[categ] = writings_df['all_tokens'].apply(partial(encode_liwc_categories, \n",
    "                                                                   category_words=liwc_dict[categ], \n",
    "                                                                   relative=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>negemo</th>\n",
       "      <th>posemo</th>\n",
       "      <th>affect</th>\n",
       "      <th>sad</th>\n",
       "      <th>anx</th>\n",
       "      <th>pronoun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007687</td>\n",
       "      <td>0.008943</td>\n",
       "      <td>0.012005</td>\n",
       "      <td>0.003770</td>\n",
       "      <td>0.009488</td>\n",
       "      <td>0.071618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negemo</th>\n",
       "      <td>0.007687</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.058048</td>\n",
       "      <td>0.456381</td>\n",
       "      <td>0.356025</td>\n",
       "      <td>0.303006</td>\n",
       "      <td>-0.011327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>posemo</th>\n",
       "      <td>0.008943</td>\n",
       "      <td>-0.058048</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.860121</td>\n",
       "      <td>-0.020983</td>\n",
       "      <td>-0.024652</td>\n",
       "      <td>-0.015620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>affect</th>\n",
       "      <td>0.012005</td>\n",
       "      <td>0.456381</td>\n",
       "      <td>0.860121</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.162322</td>\n",
       "      <td>0.131990</td>\n",
       "      <td>-0.019918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sad</th>\n",
       "      <td>0.003770</td>\n",
       "      <td>0.356025</td>\n",
       "      <td>-0.020983</td>\n",
       "      <td>0.162322</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004730</td>\n",
       "      <td>0.006895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anx</th>\n",
       "      <td>0.009488</td>\n",
       "      <td>0.303006</td>\n",
       "      <td>-0.024652</td>\n",
       "      <td>0.131990</td>\n",
       "      <td>0.004730</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.005346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pronoun</th>\n",
       "      <td>0.071618</td>\n",
       "      <td>-0.011327</td>\n",
       "      <td>-0.015620</td>\n",
       "      <td>-0.019918</td>\n",
       "      <td>0.006895</td>\n",
       "      <td>-0.005346</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            label    negemo    posemo    affect       sad       anx   pronoun\n",
       "label    1.000000  0.007687  0.008943  0.012005  0.003770  0.009488  0.071618\n",
       "negemo   0.007687  1.000000 -0.058048  0.456381  0.356025  0.303006 -0.011327\n",
       "posemo   0.008943 -0.058048  1.000000  0.860121 -0.020983 -0.024652 -0.015620\n",
       "affect   0.012005  0.456381  0.860121  1.000000  0.162322  0.131990 -0.019918\n",
       "sad      0.003770  0.356025 -0.020983  0.162322  1.000000  0.004730  0.006895\n",
       "anx      0.009488  0.303006 -0.024652  0.131990  0.004730  1.000000 -0.005346\n",
       "pronoun  0.071618 -0.011327 -0.015620 -0.019918  0.006895 -0.005346  1.000000"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['label', 'negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negemo</th>\n",
       "      <th>posemo</th>\n",
       "      <th>affect</th>\n",
       "      <th>sad</th>\n",
       "      <th>anx</th>\n",
       "      <th>pronoun</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.023493</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>0.074548</td>\n",
       "      <td>0.003242</td>\n",
       "      <td>0.002606</td>\n",
       "      <td>0.120154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.026116</td>\n",
       "      <td>0.056145</td>\n",
       "      <td>0.082611</td>\n",
       "      <td>0.003706</td>\n",
       "      <td>0.003591</td>\n",
       "      <td>0.162310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         negemo    posemo    affect       sad       anx   pronoun\n",
       "label                                                            \n",
       "0      0.023493  0.050800  0.074548  0.003242  0.002606  0.120154\n",
       "1      0.026116  0.056145  0.082611  0.003706  0.003591  0.162310"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['label', 'negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']].groupby('label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject', 'title', 'date', 'text', 'label', 'tokenized_title',\n",
       "       'title_len', 'tokenized_text', 'text_len', 'all_tokens', 'funct',\n",
       "       'article', 'negemo'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: COMET_OPTIMIZER_ID=786fc2b3654047e69f492db122f55b95\n",
      "COMET INFO: Using optimizer config: {'algorithm': 'random', 'configSpaceSize': 600000000000, 'endTime': None, 'id': '786fc2b3654047e69f492db122f55b95', 'lastUpdateTime': None, 'maxCombo': 0, 'name': '786fc2b3654047e69f492db122f55b95', 'parameters': {'batch_size': {'max': 512, 'min': 10, 'scalingType': 'loguniform', 'type': 'integer'}, 'decay': {'max': 0.5, 'min': 1e-08, 'scalingType': 'loguniform', 'type': 'float'}, 'dense_bow_units': {'max': 20, 'min': 1, 'scalingType': 'uniform', 'type': 'integer'}, 'dropout': {'max': 0.7, 'min': 0, 'scalingType': 'uniform', 'type': 'float'}, 'freeze_patience': {'max': 16, 'min': 2, 'scalingType': 'uniform', 'type': 'integer'}, 'l2_dense': {'max': 0.5, 'min': 1e-05, 'scalingType': 'loguniform', 'type': 'float'}, 'lr': {'max': 0.05, 'min': 1e-05, 'scalingType': 'loguniform', 'type': 'float'}, 'lr_reduce_factor': {'max': 0.8, 'min': 0.0001, 'scalingType': 'uniform', 'type': 'float'}, 'lr_reduce_patience': {'max': 16, 'min': 2, 'scalingType': 'uniform', 'type': 'integer'}, 'lstm_units': {'max': 100, 'min': 10, 'scalingType': 'uniform', 'type': 'integer'}, 'optimizer': {'type': 'categorical', 'values': ['adam', 'adagrad', '']}, 'positive_class_weight': {'max': 25, 'min': 1, 'scalingType': 'uniform', 'type': 'integer'}, 'trainable_embeddings': {'type': 'discrete', 'values': [True, False]}}, 'predictor': None, 'spec': {'gridSize': 10, 'maxCombo': 0, 'metric': 'loss', 'minSampleSize': 100, 'objective': 'minimize', 'retryAssignLimit': 0, 'retryLimit': 20, 'seed': 3968493229}, 'startTime': 15879681523, 'state': {'sequence_i': 0, 'sequence_pid': None, 'sequence_retry': 0}, 'status': 'running', 'suggestion_count': 0, 'trials': 1, 'version': '1.0.24'}\n",
      "COMET INFO: Optimizer metrics is 'loss' but no logged values found. Experiment ignored in sweep.\n",
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary:\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     url: https://www.comet.ml/ananana/mental/1ebbfc14aa9440e1948aabac5664d837\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     batch_binary_accuracy [29]: (0.5306122303009033, 0.9572663307189941)\n",
      "COMET INFO:     batch_f1_m [29]           : (0.0030426757875829935, 0.0336134247481823)\n",
      "COMET INFO:     batch_loss [29]           : (0.30054065585136414, 1.4031785726547241)\n",
      "COMET INFO:     batch_precision_m [29]    : (0.011335793882608414, 0.09260831028223038)\n",
      "COMET INFO:     batch_recall_m [29]       : (0.002346971072256565, 0.1666666716337204)\n",
      "COMET INFO:     sys.cpu.percent.01 [20]   : (4.8, 82.9)\n",
      "COMET INFO:     sys.cpu.percent.02 [20]   : (5.2, 73.9)\n",
      "COMET INFO:     sys.cpu.percent.03 [20]   : (4.6, 60.6)\n",
      "COMET INFO:     sys.cpu.percent.04 [20]   : (3.3, 57.8)\n",
      "COMET INFO:     sys.cpu.percent.avg [20]  : (4.925, 66.2)\n",
      "COMET INFO:     sys.gpu.0.total_memory    : (1073414144.0, 1073414144.0)\n",
      "COMET INFO:     sys.load.avg [20]         : (0.16, 4.02)\n",
      "COMET INFO:     sys.ram.total [20]        : (8277307392.0, 8277307392.0)\n",
      "COMET INFO:     sys.ram.used [20]         : (7586635776.0, 7714193408.0)\n",
      "COMET INFO:   Other [count]:\n",
      "COMET INFO:     optimizer_count       : 3\n",
      "COMET INFO:     optimizer_id          : 57eb178250e2401aa14cfc6860a4217a\n",
      "COMET INFO:     optimizer_metric      : loss\n",
      "COMET INFO:     optimizer_metric_value: None\n",
      "COMET INFO:     optimizer_parameters  : {'batch_size': 245, 'decay': 0.1402033634855056, 'dense_bow_units': 4, 'dropout': 0.46002110523805456, 'freeze_patience': 15, 'l2_dense': 0.0814165473266369, 'lr': 1.6244588122197995e-05, 'lr_reduce_factor': 0.6638163734300414, 'lr_reduce_patience': 10, 'lstm_units': 92, 'optimizer': 'adagrad', 'positive_class_weight': 5, 'set_trainable': True, 'trainable_embeddings': False}\n",
      "COMET INFO:     optimizer_pid         : 681f4971a3f98a865eb3100a742e6621e62a3420\n",
      "COMET INFO:     optimizer_process     : 2985\n",
      "COMET INFO:     optimizer_trial       : 1\n",
      "COMET INFO:     optimizer_version     : 1.0.24\n",
      "COMET INFO:     trainable_params      : 2071854\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     git-patch  : 1\n",
      "COMET INFO:     histogram3d: 1\n",
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/ananana/mental/6925a9644c034cbf9cd7f12c0a96e0f3\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 111375 samples, validate on 31863 samples\n",
      "Epoch 1/15\n",
      "111320/111375 [============================>.] - ETA: 0s - loss: 0.6696 - binary_accuracy: 0.8707 - f1_m: 0.1200 - precision_m: 0.0902 - recall_m: 0.2582\n",
      "Epoch 00001: val_loss improved from inf to 0.48204, saving model to models/experiment_best\n",
      "111375/111375 [==============================] - 317s 3ms/sample - loss: 0.6695 - binary_accuracy: 0.8707 - f1_m: 0.1201 - precision_m: 0.0902 - recall_m: 0.2584 - val_loss: 0.4820 - val_binary_accuracy: 0.7968 - val_f1_m: 0.2121 - val_precision_m: 0.1454 - val_recall_m: 0.4314\n",
      "Epoch 2/15\n",
      " 47190/111375 [===========>..................] - ETA: 2:52 - loss: 0.5882 - binary_accuracy: 0.8703 - f1_m: 0.1490 - precision_m: 0.1088 - recall_m: 0.2949"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-f94476829a7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m                           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                           \u001b[0mcallback_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfreeze_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_lr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                       model_path='models/experiment')\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-124-48d6d530785b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, x_train, y_train, x_test, y_test, batch_size, epochs, class_weight, start_epoch, workers, callback_list, model_path)\u001b[0m\n\u001b[1;32m     17\u001b[0m             callbacks = [\n\u001b[1;32m     18\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%s_best'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcallback_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             ])\n\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/comet_ml/monkey_patching.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m                     )\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Call after callbacks once we have the return value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Declare your hyperparameters search:\n",
    "tune_epochs=15\n",
    "config = {\n",
    "      \"algorithm\": \"random\",\n",
    "      \"parameters\": {\n",
    "          \"lstm_units\": {\"type\": \"integer\", \"min\": 10, \"max\": 100},\n",
    "          \"dense_bow_units\": {\"type\": \"integer\", \"min\": 1, \"max\": 20},\n",
    "          \"lr\": {\"type\": \"float\", \"min\": 0.00001, \"max\": 0.05, \"scalingType\": \"loguniform\"},\n",
    "          \"l2_dense\": {\"type\": \"float\", \"min\": 0.00001, \"max\": 0.5, \"scalingType\": \"loguniform\"},\n",
    "          \"dropout\": {\"type\": \"float\", \"min\": 0, \"max\": 0.7, \"scalingType\": \"uniform\"},\n",
    "          \"optimizer\": {\"type\": \"categorical\", \"values\": [\"adam\", \"adagrad\", \"\"]},\n",
    "          \"batch_size\": {\"type\": \"integer\", \"min\": 10, \"max\": 512, \"scalingType\": \"loguniform\"},\n",
    "          \"positive_class_weight\": {\"type\": \"integer\", \"min\": 1, \"max\": 25},\n",
    "          \"trainable_embeddings\": {\"type\": \"discrete\", \"values\": [True, False]},\n",
    "          \"freeze_patience\": {\"type\": \"integer\", \"min\": 2, \"max\": tune_epochs+1},\n",
    "          \"lr_reduce_factor\": {\"type\": \"float\", \"min\": 0.0001, \"max\": 0.8},\n",
    "          \"lr_reduce_patience\": {\"type\": \"integer\", \"min\": 2, \"max\": tune_epochs+1},\n",
    "          \"decay\": {\"type\": \"float\", \"min\": 0.00000001, \"max\": 0.5, \"scalingType\": \"loguniform\"},          \n",
    "      },\n",
    "      \"spec\": {\n",
    "          \"metric\": \"loss\",\n",
    "          \"objective\": \"minimize\",\n",
    "      },\n",
    "  }\n",
    "optimizer = Optimizer(config, api_key=\"eoBdVyznAhfg3bK9pZ58ZSXfv\")\n",
    "\n",
    "for experiment in optimizer.get_experiments(project_name=\"mental\"):\n",
    "    experiment.add_tag(\"tune\")\n",
    "    \n",
    "    # Test the model\n",
    "    hyperparams_config = {\n",
    "        param: experiment.get_parameter(param) for param in config['parameters'].keys()}\n",
    "    if not hyperparams_config['optimizer']:\n",
    "        hyperparams_config['optimizer'] = optimizers.Adam(lr=hyperparams_config['lr'], \n",
    "                                   decay=hyperparams_config['decay'])\n",
    "    model = build_model(hyperparams=hyperparams_config,\n",
    "                        hyperparams_features=hyperparams_features, \n",
    "                        embedding_matrix=embedding_matrix, emotions=emotions,\n",
    "                       stopwords_list=stopword_list)\n",
    "    freeze_layer = FreezeLayer(patience=experiment.get_parameter('freeze_patience'),\n",
    "                              set_to=not experiment.get_parameter('trainable_embeddings'))\n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            factor=experiment.get_parameter('lr_reduce_factor'),\n",
    "                                            patience=experiment.get_parameter('lr_reduce_patience'), \n",
    "                                            min_lr=0.000001, verbose=1)\n",
    "    history = train_model(model, \n",
    "            x_train, y_train, x_test, y_test,\n",
    "            epochs=tune_epochs, batch_size=experiment.get_parameter('batch_size'),\n",
    "                      class_weight={0:1, 1:experiment.get_parameter('positive_class_weight')}, \n",
    "                          workers=2,\n",
    "                          callback_list = [freeze_layer, reduce_lr],\n",
    "                      model_path='models/experiment')\n",
    "    loss = history.history['loss'][-1]\n",
    "    \n",
    "    # Report the loss, if not auto-logged:\n",
    "    experiment.log_metric(\"loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
