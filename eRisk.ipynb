{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob, os\n",
    "import numpy as np\n",
    "from comet_ml import Experiment, Optimizer\n",
    "import pickle\n",
    "import logging\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_KERAS'] = '1'\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Lambda, \\\n",
    "    CuDNNLSTM, Bidirectional, Input, concatenate, Flatten, RepeatVector, Activation, Multiply, Permute\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import callbacks, optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# only reserve 1 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('training')\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_subject_writings(subject_file):\n",
    "    writings = []\n",
    "    with open(subject_file) as sf:\n",
    "        contents = sf.read()\n",
    "        root = ET.fromstring(contents)\n",
    "        try:\n",
    "            subject = root.findall('ID')[0].text.strip()\n",
    "        except Exception:\n",
    "            print('Cannot extract ID', contents[:500], '\\n-------\\n')        \n",
    "        for w in root.iter('WRITING'):\n",
    "            subject_writings = {'subject': subject}\n",
    "            for title in w.findall('TITLE'):\n",
    "                subject_writings['title'] = title.text\n",
    "            for text in w.findall('TEXT'):\n",
    "                subject_writings['text'] = text.text\n",
    "            for date in w.findall('DATE'):\n",
    "                subject_writings['date'] = date.text\n",
    "            writings.append(subject_writings)\n",
    "    return writings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dir = '/home/bighanem/ana_data/' \n",
    "root_dir = '/home/ana/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eRisk 2020 T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir_T1 = root_dir + '/eRisk/data/eRisk2020_T1_train/eRISK2020_T1_training_data/eRISK2020_training_data/data/'\n",
    "labels_file_T1 = root_dir + '/eRisk/data//eRisk2020_T1_train/eRISK2020_T1_training_data/eRISK2020_training_data/golden_truth.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_2020(datadir_T1, labels_file_T1):\n",
    "    writings = []\n",
    "    for subject_file in os.listdir(datadir_T1):\n",
    "        print(subject_file)\n",
    "        writings.extend(read_subject_writings(os.path.join(datadir_T1, subject_file)))\n",
    "    writings_df = pd.DataFrame(writings)\n",
    "\n",
    "    labels_T1 = pd.read_csv(labels_file_T1, delimiter=' ', names=['subject', 'label'])\n",
    "    labels_T1 = labels_T1.set_index('subject')\n",
    "\n",
    "    writings_df['label'] = writings_df['subject'].apply(\n",
    "    lambda s: labels_T1.loc[s, 'label'])\n",
    "    \n",
    "    return writings_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eRisk 2019 T1 (Anorexia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadirs_T1_2019 = {\n",
    "    'train': ['2018 test/', '2018 train/positive_examples/', '2018 train/negative_examples/'],\n",
    "    'test': ['data/']\n",
    "}\n",
    "datadir_root_T1_2019 = {\n",
    "    'train': root_dir + '/eRisk/data/past/eRisk2019_T1/training data - t1/',\n",
    "    'test': root_dir + '/eRisk/data/past/eRisk2019_T1/test data - T1/'\n",
    "}\n",
    "    \n",
    "labels_files_T1_2019 = {\n",
    "    'train': ['2018 train/risk_golden_truth.txt', '2018 test/risk-golden-truth-test.txt'],\n",
    "    'test': ['T1_erisk_golden_truth.txt']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_2019(datadir_root_T1_2019,\n",
    "                   datadirs_T1_2019,\n",
    "                   labels_files_T1_2019,\n",
    "                   test_suffix='0000'):\n",
    "    writings = {'train': [], 'test': []}\n",
    "    writings_df = pd.DataFrame()\n",
    "    labels_df = pd.DataFrame()\n",
    "\n",
    "    for subset in ('train', 'test'):\n",
    "        for subdir in [os.path.join(datadir_root_T1_2019[subset], subp) for subp in datadirs_T1_2019[subset]]:\n",
    "            if subset=='train':\n",
    "                chunkdirs = [os.path.join(datadir_root_T1_2019[subset], subdir, chunkdir) \n",
    "                             for chunkdir in os.listdir(subdir)]\n",
    "            else:\n",
    "                chunkdirs = [os.path.join(datadir_root_T1_2019[subset], subdir)]\n",
    "                \n",
    "            for chunkdir in chunkdirs:\n",
    "                if not os.path.isdir(chunkdir):\n",
    "                    continue\n",
    "                for subject_file in os.listdir(chunkdir):\n",
    "                    writings[subset].extend(read_subject_writings(os.path.join(chunkdir, subject_file)))\n",
    "        writings_df_part = pd.DataFrame(writings[subset])\n",
    "        # add a suffix for users in the test -- the numbers are duplicated with the ones in train\n",
    "        if subset=='test':\n",
    "            writings_df_part['subject'] = writings_df_part['subject'].apply(lambda s: s+test_suffix)\n",
    "            print(subset, writings_df_part.subject)\n",
    "        writings_df_part['subset'] = subset\n",
    "        writings_df = pd.concat([writings_df, writings_df_part])\n",
    "        writings_df.reindex()\n",
    "\n",
    "        for label_file in labels_files_T1_2019[subset]:\n",
    "            labels = pd.read_csv(os.path.join(datadir_root_T1_2019[subset], label_file), \n",
    "                                 delimiter='\\s+', names=['subject', 'label'])\n",
    "            # add a suffix for users in the test -- the numbers are duplicated with the ones in train\n",
    "            if subset=='test':\n",
    "                labels['subject'] = labels['subject'].apply(lambda s: s+test_suffix)\n",
    "            labels_df = pd.concat([labels_df, labels])\n",
    "    labels_df = labels_df.drop_duplicates()\n",
    "    labels_df = labels_df.set_index('subject')\n",
    "\n",
    "    writings_df = writings_df.drop_duplicates()\n",
    "    \n",
    "    writings_df = writings_df.join(labels_df, on='subject')\n",
    "    \n",
    "    return writings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df = read_texts_2020(datadir_T1, labels_file_T1)\n",
    "# writings_df = read_texts_2019(datadir_root_T1_2019,\n",
    "#                    datadirs_T1_2019,\n",
    "#                    labels_files_T1_2019)\n",
    "writings_df = pickle.load(open('writings_df_selfharm_liwc', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# writings_df[writings_df['subset']=='test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ana/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>title_len</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>text_len</th>\n",
       "      <th>all_tokens</th>\n",
       "      <th>...</th>\n",
       "      <th>feel</th>\n",
       "      <th>excl</th>\n",
       "      <th>future</th>\n",
       "      <th>nonfl</th>\n",
       "      <th>ppron</th>\n",
       "      <th>shehe</th>\n",
       "      <th>i</th>\n",
       "      <th>we</th>\n",
       "      <th>you</th>\n",
       "      <th>they</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>If anyone could help with which sub to put thi...</td>\n",
       "      <td>2016-08-02 09:22:12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[if, anyone, could, help, with, which, sub, to...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[if, anyone, could, help, with, which, sub, to...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>I'm literally never gonna stop waiting...</td>\n",
       "      <td>2016-08-05 09:35:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, m, literally, never, gonna, stop, waiting]</td>\n",
       "      <td>7.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[i, m, literally, never, gonna, stop, waiting]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>This is a really interesting study! Makes sens...</td>\n",
       "      <td>2016-08-05 21:36:24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[this, is, a, really, interesting, study, make...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[this, is, a, really, interesting, study, make...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>Mostly always me during this whole charade</td>\n",
       "      <td>2016-08-09 08:39:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[mostly, always, me, during, this, whole, char...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[mostly, always, me, during, this, whole, char...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>What is your best advice to a healthy, success...</td>\n",
       "      <td>2016-11-02 05:33:33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[what, is, your, best, advice, to, a, healthy,...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[what, is, your, best, advice, to, a, healthy,...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170664</th>\n",
       "      <td>subject217</td>\n",
       "      <td>scary</td>\n",
       "      <td>2018-06-24 14:26:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[scary]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[scary]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170668</th>\n",
       "      <td>subject217</td>\n",
       "      <td>rescuing man after his car got stuck on Rub' a...</td>\n",
       "      <td>2018-07-05 15:31:29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[rescuing, man, after, his, car, got, stuck, o...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[rescuing, man, after, his, car, got, stuck, o...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170680</th>\n",
       "      <td>subject217</td>\n",
       "      <td>Gorilla in streets of Riyadh,KSA</td>\n",
       "      <td>2018-07-24 22:22:48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[gorilla, in, streets, of, riyadh, ksa]</td>\n",
       "      <td>6.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[gorilla, in, streets, of, riyadh, ksa]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170681</th>\n",
       "      <td>subject217</td>\n",
       "      <td>Gorilla in streets of Riyadh,KSA</td>\n",
       "      <td>2018-07-24 22:46:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[gorilla, in, streets, of, riyadh, ksa]</td>\n",
       "      <td>6.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[gorilla, in, streets, of, riyadh, ksa]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170696</th>\n",
       "      <td>subject217</td>\n",
       "      <td>WHAT DID YOU SAY TO ME?</td>\n",
       "      <td>2018-08-20 10:54:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[what, did, you, say, to, me]</td>\n",
       "      <td>6.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[what, did, you, say, to, me]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42757 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            subject                                              title  \\\n",
       "0       subject8292  If anyone could help with which sub to put thi...   \n",
       "1       subject8292          I'm literally never gonna stop waiting...   \n",
       "2       subject8292  This is a really interesting study! Makes sens...   \n",
       "4       subject8292         Mostly always me during this whole charade   \n",
       "7       subject8292  What is your best advice to a healthy, success...   \n",
       "...             ...                                                ...   \n",
       "170664   subject217                                              scary   \n",
       "170668   subject217  rescuing man after his car got stuck on Rub' a...   \n",
       "170680   subject217                   Gorilla in streets of Riyadh,KSA   \n",
       "170681   subject217                   Gorilla in streets of Riyadh,KSA   \n",
       "170696   subject217                            WHAT DID YOU SAY TO ME?   \n",
       "\n",
       "                       date text  label  \\\n",
       "0       2016-08-02 09:22:12  NaN      0   \n",
       "1       2016-08-05 09:35:55  NaN      0   \n",
       "2       2016-08-05 21:36:24  NaN      0   \n",
       "4       2016-08-09 08:39:41  NaN      0   \n",
       "7       2016-11-02 05:33:33  NaN      0   \n",
       "...                     ...  ...    ...   \n",
       "170664  2018-06-24 14:26:01  NaN      0   \n",
       "170668  2018-07-05 15:31:29  NaN      0   \n",
       "170680  2018-07-24 22:22:48  NaN      0   \n",
       "170681  2018-07-24 22:46:11  NaN      0   \n",
       "170696  2018-08-20 10:54:11  NaN      0   \n",
       "\n",
       "                                          tokenized_title  title_len  \\\n",
       "0       [if, anyone, could, help, with, which, sub, to...       11.0   \n",
       "1          [i, m, literally, never, gonna, stop, waiting]        7.0   \n",
       "2       [this, is, a, really, interesting, study, make...        9.0   \n",
       "4       [mostly, always, me, during, this, whole, char...        7.0   \n",
       "7       [what, is, your, best, advice, to, a, healthy,...       10.0   \n",
       "...                                                   ...        ...   \n",
       "170664                                            [scary]        1.0   \n",
       "170668  [rescuing, man, after, his, car, got, stuck, o...       12.0   \n",
       "170680            [gorilla, in, streets, of, riyadh, ksa]        6.0   \n",
       "170681            [gorilla, in, streets, of, riyadh, ksa]        6.0   \n",
       "170696                      [what, did, you, say, to, me]        6.0   \n",
       "\n",
       "       tokenized_text  text_len  \\\n",
       "0                None       NaN   \n",
       "1                None       NaN   \n",
       "2                None       NaN   \n",
       "4                None       NaN   \n",
       "7                None       NaN   \n",
       "...               ...       ...   \n",
       "170664           None       NaN   \n",
       "170668           None       NaN   \n",
       "170680           None       NaN   \n",
       "170681           None       NaN   \n",
       "170696           None       NaN   \n",
       "\n",
       "                                               all_tokens  ...      feel  \\\n",
       "0       [if, anyone, could, help, with, which, sub, to...  ...  0.000000   \n",
       "1          [i, m, literally, never, gonna, stop, waiting]  ...  0.000000   \n",
       "2       [this, is, a, really, interesting, study, make...  ...  0.000000   \n",
       "4       [mostly, always, me, during, this, whole, char...  ...  0.000000   \n",
       "7       [what, is, your, best, advice, to, a, healthy,...  ...  0.000000   \n",
       "...                                                   ...  ...       ...   \n",
       "170664                                            [scary]  ...  0.000000   \n",
       "170668  [rescuing, man, after, his, car, got, stuck, o...  ...  0.083333   \n",
       "170680            [gorilla, in, streets, of, riyadh, ksa]  ...  0.000000   \n",
       "170681            [gorilla, in, streets, of, riyadh, ksa]  ...  0.000000   \n",
       "170696                      [what, did, you, say, to, me]  ...  0.000000   \n",
       "\n",
       "            excl    future  nonfl     ppron     shehe         i   we  \\\n",
       "0       0.090909  0.090909    0.0  0.000000  0.000000  0.000000  0.0   \n",
       "1       0.000000  0.285714    0.0  0.142857  0.000000  0.142857  0.0   \n",
       "2       0.111111  0.000000    0.0  0.000000  0.000000  0.000000  0.0   \n",
       "4       0.000000  0.000000    0.0  0.142857  0.000000  0.142857  0.0   \n",
       "7       0.000000  0.000000    0.0  0.100000  0.000000  0.000000  0.0   \n",
       "...          ...       ...    ...       ...       ...       ...  ...   \n",
       "170664  0.000000  0.000000    0.0  0.000000  0.000000  0.000000  0.0   \n",
       "170668  0.000000  0.000000    0.0  0.083333  0.083333  0.000000  0.0   \n",
       "170680  0.000000  0.000000    0.0  0.000000  0.000000  0.000000  0.0   \n",
       "170681  0.000000  0.000000    0.0  0.000000  0.000000  0.000000  0.0   \n",
       "170696  0.000000  0.166667    0.0  0.333333  0.000000  0.166667  0.0   \n",
       "\n",
       "             you  they  \n",
       "0       0.000000   0.0  \n",
       "1       0.000000   0.0  \n",
       "2       0.000000   0.0  \n",
       "4       0.000000   0.0  \n",
       "7       0.100000   0.0  \n",
       "...          ...   ...  \n",
       "170664  0.000000   0.0  \n",
       "170668  0.000000   0.0  \n",
       "170680  0.000000   0.0  \n",
       "170681  0.000000   0.0  \n",
       "170696  0.166667   0.0  \n",
       "\n",
       "[42757 rows x 74 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[writings_df['text'].isna()][~writings_df['title'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f8e82e093d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "writings_df.label.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>title_len</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>text_len</th>\n",
       "      <th>all_tokens</th>\n",
       "      <th>...</th>\n",
       "      <th>feel</th>\n",
       "      <th>excl</th>\n",
       "      <th>future</th>\n",
       "      <th>nonfl</th>\n",
       "      <th>ppron</th>\n",
       "      <th>shehe</th>\n",
       "      <th>i</th>\n",
       "      <th>we</th>\n",
       "      <th>you</th>\n",
       "      <th>they</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>If anyone could help with which sub to put thi...</td>\n",
       "      <td>2016-08-02 09:22:12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[if, anyone, could, help, with, which, sub, to...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[if, anyone, could, help, with, which, sub, to...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>I'm literally never gonna stop waiting...</td>\n",
       "      <td>2016-08-05 09:35:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, m, literally, never, gonna, stop, waiting]</td>\n",
       "      <td>7.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[i, m, literally, never, gonna, stop, waiting]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>This is a really interesting study! Makes sens...</td>\n",
       "      <td>2016-08-05 21:36:24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[this, is, a, really, interesting, study, make...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[this, is, a, really, interesting, study, make...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>The only thing Frank is building ...</td>\n",
       "      <td>2016-08-07 23:35:23</td>\n",
       "      <td>... Is hype. Think about it, every time he wor...</td>\n",
       "      <td>0</td>\n",
       "      <td>[the, only, thing, frank, is, building]</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[is, hype, think, about, it, every, time, he, ...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>[is, hype, think, about, it, every, time, he, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>Mostly always me during this whole charade</td>\n",
       "      <td>2016-08-09 08:39:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[mostly, always, me, during, this, whole, char...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[mostly, always, me, during, this, whole, char...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170693</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 11:29:21</td>\n",
       "      <td>this is my personal experience ,it may not ref...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[this, is, my, personal, experience, it, may, ...</td>\n",
       "      <td>153.0</td>\n",
       "      <td>[this, is, my, personal, experience, it, may, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006536</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.065359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.006536</td>\n",
       "      <td>0.026144</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170694</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 16:17:34</td>\n",
       "      <td>stop looking at 20 million saudis as one entit...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[stop, looking, at, 20, million, saudis, as, o...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>[stop, looking, at, 20, million, saudis, as, o...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170695</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 20:00:31</td>\n",
       "      <td>i am aware of stats now and then. i was just s...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[i, am, aware, of, stats, now, and, then, i, w...</td>\n",
       "      <td>198.0</td>\n",
       "      <td>[i, am, aware, of, stats, now, and, then, i, w...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.085859</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.116162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025253</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170696</th>\n",
       "      <td>subject217</td>\n",
       "      <td>WHAT DID YOU SAY TO ME?</td>\n",
       "      <td>2018-08-20 10:54:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[what, did, you, say, to, me]</td>\n",
       "      <td>6.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[what, did, you, say, to, me]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170697</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-20 12:07:44</td>\n",
       "      <td>me smellz fish,me find no fish!...what the fuc...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[me, smellz, fish, me, find, no, fish, what, t...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>[me, smellz, fish, me, find, no, fish, what, t...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170698 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            subject                                              title  \\\n",
       "0       subject8292  If anyone could help with which sub to put thi...   \n",
       "1       subject8292          I'm literally never gonna stop waiting...   \n",
       "2       subject8292  This is a really interesting study! Makes sens...   \n",
       "3       subject8292               The only thing Frank is building ...   \n",
       "4       subject8292         Mostly always me during this whole charade   \n",
       "...             ...                                                ...   \n",
       "170693   subject217                                                NaN   \n",
       "170694   subject217                                                NaN   \n",
       "170695   subject217                                                NaN   \n",
       "170696   subject217                            WHAT DID YOU SAY TO ME?   \n",
       "170697   subject217                                                NaN   \n",
       "\n",
       "                       date  \\\n",
       "0       2016-08-02 09:22:12   \n",
       "1       2016-08-05 09:35:55   \n",
       "2       2016-08-05 21:36:24   \n",
       "3       2016-08-07 23:35:23   \n",
       "4       2016-08-09 08:39:41   \n",
       "...                     ...   \n",
       "170693  2018-08-19 11:29:21   \n",
       "170694  2018-08-19 16:17:34   \n",
       "170695  2018-08-19 20:00:31   \n",
       "170696  2018-08-20 10:54:11   \n",
       "170697  2018-08-20 12:07:44   \n",
       "\n",
       "                                                     text  label  \\\n",
       "0                                                     NaN      0   \n",
       "1                                                     NaN      0   \n",
       "2                                                     NaN      0   \n",
       "3       ... Is hype. Think about it, every time he wor...      0   \n",
       "4                                                     NaN      0   \n",
       "...                                                   ...    ...   \n",
       "170693  this is my personal experience ,it may not ref...      0   \n",
       "170694  stop looking at 20 million saudis as one entit...      0   \n",
       "170695  i am aware of stats now and then. i was just s...      0   \n",
       "170696                                                NaN      0   \n",
       "170697  me smellz fish,me find no fish!...what the fuc...      0   \n",
       "\n",
       "                                          tokenized_title  title_len  \\\n",
       "0       [if, anyone, could, help, with, which, sub, to...       11.0   \n",
       "1          [i, m, literally, never, gonna, stop, waiting]        7.0   \n",
       "2       [this, is, a, really, interesting, study, make...        9.0   \n",
       "3                 [the, only, thing, frank, is, building]        6.0   \n",
       "4       [mostly, always, me, during, this, whole, char...        7.0   \n",
       "...                                                   ...        ...   \n",
       "170693                                               None        NaN   \n",
       "170694                                               None        NaN   \n",
       "170695                                               None        NaN   \n",
       "170696                      [what, did, you, say, to, me]        6.0   \n",
       "170697                                               None        NaN   \n",
       "\n",
       "                                           tokenized_text  text_len  \\\n",
       "0                                                    None       NaN   \n",
       "1                                                    None       NaN   \n",
       "2                                                    None       NaN   \n",
       "3       [is, hype, think, about, it, every, time, he, ...      26.0   \n",
       "4                                                    None       NaN   \n",
       "...                                                   ...       ...   \n",
       "170693  [this, is, my, personal, experience, it, may, ...     153.0   \n",
       "170694  [stop, looking, at, 20, million, saudis, as, o...      15.0   \n",
       "170695  [i, am, aware, of, stats, now, and, then, i, w...     198.0   \n",
       "170696                                               None       NaN   \n",
       "170697  [me, smellz, fish, me, find, no, fish, what, t...      11.0   \n",
       "\n",
       "                                               all_tokens  ...      feel  \\\n",
       "0       [if, anyone, could, help, with, which, sub, to...  ...  0.000000   \n",
       "1          [i, m, literally, never, gonna, stop, waiting]  ...  0.000000   \n",
       "2       [this, is, a, really, interesting, study, make...  ...  0.000000   \n",
       "3       [is, hype, think, about, it, every, time, he, ...  ...  0.000000   \n",
       "4       [mostly, always, me, during, this, whole, char...  ...  0.000000   \n",
       "...                                                   ...  ...       ...   \n",
       "170693  [this, is, my, personal, experience, it, may, ...  ...  0.006536   \n",
       "170694  [stop, looking, at, 20, million, saudis, as, o...  ...  0.000000   \n",
       "170695  [i, am, aware, of, stats, now, and, then, i, w...  ...  0.005051   \n",
       "170696                      [what, did, you, say, to, me]  ...  0.000000   \n",
       "170697  [me, smellz, fish, me, find, no, fish, what, t...  ...  0.000000   \n",
       "\n",
       "            excl    future     nonfl     ppron     shehe         i        we  \\\n",
       "0       0.090909  0.090909  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1       0.000000  0.285714  0.000000  0.142857  0.000000  0.142857  0.000000   \n",
       "2       0.111111  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3       0.000000  0.062500  0.000000  0.062500  0.031250  0.000000  0.031250   \n",
       "4       0.000000  0.000000  0.000000  0.142857  0.000000  0.142857  0.000000   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "170693  0.039216  0.065359  0.000000  0.058824  0.006536  0.026144  0.019608   \n",
       "170694  0.066667  0.133333  0.000000  0.066667  0.000000  0.000000  0.000000   \n",
       "170695  0.075758  0.085859  0.005051  0.116162  0.000000  0.090909  0.000000   \n",
       "170696  0.000000  0.166667  0.000000  0.333333  0.000000  0.166667  0.000000   \n",
       "170697  0.000000  0.000000  0.000000  0.181818  0.000000  0.181818  0.000000   \n",
       "\n",
       "             you      they  \n",
       "0       0.000000  0.000000  \n",
       "1       0.000000  0.000000  \n",
       "2       0.000000  0.000000  \n",
       "3       0.000000  0.000000  \n",
       "4       0.000000  0.000000  \n",
       "...          ...       ...  \n",
       "170693  0.000000  0.006536  \n",
       "170694  0.066667  0.000000  \n",
       "170695  0.025253  0.000000  \n",
       "170696  0.166667  0.000000  \n",
       "170697  0.000000  0.000000  \n",
       "\n",
       "[170698 rows x 74 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def tokenize(t):\n",
    "    return tokenizer.tokenize(t.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'wasn', 't', 'ready', 'to', 'leave', 'buh', 'buw', 'dd', 'sasa']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"I wasn't ready to leave! buh-buw(dd). Sasa .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['tokenized_title'] = writings_df['title'].apply(lambda t: tokenize(t) if type(t)==str and t else None)\n",
    "writings_df['title_len'] = writings_df['tokenized_title'].apply(lambda t: len(t) if type(t)==list and t else None)\n",
    "writings_df['tokenized_text'] = writings_df['text'].apply(lambda t: tokenize(t) if type(t)==str and t else None)\n",
    "writings_df['text_len'] = writings_df['tokenized_text'].apply(lambda t: len(t) if type(t)==list and t else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    127604.00000\n",
       "mean         32.35415\n",
       "std          82.68303\n",
       "min           1.00000\n",
       "25%           6.00000\n",
       "50%          13.00000\n",
       "75%          31.00000\n",
       "max        7201.00000\n",
       "Name: text_len, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.text_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    49752.000000\n",
       "mean        10.701922\n",
       "std          9.282147\n",
       "min          1.000000\n",
       "25%          4.000000\n",
       "50%          8.000000\n",
       "75%         14.000000\n",
       "max        149.000000\n",
       "Name: title_len, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.title_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title_len</th>\n",
       "      <th>text_len</th>\n",
       "      <th>funct</th>\n",
       "      <th>article</th>\n",
       "      <th>affect</th>\n",
       "      <th>negemo</th>\n",
       "      <th>sad</th>\n",
       "      <th>cogmech</th>\n",
       "      <th>inhib</th>\n",
       "      <th>...</th>\n",
       "      <th>feel</th>\n",
       "      <th>excl</th>\n",
       "      <th>future</th>\n",
       "      <th>nonfl</th>\n",
       "      <th>ppron</th>\n",
       "      <th>shehe</th>\n",
       "      <th>i</th>\n",
       "      <th>we</th>\n",
       "      <th>you</th>\n",
       "      <th>they</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subject0</th>\n",
       "      <td>0</td>\n",
       "      <td>20.285714</td>\n",
       "      <td>31.711712</td>\n",
       "      <td>0.544556</td>\n",
       "      <td>0.044899</td>\n",
       "      <td>0.100554</td>\n",
       "      <td>0.021298</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>0.156881</td>\n",
       "      <td>0.007290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009250</td>\n",
       "      <td>0.029561</td>\n",
       "      <td>0.154795</td>\n",
       "      <td>0.001963</td>\n",
       "      <td>0.135275</td>\n",
       "      <td>0.002919</td>\n",
       "      <td>0.084605</td>\n",
       "      <td>0.003715</td>\n",
       "      <td>0.038377</td>\n",
       "      <td>0.005659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1027</th>\n",
       "      <td>0</td>\n",
       "      <td>7.769231</td>\n",
       "      <td>1.190476</td>\n",
       "      <td>0.190928</td>\n",
       "      <td>0.005636</td>\n",
       "      <td>0.013322</td>\n",
       "      <td>0.004919</td>\n",
       "      <td>0.004202</td>\n",
       "      <td>0.051015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.004919</td>\n",
       "      <td>0.087587</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.116719</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.076786</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1055</th>\n",
       "      <td>0</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>79.983193</td>\n",
       "      <td>0.541480</td>\n",
       "      <td>0.048654</td>\n",
       "      <td>0.052665</td>\n",
       "      <td>0.025766</td>\n",
       "      <td>0.002609</td>\n",
       "      <td>0.161580</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>0.024850</td>\n",
       "      <td>0.125660</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.095239</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.038783</td>\n",
       "      <td>0.007599</td>\n",
       "      <td>0.021944</td>\n",
       "      <td>0.021913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1064</th>\n",
       "      <td>1</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>68.410256</td>\n",
       "      <td>0.544914</td>\n",
       "      <td>0.029865</td>\n",
       "      <td>0.140286</td>\n",
       "      <td>0.038420</td>\n",
       "      <td>0.012897</td>\n",
       "      <td>0.148135</td>\n",
       "      <td>0.005281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003128</td>\n",
       "      <td>0.041912</td>\n",
       "      <td>0.150172</td>\n",
       "      <td>0.001942</td>\n",
       "      <td>0.125145</td>\n",
       "      <td>0.002882</td>\n",
       "      <td>0.086882</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.030292</td>\n",
       "      <td>0.004286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1089</th>\n",
       "      <td>0</td>\n",
       "      <td>9.823529</td>\n",
       "      <td>13.254902</td>\n",
       "      <td>0.440338</td>\n",
       "      <td>0.048374</td>\n",
       "      <td>0.082136</td>\n",
       "      <td>0.027137</td>\n",
       "      <td>0.005128</td>\n",
       "      <td>0.125983</td>\n",
       "      <td>0.003108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008413</td>\n",
       "      <td>0.024749</td>\n",
       "      <td>0.104063</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>0.077539</td>\n",
       "      <td>0.006873</td>\n",
       "      <td>0.028960</td>\n",
       "      <td>0.006127</td>\n",
       "      <td>0.027354</td>\n",
       "      <td>0.008226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9917</th>\n",
       "      <td>1</td>\n",
       "      <td>8.983607</td>\n",
       "      <td>95.806897</td>\n",
       "      <td>0.646948</td>\n",
       "      <td>0.040454</td>\n",
       "      <td>0.061069</td>\n",
       "      <td>0.034207</td>\n",
       "      <td>0.007909</td>\n",
       "      <td>0.224470</td>\n",
       "      <td>0.004510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006677</td>\n",
       "      <td>0.048364</td>\n",
       "      <td>0.152385</td>\n",
       "      <td>0.001657</td>\n",
       "      <td>0.120856</td>\n",
       "      <td>0.001722</td>\n",
       "      <td>0.105751</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.004625</td>\n",
       "      <td>0.007889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9918</th>\n",
       "      <td>0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.900901</td>\n",
       "      <td>0.410173</td>\n",
       "      <td>0.045823</td>\n",
       "      <td>0.102763</td>\n",
       "      <td>0.044945</td>\n",
       "      <td>0.003296</td>\n",
       "      <td>0.118872</td>\n",
       "      <td>0.005408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004256</td>\n",
       "      <td>0.023128</td>\n",
       "      <td>0.090074</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.083653</td>\n",
       "      <td>0.013338</td>\n",
       "      <td>0.036926</td>\n",
       "      <td>0.007207</td>\n",
       "      <td>0.021180</td>\n",
       "      <td>0.005003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject992</th>\n",
       "      <td>0</td>\n",
       "      <td>5.872928</td>\n",
       "      <td>19.914122</td>\n",
       "      <td>0.394271</td>\n",
       "      <td>0.053367</td>\n",
       "      <td>0.086128</td>\n",
       "      <td>0.038611</td>\n",
       "      <td>0.003050</td>\n",
       "      <td>0.113686</td>\n",
       "      <td>0.005810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003799</td>\n",
       "      <td>0.024507</td>\n",
       "      <td>0.079539</td>\n",
       "      <td>0.004855</td>\n",
       "      <td>0.069299</td>\n",
       "      <td>0.006907</td>\n",
       "      <td>0.032040</td>\n",
       "      <td>0.003941</td>\n",
       "      <td>0.023434</td>\n",
       "      <td>0.002977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9949</th>\n",
       "      <td>0</td>\n",
       "      <td>10.609756</td>\n",
       "      <td>42.346979</td>\n",
       "      <td>0.435481</td>\n",
       "      <td>0.059932</td>\n",
       "      <td>0.073305</td>\n",
       "      <td>0.031003</td>\n",
       "      <td>0.003585</td>\n",
       "      <td>0.123296</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>0.024724</td>\n",
       "      <td>0.080135</td>\n",
       "      <td>0.001819</td>\n",
       "      <td>0.067461</td>\n",
       "      <td>0.012940</td>\n",
       "      <td>0.029135</td>\n",
       "      <td>0.006191</td>\n",
       "      <td>0.013108</td>\n",
       "      <td>0.006086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9961</th>\n",
       "      <td>0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>26.389313</td>\n",
       "      <td>0.380818</td>\n",
       "      <td>0.030147</td>\n",
       "      <td>0.118331</td>\n",
       "      <td>0.026220</td>\n",
       "      <td>0.001940</td>\n",
       "      <td>0.101743</td>\n",
       "      <td>0.007136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>0.018475</td>\n",
       "      <td>0.073481</td>\n",
       "      <td>0.001981</td>\n",
       "      <td>0.086225</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>0.053193</td>\n",
       "      <td>0.002178</td>\n",
       "      <td>0.027518</td>\n",
       "      <td>0.001803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>340 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             label  title_len   text_len     funct   article    affect  \\\n",
       "subject                                                                  \n",
       "subject0         0  20.285714  31.711712  0.544556  0.044899  0.100554   \n",
       "subject1027      0   7.769231   1.190476  0.190928  0.005636  0.013322   \n",
       "subject1055      0  16.666667  79.983193  0.541480  0.048654  0.052665   \n",
       "subject1064      1  13.000000  68.410256  0.544914  0.029865  0.140286   \n",
       "subject1089      0   9.823529  13.254902  0.440338  0.048374  0.082136   \n",
       "...            ...        ...        ...       ...       ...       ...   \n",
       "subject9917      1   8.983607  95.806897  0.646948  0.040454  0.061069   \n",
       "subject9918      0   5.000000  11.900901  0.410173  0.045823  0.102763   \n",
       "subject992       0   5.872928  19.914122  0.394271  0.053367  0.086128   \n",
       "subject9949      0  10.609756  42.346979  0.435481  0.059932  0.073305   \n",
       "subject9961      0   5.000000  26.389313  0.380818  0.030147  0.118331   \n",
       "\n",
       "               negemo       sad   cogmech     inhib  ...      feel      excl  \\\n",
       "subject                                              ...                       \n",
       "subject0     0.021298  0.001919  0.156881  0.007290  ...  0.009250  0.029561   \n",
       "subject1027  0.004919  0.004202  0.051015  0.000000  ...  0.011765  0.004919   \n",
       "subject1055  0.025766  0.002609  0.161580  0.005952  ...  0.002272  0.024850   \n",
       "subject1064  0.038420  0.012897  0.148135  0.005281  ...  0.003128  0.041912   \n",
       "subject1089  0.027137  0.005128  0.125983  0.003108  ...  0.008413  0.024749   \n",
       "...               ...       ...       ...       ...  ...       ...       ...   \n",
       "subject9917  0.034207  0.007909  0.224470  0.004510  ...  0.006677  0.048364   \n",
       "subject9918  0.044945  0.003296  0.118872  0.005408  ...  0.004256  0.023128   \n",
       "subject992   0.038611  0.003050  0.113686  0.005810  ...  0.003799  0.024507   \n",
       "subject9949  0.031003  0.003585  0.123296  0.004145  ...  0.002448  0.024724   \n",
       "subject9961  0.026220  0.001940  0.101743  0.007136  ...  0.002923  0.018475   \n",
       "\n",
       "               future     nonfl     ppron     shehe         i        we  \\\n",
       "subject                                                                   \n",
       "subject0     0.154795  0.001963  0.135275  0.002919  0.084605  0.003715   \n",
       "subject1027  0.087587  0.000717  0.116719  0.000717  0.076786  0.000000   \n",
       "subject1055  0.125660  0.001161  0.095239  0.005000  0.038783  0.007599   \n",
       "subject1064  0.150172  0.001942  0.125145  0.002882  0.086882  0.000803   \n",
       "subject1089  0.104063  0.001497  0.077539  0.006873  0.028960  0.006127   \n",
       "...               ...       ...       ...       ...       ...       ...   \n",
       "subject9917  0.152385  0.001657  0.120856  0.001722  0.105751  0.000779   \n",
       "subject9918  0.090074  0.001046  0.083653  0.013338  0.036926  0.007207   \n",
       "subject992   0.079539  0.004855  0.069299  0.006907  0.032040  0.003941   \n",
       "subject9949  0.080135  0.001819  0.067461  0.012940  0.029135  0.006191   \n",
       "subject9961  0.073481  0.001981  0.086225  0.001534  0.053193  0.002178   \n",
       "\n",
       "                  you      they  \n",
       "subject                          \n",
       "subject0     0.038377  0.005659  \n",
       "subject1027  0.039216  0.000000  \n",
       "subject1055  0.021944  0.021913  \n",
       "subject1064  0.030292  0.004286  \n",
       "subject1089  0.027354  0.008226  \n",
       "...               ...       ...  \n",
       "subject9917  0.004625  0.007889  \n",
       "subject9918  0.021180  0.005003  \n",
       "subject992   0.023434  0.002977  \n",
       "subject9949  0.013108  0.006086  \n",
       "subject9961  0.027518  0.001803  \n",
       "\n",
       "[340 rows x 67 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.groupby('subject').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title_len</th>\n",
       "      <th>text_len</th>\n",
       "      <th>all_tokens</th>\n",
       "      <th>funct</th>\n",
       "      <th>article</th>\n",
       "      <th>affect</th>\n",
       "      <th>negemo</th>\n",
       "      <th>sad</th>\n",
       "      <th>cogmech</th>\n",
       "      <th>...</th>\n",
       "      <th>feel</th>\n",
       "      <th>excl</th>\n",
       "      <th>future</th>\n",
       "      <th>nonfl</th>\n",
       "      <th>ppron</th>\n",
       "      <th>shehe</th>\n",
       "      <th>i</th>\n",
       "      <th>we</th>\n",
       "      <th>you</th>\n",
       "      <th>they</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>299</td>\n",
       "      <td>296</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>...</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>40</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       date  title_len  text_len  all_tokens  funct  article  affect  negemo  \\\n",
       "label                                                                          \n",
       "0       299        296       299         299    299      299     299     299   \n",
       "1        41         40        41          41     41       41      41      41   \n",
       "\n",
       "       sad  cogmech  ...  feel  excl  future  nonfl  ppron  shehe    i   we  \\\n",
       "label                ...                                                      \n",
       "0      299      299  ...   299   299     299    299    299    299  299  299   \n",
       "1       41       41  ...    41    41      41     41     41     41   41   41   \n",
       "\n",
       "       you  they  \n",
       "label             \n",
       "0      299   299  \n",
       "1       41    41  \n",
       "\n",
       "[2 rows x 68 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.groupby('subject').max().groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of posts per user 146.35882352941175\n",
      "Average number of comments per user 376.2970588235294\n"
     ]
    }
   ],
   "source": [
    "print(\"Average number of posts per user\", writings_df.groupby('subject').count().title.mean())\n",
    "print(\"Average number of comments per user\", writings_df.groupby('subject').count().text.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>title_len</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>text_len</th>\n",
       "      <th>all_tokens</th>\n",
       "      <th>...</th>\n",
       "      <th>feel</th>\n",
       "      <th>excl</th>\n",
       "      <th>future</th>\n",
       "      <th>nonfl</th>\n",
       "      <th>ppron</th>\n",
       "      <th>shehe</th>\n",
       "      <th>i</th>\n",
       "      <th>we</th>\n",
       "      <th>you</th>\n",
       "      <th>they</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>Okay friends so I messed up and posted to do a...</td>\n",
       "      <td>2017-04-25 22:37:57</td>\n",
       "      <td>Sorry for that, I truly didn't think it was go...</td>\n",
       "      <td>0</td>\n",
       "      <td>[okay, friends, so, i, messed, up, and, posted...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>[sorry, for, that, i, truly, didn, t, think, i...</td>\n",
       "      <td>120.0</td>\n",
       "      <td>[sorry, for, that, i, truly, didn, t, think, i...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103896</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-09-16 06:29:13</td>\n",
       "      <td>You've got plenty of time to fix that. You can...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you, ve, got, plenty, of, time, to, fix, that...</td>\n",
       "      <td>104.0</td>\n",
       "      <td>[you, ve, got, plenty, of, time, to, fix, that...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-11-24 01:33:22</td>\n",
       "      <td>LCD, Glass animals, Kendrick, The Weeknd, Jack...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[lcd, glass, animals, kendrick, the, weeknd, j...</td>\n",
       "      <td>127.0</td>\n",
       "      <td>[lcd, glass, animals, kendrick, the, weeknd, j...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015748</td>\n",
       "      <td>0.007874</td>\n",
       "      <td>0.047244</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055118</td>\n",
       "      <td>0.015748</td>\n",
       "      <td>0.039370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>Getting that coachella bod</td>\n",
       "      <td>2018-01-09 00:54:06</td>\n",
       "      <td>First I want to say whatever skin is your skin...</td>\n",
       "      <td>0</td>\n",
       "      <td>[getting, that, coachella, bod]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[first, i, want, to, say, whatever, skin, is, ...</td>\n",
       "      <td>149.0</td>\n",
       "      <td>[first, i, want, to, say, whatever, skin, is, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013072</td>\n",
       "      <td>0.052288</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045752</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026144</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-03-12 17:14:03</td>\n",
       "      <td>Not the same but me and my wife saw a man and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[not, the, same, but, me, and, my, wife, saw, ...</td>\n",
       "      <td>151.0</td>\n",
       "      <td>[not, the, same, but, me, and, my, wife, saw, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026490</td>\n",
       "      <td>0.066225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099338</td>\n",
       "      <td>0.046358</td>\n",
       "      <td>0.052980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170652</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-05-28 12:23:00</td>\n",
       "      <td>/r/keto /r/ketorecipes /r/ketodessert all are ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[r, keto, r, ketorecipes, r, ketodessert, all,...</td>\n",
       "      <td>197.0</td>\n",
       "      <td>[r, keto, r, ketorecipes, r, ketodessert, all,...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005076</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>0.076142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030457</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170653</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-05-28 12:32:36</td>\n",
       "      <td>its okay dont worry . as long as you don't exc...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[its, okay, dont, worry, as, long, as, you, do...</td>\n",
       "      <td>109.0</td>\n",
       "      <td>[its, okay, dont, worry, as, long, as, you, do...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027523</td>\n",
       "      <td>0.064220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036697</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170662</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-06-20 00:33:57</td>\n",
       "      <td>the national number is :1919 here are more com...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[the, national, number, is, 1919, here, are, m...</td>\n",
       "      <td>115.0</td>\n",
       "      <td>[the, national, number, is, 1919, here, are, m...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170693</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 11:29:21</td>\n",
       "      <td>this is my personal experience ,it may not ref...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[this, is, my, personal, experience, it, may, ...</td>\n",
       "      <td>153.0</td>\n",
       "      <td>[this, is, my, personal, experience, it, may, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006536</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.065359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.006536</td>\n",
       "      <td>0.026144</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170695</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 20:00:31</td>\n",
       "      <td>i am aware of stats now and then. i was just s...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[i, am, aware, of, stats, now, and, then, i, w...</td>\n",
       "      <td>198.0</td>\n",
       "      <td>[i, am, aware, of, stats, now, and, then, i, w...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.085859</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.116162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025253</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7655 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            subject                                              title  \\\n",
       "122     subject8292  Okay friends so I messed up and posted to do a...   \n",
       "390     subject8292                                                NaN   \n",
       "498     subject8292                                                NaN   \n",
       "752     subject8292                         Getting that coachella bod   \n",
       "904     subject8292                                                NaN   \n",
       "...             ...                                                ...   \n",
       "170652   subject217                                                NaN   \n",
       "170653   subject217                                                NaN   \n",
       "170662   subject217                                                NaN   \n",
       "170693   subject217                                                NaN   \n",
       "170695   subject217                                                NaN   \n",
       "\n",
       "                       date  \\\n",
       "122     2017-04-25 22:37:57   \n",
       "390     2017-09-16 06:29:13   \n",
       "498     2017-11-24 01:33:22   \n",
       "752     2018-01-09 00:54:06   \n",
       "904     2018-03-12 17:14:03   \n",
       "...                     ...   \n",
       "170652  2018-05-28 12:23:00   \n",
       "170653  2018-05-28 12:32:36   \n",
       "170662  2018-06-20 00:33:57   \n",
       "170693  2018-08-19 11:29:21   \n",
       "170695  2018-08-19 20:00:31   \n",
       "\n",
       "                                                     text  label  \\\n",
       "122     Sorry for that, I truly didn't think it was go...      0   \n",
       "390     You've got plenty of time to fix that. You can...      0   \n",
       "498     LCD, Glass animals, Kendrick, The Weeknd, Jack...      0   \n",
       "752     First I want to say whatever skin is your skin...      0   \n",
       "904     Not the same but me and my wife saw a man and ...      0   \n",
       "...                                                   ...    ...   \n",
       "170652  /r/keto /r/ketorecipes /r/ketodessert all are ...      0   \n",
       "170653  its okay dont worry . as long as you don't exc...      0   \n",
       "170662  the national number is :1919 here are more com...      0   \n",
       "170693  this is my personal experience ,it may not ref...      0   \n",
       "170695  i am aware of stats now and then. i was just s...      0   \n",
       "\n",
       "                                          tokenized_title  title_len  \\\n",
       "122     [okay, friends, so, i, messed, up, and, posted...       34.0   \n",
       "390                                                  None        NaN   \n",
       "498                                                  None        NaN   \n",
       "752                       [getting, that, coachella, bod]        4.0   \n",
       "904                                                  None        NaN   \n",
       "...                                                   ...        ...   \n",
       "170652                                               None        NaN   \n",
       "170653                                               None        NaN   \n",
       "170662                                               None        NaN   \n",
       "170693                                               None        NaN   \n",
       "170695                                               None        NaN   \n",
       "\n",
       "                                           tokenized_text  text_len  \\\n",
       "122     [sorry, for, that, i, truly, didn, t, think, i...     120.0   \n",
       "390     [you, ve, got, plenty, of, time, to, fix, that...     104.0   \n",
       "498     [lcd, glass, animals, kendrick, the, weeknd, j...     127.0   \n",
       "752     [first, i, want, to, say, whatever, skin, is, ...     149.0   \n",
       "904     [not, the, same, but, me, and, my, wife, saw, ...     151.0   \n",
       "...                                                   ...       ...   \n",
       "170652  [r, keto, r, ketorecipes, r, ketodessert, all,...     197.0   \n",
       "170653  [its, okay, dont, worry, as, long, as, you, do...     109.0   \n",
       "170662  [the, national, number, is, 1919, here, are, m...     115.0   \n",
       "170693  [this, is, my, personal, experience, it, may, ...     153.0   \n",
       "170695  [i, am, aware, of, stats, now, and, then, i, w...     198.0   \n",
       "\n",
       "                                               all_tokens  ...      feel  \\\n",
       "122     [sorry, for, that, i, truly, didn, t, think, i...  ...  0.006494   \n",
       "390     [you, ve, got, plenty, of, time, to, fix, that...  ...  0.000000   \n",
       "498     [lcd, glass, animals, kendrick, the, weeknd, j...  ...  0.015748   \n",
       "752     [first, i, want, to, say, whatever, skin, is, ...  ...  0.013072   \n",
       "904     [not, the, same, but, me, and, my, wife, saw, ...  ...  0.000000   \n",
       "...                                                   ...  ...       ...   \n",
       "170652  [r, keto, r, ketorecipes, r, ketodessert, all,...  ...  0.005076   \n",
       "170653  [its, okay, dont, worry, as, long, as, you, do...  ...  0.000000   \n",
       "170662  [the, national, number, is, 1919, here, are, m...  ...  0.000000   \n",
       "170693  [this, is, my, personal, experience, it, may, ...  ...  0.006536   \n",
       "170695  [i, am, aware, of, stats, now, and, then, i, w...  ...  0.005051   \n",
       "\n",
       "            excl    future     nonfl     ppron     shehe         i        we  \\\n",
       "122     0.012987  0.142857  0.000000  0.110390  0.000000  0.103896  0.006494   \n",
       "390     0.000000  0.163462  0.000000  0.163462  0.000000  0.009615  0.000000   \n",
       "498     0.007874  0.047244  0.000000  0.055118  0.015748  0.039370  0.000000   \n",
       "752     0.052288  0.098039  0.000000  0.071895  0.000000  0.045752  0.000000   \n",
       "904     0.026490  0.066225  0.000000  0.099338  0.046358  0.052980  0.000000   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "170652  0.020305  0.076142  0.000000  0.076142  0.000000  0.045685  0.000000   \n",
       "170653  0.027523  0.064220  0.000000  0.045872  0.000000  0.009174  0.000000   \n",
       "170662  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "170693  0.039216  0.065359  0.000000  0.058824  0.006536  0.026144  0.019608   \n",
       "170695  0.075758  0.085859  0.005051  0.116162  0.000000  0.090909  0.000000   \n",
       "\n",
       "             you      they  \n",
       "122     0.000000  0.000000  \n",
       "390     0.153846  0.000000  \n",
       "498     0.000000  0.000000  \n",
       "752     0.026144  0.000000  \n",
       "904     0.000000  0.000000  \n",
       "...          ...       ...  \n",
       "170652  0.030457  0.000000  \n",
       "170653  0.036697  0.000000  \n",
       "170662  0.000000  0.000000  \n",
       "170693  0.000000  0.006536  \n",
       "170695  0.025253  0.000000  \n",
       "\n",
       "[7655 rows x 74 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[(~writings_df['text_len'].isnull()) & (writings_df['text_len'] > 100)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features and encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_features = {\n",
    "    \"max_features\": 40000,\n",
    "    # cut texts after this number of words\n",
    "    # (among top max_features most common words)\n",
    "    \"maxlen\": 100,\n",
    "    \"embedding_dim\": 50,\n",
    "    \"user_level\": True,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_NRC(nrc_path):\n",
    "    word_emotions = {}\n",
    "    emotion_words = {}\n",
    "    with open(nrc_path) as in_f:\n",
    "        for line in in_f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            word, emotion, label = line.split()\n",
    "            if word not in word_emotions:\n",
    "                word_emotions[word] = set()\n",
    "            if emotion not in emotion_words:\n",
    "                emotion_words[emotion] = set()\n",
    "            label = int(label)\n",
    "            if label:\n",
    "                word_emotions[word].add(emotion)\n",
    "                emotion_words[emotion].add(word)\n",
    "    return emotion_words\n",
    "\n",
    "nrc_lexicon_path = root_dir + '/resources/NRC-Sentiment-Emotion-Lexicons/NRC-Sentiment-Emotion-Lexicons/NRC-Emotion-Lexicon-v0.92/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'\n",
    "nrc_lexicon = load_NRC(nrc_lexicon_path)\n",
    "emotions = list(nrc_lexicon.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_emotions(tokens, emotion_lexicon, emotions, relative=True):\n",
    "    text_len = len(tokens)\n",
    "    encoded_emotions = [0 for e in emotions]\n",
    "    for i, emotion in enumerate(emotions):\n",
    "        try:\n",
    "            emotion_words = [t for t in tokens if t in emotion_lexicon[emotion]]\n",
    "            if relative:\n",
    "                encoded_emotions[i] = len(emotion_words) / len(tokens)\n",
    "            else:\n",
    "                encoded_emotions[i] = len(emotion_words)\n",
    "        except ValueError:\n",
    "            print(\"Emotion not found.\")\n",
    "    return encoded_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from liwc_readDict import readDict\n",
    "\n",
    "liwc = readDict(root_dir + '/resources/liwc.dic')\n",
    "\n",
    "categories = set([c for (w,c) in liwc])\n",
    "len(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Char n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(tokens):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Personal pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_person_pronouns = {\"i\", \"me\", \"my\", \"mine\", \"myself\"}\n",
    "def encode_pronouns(tokens, pronouns={\"i\", \"me\", \"my\", \"mine\", \"myself\"}, relative=True):\n",
    "    if not tokens:\n",
    "        return np.nan\n",
    "    text_len = len(tokens)\n",
    "    nr_pronouns = len([t for t in tokens if t in pronouns])\n",
    "    if relative:\n",
    "        return nr_pronouns/text_len\n",
    "    else:\n",
    "        return nr_pronouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words(\"english\")\n",
    "def encode_stopwords(tokens, stopwords=stopword_list):\n",
    "    encoded_stopwords = [0 for s in stopword_list]\n",
    "    if not tokens:\n",
    "        return encoded_stopwords\n",
    "    for i, stopword in enumerate(stopwords):\n",
    "        if stopword in tokens:\n",
    "            encoded_stopwords[i] += 1\n",
    "    return encoded_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def load_erisk_data(writings_df, voc_size, emotion_lexicon, seq_len, emotions =  \n",
    "                    ['anger', 'anticipation', 'disgust', 'fear', 'joy', \n",
    "                     'negative', 'positive', 'sadness', 'surprise', 'trust'],\n",
    "                    liwc_categories = categories,\n",
    "                    pronouns = [\"i\", \"me\", \"my\", \"mine\", \"myself\"],\n",
    "                    train_prop=0.7, valid_prop=0.3, test_slice=2,\n",
    "                    nr_slices=5,\n",
    "                    min_post_len=3, min_word_len=1, \n",
    "                    user_level=True, vocabulary=None,\n",
    "                   logger=logger):\n",
    "    logger.debug(\"Loading data...\\n\")\n",
    "    if not vocabulary:\n",
    "        vocabulary = {}\n",
    "        word_freqs = Counter()\n",
    "        for words in writings_df.tokenized_text:\n",
    "            word_freqs.update(words)\n",
    "        for words in writings_df.tokenized_title:\n",
    "            word_freqs.update(words)\n",
    "        i = 1\n",
    "        for w, f in word_freqs.most_common(voc_size-2): # keeping voc_size-1 for unk\n",
    "            if len(w) < min_word_len:\n",
    "                continue\n",
    "            vocabulary[w] = i\n",
    "            i += 1\n",
    "    tokens_data_train = []\n",
    "    categ_data_train = []\n",
    "    sparse_data_train = []\n",
    "    tokens_data_valid = []\n",
    "    categ_data_valid = []\n",
    "    sparse_data_valid = []\n",
    "    tokens_data_test = []\n",
    "    categ_data_test = []\n",
    "    sparse_data_test = []\n",
    "    labels_train = []\n",
    "    users_train = []\n",
    "    labels_valid = []\n",
    "    users_valid = []\n",
    "    users_test = []\n",
    "    labels_test = []\n",
    "    if 'subset' in writings_df.columns:\n",
    "        training_subjects = list(set(writings_df[writings_df['subset']=='train'].subject))\n",
    "        test_subjects = list(set(writings_df[writings_df['subset']=='test'].subject))\n",
    "    else:\n",
    "        all_subjects = sorted(list(set(writings_df.subject)))\n",
    "        training_subjects_size = int(len(all_subjects) * train_prop)\n",
    "        test_subjects_size = len(all_subjects) - training_subjects_size\n",
    "        # Cross-validation, with fixed slice as input\n",
    "        test_prop = 1-train_prop\n",
    "        test_slice = min(test_slice, nr_slices)\n",
    "        logger.debug(\"start index: %f, from %f\\n\" % (\n",
    "            len(all_subjects)*(1/nr_slices)*test_slice, test_prop*test_slice))\n",
    "        start_slice = int(len(all_subjects)*(1/nr_slices)*test_slice)\n",
    "        test_subjects = all_subjects[start_slice: start_slice+test_subjects_size]\n",
    "        training_subjects = [s for s in all_subjects if s not in test_subjects]\n",
    "    training_subjects = sorted(training_subjects) # ensuring reproducibility\n",
    "    valid_subjects_size = int(len(training_subjects) * valid_prop)\n",
    "    valid_subjects = training_subjects[:valid_subjects_size]\n",
    "    training_subjects = training_subjects[valid_subjects_size:]\n",
    "    categories = [c for c in liwc_categories if c in writings_df.columns]\n",
    "    logger.debug(\"%d training users, %d validation users, %d test users.\" % (\n",
    "        len(training_subjects), \n",
    "          len(valid_subjects),\n",
    "          len(test_subjects)))\n",
    "#     training_rows = writings_df[writings_df['subject'].isin(training_subjects)].sample(frac=1) # shuffling\n",
    "#     test_rows = writings_df[~writings_df['subject'].isin(training_subjects)].sample(frac=1)\n",
    "#     positive_training_users = training_rows.groupby('subject').max().groupby('label').count().date[1]\n",
    "#     positive_test_users = test_rows.groupby('subject').max().groupby('label').count().date[1]\n",
    "#     print(\"Positive training users: \", positive_training_users, \", positive test users: \", positive_test_users)\n",
    "    def encode_text(tokens):\n",
    "        # Using voc_size-1 value for OOV token\n",
    "        encoded_tokens = [vocabulary.get(w, voc_size-1) for w in tokens]\n",
    "        encoded_emotions = encode_emotions(tokens, emotion_lexicon, emotions)\n",
    "        encoded_pronouns = encode_pronouns(tokens, pronouns)\n",
    "        encoded_stopwords = encode_stopwords(tokens)\n",
    "        return (encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords)\n",
    "    user_level_texts = {}\n",
    "    for row in writings_df.itertuples():\n",
    "        words = []\n",
    "        if row.tokenized_title:\n",
    "            words.extend(row.tokenized_title)\n",
    "        if row.tokenized_text:\n",
    "            words.extend(row.tokenized_text)\n",
    "        if not words or len(words)<min_post_len:\n",
    "            continue\n",
    "        label = row.label\n",
    "        liwc_categs = [getattr(row, categ) for categ in categories]\n",
    "        if row.subject not in user_level_texts.keys():\n",
    "            user_level_texts[row.subject] = {}\n",
    "            user_level_texts[row.subject]['texts'] = [words]\n",
    "            user_level_texts[row.subject]['label'] = label\n",
    "            user_level_texts[row.subject]['liwc'] = [liwc_categs]\n",
    "        else:\n",
    "            user_level_texts[row.subject]['texts'].append(words) # TODO: sort datapoints chronologically\n",
    "            user_level_texts[row.subject]['liwc'].append(liwc_categs)\n",
    "\n",
    "    for subject in user_level_texts.keys():\n",
    "        texts = user_level_texts[subject]['texts']\n",
    "        label = user_level_texts[subject]['label']\n",
    "        if user_level:\n",
    "            all_words = [sum(texts, [])] # merge all texts in one list\n",
    "            liwc_aggreg = [np.array(user_level_texts[subject]['liwc']).mean(axis=0).tolist()]\n",
    "        else:\n",
    "            all_words = texts\n",
    "            liwc_aggreg = user_level_texts[subject]['liwc']\n",
    "        for i, words in enumerate(all_words):\n",
    "            encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords = encode_text(words)\n",
    "            subject_id = int(subject.split('t')[1])\n",
    "            if subject in training_subjects:\n",
    "                tokens_data_train.append(encoded_tokens)\n",
    "                categ_data_train.append(encoded_emotions + [encoded_pronouns] + liwc_aggreg[i])\n",
    "                sparse_data_train.append(encoded_stopwords)\n",
    "                labels_train.append(label)\n",
    "                users_train.append(subject_id)\n",
    "            elif subject in valid_subjects:\n",
    "                tokens_data_valid.append(encoded_tokens)\n",
    "                categ_data_valid.append(encoded_emotions + [encoded_pronouns]  + liwc_aggreg[i])\n",
    "                sparse_data_valid.append(encoded_stopwords)\n",
    "                labels_valid.append(label)\n",
    "                users_valid.append(subject_id)\n",
    "            else:\n",
    "                tokens_data_test.append(encoded_tokens)\n",
    "                categ_data_test.append(encoded_emotions + [encoded_pronouns] + liwc_aggreg[i])\n",
    "                sparse_data_test.append(encoded_stopwords)\n",
    "                labels_test.append(label)\n",
    "                users_test.append(subject_id)\n",
    "        \n",
    "    # using zeros for padding\n",
    "    tokens_data_train_padded = sequence.pad_sequences(tokens_data_train, maxlen=seq_len)\n",
    "    tokens_data_valid_padded = sequence.pad_sequences(tokens_data_valid, maxlen=seq_len)\n",
    "    tokens_data_test_padded = sequence.pad_sequences(tokens_data_test, maxlen=seq_len)\n",
    "        \n",
    "    return ([np.array(tokens_data_train_padded), np.array(categ_data_train), np.array(sparse_data_train),\n",
    "            np.array(users_train)],\n",
    "            np.array(labels_train)), \\\n",
    "            ([np.array(tokens_data_valid_padded),\n",
    "              np.array(categ_data_valid), np.array(sparse_data_valid),\n",
    "            np.array(users_valid)],\n",
    "            np.array(labels_valid)), \\\n",
    "            ([np.array(tokens_data_test_padded), np.array(categ_data_test), np.array(sparse_data_test),\n",
    "             np.array(users_test)],\n",
    "             np.array(labels_test)), vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "start index: 136.000000, from 0.600000\n",
      "\n",
      "166 training users, 71 validation users, 103 test users.\n"
     ]
    }
   ],
   "source": [
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "(x_train, y_train), (x_valid, y_valid), (x_test, y_test), voc = load_erisk_data(writings_df, \n",
    "                                                            seq_len=hyperparams_features['maxlen'],\n",
    "                                                            voc_size=hyperparams_features['max_features'],\n",
    "                                                           emotion_lexicon=nrc_lexicon,\n",
    "                                                           emotions=emotions,\n",
    "                                                           user_level=hyperparams_features['user_level'],\n",
    "                                                                                logger=logger\n",
    "#                                                            vocabulary=pickle.load(open('vocabulary20K_selfharm.pkl', 'rb'))\n",
    "                                                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166 train sequences\n",
      "71 train sequences\n",
      "103 test sequences\n"
     ]
    }
   ],
   "source": [
    "x_train_seq, x_train_categ, x_train_sparse, x_train_users = x_train\n",
    "x_valid_seq, x_valid_categ, x_valid_sparse, x_valid_users = x_valid\n",
    "x_test_seq, x_test_categ, x_test_sparse, x_test_users = x_test\n",
    "print(len(x_train_seq), 'train sequences')\n",
    "print(len(x_valid_seq), 'train sequences')\n",
    "print(len(x_test_seq), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 positive training examples\n",
      "5 positive validation examples\n",
      "10 positive test examples\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(y_train).sum(), \"positive training examples\")\n",
    "print(pd.Series(y_valid).sum(), \"positive validation examples\")\n",
    "\n",
    "print(pd.Series(y_test).sum(), \"positive test examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[   21,  1364,    10, ...,   377,  3718,   656],\n",
       "        [   12,    13,  6836, ...,    12,    25,   840],\n",
       "        [   24,     1,  1870, ..., 23444,   400,   518],\n",
       "        ...,\n",
       "        [   11,   255,   305, ...,  2510,     5,  1405],\n",
       "        [    2,    55,    15, ...,     3,     4,   334],\n",
       "        [    5,   413,    19, ...,    42,     2,    42]], dtype=int32),\n",
       " array([[0.01088049, 0.02307188, 0.00786541, ..., 0.01309675, 0.        ,\n",
       "         0.01402339],\n",
       "        [0.00595238, 0.01190476, 0.00892857, ..., 0.01454926, 0.        ,\n",
       "         0.01465054],\n",
       "        [0.00773015, 0.02588428, 0.00541696, ..., 0.01066934, 0.        ,\n",
       "         0.01944905],\n",
       "        ...,\n",
       "        [0.00824629, 0.02253986, 0.00879604, ..., 0.02449821, 0.        ,\n",
       "         0.01450942],\n",
       "        [0.01238583, 0.02179629, 0.00650429, ..., 0.02078657, 0.        ,\n",
       "         0.02072154],\n",
       "        [0.03150751, 0.02423655, 0.01793505, ..., 0.00972026, 0.        ,\n",
       "         0.01039306]]),\n",
       " array([[1, 1, 1, ..., 0, 1, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 1, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 1, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0]]),\n",
       " array([8292, 7982, 9260, 9918, 4284, 9829, 7661, 8361, 4831, 9077, 4513,\n",
       "        9197, 4143, 4226, 7627, 4510, 8001, 9285, 4414, 9961, 8065, 8225,\n",
       "        9949, 8329, 9411, 7857, 9811, 9156, 4527, 3301, 4074, 8990, 7830,\n",
       "        8395, 4247, 3667,  992, 7764, 3283, 7678, 4333, 8200, 9039, 7698,\n",
       "        9652, 9725, 3994, 3644, 8094,  974, 9575, 4570, 4729, 7499, 4071,\n",
       "        7740, 8721, 9222, 8432, 8472, 4198, 7777, 4762, 7637,   47, 8795,\n",
       "        4785, 4227, 7428, 4563, 9014, 7435, 8626, 4459, 8233, 8822, 3277,\n",
       "        7669, 4795, 4002, 3928, 9318, 3612, 9114, 4719, 7439, 8973, 3844,\n",
       "        7898, 3605, 9381, 3191, 4196, 8882, 8845, 4777, 4479, 9393, 4644,\n",
       "        7338, 9729, 7952, 9917, 3868, 3674,  855, 4318, 9249, 3737, 9095,\n",
       "        3227, 7355, 3883, 8933, 8802, 8657, 3635, 8978, 4702, 8062, 3555,\n",
       "        8357, 9492, 3914, 7581, 3725, 8481, 7946,  747,  814, 9160, 8770,\n",
       "        9497,  807, 4014, 8064, 8081, 7462, 8193, 4526,  463, 4379, 3904,\n",
       "        4392, 8581, 9242,  379, 3881, 8565, 4505, 3977, 7489, 8544, 7377,\n",
       "        8769, 3270, 3224, 7801, 3596, 4278, 3357,  796, 7692, 7560, 8726,\n",
       "         835])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.59285714, 3.19230769])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 40000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "def load_embeddings(path, embedding_dim, voc):\n",
    "    # random matrix with mean value = 0\n",
    "    embedding_matrix = np.random.random((len(voc)+2, embedding_dim)) - 0.5 # voc + unk + pad value(0)\n",
    "#     embedding_matrix = np.zeros((len(voc)+1, embedding_dim))\n",
    "\n",
    "    f = open(path)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_i = voc.get(word)\n",
    "        if word_i is not None:\n",
    "            embedding_matrix[word_i] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print('Total %s word vectors.' % len(embedding_matrix))\n",
    "\n",
    " \n",
    "    return embedding_matrix\n",
    "\n",
    "# pretrained_embeddings_path = '/home/ana/resources/glove.6B/glove.6B.%dd.txt' % hyperparams_features['embedding_dim']\n",
    "pretrained_embeddings_path = root_dir + '/resources/glove.twitter.27B/glove.twitter.27B.%dd.txt' % hyperparams_features['embedding_dim']\n",
    "embedding_matrix = load_embeddings(pretrained_embeddings_path, hyperparams_features['embedding_dim'], voc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166, 179)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'lstm_units': 100,\n",
    "    'dense_bow_units': 5,\n",
    "    'dropout': 0.0,\n",
    "    'l2_dense': 0.00011,\n",
    "    'l2_embeddings': 0.00001,\n",
    "    'optimizer': 'adam',\n",
    "    'decay': 0.0001,\n",
    "    'lr': 0.001,\n",
    "    \"batch_size\": 32,\n",
    "    \"trainable_embeddings\": False,\n",
    "    \"reduce_lr_factor\": 0.002,\n",
    "    \"reduce_lr_patience\": 50,\n",
    "    \"freeze_patience\": 50,\n",
    "    'threshold': 0.5,\n",
    "    'ignore_layer': []\n",
    "\n",
    "}\n",
    "if not hyperparams['optimizer']:\n",
    "    hyperparams['optimizer'] = optimizers.Adam(lr=hyperparams['lr'], #beta_1=0.9, beta_2=0.999, epsilon=0.0001,\n",
    "                                   decay=hyperparams['decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics():\n",
    "    def __init__(self, threshold=0.5):\n",
    "        self.threshold=threshold\n",
    "        \n",
    "    def recall_m(self, y_true, y_pred):\n",
    "            y_labels = y_true#tf.reshape(y_true[:], (-1,1))\n",
    "    #         y_pred = tf.reshape(y_pred, (-1,1))\n",
    "            y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), self.threshold), K.floatx())        \n",
    "            possible_positives = K.sum(K.round(K.clip(y_labels, 0, 1)))\n",
    "            true_positives = K.sum(K.round(K.clip(y_labels * y_pred, 0, 1)))\n",
    "            recall = true_positives / (possible_positives + K.epsilon())\n",
    "            return recall\n",
    "\n",
    "    def precision_m(self, y_true, y_pred):\n",
    "            y_labels = y_true#tf.reshape(y_true[:], (-1,1))\n",
    "    #         y_pred = tf.reshape(y_pred, (-1,1))\n",
    "            y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), self.threshold), K.floatx())        \n",
    "            true_positives = K.sum(K.round(K.clip(y_labels * y_pred, 0, 1)))\n",
    "            predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "            precision = true_positives / (predicted_positives + K.epsilon())\n",
    "            return precision\n",
    "\n",
    "    def f1_m(self, y_true, y_pred):\n",
    "        precision = self.precision_m(y_true, y_pred)\n",
    "        recall = self.recall_m(y_true, y_pred)\n",
    "        return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def binary_crossentropy_custom(y_true, y_pred):\n",
    "    y_labels = y_true#tf.reshape(y_true[0],(1,-1))\n",
    "    return K.binary_crossentropy(y_labels, \n",
    "                                 y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopwords_list,\n",
    "                liwc_categories,\n",
    "               ignore_layer=[]):\n",
    "\n",
    "    tokens_features = Input(shape=(hyperparams_features['maxlen'],), name='word_seq')\n",
    "    embedding_layer = Embedding(hyperparams_features['max_features'], \n",
    "                                hyperparams_features['embedding_dim'], \n",
    "                                input_length=hyperparams_features['maxlen'],\n",
    "#                                 mask_zero=True,\n",
    "                                embeddings_regularizer=regularizers.l2(hyperparams['l2_embeddings']),\n",
    "                                weights=[embedding_matrix], \n",
    "                                trainable=hyperparams['trainable_embeddings'],\n",
    "                               name='embeddings_layer')(\n",
    "        tokens_features)\n",
    "#     lstm_layers = Bidirectional(LSTM(hyperparams['lstm_units']))(embedding_layer)\n",
    "    if tf.test.is_gpu_available():\n",
    "        lstm_layers = CuDNNLSTM(hyperparams['lstm_units'], \n",
    "                      name='LSTM_layer')(embedding_layer)\n",
    "    else:\n",
    "        lstm_layers = LSTM(hyperparams['lstm_units'], \n",
    "                      name='LSTM_layer')(embedding_layer)\n",
    "    \n",
    "    # Attention\n",
    "    attention = Dense(1, activation='tanh', name='attention')(lstm_layers)\n",
    "    attention = Flatten()(attention)\n",
    "    attention = Activation('softmax')(attention)\n",
    "    attention = RepeatVector(hyperparams['lstm_units'])(attention)\n",
    "    print(attention.shape)\n",
    "    attention = Permute([2, 1])(attention)\n",
    "    print(attention.shape)\n",
    "\n",
    "#     sent_representation = merge([input, attention], mode='mul')\n",
    "#     # Compilation error, so changed to Multiply()`\n",
    "    sent_representation = Multiply()([lstm_layers, attention])\n",
    "    sent_representation = Lambda(lambda xin: K.sum(xin, axis=1), \n",
    "                                 output_shape=(hyperparams['lstm_units'],)\n",
    "                                )(sent_representation)\n",
    "#     sent_representation = K.sum(sent_representation, axis=2)`\n",
    "    # whatever value i put in axis, I get error. Please help.`\n",
    "    print(sent_representation.shape)\n",
    "\n",
    "    sent_representation = Dropout(hyperparams['dropout'], name='lstm_att_dropout')(sent_representation)\n",
    "    numerical_features = Input(shape=(len(emotions) + 1 + len(liwc_categories),), name='numeric_input') # emotions and pronouns\n",
    "    dense_layer = Dense(units=1,\n",
    "                        kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                        name='numerical_dense_layer',\n",
    "                       )(numerical_features)\n",
    "    sparse_features = Input(shape=(len(stopwords_list),), name='sparse_input') # stopwords\n",
    "    subjects = Input(shape=(1,), name='subjects')\n",
    "    dense_layer_sparse = Dense(units=hyperparams['dense_bow_units'],\n",
    "                              name='sparse_feat_dense_layer',\n",
    "                                kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                              )(sparse_features)\n",
    "\n",
    "    all_layers = {\n",
    "        'lstm_layers': sent_representation,\n",
    "        'numerical_dense_layer': numerical_features,# dense_layer,\n",
    "        'sparse_feat_dense_layer': sparse_features#dense_layer_sparse,\n",
    "    }\n",
    "    layers_to_merge = []\n",
    "    for n, l in all_layers.items():\n",
    "        if n in ignore_layer:\n",
    "            continue\n",
    "        layers_to_merge.append(l)\n",
    "        \n",
    "    if len(layers_to_merge) == 1:\n",
    "        merged_layers = layers_to_merge[0]\n",
    "    else:\n",
    "        merged_layers = concatenate(layers_to_merge)\n",
    "    output_layer = Dense(1, activation='sigmoid',\n",
    "                         name='output_layer',\n",
    "                        kernel_regularizer=regularizers.l2(hyperparams['l2_dense']))(merged_layers)\n",
    "\n",
    "    # Compile model\n",
    "    model = Model(inputs=[tokens_features, numerical_features, sparse_features, subjects], \n",
    "                  outputs=output_layer)\n",
    "    metrics_class = Metrics(threshold=hyperparams['threshold'])\n",
    "    model.compile(hyperparams['optimizer'], binary_crossentropy_custom,\n",
    "                  metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 100, 1)\n",
      "(?, 1, 100)\n",
      "(?, 100)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "word_seq (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embeddings_layer (Embedding)    (None, 100, 50)      2000000     word_seq[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "LSTM_layer (LSTM)               (None, 100)          60400       embeddings_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention (Dense)               (None, 1)            101         LSTM_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 1)            0           attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 1)            0           flatten_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_13 (RepeatVector) (None, 100, 1)       0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "permute_12 (Permute)            (None, 1, 100)       0           repeat_vector_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "multiply_12 (Multiply)          (None, 1, 100)       0           LSTM_layer[0][0]                 \n",
      "                                                                 permute_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 100)          0           multiply_12[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_att_dropout (Dropout)      (None, 100)          0           lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "numeric_input (InputLayer)      (None, 75)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sparse_input (InputLayer)       (None, 179)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 354)          0           lstm_att_dropout[0][0]           \n",
      "                                                                 numeric_input[0][0]              \n",
      "                                                                 sparse_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 1)            355         concatenate_10[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 2,060,856\n",
      "Trainable params: 60,856\n",
      "Non-trainable params: 2,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopword_list,\n",
    "                    liwc_categories=[c for c in categories if c in writings_df.columns]\n",
    ",\n",
    "                   ignore_layer=hyperparams['ignore_layer'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, 'models/lstm_plus_ablated3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary:\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     url: https://www.comet.ml/ananana/mental/9e337f32bd2a434a999dc93c4eab3d28\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     batch_f1_m [100]        : (0.0, 1.0)\n",
      "COMET INFO:     batch_loss [100]        : (7.923710346221924, 8.693343162536621)\n",
      "COMET INFO:     batch_precision_m [100] : (0.0, 1.0)\n",
      "COMET INFO:     batch_recall_m [100]    : (0.0, 1.0)\n",
      "COMET INFO:     epoch_duration [100]    : (23.11319889499282, 27.83345008699689)\n",
      "COMET INFO:     f1_m [100]              : (0.18202191591262817, 1.0)\n",
      "COMET INFO:     loss [100]              : (7.923916156033435, 8.66281114141625)\n",
      "COMET INFO:     lr [100]                : (1.9999999949504854e-06, 0.0010000000474974513)\n",
      "COMET INFO:     precision_m [100]       : (0.13262322545051575, 1.0)\n",
      "COMET INFO:     recall_m [100]          : (0.34761905670166016, 1.0)\n",
      "COMET INFO:     step                    : 600\n",
      "COMET INFO:     sys.cpu.percent.01 [44] : (13.9, 97.6)\n",
      "COMET INFO:     sys.cpu.percent.02 [44] : (14.2, 40.7)\n",
      "COMET INFO:     sys.cpu.percent.03 [44] : (12.6, 50.4)\n",
      "COMET INFO:     sys.cpu.percent.04 [44] : (14.1, 45.2)\n",
      "COMET INFO:     sys.cpu.percent.avg [44]: (14.55, 58.17500000000001)\n",
      "COMET INFO:     sys.gpu.0.total_memory  : (1073414144.0, 1073414144.0)\n",
      "COMET INFO:     sys.load.avg [44]       : (0.4, 2.97)\n",
      "COMET INFO:     sys.ram.total [44]      : (8277331968.0, 8277331968.0)\n",
      "COMET INFO:     sys.ram.used [44]       : (7165841408.0, 7533350912.0)\n",
      "COMET INFO:     val_f1_m [100]          : (0.10549942404031754, 0.8666666150093079)\n",
      "COMET INFO:     val_loss [100]          : (8.037364220955002, 8.96692355249969)\n",
      "COMET INFO:     val_precision_m [100]   : (0.05879752337932587, 0.8333332538604736)\n",
      "COMET INFO:     val_recall_m [100]      : (0.2777777910232544, 1.0)\n",
      "COMET INFO:   Other [count]:\n",
      "COMET INFO:     trainable_params: 2060856\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     git-patch  : 1\n",
      "COMET INFO:     histogram3d: 303\n",
      "COMET INFO: ----------------------------\n",
      "COMET INFO: old comet version (3.0.2) detected. current: 3.1.0 please update your comet lib with command: `pip install --no-cache-dir --upgrade comet_ml`\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/ananana/mental/f67a38789d41404db1250be24359bb3a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(api_key=\"eoBdVyznAhfg3bK9pZ58ZSXfv\",\n",
    "                        project_name=\"mental\", workspace=\"ananana\")\n",
    "\n",
    "experiment.log_parameters(hyperparams_features)\n",
    "\n",
    "experiment.log_parameter('emotion_lexicon', nrc_lexicon_path)\n",
    "experiment.log_parameter('emotions', emotions)\n",
    "experiment.log_parameter('embeddings_path', pretrained_embeddings_path)\n",
    "if 'subset' in writings_df.columns:\n",
    "    experiment.add_tag('anorexia')\n",
    "\n",
    "experiment.log_parameters(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'i': 2,\n",
       " 'to': 3,\n",
       " 'a': 4,\n",
       " 'and': 5,\n",
       " 'of': 6,\n",
       " 'it': 7,\n",
       " 'in': 8,\n",
       " 'you': 9,\n",
       " 'that': 10,\n",
       " 'is': 11,\n",
       " 's': 12,\n",
       " 'for': 13,\n",
       " 'this': 14,\n",
       " 't': 15,\n",
       " 'on': 16,\n",
       " 'with': 17,\n",
       " 'but': 18,\n",
       " 'my': 19,\n",
       " '8217': 20,\n",
       " 'be': 21,\n",
       " 'was': 22,\n",
       " 'have': 23,\n",
       " 'are': 24,\n",
       " 'not': 25,\n",
       " 'they': 26,\n",
       " 'as': 27,\n",
       " 'if': 28,\n",
       " 'so': 29,\n",
       " 'just': 30,\n",
       " 'what': 31,\n",
       " 'can': 32,\n",
       " 'like': 33,\n",
       " 'he': 34,\n",
       " 'or': 35,\n",
       " 'at': 36,\n",
       " 'we': 37,\n",
       " 'me': 38,\n",
       " 'from': 39,\n",
       " 'your': 40,\n",
       " 'm': 41,\n",
       " 'do': 42,\n",
       " 'com': 43,\n",
       " 'all': 44,\n",
       " 'about': 45,\n",
       " 'an': 46,\n",
       " 'one': 47,\n",
       " 'there': 48,\n",
       " 'would': 49,\n",
       " 'out': 50,\n",
       " 'up': 51,\n",
       " 'when': 52,\n",
       " 'more': 53,\n",
       " 'get': 54,\n",
       " 'don': 55,\n",
       " 'people': 56,\n",
       " 'by': 57,\n",
       " 'will': 58,\n",
       " 'no': 59,\n",
       " 'how': 60,\n",
       " 'https': 61,\n",
       " 'gt': 62,\n",
       " 'has': 63,\n",
       " 'them': 64,\n",
       " 'his': 65,\n",
       " 'time': 66,\n",
       " 'some': 67,\n",
       " 're': 68,\n",
       " 'know': 69,\n",
       " 'think': 70,\n",
       " 'who': 71,\n",
       " 'their': 72,\n",
       " 'because': 73,\n",
       " 'had': 74,\n",
       " 'she': 75,\n",
       " 'here': 76,\n",
       " 'good': 77,\n",
       " 'really': 78,\n",
       " 'www': 79,\n",
       " 'r': 80,\n",
       " 'now': 81,\n",
       " 've': 82,\n",
       " 'been': 83,\n",
       " 'only': 84,\n",
       " 'her': 85,\n",
       " 'also': 86,\n",
       " 'were': 87,\n",
       " 'than': 88,\n",
       " 'see': 89,\n",
       " 'any': 90,\n",
       " 'http': 91,\n",
       " 'even': 92,\n",
       " 'make': 93,\n",
       " 'other': 94,\n",
       " 'then': 95,\n",
       " '128056': 96,\n",
       " 'much': 97,\n",
       " '1': 98,\n",
       " 'which': 99,\n",
       " 'him': 100,\n",
       " 'could': 101,\n",
       " 'go': 102,\n",
       " '2': 103,\n",
       " 'first': 104,\n",
       " 'd': 105,\n",
       " 'want': 106,\n",
       " 'new': 107,\n",
       " 'why': 108,\n",
       " 'well': 109,\n",
       " 'did': 110,\n",
       " 'too': 111,\n",
       " 'right': 112,\n",
       " 'way': 113,\n",
       " 'into': 114,\n",
       " 'very': 115,\n",
       " 'after': 116,\n",
       " 'being': 117,\n",
       " 'over': 118,\n",
       " 'back': 119,\n",
       " 'still': 120,\n",
       " 'got': 121,\n",
       " 'most': 122,\n",
       " 'reddit': 123,\n",
       " 'should': 124,\n",
       " 'something': 125,\n",
       " 'post': 126,\n",
       " 'going': 127,\n",
       " 'll': 128,\n",
       " '3': 129,\n",
       " 'our': 130,\n",
       " 'these': 131,\n",
       " 'thanks': 132,\n",
       " 'never': 133,\n",
       " '8220': 134,\n",
       " 'need': 135,\n",
       " '8221': 136,\n",
       " 'say': 137,\n",
       " 'where': 138,\n",
       " 'us': 139,\n",
       " 'use': 140,\n",
       " 'am': 141,\n",
       " 'love': 142,\n",
       " 'work': 143,\n",
       " 'day': 144,\n",
       " 'same': 145,\n",
       " 'off': 146,\n",
       " 'sure': 147,\n",
       " 'game': 148,\n",
       " 'before': 149,\n",
       " 'said': 150,\n",
       " 'didn': 151,\n",
       " '10': 152,\n",
       " 'lot': 153,\n",
       " 'those': 154,\n",
       " 'thing': 155,\n",
       " 'years': 156,\n",
       " 'someone': 157,\n",
       " 'does': 158,\n",
       " 'take': 159,\n",
       " 'best': 160,\n",
       " 'made': 161,\n",
       " 'feel': 162,\n",
       " 'great': 163,\n",
       " 'two': 164,\n",
       " 'actually': 165,\n",
       " 'better': 166,\n",
       " 'look': 167,\n",
       " '_': 168,\n",
       " 'its': 169,\n",
       " '5': 170,\n",
       " 'every': 171,\n",
       " 'down': 172,\n",
       " 'year': 173,\n",
       " 'pretty': 174,\n",
       " 'life': 175,\n",
       " 'doesn': 176,\n",
       " 'amp': 177,\n",
       " 'things': 178,\n",
       " 'many': 179,\n",
       " 'though': 180,\n",
       " '4': 181,\n",
       " 'while': 182,\n",
       " 'last': 183,\n",
       " 'always': 184,\n",
       " 'around': 185,\n",
       " 'man': 186,\n",
       " 'imgur': 187,\n",
       " 'thank': 188,\n",
       " 'world': 189,\n",
       " 'give': 190,\n",
       " 'find': 191,\n",
       " 'little': 192,\n",
       " 'help': 193,\n",
       " 'anything': 194,\n",
       " 'u': 195,\n",
       " 'through': 196,\n",
       " 'trump': 197,\n",
       " 'long': 198,\n",
       " 'used': 199,\n",
       " 'o': 200,\n",
       " 'yeah': 201,\n",
       " 'may': 202,\n",
       " 'few': 203,\n",
       " 'ever': 204,\n",
       " 'thought': 205,\n",
       " 'since': 206,\n",
       " 'yes': 207,\n",
       " 'doing': 208,\n",
       " 'getting': 209,\n",
       " 'shit': 210,\n",
       " 'anyone': 211,\n",
       " 'comments': 212,\n",
       " 'probably': 213,\n",
       " 'let': 214,\n",
       " 'point': 215,\n",
       " 'person': 216,\n",
       " 'bad': 217,\n",
       " 'both': 218,\n",
       " 'watch': 219,\n",
       " 'another': 220,\n",
       " 'again': 221,\n",
       " 'own': 222,\n",
       " 'might': 223,\n",
       " 'isn': 224,\n",
       " 'looking': 225,\n",
       " 'maybe': 226,\n",
       " 'everyone': 227,\n",
       " 'old': 228,\n",
       " 'come': 229,\n",
       " 'put': 230,\n",
       " 'mean': 231,\n",
       " 'without': 232,\n",
       " 'try': 233,\n",
       " 'please': 234,\n",
       " 'looks': 235,\n",
       " 'keep': 236,\n",
       " 'trying': 237,\n",
       " 'big': 238,\n",
       " 'video': 239,\n",
       " 'different': 240,\n",
       " 'show': 241,\n",
       " 'guy': 242,\n",
       " 'next': 243,\n",
       " 'e': 244,\n",
       " 'part': 245,\n",
       " 'play': 246,\n",
       " 'using': 247,\n",
       " 'makes': 248,\n",
       " 'already': 249,\n",
       " 'enough': 250,\n",
       " '2018': 251,\n",
       " 'found': 252,\n",
       " 'money': 253,\n",
       " 'oh': 254,\n",
       " 'real': 255,\n",
       " 'such': 256,\n",
       " '9685': 257,\n",
       " 'high': 258,\n",
       " 'end': 259,\n",
       " 'between': 260,\n",
       " '12388': 261,\n",
       " '6': 262,\n",
       " 'done': 263,\n",
       " 'read': 264,\n",
       " 'link': 265,\n",
       " 'lol': 266,\n",
       " 'nice': 267,\n",
       " 'having': 268,\n",
       " 'away': 269,\n",
       " 'tell': 270,\n",
       " 'nothing': 271,\n",
       " 'kind': 272,\n",
       " 'making': 273,\n",
       " 'question': 274,\n",
       " 'live': 275,\n",
       " 'fuck': 276,\n",
       " 'v': 277,\n",
       " 'start': 278,\n",
       " 'else': 279,\n",
       " 'team': 280,\n",
       " 'today': 281,\n",
       " 'hard': 282,\n",
       " '0': 283,\n",
       " 'against': 284,\n",
       " 'once': 285,\n",
       " 'days': 286,\n",
       " 'seen': 287,\n",
       " 'far': 288,\n",
       " 'bit': 289,\n",
       " 'able': 290,\n",
       " 'org': 291,\n",
       " 'place': 292,\n",
       " 'week': 293,\n",
       " 'everything': 294,\n",
       " 'each': 295,\n",
       " 'free': 296,\n",
       " 'wrong': 297,\n",
       " 'school': 298,\n",
       " 'least': 299,\n",
       " 'times': 300,\n",
       " 'less': 301,\n",
       " 'edit': 302,\n",
       " 'source': 303,\n",
       " 'idea': 304,\n",
       " 'top': 305,\n",
       " 'went': 306,\n",
       " 'during': 307,\n",
       " 'name': 308,\n",
       " 'believe': 309,\n",
       " 'until': 310,\n",
       " 'won': 311,\n",
       " 'state': 312,\n",
       " 'definitely': 313,\n",
       " 'seems': 314,\n",
       " 'guys': 315,\n",
       " 'saying': 316,\n",
       " 'home': 317,\n",
       " 'hope': 318,\n",
       " 'title': 319,\n",
       " 'story': 320,\n",
       " '7': 321,\n",
       " 'youtube': 322,\n",
       " 'fucking': 323,\n",
       " 'small': 324,\n",
       " 'says': 325,\n",
       " 'full': 326,\n",
       " 'rep': 327,\n",
       " 'n': 328,\n",
       " 'yet': 329,\n",
       " 'case': 330,\n",
       " 'sorry': 331,\n",
       " 'stop': 332,\n",
       " 'twitter': 333,\n",
       " 'friend': 334,\n",
       " 'under': 335,\n",
       " 'stuff': 336,\n",
       " 'reason': 337,\n",
       " 'ago': 338,\n",
       " 'support': 339,\n",
       " 'change': 340,\n",
       " '8': 341,\n",
       " 'job': 342,\n",
       " 'however': 343,\n",
       " 'house': 344,\n",
       " 'wasn': 345,\n",
       " 'family': 346,\n",
       " 'left': 347,\n",
       " 'second': 348,\n",
       " 'friends': 349,\n",
       " 'etc': 350,\n",
       " 'guess': 351,\n",
       " 'comment': 352,\n",
       " 'true': 353,\n",
       " 'problem': 354,\n",
       " 'power': 355,\n",
       " 'gets': 356,\n",
       " 'myself': 357,\n",
       " 'called': 358,\n",
       " 'news': 359,\n",
       " 'buy': 360,\n",
       " 'either': 361,\n",
       " 'rules': 362,\n",
       " 'sub': 363,\n",
       " 'set': 364,\n",
       " 'wanted': 365,\n",
       " 'happy': 366,\n",
       " 'w': 367,\n",
       " 'water': 368,\n",
       " 'fun': 369,\n",
       " 'white': 370,\n",
       " 'almost': 371,\n",
       " 'fact': 372,\n",
       " 'wiki': 373,\n",
       " 'experience': 374,\n",
       " 'open': 375,\n",
       " 'call': 376,\n",
       " 'whole': 377,\n",
       " 'started': 378,\n",
       " 'understand': 379,\n",
       " 'wouldn': 380,\n",
       " '20': 381,\n",
       " 'night': 382,\n",
       " '9': 383,\n",
       " 'article': 384,\n",
       " 'h': 385,\n",
       " 'side': 386,\n",
       " '100': 387,\n",
       " 'check': 388,\n",
       " 'came': 389,\n",
       " 'dog': 390,\n",
       " 'took': 391,\n",
       " 'net': 392,\n",
       " 'possible': 393,\n",
       " 'months': 394,\n",
       " 'jpg': 395,\n",
       " 'hours': 396,\n",
       " 'face': 397,\n",
       " 'food': 398,\n",
       " 'saw': 399,\n",
       " 'playing': 400,\n",
       " 'doi': 401,\n",
       " 'care': 402,\n",
       " '12': 403,\n",
       " 'internet': 404,\n",
       " 'study': 405,\n",
       " 'talking': 406,\n",
       " 'remember': 407,\n",
       " 'content': 408,\n",
       " 'mind': 409,\n",
       " 'de': 410,\n",
       " 'ask': 411,\n",
       " 'level': 412,\n",
       " 'told': 413,\n",
       " 'instead': 414,\n",
       " 'season': 415,\n",
       " 'god': 416,\n",
       " 'cool': 417,\n",
       " 'thinking': 418,\n",
       " 'working': 419,\n",
       " 'must': 420,\n",
       " 'human': 421,\n",
       " 'based': 422,\n",
       " 'country': 423,\n",
       " 'talk': 424,\n",
       " '225': 425,\n",
       " 'p': 426,\n",
       " 'movie': 427,\n",
       " 'aren': 428,\n",
       " 'run': 429,\n",
       " 'body': 430,\n",
       " '233': 431,\n",
       " 'agree': 432,\n",
       " 'posted': 433,\n",
       " '227': 434,\n",
       " 'black': 435,\n",
       " '65039': 436,\n",
       " 'awesome': 437,\n",
       " 'party': 438,\n",
       " 'wait': 439,\n",
       " 'games': 440,\n",
       " 'journal': 441,\n",
       " 'others': 442,\n",
       " 'group': 443,\n",
       " 'likely': 444,\n",
       " 'exactly': 445,\n",
       " 'worth': 446,\n",
       " 'government': 447,\n",
       " 'gonna': 448,\n",
       " 'que': 449,\n",
       " 'coming': 450,\n",
       " 'women': 451,\n",
       " 'means': 452,\n",
       " 'tried': 453,\n",
       " 'information': 454,\n",
       " '30': 455,\n",
       " 'pay': 456,\n",
       " 'haven': 457,\n",
       " 'head': 458,\n",
       " 'future': 459,\n",
       " '2017': 460,\n",
       " 'hey': 461,\n",
       " 'c': 462,\n",
       " 'taking': 463,\n",
       " 'single': 464,\n",
       " 'literally': 465,\n",
       " 'hear': 466,\n",
       " 'hate': 467,\n",
       " 'super': 468,\n",
       " 'self': 469,\n",
       " 'goes': 470,\n",
       " 'health': 471,\n",
       " 'amazing': 472,\n",
       " 'hand': 473,\n",
       " 'message': 474,\n",
       " 'within': 475,\n",
       " 'issue': 476,\n",
       " 'comes': 477,\n",
       " 'happened': 478,\n",
       " 'sounds': 479,\n",
       " 'system': 480,\n",
       " 'sense': 481,\n",
       " 'car': 482,\n",
       " 'couple': 483,\n",
       " 'type': 484,\n",
       " 'half': 485,\n",
       " 'social': 486,\n",
       " 'usually': 487,\n",
       " 'facebook': 488,\n",
       " 'order': 489,\n",
       " '3901': 490,\n",
       " '8211': 491,\n",
       " 'close': 492,\n",
       " '3900': 493,\n",
       " 'comic': 494,\n",
       " 'book': 495,\n",
       " 'three': 496,\n",
       " 'children': 497,\n",
       " 'needs': 498,\n",
       " 'interesting': 499,\n",
       " 'futurology': 500,\n",
       " 'number': 501,\n",
       " 'past': 502,\n",
       " 'data': 503,\n",
       " 'girl': 504,\n",
       " 'quite': 505,\n",
       " 'low': 506,\n",
       " 'kids': 507,\n",
       " '128514': 508,\n",
       " 'reference': 509,\n",
       " 'course': 510,\n",
       " 'subreddit': 511,\n",
       " 'dont': 512,\n",
       " 'ok': 513,\n",
       " 'heard': 514,\n",
       " 'weeks': 515,\n",
       " 'yourself': 516,\n",
       " '000': 517,\n",
       " 'together': 518,\n",
       " 'front': 519,\n",
       " 'especially': 520,\n",
       " 'war': 521,\n",
       " 'important': 522,\n",
       " 'control': 523,\n",
       " 'picture': 524,\n",
       " 'happen': 525,\n",
       " 'sometimes': 526,\n",
       " 'fine': 527,\n",
       " 'eat': 528,\n",
       " 'parents': 529,\n",
       " 'list': 530,\n",
       " 'rather': 531,\n",
       " 'line': 532,\n",
       " 'law': 533,\n",
       " 'thread': 534,\n",
       " 'opinion': 535,\n",
       " 'release': 536,\n",
       " 'often': 537,\n",
       " 'public': 538,\n",
       " 'works': 539,\n",
       " 'seem': 540,\n",
       " '8216': 541,\n",
       " '50': 542,\n",
       " 'later': 543,\n",
       " 'vote': 544,\n",
       " 'non': 545,\n",
       " 'american': 546,\n",
       " 'matter': 547,\n",
       " 'favorite': 548,\n",
       " 'posts': 549,\n",
       " 'phone': 550,\n",
       " '15': 551,\n",
       " 'wish': 552,\n",
       " 'original': 553,\n",
       " 'deal': 554,\n",
       " 'b': 555,\n",
       " 'hit': 556,\n",
       " 'google': 557,\n",
       " '8212': 558,\n",
       " 'leave': 559,\n",
       " 'win': 560,\n",
       " 'completely': 561,\n",
       " 'im': 562,\n",
       " 'add': 563,\n",
       " 'damn': 564,\n",
       " 'ones': 565,\n",
       " 'example': 566,\n",
       " 'space': 567,\n",
       " 'due': 568,\n",
       " 'cat': 569,\n",
       " 'lost': 570,\n",
       " 'entire': 571,\n",
       " 'fight': 572,\n",
       " 'absolutely': 573,\n",
       " 'kill': 574,\n",
       " 'basically': 575,\n",
       " 'whether': 576,\n",
       " '11': 577,\n",
       " 'image': 578,\n",
       " 'haha': 579,\n",
       " 'history': 580,\n",
       " 'removed': 581,\n",
       " 'early': 582,\n",
       " 'cause': 583,\n",
       " 'easy': 584,\n",
       " 'die': 585,\n",
       " 'room': 586,\n",
       " 'op': 587,\n",
       " 'couldn': 588,\n",
       " 'media': 589,\n",
       " 'become': 590,\n",
       " 'neutrality': 591,\n",
       " 'players': 592,\n",
       " 'death': 593,\n",
       " 'baby': 594,\n",
       " 'men': 595,\n",
       " 'okay': 596,\n",
       " 'minutes': 597,\n",
       " 'similar': 598,\n",
       " 'answer': 599,\n",
       " 'age': 600,\n",
       " 'company': 601,\n",
       " 'child': 602,\n",
       " 'sound': 603,\n",
       " 'gif': 604,\n",
       " 'music': 605,\n",
       " 'huge': 606,\n",
       " 'move': 607,\n",
       " 'whatever': 608,\n",
       " 'results': 609,\n",
       " 'weight': 610,\n",
       " 'dude': 611,\n",
       " 'page': 612,\n",
       " 'city': 613,\n",
       " 'questions': 614,\n",
       " 'pain': 615,\n",
       " 'hell': 616,\n",
       " 'online': 617,\n",
       " 'ban': 618,\n",
       " 'woman': 619,\n",
       " 'community': 620,\n",
       " 'status': 621,\n",
       " 'soon': 622,\n",
       " 'fire': 623,\n",
       " 'month': 624,\n",
       " 'turn': 625,\n",
       " 'class': 626,\n",
       " 'dead': 627,\n",
       " 'red': 628,\n",
       " 'stay': 629,\n",
       " 'quality': 630,\n",
       " 'watching': 631,\n",
       " 'police': 632,\n",
       " 'simply': 633,\n",
       " 'per': 634,\n",
       " 'business': 635,\n",
       " 'president': 636,\n",
       " 'asked': 637,\n",
       " 'general': 638,\n",
       " 'en': 639,\n",
       " 'weird': 640,\n",
       " 'x': 641,\n",
       " 'chance': 642,\n",
       " 'wants': 643,\n",
       " 'research': 644,\n",
       " 'act': 645,\n",
       " 'seeing': 646,\n",
       " 'area': 647,\n",
       " 'higher': 648,\n",
       " 'court': 649,\n",
       " 'song': 650,\n",
       " 'linked': 651,\n",
       " 'reading': 652,\n",
       " 'press': 653,\n",
       " 'dad': 654,\n",
       " 'light': 655,\n",
       " 'rest': 656,\n",
       " 'advice': 657,\n",
       " 'price': 658,\n",
       " 'wife': 659,\n",
       " 'photo': 660,\n",
       " 'shot': 661,\n",
       " 'mine': 662,\n",
       " 'episode': 663,\n",
       " 'project': 664,\n",
       " 'amount': 665,\n",
       " 'sex': 666,\n",
       " 'finally': 667,\n",
       " 'large': 668,\n",
       " 'mods': 669,\n",
       " 'outside': 670,\n",
       " 'shows': 671,\n",
       " 'honestly': 672,\n",
       " 'series': 673,\n",
       " 'funny': 674,\n",
       " 'account': 675,\n",
       " 'taken': 676,\n",
       " 'risk': 677,\n",
       " 'save': 678,\n",
       " 'states': 679,\n",
       " 'given': 680,\n",
       " 'knew': 681,\n",
       " 'mom': 682,\n",
       " 'felt': 683,\n",
       " 'sleep': 684,\n",
       " 'perfect': 685,\n",
       " 'word': 686,\n",
       " 'player': 687,\n",
       " 'issues': 688,\n",
       " 'behind': 689,\n",
       " 'j': 690,\n",
       " 'hot': 691,\n",
       " 'played': 692,\n",
       " 'fan': 693,\n",
       " 'across': 694,\n",
       " 'wow': 695,\n",
       " 'running': 696,\n",
       " 'copy': 697,\n",
       " 'blue': 698,\n",
       " 'cut': 699,\n",
       " 'anyway': 700,\n",
       " 'normal': 701,\n",
       " '2016': 702,\n",
       " 'share': 703,\n",
       " 'uk': 704,\n",
       " 'store': 705,\n",
       " 'feeling': 706,\n",
       " 'current': 707,\n",
       " 'young': 708,\n",
       " 'points': 709,\n",
       " 'abstract': 710,\n",
       " 'kid': 711,\n",
       " 'takes': 712,\n",
       " 'vs': 713,\n",
       " 'short': 714,\n",
       " 'actual': 715,\n",
       " 'g': 716,\n",
       " 'rule': 717,\n",
       " 'although': 718,\n",
       " 'looked': 719,\n",
       " 'recently': 720,\n",
       " 'site': 721,\n",
       " 'xbox': 722,\n",
       " 'bring': 723,\n",
       " 'moment': 724,\n",
       " 'enjoy': 725,\n",
       " 'late': 726,\n",
       " 'f': 727,\n",
       " 'specific': 728,\n",
       " '18': 729,\n",
       " 'happens': 730,\n",
       " 'related': 731,\n",
       " 'stupid': 732,\n",
       " 'version': 733,\n",
       " 'words': 734,\n",
       " 'middle': 735,\n",
       " 'imagine': 736,\n",
       " 'sign': 737,\n",
       " 'bill': 738,\n",
       " 'test': 739,\n",
       " 'along': 740,\n",
       " 'longer': 741,\n",
       " 'poor': 742,\n",
       " 'break': 743,\n",
       " '25': 744,\n",
       " 'tv': 745,\n",
       " 'garfield': 746,\n",
       " 'giving': 747,\n",
       " 'crazy': 748,\n",
       " 'college': 749,\n",
       " 'near': 750,\n",
       " 'unless': 751,\n",
       " 'situation': 752,\n",
       " 'eating': 753,\n",
       " 'million': 754,\n",
       " 'needed': 755,\n",
       " 'learn': 756,\n",
       " 'joke': 757,\n",
       " 'follow': 758,\n",
       " 'living': 759,\n",
       " 'themselves': 760,\n",
       " 'known': 761,\n",
       " 'instagram': 762,\n",
       " 'blood': 763,\n",
       " 'l': 764,\n",
       " 'pick': 765,\n",
       " 'inside': 766,\n",
       " 'totally': 767,\n",
       " 'star': 768,\n",
       " 'evidence': 769,\n",
       " 'common': 770,\n",
       " 'discussion': 771,\n",
       " 'beautiful': 772,\n",
       " 'png': 773,\n",
       " 'album': 774,\n",
       " 'personal': 775,\n",
       " 'serious': 776,\n",
       " 'bought': 777,\n",
       " 'interested': 778,\n",
       " '16': 779,\n",
       " 'currently': 780,\n",
       " 'build': 781,\n",
       " 'plan': 782,\n",
       " 'gave': 783,\n",
       " 'effect': 784,\n",
       " 'killed': 785,\n",
       " 'clear': 786,\n",
       " 'gay': 787,\n",
       " 'pkk': 788,\n",
       " 'character': 789,\n",
       " 'asking': 790,\n",
       " 'worked': 791,\n",
       " 'fast': 792,\n",
       " 'pass': 793,\n",
       " 'relationship': 794,\n",
       " 'local': 795,\n",
       " 'office': 796,\n",
       " 'air': 797,\n",
       " 'league': 798,\n",
       " 'national': 799,\n",
       " 'political': 800,\n",
       " 'several': 801,\n",
       " 'size': 802,\n",
       " 'main': 803,\n",
       " 'gun': 804,\n",
       " 'process': 805,\n",
       " 'brain': 806,\n",
       " 'report': 807,\n",
       " 'difference': 808,\n",
       " 'above': 809,\n",
       " 'sort': 810,\n",
       " 'explain': 811,\n",
       " 'fair': 812,\n",
       " 'major': 813,\n",
       " 'science': 814,\n",
       " 'term': 815,\n",
       " 'certain': 816,\n",
       " 'lose': 817,\n",
       " 'film': 818,\n",
       " 'html': 819,\n",
       " 'boy': 820,\n",
       " '11088': 821,\n",
       " 'eyes': 822,\n",
       " 'strong': 823,\n",
       " 'response': 824,\n",
       " 'anymore': 825,\n",
       " 'himself': 826,\n",
       " 'glad': 827,\n",
       " 'problems': 828,\n",
       " 'obviously': 829,\n",
       " 'associated': 830,\n",
       " 'worst': 831,\n",
       " 'morning': 832,\n",
       " 'knows': 833,\n",
       " 'allowed': 834,\n",
       " 'sell': 835,\n",
       " 'trade': 836,\n",
       " 'congress': 837,\n",
       " 'positive': 838,\n",
       " 'alone': 839,\n",
       " 'fat': 840,\n",
       " 'hold': 841,\n",
       " 'attack': 842,\n",
       " 'card': 843,\n",
       " 'action': 844,\n",
       " 'form': 845,\n",
       " 'credit': 846,\n",
       " 'kinda': 847,\n",
       " 'hour': 848,\n",
       " 'recommend': 849,\n",
       " 'mod': 850,\n",
       " 'starting': 851,\n",
       " 'service': 852,\n",
       " 'decided': 853,\n",
       " '99': 854,\n",
       " 'ass': 855,\n",
       " 'piece': 856,\n",
       " 'special': 857,\n",
       " 'personally': 858,\n",
       " 'effects': 859,\n",
       " 'worse': 860,\n",
       " 'paste': 861,\n",
       " 'average': 862,\n",
       " 'market': 863,\n",
       " 'seriously': 864,\n",
       " 'appreciate': 865,\n",
       " 'heart': 866,\n",
       " 'y': 867,\n",
       " 'dark': 868,\n",
       " 'earth': 869,\n",
       " 'key': 870,\n",
       " 'drop': 871,\n",
       " '160': 872,\n",
       " 'til': 873,\n",
       " 'hi': 874,\n",
       " 'skin': 875,\n",
       " 'event': 876,\n",
       " 'including': 877,\n",
       " 'product': 878,\n",
       " 'wikipedia': 879,\n",
       " 'thoughts': 880,\n",
       " 'simple': 881,\n",
       " 'review': 882,\n",
       " 'nature': 883,\n",
       " 'united': 884,\n",
       " 'luck': 885,\n",
       " 'hands': 886,\n",
       " 'bed': 887,\n",
       " '40': 888,\n",
       " 'ideas': 889,\n",
       " 'send': 890,\n",
       " 'gone': 891,\n",
       " 'value': 892,\n",
       " 'view': 893,\n",
       " 'posting': 894,\n",
       " 'safe': 895,\n",
       " 'extra': 896,\n",
       " 'gives': 897,\n",
       " 'hair': 898,\n",
       " 'attention': 899,\n",
       " 'according': 900,\n",
       " 'drive': 901,\n",
       " 'meme': 902,\n",
       " 'gold': 903,\n",
       " '13': 904,\n",
       " 'info': 905,\n",
       " 'drug': 906,\n",
       " 'mother': 907,\n",
       " 'green': 908,\n",
       " 'except': 909,\n",
       " 'itself': 910,\n",
       " 'k': 911,\n",
       " 'rights': 912,\n",
       " 'race': 913,\n",
       " 'spend': 914,\n",
       " 'ball': 915,\n",
       " 'provide': 916,\n",
       " 'option': 917,\n",
       " 'america': 918,\n",
       " 'iamhoneydill': 919,\n",
       " 'cost': 920,\n",
       " 'app': 921,\n",
       " 'son': 922,\n",
       " 'academic': 923,\n",
       " 'mostly': 924,\n",
       " 'stand': 925,\n",
       " 'force': 926,\n",
       " 'decision': 927,\n",
       " 'avoid': 928,\n",
       " 'available': 929,\n",
       " 'potential': 930,\n",
       " 'create': 931,\n",
       " 'final': 932,\n",
       " 'members': 933,\n",
       " 'energy': 934,\n",
       " 'meant': 935,\n",
       " 'website': 936,\n",
       " '14': 937,\n",
       " 'videos': 938,\n",
       " 'shouldn': 939,\n",
       " 'sad': 940,\n",
       " 'walk': 941,\n",
       " 'fake': 942,\n",
       " 'quick': 943,\n",
       " 'total': 944,\n",
       " 'fit': 945,\n",
       " 'proof': 946,\n",
       " 'lower': 947,\n",
       " 'straight': 948,\n",
       " '24': 949,\n",
       " 'wonder': 950,\n",
       " 'holy': 951,\n",
       " 'ready': 952,\n",
       " 'offers': 953,\n",
       " 'feels': 954,\n",
       " 'king': 955,\n",
       " 'art': 956,\n",
       " 'campaign': 957,\n",
       " 'user': 958,\n",
       " '17': 959,\n",
       " 'countries': 960,\n",
       " 'users': 961,\n",
       " 'compared': 962,\n",
       " 'majority': 963,\n",
       " 'recent': 964,\n",
       " 'park': 965,\n",
       " 'access': 966,\n",
       " 'allow': 967,\n",
       " 'note': 968,\n",
       " 'language': 969,\n",
       " 'clinton': 970,\n",
       " 'correct': 971,\n",
       " 'search': 972,\n",
       " 'south': 973,\n",
       " 'spent': 974,\n",
       " 'changed': 975,\n",
       " 'choice': 976,\n",
       " 'pro': 977,\n",
       " 'damage': 978,\n",
       " 'lead': 979,\n",
       " 'figure': 980,\n",
       " 'paper': 981,\n",
       " 'following': 982,\n",
       " 'plus': 983,\n",
       " 'cannot': 984,\n",
       " 'co': 985,\n",
       " 'turned': 986,\n",
       " '8201': 987,\n",
       " 'multiple': 988,\n",
       " 'four': 989,\n",
       " 'welcome': 990,\n",
       " 'further': 991,\n",
       " 'ability': 992,\n",
       " 'legal': 993,\n",
       " 'official': 994,\n",
       " 'popular': 995,\n",
       " 'expect': 996,\n",
       " 'offer': 997,\n",
       " 'lives': 998,\n",
       " 'upvote': 999,\n",
       " 'added': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightsHistory(callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.log_weights(0)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.log_weights(epoch)\n",
    "        \n",
    "    def log_weights(self, step):\n",
    "        for layer_name in ['LSTM_layer', 'embeddings_layer', 'sparse_feat_dense_layer', 'output_layer']:\n",
    "            try:\n",
    "                layer = self.model.get_layer(layer_name)\n",
    "                experiment.log_histogram_3d(layer.get_weights()[0], \n",
    "                                            name=layer_name,\n",
    "                                   step=step)  \n",
    "            except Exception as e:\n",
    "                logger.debug(\"Logging weights error: \" + str(e) + \"\\n\")\n",
    "                # layer probably does not exist\n",
    "                pass\n",
    "        \n",
    "class FreezeLayer(callbacks.Callback):\n",
    "    def __init__(self, logs={}, patience=5, layer='embeddings_layer', verbose=1, set_to=False):\n",
    "        super(FreezeLayer, self).__init__()\n",
    "        self.freeze_epoch = patience\n",
    "        self.freeze_layer = layer\n",
    "        self.verbose = verbose\n",
    "        self.set_to = set_to\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        if epoch == self.freeze_epoch:\n",
    "            try:\n",
    "                layer = model.get_layer(self.freeze_layer)\n",
    "                old_value = layer.trainable\n",
    "                layer.trainable = self.set_to\n",
    "                model.compile()\n",
    "                if self.verbose:\n",
    "                    logger.debug(\"Setting %s layer from %s to trainable=%s...\\n\" % (layer.name, old_value,\n",
    "                                                                   model.get_layer(self.freeze_layer).trainable))\n",
    "            except Exception as e:\n",
    "                # layer probably does not exist\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, \n",
    "                x_train, y_train, x_test, y_test, \n",
    "                batch_size, epochs, class_weight, start_epoch=0, workers=4,\n",
    "                callback_list = [],\n",
    "                model_path='/tmp/model',\n",
    "               verbose=1):\n",
    "    print('Train...')\n",
    "    experiment.log_parameter('class_weight', class_weight.values())\n",
    "    experiment.log_parameter('callbacks', callbacks)\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs, initial_epoch=start_epoch, \n",
    "              class_weight=class_weight,\n",
    "              validation_data=[x_test, y_test],\n",
    "                        verbose=verbose,\n",
    "#               validation_split=0.3,\n",
    "                       workers=workers,\n",
    "            callbacks = [\n",
    "                callbacks.ModelCheckpoint(filepath='%s_best' % model_path, verbose=1, \n",
    "                                          save_best_only=True),#, save_weights_only=True),\n",
    "                callbacks.EarlyStopping(patience=70), *callback_list\n",
    "            ])\n",
    "    model.save(model_path)#, save_weights_only=True)\n",
    "    experiment.log_parameter('model_path', model_path)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 166 samples, validate on 71 samples\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "Epoch 1/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.6594 - f1_m: 0.2365 - precision_m: 0.1587 - recall_m: 0.6333\n",
      "Epoch 00001: val_loss improved from inf to 9.08400, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 27s 165ms/sample - loss: 8.6837 - f1_m: 0.1971 - precision_m: 0.1322 - recall_m: 0.5278 - val_loss: 9.0840 - val_f1_m: 0.1017 - val_precision_m: 0.0553 - val_recall_m: 0.6667\n",
      "Epoch 2/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.5818 - f1_m: 0.2849 - precision_m: 0.1705 - recall_m: 0.9214\n",
      "Epoch 00002: val_loss improved from 9.08400 to 8.86473, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 142ms/sample - loss: 8.5909 - f1_m: 0.2374 - precision_m: 0.1421 - recall_m: 0.7679 - val_loss: 8.8647 - val_f1_m: 0.1055 - val_precision_m: 0.0575 - val_recall_m: 0.6667\n",
      "Epoch 3/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.5636 - f1_m: 0.2731 - precision_m: 0.1671 - recall_m: 0.8100\n",
      "Epoch 00003: val_loss improved from 8.86473 to 8.69642, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 142ms/sample - loss: 8.5690 - f1_m: 0.2276 - precision_m: 0.1392 - recall_m: 0.6750 - val_loss: 8.6964 - val_f1_m: 0.1280 - val_precision_m: 0.0708 - val_recall_m: 0.6667\n",
      "Epoch 4/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.5635 - f1_m: 0.3300 - precision_m: 0.2352 - recall_m: 0.7167\n",
      "Epoch 00004: val_loss improved from 8.69642 to 8.61148, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 143ms/sample - loss: 8.5617 - f1_m: 0.3416 - precision_m: 0.2377 - recall_m: 0.7639 - val_loss: 8.6115 - val_f1_m: 0.3662 - val_precision_m: 0.2476 - val_recall_m: 1.0000\n",
      "Epoch 5/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.4835 - f1_m: 0.3825 - precision_m: 0.2459 - recall_m: 0.9267\n",
      "Epoch 00005: val_loss did not improve from 8.61148\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 23s 142ms/sample - loss: 8.4804 - f1_m: 0.3854 - precision_m: 0.2466 - recall_m: 0.9389 - val_loss: 8.9206 - val_f1_m: 0.1121 - val_precision_m: 0.0614 - val_recall_m: 0.6667\n",
      "Epoch 6/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.4730 - f1_m: 0.3304 - precision_m: 0.2058 - recall_m: 0.9750\n",
      "Epoch 00006: val_loss did not improve from 8.61148\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 144ms/sample - loss: 8.4675 - f1_m: 0.3587 - precision_m: 0.2270 - recall_m: 0.9792 - val_loss: 8.6328 - val_f1_m: 0.2709 - val_precision_m: 0.1601 - val_recall_m: 1.0000\n",
      "Epoch 7/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.4128 - f1_m: 0.5417 - precision_m: 0.4149 - recall_m: 0.9500\n",
      "Epoch 00007: val_loss improved from 8.61148 to 8.59742, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 26s 156ms/sample - loss: 8.4066 - f1_m: 0.5625 - precision_m: 0.4291 - recall_m: 0.9583 - val_loss: 8.5974 - val_f1_m: 0.1667 - val_precision_m: 0.1111 - val_recall_m: 0.3333\n",
      "Epoch 8/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.3844 - f1_m: 0.4299 - precision_m: 0.2969 - recall_m: 0.9444\n",
      "Epoch 00008: val_loss did not improve from 8.59742\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 146ms/sample - loss: 8.3811 - f1_m: 0.4249 - precision_m: 0.2891 - recall_m: 0.9537 - val_loss: 8.6694 - val_f1_m: 0.2034 - val_precision_m: 0.1214 - val_recall_m: 0.6667\n",
      "Epoch 9/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.3608 - f1_m: 0.5137 - precision_m: 0.4055 - recall_m: 0.7967\n",
      "Epoch 00009: val_loss improved from 8.59742 to 8.59167, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 25s 153ms/sample - loss: 8.3623 - f1_m: 0.5947 - precision_m: 0.5046 - recall_m: 0.8306 - val_loss: 8.5917 - val_f1_m: 0.1826 - val_precision_m: 0.1072 - val_recall_m: 0.6667\n",
      "Epoch 10/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.3186 - f1_m: 0.4796 - precision_m: 0.3300 - recall_m: 0.9214\n",
      "Epoch 00010: val_loss did not improve from 8.59167\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 147ms/sample - loss: 8.3175 - f1_m: 0.4830 - precision_m: 0.3306 - recall_m: 0.9345 - val_loss: 8.6910 - val_f1_m: 0.1449 - val_precision_m: 0.0926 - val_recall_m: 0.3333\n",
      "Epoch 11/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.3116 - f1_m: 0.4646 - precision_m: 0.3180 - recall_m: 0.9267\n",
      "Epoch 00011: val_loss improved from 8.59167 to 8.50955, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 25s 150ms/sample - loss: 8.3030 - f1_m: 0.5205 - precision_m: 0.3761 - recall_m: 0.9389 - val_loss: 8.5095 - val_f1_m: 0.4127 - val_precision_m: 0.2778 - val_recall_m: 1.0000\n",
      "Epoch 12/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.2384 - f1_m: 0.5931 - precision_m: 0.4511 - recall_m: 0.9429\n",
      "Epoch 00012: val_loss improved from 8.50955 to 8.27466, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 25s 152ms/sample - loss: 8.2402 - f1_m: 0.4943 - precision_m: 0.3759 - recall_m: 0.7857 - val_loss: 8.2747 - val_f1_m: 0.5556 - val_precision_m: 0.5556 - val_recall_m: 0.5556\n",
      "Epoch 13/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.2341 - f1_m: 0.6270 - precision_m: 0.5385 - recall_m: 0.9550\n",
      "Epoch 00013: val_loss did not improve from 8.27466\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 146ms/sample - loss: 8.2310 - f1_m: 0.6336 - precision_m: 0.5321 - recall_m: 0.9625 - val_loss: 8.3709 - val_f1_m: 0.4768 - val_precision_m: 0.3241 - val_recall_m: 1.0000\n",
      "Epoch 14/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.2195 - f1_m: 0.6707 - precision_m: 0.6130 - recall_m: 0.8450\n",
      "Epoch 00014: val_loss improved from 8.27466 to 8.23684, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 142ms/sample - loss: 8.2194 - f1_m: 0.7256 - precision_m: 0.6775 - recall_m: 0.8708 - val_loss: 8.2368 - val_f1_m: 0.4040 - val_precision_m: 0.2917 - val_recall_m: 0.6667\n",
      "Epoch 15/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.2162 - f1_m: 0.7386 - precision_m: 0.7155 - recall_m: 0.8714\n",
      "Epoch 00015: val_loss did not improve from 8.23684\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 144ms/sample - loss: 8.2074 - f1_m: 0.7822 - precision_m: 0.7629 - recall_m: 0.8929 - val_loss: 8.9350 - val_f1_m: 0.3645 - val_precision_m: 0.2488 - val_recall_m: 1.0000\n",
      "Epoch 16/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.2641 - f1_m: 0.5105 - precision_m: 0.3797 - recall_m: 0.8267\n",
      "Epoch 00016: val_loss improved from 8.23684 to 8.21487, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 148ms/sample - loss: 8.2604 - f1_m: 0.5365 - precision_m: 0.3997 - recall_m: 0.8556 - val_loss: 8.2149 - val_f1_m: 0.4444 - val_precision_m: 0.3889 - val_recall_m: 0.5556\n",
      "Epoch 17/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/166 [===========================>..] - ETA: 0s - loss: 8.2313 - f1_m: 0.6575 - precision_m: 0.5423 - recall_m: 0.8933\n",
      "Epoch 00017: val_loss did not improve from 8.21487\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 145ms/sample - loss: 8.2277 - f1_m: 0.6590 - precision_m: 0.5353 - recall_m: 0.9111 - val_loss: 8.7551 - val_f1_m: 0.2954 - val_precision_m: 0.1786 - val_recall_m: 1.0000\n",
      "Epoch 18/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.2004 - f1_m: 0.5843 - precision_m: 0.4300 - recall_m: 0.9667\n",
      "Epoch 00018: val_loss improved from 8.21487 to 8.21194, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 23s 141ms/sample - loss: 8.1947 - f1_m: 0.6536 - precision_m: 0.5250 - recall_m: 0.9722 - val_loss: 8.2119 - val_f1_m: 0.4000 - val_precision_m: 0.3056 - val_recall_m: 0.6667\n",
      "Epoch 19/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.0992 - f1_m: 0.8521 - precision_m: 0.7814 - recall_m: 0.9500\n",
      "Epoch 00019: val_loss did not improve from 8.21194\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 146ms/sample - loss: 8.1044 - f1_m: 0.8767 - precision_m: 0.8179 - recall_m: 0.9583 - val_loss: 8.2748 - val_f1_m: 0.3968 - val_precision_m: 0.4000 - val_recall_m: 0.5833\n",
      "Epoch 20/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.1377 - f1_m: 0.7449 - precision_m: 0.6778 - recall_m: 0.9417\n",
      "Epoch 00020: val_loss did not improve from 8.21194\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 144ms/sample - loss: 8.1326 - f1_m: 0.6207 - precision_m: 0.5648 - recall_m: 0.7847 - val_loss: 8.2843 - val_f1_m: 0.3333 - val_precision_m: 0.3000 - val_recall_m: 0.3889\n",
      "Epoch 21/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.1215 - f1_m: 0.8316 - precision_m: 0.8081 - recall_m: 0.8667\n",
      "Epoch 00021: val_loss did not improve from 8.21194\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 146ms/sample - loss: 8.1177 - f1_m: 0.8041 - precision_m: 0.7567 - recall_m: 0.8889 - val_loss: 8.5830 - val_f1_m: 0.3070 - val_precision_m: 0.2000 - val_recall_m: 0.6667\n",
      "Epoch 22/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.1699 - f1_m: 0.5792 - precision_m: 0.4160 - recall_m: 1.0000\n",
      "Epoch 00022: val_loss did not improve from 8.21194\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 144ms/sample - loss: 8.1805 - f1_m: 0.4827 - precision_m: 0.3467 - recall_m: 0.8333 - val_loss: 8.3208 - val_f1_m: 0.3667 - val_precision_m: 0.2540 - val_recall_m: 0.6667\n",
      "Epoch 23/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.2628 - f1_m: 0.7422 - precision_m: 0.8071 - recall_m: 0.8057\n",
      "Epoch 00023: val_loss improved from 8.21194 to 8.12964, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 146ms/sample - loss: 8.2741 - f1_m: 0.7019 - precision_m: 0.8393 - recall_m: 0.7270 - val_loss: 8.1296 - val_f1_m: 0.5524 - val_precision_m: 0.4722 - val_recall_m: 0.6667\n",
      "Epoch 24/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.1692 - f1_m: 0.6740 - precision_m: 0.5780 - recall_m: 0.9000\n",
      "Epoch 00024: val_loss did not improve from 8.12964\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 25s 148ms/sample - loss: 8.1631 - f1_m: 0.7283 - precision_m: 0.6483 - recall_m: 0.9167 - val_loss: 8.6174 - val_f1_m: 0.2889 - val_precision_m: 0.1725 - val_recall_m: 1.0000\n",
      "Epoch 25/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.1955 - f1_m: 0.5245 - precision_m: 0.3727 - recall_m: 1.0000\n",
      "Epoch 00025: val_loss did not improve from 8.12964\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 145ms/sample - loss: 8.1932 - f1_m: 0.5204 - precision_m: 0.3662 - recall_m: 1.0000 - val_loss: 8.4489 - val_f1_m: 0.2889 - val_precision_m: 0.1941 - val_recall_m: 0.6667\n",
      "Epoch 26/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.1050 - f1_m: 0.7578 - precision_m: 0.6136 - recall_m: 1.0000\n",
      "Epoch 00026: val_loss did not improve from 8.12964\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 145ms/sample - loss: 8.1082 - f1_m: 0.7982 - precision_m: 0.6780 - recall_m: 1.0000 - val_loss: 8.1659 - val_f1_m: 0.6667 - val_precision_m: 0.6222 - val_recall_m: 0.8889\n",
      "Epoch 27/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.0481 - f1_m: 0.9247 - precision_m: 0.8733 - recall_m: 1.0000\n",
      "Epoch 00027: val_loss did not improve from 8.12964\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 147ms/sample - loss: 8.0480 - f1_m: 0.7706 - precision_m: 0.7278 - recall_m: 0.8333 - val_loss: 8.2468 - val_f1_m: 0.2771 - val_precision_m: 0.1917 - val_recall_m: 0.5000\n",
      "Epoch 28/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.0404 - f1_m: 0.9096 - precision_m: 0.8767 - recall_m: 0.9500\n",
      "Epoch 00028: val_loss did not improve from 8.12964\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 147ms/sample - loss: 8.0443 - f1_m: 0.7580 - precision_m: 0.7306 - recall_m: 0.7917 - val_loss: 8.2424 - val_f1_m: 0.3452 - val_precision_m: 0.3056 - val_recall_m: 0.5833\n",
      "Epoch 29/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.0351 - f1_m: 0.9264 - precision_m: 0.8981 - recall_m: 0.9600\n",
      "Epoch 00029: val_loss did not improve from 8.12964\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 23s 141ms/sample - loss: 8.0344 - f1_m: 0.9387 - precision_m: 0.9151 - recall_m: 0.9667 - val_loss: 8.4994 - val_f1_m: 0.5619 - val_precision_m: 0.4722 - val_recall_m: 1.0000\n",
      "Epoch 30/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.0627 - f1_m: 0.7827 - precision_m: 0.6548 - recall_m: 1.0000\n",
      "Epoch 00030: val_loss did not improve from 8.12964\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 145ms/sample - loss: 8.0673 - f1_m: 0.6523 - precision_m: 0.5456 - recall_m: 0.8333 - val_loss: 8.2151 - val_f1_m: 0.5556 - val_precision_m: 0.5556 - val_recall_m: 0.8333\n",
      "Epoch 31/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9861 - f1_m: 0.9667 - precision_m: 0.9429 - recall_m: 1.0000\n",
      "Epoch 00031: val_loss did not improve from 8.12964\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 25s 152ms/sample - loss: 7.9844 - f1_m: 0.9722 - precision_m: 0.9524 - recall_m: 1.0000 - val_loss: 8.2001 - val_f1_m: 0.4444 - val_precision_m: 0.4444 - val_recall_m: 0.4444\n",
      "Epoch 32/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.0095 - f1_m: 0.9160 - precision_m: 0.9048 - recall_m: 0.9500\n",
      "Epoch 00032: val_loss did not improve from 8.12964\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 25s 150ms/sample - loss: 8.0334 - f1_m: 0.7634 - precision_m: 0.7540 - recall_m: 0.7917 - val_loss: 8.4590 - val_f1_m: 0.2083 - val_precision_m: 0.1515 - val_recall_m: 0.3333\n",
      "Epoch 33/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.0608 - f1_m: 0.7971 - precision_m: 0.7168 - recall_m: 0.9333\n",
      "Epoch 00033: val_loss improved from 8.12964 to 8.10705, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 23s 141ms/sample - loss: 8.0602 - f1_m: 0.6643 - precision_m: 0.5974 - recall_m: 0.7778 - val_loss: 8.1070 - val_f1_m: 0.6190 - val_precision_m: 0.6667 - val_recall_m: 0.5833\n",
      "Epoch 34/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/166 [===========================>..] - ETA: 0s - loss: 8.0665 - f1_m: 0.9685 - precision_m: 0.9417 - recall_m: 1.0000\n",
      "Epoch 00034: val_loss did not improve from 8.10705\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 144ms/sample - loss: 8.0633 - f1_m: 0.8071 - precision_m: 0.7847 - recall_m: 0.8333 - val_loss: 8.2121 - val_f1_m: 0.3333 - val_precision_m: 0.2222 - val_recall_m: 0.6667\n",
      "Epoch 35/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.0558 - f1_m: 0.8010 - precision_m: 0.7111 - recall_m: 1.0000\n",
      "Epoch 00035: val_loss did not improve from 8.10705\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 143ms/sample - loss: 8.0527 - f1_m: 0.8341 - precision_m: 0.7593 - recall_m: 1.0000 - val_loss: 8.3135 - val_f1_m: 0.2222 - val_precision_m: 0.1667 - val_recall_m: 0.3333\n",
      "Epoch 36/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.0300 - f1_m: 0.8365 - precision_m: 0.7246 - recall_m: 1.0000\n",
      "Epoch 00036: val_loss did not improve from 8.10705\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 23s 142ms/sample - loss: 8.0278 - f1_m: 0.8637 - precision_m: 0.7705 - recall_m: 1.0000 - val_loss: 8.1491 - val_f1_m: 0.4167 - val_precision_m: 0.3810 - val_recall_m: 0.6667\n",
      "Epoch 37/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9877 - f1_m: 0.9714 - precision_m: 0.9500 - recall_m: 1.0000\n",
      "Epoch 00037: val_loss improved from 8.10705 to 8.10280, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 146ms/sample - loss: 7.9858 - f1_m: 0.8095 - precision_m: 0.7917 - recall_m: 0.8333 - val_loss: 8.1028 - val_f1_m: 0.3556 - val_precision_m: 0.3333 - val_recall_m: 0.3889\n",
      "Epoch 38/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9735 - f1_m: 0.9714 - precision_m: 0.9500 - recall_m: 1.0000\n",
      "Epoch 00038: val_loss did not improve from 8.10280\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 23s 140ms/sample - loss: 7.9722 - f1_m: 0.8095 - precision_m: 0.7917 - recall_m: 0.8333 - val_loss: 8.2317 - val_f1_m: 0.5238 - val_precision_m: 0.4000 - val_recall_m: 0.8889\n",
      "Epoch 39/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9980 - f1_m: 0.8605 - precision_m: 0.7855 - recall_m: 1.0000\n",
      "Epoch 00039: val_loss did not improve from 8.10280\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 144ms/sample - loss: 7.9984 - f1_m: 0.7171 - precision_m: 0.6545 - recall_m: 0.8333 - val_loss: 8.1694 - val_f1_m: 0.5722 - val_precision_m: 0.5476 - val_recall_m: 0.8889\n",
      "Epoch 40/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9556 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00040: val_loss improved from 8.10280 to 8.08537, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 142ms/sample - loss: 7.9562 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 8.0854 - val_f1_m: 0.2500 - val_precision_m: 0.3333 - val_recall_m: 0.2000\n",
      "Epoch 41/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9484 - f1_m: 0.9818 - precision_m: 0.9667 - recall_m: 1.0000\n",
      "Epoch 00041: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 144ms/sample - loss: 7.9489 - f1_m: 0.8182 - precision_m: 0.8056 - recall_m: 0.8333 - val_loss: 8.1918 - val_f1_m: 0.3485 - val_precision_m: 0.2540 - val_recall_m: 0.5833\n",
      "Epoch 42/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9443 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00042: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 146ms/sample - loss: 7.9444 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 8.2248 - val_f1_m: 0.4444 - val_precision_m: 0.3333 - val_recall_m: 0.6667\n",
      "Epoch 43/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9372 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00043: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 147ms/sample - loss: 7.9368 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 8.2182 - val_f1_m: 0.6349 - val_precision_m: 0.5667 - val_recall_m: 0.8889\n",
      "Epoch 44/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9346 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00044: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 23s 140ms/sample - loss: 7.9345 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 8.2207 - val_f1_m: 0.4127 - val_precision_m: 0.3333 - val_recall_m: 0.5556\n",
      "Epoch 45/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9326 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00045: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 146ms/sample - loss: 7.9327 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 8.2481 - val_f1_m: 0.3175 - val_precision_m: 0.2333 - val_recall_m: 0.5000\n",
      "Epoch 46/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9314 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00046: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 143ms/sample - loss: 7.9313 - f1_m: 0.8333 - precision_m: 0.8333 - recall_m: 0.8333 - val_loss: 8.2748 - val_f1_m: 0.3833 - val_precision_m: 0.2833 - val_recall_m: 0.6667\n",
      "Epoch 47/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9299 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00047: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 148ms/sample - loss: 7.9299 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 8.2962 - val_f1_m: 0.6508 - val_precision_m: 0.5667 - val_recall_m: 0.8333\n",
      "Epoch 48/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9290 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00048: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 143ms/sample - loss: 7.9289 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 8.3137 - val_f1_m: 0.4333 - val_precision_m: 0.3333 - val_recall_m: 0.6667\n",
      "Epoch 49/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9279 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00049: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 23s 141ms/sample - loss: 7.9283 - f1_m: 0.8333 - precision_m: 0.8333 - recall_m: 0.8333 - val_loss: 8.3260 - val_f1_m: 0.6481 - val_precision_m: 0.5556 - val_recall_m: 0.8889\n",
      "Epoch 50/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9287 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00050: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 143ms/sample - loss: 7.9287 - f1_m: 0.8333 - precision_m: 0.8333 - recall_m: 0.8333 - val_loss: 8.3335 - val_f1_m: 0.3571 - val_precision_m: 0.2667 - val_recall_m: 0.5556\n",
      "Epoch 51/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9278 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00051: val_loss did not improve from 8.08537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 144ms/sample - loss: 7.9278 - f1_m: 0.8333 - precision_m: 0.8333 - recall_m: 0.8333 - val_loss: 8.3240 - val_f1_m: 0.3571 - val_precision_m: 0.2667 - val_recall_m: 0.5556\n",
      "Epoch 52/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9274 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00052: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 145ms/sample - loss: 7.9274 - f1_m: 0.8333 - precision_m: 0.8333 - recall_m: 0.8333 - val_loss: 8.3349 - val_f1_m: 0.3386 - val_precision_m: 0.2619 - val_recall_m: 0.5556\n",
      "Epoch 53/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9278 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00053: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 23s 141ms/sample - loss: 7.9278 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 8.2593 - val_f1_m: 0.2424 - val_precision_m: 0.1905 - val_recall_m: 0.3333\n",
      "Epoch 54/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9289 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00054: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 142ms/sample - loss: 7.9290 - f1_m: 0.8333 - precision_m: 0.8333 - recall_m: 0.8333 - val_loss: 8.2575 - val_f1_m: 0.6190 - val_precision_m: 0.5333 - val_recall_m: 0.8333\n",
      "Epoch 55/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9286 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00055: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 142ms/sample - loss: 7.9287 - f1_m: 0.8333 - precision_m: 0.8333 - recall_m: 0.8333 - val_loss: 8.2433 - val_f1_m: 0.3000 - val_precision_m: 0.2778 - val_recall_m: 0.4444\n",
      "Epoch 56/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9279 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00056: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 145ms/sample - loss: 7.9278 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 8.2501 - val_f1_m: 0.3333 - val_precision_m: 0.2778 - val_recall_m: 0.4444\n",
      "Epoch 57/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9274 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00057: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 143ms/sample - loss: 7.9275 - f1_m: 0.8333 - precision_m: 0.8333 - recall_m: 0.8333 - val_loss: 8.2858 - val_f1_m: 0.3000 - val_precision_m: 0.2500 - val_recall_m: 0.5000\n",
      "Epoch 58/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9271 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00058: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 146ms/sample - loss: 7.9270 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 8.3192 - val_f1_m: 0.3238 - val_precision_m: 0.2778 - val_recall_m: 0.3889\n",
      "Epoch 59/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9260 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00059: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 143ms/sample - loss: 7.9260 - f1_m: 0.8333 - precision_m: 0.8333 - recall_m: 0.8333 - val_loss: 8.3897 - val_f1_m: 0.2857 - val_precision_m: 0.2333 - val_recall_m: 0.3889\n",
      "Epoch 60/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9258 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00060: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 145ms/sample - loss: 7.9257 - f1_m: 0.8333 - precision_m: 0.8333 - recall_m: 0.8333 - val_loss: 8.4199 - val_f1_m: 0.5460 - val_precision_m: 0.4111 - val_recall_m: 0.8333\n",
      "Epoch 61/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9253 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00061: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 143ms/sample - loss: 7.9253 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 8.4244 - val_f1_m: 0.3571 - val_precision_m: 0.2667 - val_recall_m: 0.5556\n",
      "Epoch 62/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9252 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00062: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 147ms/sample - loss: 7.9252 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 8.4250 - val_f1_m: 0.3333 - val_precision_m: 0.2667 - val_recall_m: 0.5833\n",
      "Epoch 63/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9248 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00063: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 144ms/sample - loss: 7.9249 - f1_m: 0.8333 - precision_m: 0.8333 - recall_m: 0.8333 - val_loss: 8.4258 - val_f1_m: 0.3704 - val_precision_m: 0.2778 - val_recall_m: 0.5556\n",
      "Epoch 64/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9247 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00064: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 25s 149ms/sample - loss: 7.9247 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 8.4310 - val_f1_m: 0.4333 - val_precision_m: 0.3556 - val_recall_m: 0.5556\n",
      "Epoch 65/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9247 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00065: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 146ms/sample - loss: 7.9248 - f1_m: 0.8333 - precision_m: 0.8333 - recall_m: 0.8333 - val_loss: 8.4290 - val_f1_m: 0.6349 - val_precision_m: 0.5500 - val_recall_m: 0.8333\n",
      "Epoch 66/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9244 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00066: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 25s 148ms/sample - loss: 7.9244 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 8.3772 - val_f1_m: 0.5556 - val_precision_m: 0.4111 - val_recall_m: 0.8889\n",
      "Epoch 67/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9243 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00067: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 25s 150ms/sample - loss: 7.9243 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 8.3625 - val_f1_m: 0.3889 - val_precision_m: 0.3000 - val_recall_m: 0.5556\n",
      "Epoch 68/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9243 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00068: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 147ms/sample - loss: 7.9243 - f1_m: 0.8333 - precision_m: 0.8333 - recall_m: 0.8333 - val_loss: 8.3752 - val_f1_m: 0.2424 - val_precision_m: 0.2222 - val_recall_m: 0.2667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9242 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00069: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 23s 141ms/sample - loss: 7.9242 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 8.3638 - val_f1_m: 0.3556 - val_precision_m: 0.2778 - val_recall_m: 0.5000\n",
      "Epoch 70/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9241 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00070: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 142ms/sample - loss: 7.9241 - f1_m: 0.8333 - precision_m: 0.8333 - recall_m: 0.8333 - val_loss: 8.3559 - val_f1_m: 0.6667 - val_precision_m: 0.5833 - val_recall_m: 0.8333\n",
      "Epoch 71/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9241 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00071: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 143ms/sample - loss: 7.9240 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 8.3489 - val_f1_m: 0.4722 - val_precision_m: 0.3667 - val_recall_m: 0.6667\n",
      "Epoch 72/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9240 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00072: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 25s 150ms/sample - loss: 7.9240 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 8.3621 - val_f1_m: 0.5333 - val_precision_m: 0.5000 - val_recall_m: 0.5833\n",
      "Epoch 73/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9239 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00073: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 142ms/sample - loss: 7.9239 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 8.3776 - val_f1_m: 0.6667 - val_precision_m: 0.5778 - val_recall_m: 0.8889\n",
      "Epoch 74/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9238 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00074: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 23s 141ms/sample - loss: 7.9238 - f1_m: 0.8333 - precision_m: 0.8333 - recall_m: 0.8333 - val_loss: 8.3801 - val_f1_m: 0.4127 - val_precision_m: 0.3333 - val_recall_m: 0.5556\n",
      "Epoch 75/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9237 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00075: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 146ms/sample - loss: 7.9237 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 8.3662 - val_f1_m: 0.6667 - val_precision_m: 0.6111 - val_recall_m: 0.8333\n",
      "Epoch 76/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9238 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00076: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 146ms/sample - loss: 7.9238 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 8.3741 - val_f1_m: 0.4127 - val_precision_m: 0.3000 - val_recall_m: 0.6667\n",
      "Epoch 77/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9237 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00077: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 25s 148ms/sample - loss: 7.9237 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 8.3934 - val_f1_m: 0.3833 - val_precision_m: 0.2833 - val_recall_m: 0.6667\n",
      "Epoch 78/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9236 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00078: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 25s 149ms/sample - loss: 7.9236 - f1_m: 0.8333 - precision_m: 0.8333 - recall_m: 0.8333 - val_loss: 8.3966 - val_f1_m: 0.3889 - val_precision_m: 0.3000 - val_recall_m: 0.5556\n",
      "Epoch 79/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9236 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00079: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 147ms/sample - loss: 7.9236 - f1_m: 0.8333 - precision_m: 0.8333 - recall_m: 0.8333 - val_loss: 8.3846 - val_f1_m: 0.3704 - val_precision_m: 0.2778 - val_recall_m: 0.5556\n",
      "Epoch 80/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9236 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00080: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 25s 151ms/sample - loss: 7.9236 - f1_m: 0.8333 - precision_m: 0.8333 - recall_m: 0.8333 - val_loss: 8.3678 - val_f1_m: 0.3968 - val_precision_m: 0.3333 - val_recall_m: 0.5000\n",
      "Epoch 81/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9236 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00081: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 142ms/sample - loss: 7.9235 - f1_m: 0.8333 - precision_m: 0.8333 - recall_m: 0.8333 - val_loss: 8.3662 - val_f1_m: 0.3810 - val_precision_m: 0.3167 - val_recall_m: 0.5000\n",
      "Epoch 82/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9235 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00082: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 25s 149ms/sample - loss: 7.9235 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 8.3792 - val_f1_m: 0.6000 - val_precision_m: 0.5278 - val_recall_m: 0.8333\n",
      "Epoch 83/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9235 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00083: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 146ms/sample - loss: 7.9235 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 8.4050 - val_f1_m: 0.5556 - val_precision_m: 0.4444 - val_recall_m: 0.8333\n",
      "Epoch 84/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9234 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00084: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 144ms/sample - loss: 7.9235 - f1_m: 0.8333 - precision_m: 0.8333 - recall_m: 0.8333 - val_loss: 8.3555 - val_f1_m: 0.5222 - val_precision_m: 0.3889 - val_recall_m: 0.8333\n",
      "Epoch 85/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9236 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00085: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 144ms/sample - loss: 7.9236 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 8.1581 - val_f1_m: 0.3571 - val_precision_m: 0.3333 - val_recall_m: 0.3889\n",
      "Epoch 86/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.4578 - f1_m: 0.6441 - precision_m: 0.5783 - recall_m: 0.9500\n",
      "Epoch 00086: val_loss did not improve from 8.08537\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 145ms/sample - loss: 8.5133 - f1_m: 0.5367 - precision_m: 0.4819 - recall_m: 0.7917 - val_loss: 8.3389 - val_f1_m: 0.3238 - val_precision_m: 0.2286 - val_recall_m: 0.5556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.3979 - f1_m: 0.6888 - precision_m: 0.8000 - recall_m: 0.7029\n",
      "Epoch 00087: val_loss improved from 8.08537 to 8.08109, saving model to models/mlp_user_anorex_best\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 147ms/sample - loss: 8.3832 - f1_m: 0.5740 - precision_m: 0.6667 - recall_m: 0.5857 - val_loss: 8.0811 - val_f1_m: 0.4815 - val_precision_m: 0.4667 - val_recall_m: 0.5000\n",
      "Epoch 88/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.2490 - f1_m: 0.8185 - precision_m: 0.8533 - recall_m: 0.8429\n",
      "Epoch 00088: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 143ms/sample - loss: 8.2441 - f1_m: 0.8487 - precision_m: 0.8778 - recall_m: 0.8690 - val_loss: 8.3342 - val_f1_m: 0.4111 - val_precision_m: 0.2611 - val_recall_m: 1.0000\n",
      "Epoch 89/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.1959 - f1_m: 0.5789 - precision_m: 0.4120 - recall_m: 1.0000\n",
      "Epoch 00089: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 145ms/sample - loss: 8.1910 - f1_m: 0.5935 - precision_m: 0.4267 - recall_m: 1.0000 - val_loss: 8.6608 - val_f1_m: 0.2719 - val_precision_m: 0.1578 - val_recall_m: 1.0000\n",
      "Epoch 90/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.1722 - f1_m: 0.5918 - precision_m: 0.4540 - recall_m: 1.0000\n",
      "Epoch 00090: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 142ms/sample - loss: 8.1674 - f1_m: 0.6598 - precision_m: 0.5450 - recall_m: 1.0000 - val_loss: 8.1385 - val_f1_m: 0.8333 - val_precision_m: 0.8000 - val_recall_m: 0.8889\n",
      "Epoch 91/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.1745 - f1_m: 0.9000 - precision_m: 0.9667 - recall_m: 0.8667\n",
      "Epoch 00091: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 144ms/sample - loss: 8.1685 - f1_m: 0.7500 - precision_m: 0.8056 - recall_m: 0.7222 - val_loss: 8.1454 - val_f1_m: 0.2222 - val_precision_m: 0.1905 - val_recall_m: 0.2667\n",
      "Epoch 92/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.0551 - f1_m: 0.8316 - precision_m: 0.7311 - recall_m: 1.0000\n",
      "Epoch 00092: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 144ms/sample - loss: 8.0594 - f1_m: 0.8041 - precision_m: 0.6926 - recall_m: 1.0000 - val_loss: 8.3688 - val_f1_m: 0.2872 - val_precision_m: 0.1833 - val_recall_m: 0.6667\n",
      "Epoch 93/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.0581 - f1_m: 0.7453 - precision_m: 0.6114 - recall_m: 1.0000\n",
      "Epoch 00093: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 145ms/sample - loss: 8.0540 - f1_m: 0.7877 - precision_m: 0.6761 - recall_m: 1.0000 - val_loss: 8.2009 - val_f1_m: 0.5926 - val_precision_m: 0.5111 - val_recall_m: 0.8889\n",
      "Epoch 94/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.0132 - f1_m: 0.9010 - precision_m: 0.8343 - recall_m: 1.0000\n",
      "Epoch 00094: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 143ms/sample - loss: 8.0107 - f1_m: 0.9175 - precision_m: 0.8619 - recall_m: 1.0000 - val_loss: 8.1252 - val_f1_m: 0.4190 - val_precision_m: 0.4167 - val_recall_m: 0.5833\n",
      "Epoch 95/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9932 - f1_m: 0.9581 - precision_m: 0.9250 - recall_m: 1.0000\n",
      "Epoch 00095: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 23s 141ms/sample - loss: 7.9914 - f1_m: 0.9651 - precision_m: 0.9375 - recall_m: 1.0000 - val_loss: 8.1575 - val_f1_m: 0.3667 - val_precision_m: 0.2778 - val_recall_m: 0.5833\n",
      "Epoch 96/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9740 - f1_m: 0.9532 - precision_m: 0.9167 - recall_m: 1.0000\n",
      "Epoch 00096: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 142ms/sample - loss: 7.9785 - f1_m: 0.9055 - precision_m: 0.8472 - recall_m: 1.0000 - val_loss: 8.1413 - val_f1_m: 0.1905 - val_precision_m: 0.1667 - val_recall_m: 0.2222\n",
      "Epoch 97/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.0307 - f1_m: 0.9538 - precision_m: 0.9667 - recall_m: 0.9429\n",
      "Epoch 00097: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 142ms/sample - loss: 8.0419 - f1_m: 0.8782 - precision_m: 0.8611 - recall_m: 0.9524 - val_loss: 8.1244 - val_f1_m: 0.4889 - val_precision_m: 0.3889 - val_recall_m: 0.6667\n",
      "Epoch 98/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.4166 - f1_m: 0.7006 - precision_m: 0.9267 - recall_m: 0.6167\n",
      "Epoch 00098: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 145ms/sample - loss: 8.4506 - f1_m: 0.5838 - precision_m: 0.7722 - recall_m: 0.5139 - val_loss: 8.0897 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 99/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.5821 - f1_m: 0.5367 - precision_m: 0.7000 - recall_m: 0.5057\n",
      "Epoch 00099: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 144ms/sample - loss: 8.5675 - f1_m: 0.4472 - precision_m: 0.5833 - recall_m: 0.4214 - val_loss: 8.2950 - val_f1_m: 0.3556 - val_precision_m: 0.3056 - val_recall_m: 0.5556\n",
      "Epoch 100/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.2817 - f1_m: 0.5632 - precision_m: 0.4450 - recall_m: 0.8914\n",
      "Epoch 00100: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 23s 141ms/sample - loss: 8.2724 - f1_m: 0.6122 - precision_m: 0.4958 - recall_m: 0.9095 - val_loss: 8.7150 - val_f1_m: 0.1635 - val_precision_m: 0.0939 - val_recall_m: 0.6667\n",
      "Epoch 101/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.2774 - f1_m: 0.4480 - precision_m: 0.2909 - recall_m: 1.0000\n",
      "Epoch 00101: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 23s 141ms/sample - loss: 8.2695 - f1_m: 0.5067 - precision_m: 0.3535 - recall_m: 1.0000 - val_loss: 8.8025 - val_f1_m: 0.2323 - val_precision_m: 0.1333 - val_recall_m: 1.0000\n",
      "Epoch 102/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.2692 - f1_m: 0.4497 - precision_m: 0.3017 - recall_m: 1.0000\n",
      "Epoch 00102: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 145ms/sample - loss: 8.2700 - f1_m: 0.3748 - precision_m: 0.2514 - recall_m: 0.8333 - val_loss: 8.5956 - val_f1_m: 0.3284 - val_precision_m: 0.2032 - val_recall_m: 1.0000\n",
      "Epoch 103/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.2110 - f1_m: 0.6055 - precision_m: 0.4713 - recall_m: 0.9600\n",
      "Epoch 00103: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 145ms/sample - loss: 8.2164 - f1_m: 0.5046 - precision_m: 0.3928 - recall_m: 0.8000 - val_loss: 8.3290 - val_f1_m: 0.5265 - val_precision_m: 0.4028 - val_recall_m: 1.0000\n",
      "Epoch 104/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.1762 - f1_m: 0.7250 - precision_m: 0.5944 - recall_m: 0.9750\n",
      "Epoch 00104: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166/166 [==============================] - 24s 144ms/sample - loss: 8.1733 - f1_m: 0.6042 - precision_m: 0.4953 - recall_m: 0.8125 - val_loss: 8.2126 - val_f1_m: 0.3571 - val_precision_m: 0.2667 - val_recall_m: 0.5556\n",
      "Epoch 105/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.1863 - f1_m: 0.8323 - precision_m: 0.7562 - recall_m: 0.9444\n",
      "Epoch 00105: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 25s 148ms/sample - loss: 8.1788 - f1_m: 0.6936 - precision_m: 0.6302 - recall_m: 0.7870 - val_loss: 8.1961 - val_f1_m: 0.3889 - val_precision_m: 0.2778 - val_recall_m: 0.6667\n",
      "Epoch 106/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.1786 - f1_m: 0.7302 - precision_m: 0.6550 - recall_m: 0.9083\n",
      "Epoch 00106: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 147ms/sample - loss: 8.1733 - f1_m: 0.7752 - precision_m: 0.7125 - recall_m: 0.9236 - val_loss: 8.2244 - val_f1_m: 0.6190 - val_precision_m: 0.5556 - val_recall_m: 0.8889\n",
      "Epoch 107/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.1206 - f1_m: 0.7612 - precision_m: 0.6477 - recall_m: 0.9500\n",
      "Epoch 00107: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 147ms/sample - loss: 8.1246 - f1_m: 0.6343 - precision_m: 0.5398 - recall_m: 0.7917 - val_loss: 8.2900 - val_f1_m: 0.4481 - val_precision_m: 0.3175 - val_recall_m: 0.8333\n",
      "Epoch 108/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.0961 - f1_m: 0.7768 - precision_m: 0.6418 - recall_m: 1.0000\n",
      "Epoch 00108: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 146ms/sample - loss: 8.1000 - f1_m: 0.6473 - precision_m: 0.5349 - recall_m: 0.8333 - val_loss: 8.2610 - val_f1_m: 0.4815 - val_precision_m: 0.4952 - val_recall_m: 0.8333\n",
      "Epoch 109/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.0862 - f1_m: 0.7826 - precision_m: 0.6500 - recall_m: 1.0000\n",
      "Epoch 00109: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 23s 140ms/sample - loss: 8.0819 - f1_m: 0.8188 - precision_m: 0.7083 - recall_m: 1.0000 - val_loss: 8.2033 - val_f1_m: 0.4127 - val_precision_m: 0.3556 - val_recall_m: 0.5556\n",
      "Epoch 110/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.0773 - f1_m: 0.8160 - precision_m: 0.7094 - recall_m: 1.0000\n",
      "Epoch 00110: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 25s 149ms/sample - loss: 8.0743 - f1_m: 0.8466 - precision_m: 0.7579 - recall_m: 1.0000 - val_loss: 8.1886 - val_f1_m: 0.2424 - val_precision_m: 0.2222 - val_recall_m: 0.2667\n",
      "Epoch 111/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.0514 - f1_m: 0.8835 - precision_m: 0.8071 - recall_m: 1.0000\n",
      "Epoch 00111: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 144ms/sample - loss: 8.0538 - f1_m: 0.7363 - precision_m: 0.6726 - recall_m: 0.8333 - val_loss: 8.1705 - val_f1_m: 0.3333 - val_precision_m: 0.2667 - val_recall_m: 0.5833\n",
      "Epoch 112/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.0437 - f1_m: 0.8974 - precision_m: 0.8273 - recall_m: 1.0000\n",
      "Epoch 00112: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 147ms/sample - loss: 8.0463 - f1_m: 0.7478 - precision_m: 0.6894 - recall_m: 0.8333 - val_loss: 8.1512 - val_f1_m: 0.3333 - val_precision_m: 0.2444 - val_recall_m: 0.5556\n",
      "Epoch 113/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.0197 - f1_m: 0.9560 - precision_m: 0.9214 - recall_m: 1.0000\n",
      "Epoch 00113: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 25s 148ms/sample - loss: 8.0204 - f1_m: 0.9634 - precision_m: 0.9345 - recall_m: 1.0000 - val_loss: 8.1473 - val_f1_m: 0.5952 - val_precision_m: 0.5111 - val_recall_m: 0.8333\n",
      "Epoch 114/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 8.0036 - f1_m: 0.9600 - precision_m: 0.9333 - recall_m: 1.0000\n",
      "Epoch 00114: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 147ms/sample - loss: 8.0099 - f1_m: 0.9111 - precision_m: 0.8611 - recall_m: 1.0000 - val_loss: 8.1734 - val_f1_m: 0.3152 - val_precision_m: 0.2262 - val_recall_m: 0.5833\n",
      "Epoch 115/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9998 - f1_m: 0.9532 - precision_m: 0.9167 - recall_m: 1.0000\n",
      "Epoch 00115: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 145ms/sample - loss: 7.9975 - f1_m: 0.9610 - precision_m: 0.9306 - recall_m: 1.0000 - val_loss: 8.1721 - val_f1_m: 0.6111 - val_precision_m: 0.5278 - val_recall_m: 0.8333\n",
      "Epoch 116/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9989 - f1_m: 0.9314 - precision_m: 0.8833 - recall_m: 1.0000\n",
      "Epoch 00116: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 147ms/sample - loss: 8.0001 - f1_m: 0.9429 - precision_m: 0.9028 - recall_m: 1.0000 - val_loss: 8.1682 - val_f1_m: 0.5222 - val_precision_m: 0.3833 - val_recall_m: 0.8889\n",
      "Epoch 117/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9800 - f1_m: 0.9685 - precision_m: 0.9417 - recall_m: 1.0000\n",
      "Epoch 00117: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 147ms/sample - loss: 7.9795 - f1_m: 0.9737 - precision_m: 0.9514 - recall_m: 1.0000 - val_loss: 8.1713 - val_f1_m: 0.3452 - val_precision_m: 0.2667 - val_recall_m: 0.5000\n",
      "Epoch 118/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9778 - f1_m: 0.9378 - precision_m: 0.8933 - recall_m: 1.0000\n",
      "Epoch 00118: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 145ms/sample - loss: 7.9772 - f1_m: 0.9481 - precision_m: 0.9111 - recall_m: 1.0000 - val_loss: 8.1787 - val_f1_m: 0.5079 - val_precision_m: 0.3889 - val_recall_m: 0.8889\n",
      "Epoch 119/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9692 - f1_m: 0.9532 - precision_m: 0.9167 - recall_m: 1.0000\n",
      "Epoch 00119: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 145ms/sample - loss: 7.9717 - f1_m: 0.9610 - precision_m: 0.9306 - recall_m: 1.0000 - val_loss: 8.1852 - val_f1_m: 0.4040 - val_precision_m: 0.2917 - val_recall_m: 0.6667\n",
      "Epoch 120/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9686 - f1_m: 0.9560 - precision_m: 0.9214 - recall_m: 1.0000\n",
      "Epoch 00120: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 25s 149ms/sample - loss: 7.9674 - f1_m: 0.9634 - precision_m: 0.9345 - recall_m: 1.0000 - val_loss: 8.1982 - val_f1_m: 0.3148 - val_precision_m: 0.2222 - val_recall_m: 0.5556\n",
      "Epoch 121/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9631 - f1_m: 0.9624 - precision_m: 0.9314 - recall_m: 1.0000\n",
      "Epoch 00121: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 142ms/sample - loss: 7.9636 - f1_m: 0.9687 - precision_m: 0.9429 - recall_m: 1.0000 - val_loss: 8.1916 - val_f1_m: 0.5833 - val_precision_m: 0.5143 - val_recall_m: 0.8889\n",
      "Epoch 122/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9585 - f1_m: 0.9867 - precision_m: 0.9750 - recall_m: 1.0000\n",
      "Epoch 00122: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 24s 143ms/sample - loss: 7.9582 - f1_m: 0.8222 - precision_m: 0.8125 - recall_m: 0.8333 - val_loss: 8.1901 - val_f1_m: 0.5407 - val_precision_m: 0.4861 - val_recall_m: 0.8333\n",
      "Epoch 123/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9546 - f1_m: 0.9600 - precision_m: 0.9333 - recall_m: 1.0000\n",
      "Epoch 00123: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 26s 155ms/sample - loss: 7.9584 - f1_m: 0.9667 - precision_m: 0.9444 - recall_m: 1.0000 - val_loss: 8.1954 - val_f1_m: 0.2952 - val_precision_m: 0.2095 - val_recall_m: 0.5000\n",
      "Epoch 124/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9547 - f1_m: 0.9749 - precision_m: 0.9528 - recall_m: 1.0000\n",
      "Epoch 00124: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 25s 149ms/sample - loss: 7.9543 - f1_m: 0.8124 - precision_m: 0.7940 - recall_m: 0.8333 - val_loss: 8.2220 - val_f1_m: 0.2222 - val_precision_m: 0.1905 - val_recall_m: 0.2667\n",
      "Epoch 125/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9535 - f1_m: 0.9624 - precision_m: 0.9314 - recall_m: 1.0000\n",
      "Epoch 00125: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 25s 151ms/sample - loss: 7.9527 - f1_m: 0.9687 - precision_m: 0.9429 - recall_m: 1.0000 - val_loss: 8.2153 - val_f1_m: 0.3238 - val_precision_m: 0.2500 - val_recall_m: 0.5556\n",
      "Epoch 126/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9470 - f1_m: 0.9778 - precision_m: 0.9600 - recall_m: 1.0000\n",
      "Epoch 00126: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 31s 185ms/sample - loss: 7.9469 - f1_m: 0.9815 - precision_m: 0.9667 - recall_m: 1.0000 - val_loss: 8.2107 - val_f1_m: 0.5741 - val_precision_m: 0.5000 - val_recall_m: 0.7778\n",
      "Epoch 127/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9456 - f1_m: 0.9778 - precision_m: 0.9600 - recall_m: 1.0000\n",
      "Epoch 00127: val_loss did not improve from 8.08109\n",
      "Logging weights error: No such layer: sparse_feat_dense_layer\n",
      "\n",
      "166/166 [==============================] - 26s 156ms/sample - loss: 7.9452 - f1_m: 0.8148 - precision_m: 0.8000 - recall_m: 0.8333 - val_loss: 8.2034 - val_f1_m: 0.1667 - val_precision_m: 0.1111 - val_recall_m: 0.3333\n",
      "Epoch 128/300\n",
      "160/166 [===========================>..] - ETA: 0s - loss: 7.9430 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000\n",
      "Epoch 00128: val_loss did not improve from 8.08109\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-99-d63e84c48579>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, x_train, y_train, x_test, y_test, batch_size, epochs, class_weight, start_epoch, workers, callback_list, model_path, verbose)\u001b[0m\n\u001b[1;32m     20\u001b[0m                 callbacks.ModelCheckpoint(filepath='%s_best' % model_path, verbose=1, \n\u001b[1;32m     21\u001b[0m                                           save_best_only=True),#, save_weights_only=True),\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcallback_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             ])\n\u001b[1;32m     24\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, save_weights_only=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/comet_ml/monkey_patching.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m                     )\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Call after callbacks once we have the return value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m           training_utils.make_logs(model, val_results, mode, prefix='val_'))\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m     \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m   \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs, mode)\u001b[0m\n\u001b[1;32m    249\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-98-18d390ece966>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-98-18d390ece966>\u001b[0m in \u001b[0;36mlog_weights\u001b[0;34m(self, step)\u001b[0m\n\u001b[1;32m     12\u001b[0m                 experiment.log_histogram_3d(layer.get_weights()[0], \n\u001b[1;32m     13\u001b[0m                                             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                                    step=step)  \n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Logging weights error: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/comet_ml/experiment.py\u001b[0m in \u001b[0;36mlog_histogram_3d\u001b[0;34m(self, values, name, step, **kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m             \u001b[0mhistogram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m             \u001b[0mhistogram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"histogram_3d.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/comet_ml/utils.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, values, counts, max_skip_count)\u001b[0m\n\u001b[1;32m   1065\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcounts_compressed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "freeze_layer = FreezeLayer(hyperparams['freeze_patience'], set_to=not hyperparams['trainable_embeddings'])\n",
    "weights_history = WeightsHistory()\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=hyperparams['reduce_lr_factor'],\n",
    "                          patience=hyperparams['reduce_lr_patience'], min_lr=0.000001, verbose=1)\n",
    "\n",
    "model, history = train_model(model, x_train, y_train, x_valid, y_valid,\n",
    "           epochs=300, batch_size=hyperparams['batch_size'],\n",
    "                      class_weight={0:0.5, 1:5}, start_epoch=0,\n",
    "                      callback_list = [freeze_layer, weights_history, reduce_lr],\n",
    "                      model_path='models/mlp_user_anorex', workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.13346808e-01],\n",
       "        [ 1.49938107e-01],\n",
       "        [-4.02613357e-02],\n",
       "        [-4.96538496e-03],\n",
       "        [-1.31627843e-01],\n",
       "        [ 1.80921361e-01],\n",
       "        [ 5.91040961e-02],\n",
       "        [ 8.27158988e-02],\n",
       "        [-1.11491986e-01],\n",
       "        [-9.24129561e-02],\n",
       "        [ 1.72996402e-01],\n",
       "        [ 1.82813600e-01],\n",
       "        [-3.86944227e-02],\n",
       "        [ 3.28002982e-02],\n",
       "        [ 6.13929220e-02],\n",
       "        [ 1.47554502e-01],\n",
       "        [ 1.34893045e-01],\n",
       "        [ 1.24246009e-01],\n",
       "        [-1.52474254e-01],\n",
       "        [ 7.69707561e-02],\n",
       "        [ 5.58837987e-02],\n",
       "        [ 5.96134402e-02],\n",
       "        [-6.24312973e-03],\n",
       "        [-1.55915285e-03],\n",
       "        [ 8.63053873e-02],\n",
       "        [ 4.75549996e-02],\n",
       "        [-7.87871256e-02],\n",
       "        [-1.49628788e-01],\n",
       "        [ 8.46894681e-02],\n",
       "        [ 8.07042792e-02],\n",
       "        [ 1.17896684e-01],\n",
       "        [ 1.13564476e-01],\n",
       "        [-3.49921137e-02],\n",
       "        [ 1.46593889e-02],\n",
       "        [-3.47690051e-03],\n",
       "        [ 4.25638892e-02],\n",
       "        [-9.71212089e-02],\n",
       "        [ 5.64015321e-02],\n",
       "        [ 7.16500357e-02],\n",
       "        [-1.08462475e-01],\n",
       "        [ 1.33519862e-02],\n",
       "        [ 2.22153589e-03],\n",
       "        [-4.60857302e-02],\n",
       "        [ 6.66336864e-02],\n",
       "        [-4.60681356e-02],\n",
       "        [ 8.37811008e-02],\n",
       "        [-1.95291251e-01],\n",
       "        [ 9.98888165e-02],\n",
       "        [-1.25741586e-02],\n",
       "        [ 6.38722163e-03],\n",
       "        [ 3.69350053e-03],\n",
       "        [-6.87789768e-02],\n",
       "        [ 2.13629276e-01],\n",
       "        [-1.95351336e-02],\n",
       "        [-5.37343025e-02],\n",
       "        [ 2.23885521e-01],\n",
       "        [ 1.48967400e-01],\n",
       "        [-1.25706531e-02],\n",
       "        [ 4.98889163e-02],\n",
       "        [-4.89821509e-02],\n",
       "        [ 3.96624170e-02],\n",
       "        [ 1.49418265e-01],\n",
       "        [ 4.33165729e-02],\n",
       "        [ 2.60219909e-02],\n",
       "        [ 1.35336146e-01],\n",
       "        [ 1.49108022e-01],\n",
       "        [-6.28161877e-02],\n",
       "        [-4.51554768e-02],\n",
       "        [-1.20844632e-01],\n",
       "        [ 1.26302913e-01],\n",
       "        [ 2.25514313e-03],\n",
       "        [ 6.29298240e-02],\n",
       "        [ 1.62328392e-01],\n",
       "        [ 1.90966316e-02],\n",
       "        [-1.42699433e-02],\n",
       "        [ 1.19457603e-01],\n",
       "        [ 1.05168574e-01],\n",
       "        [-8.66559371e-02],\n",
       "        [ 6.24882206e-02],\n",
       "        [ 1.31257884e-02],\n",
       "        [ 6.11451901e-02],\n",
       "        [-1.36410519e-01],\n",
       "        [-1.56491458e-01],\n",
       "        [-9.77962166e-02],\n",
       "        [ 2.19149898e-09],\n",
       "        [-3.16750421e-03],\n",
       "        [ 1.24134047e-09],\n",
       "        [ 1.87178713e-07],\n",
       "        [ 1.57135278e-01],\n",
       "        [ 1.06525980e-01],\n",
       "        [-2.26811338e-02],\n",
       "        [ 4.18008827e-02],\n",
       "        [-1.62558272e-01],\n",
       "        [ 1.64229590e-02],\n",
       "        [-9.99655724e-02],\n",
       "        [ 1.05370432e-01],\n",
       "        [ 8.38235393e-02],\n",
       "        [ 2.94409547e-04],\n",
       "        [ 1.86582226e-02],\n",
       "        [-6.33901507e-02],\n",
       "        [-3.20388302e-02],\n",
       "        [-2.14377176e-02],\n",
       "        [-1.77424497e-09],\n",
       "        [-8.61299336e-02],\n",
       "        [ 8.12208056e-02],\n",
       "        [ 1.12673894e-01],\n",
       "        [-4.34028581e-02],\n",
       "        [-1.59247026e-01],\n",
       "        [-1.63344234e-01],\n",
       "        [-4.84324880e-02],\n",
       "        [-2.22532614e-03],\n",
       "        [-5.52749820e-02],\n",
       "        [-9.28694233e-02],\n",
       "        [-2.63031006e-01],\n",
       "        [-1.20948873e-01],\n",
       "        [ 3.01445127e-02],\n",
       "        [-9.72760206e-10],\n",
       "        [ 5.28047495e-02],\n",
       "        [ 1.81494921e-01],\n",
       "        [-1.19286459e-02],\n",
       "        [ 9.45564359e-02],\n",
       "        [ 1.42100817e-02],\n",
       "        [-1.22201331e-01],\n",
       "        [ 9.81219113e-02],\n",
       "        [ 1.26256391e-01],\n",
       "        [ 1.07745171e-01],\n",
       "        [-3.81029248e-02],\n",
       "        [-4.60895933e-02],\n",
       "        [-9.63097066e-02],\n",
       "        [ 3.01120859e-02],\n",
       "        [ 4.29925211e-02],\n",
       "        [ 1.38276611e-02],\n",
       "        [-1.64101794e-02],\n",
       "        [-3.34986970e-02],\n",
       "        [ 2.18760651e-02],\n",
       "        [ 1.43932253e-01],\n",
       "        [-6.37985989e-02],\n",
       "        [-7.48375878e-02],\n",
       "        [ 1.57065034e-01],\n",
       "        [-8.44406243e-03],\n",
       "        [-2.66874004e-02],\n",
       "        [-1.36280432e-02],\n",
       "        [-3.22498865e-02],\n",
       "        [-1.30448103e-01],\n",
       "        [-8.58247504e-02],\n",
       "        [ 1.56724993e-02],\n",
       "        [ 5.79428300e-03],\n",
       "        [-1.66645441e-02],\n",
       "        [-1.35751560e-01],\n",
       "        [ 7.85717368e-02],\n",
       "        [-3.83953825e-02],\n",
       "        [ 1.39426947e-01],\n",
       "        [ 1.19833298e-01],\n",
       "        [ 5.48574738e-02],\n",
       "        [-8.08766186e-02],\n",
       "        [-9.81495380e-02],\n",
       "        [-7.93948919e-02],\n",
       "        [-7.93120936e-02],\n",
       "        [ 4.44845222e-02],\n",
       "        [-1.19267583e-01],\n",
       "        [-1.21423520e-01],\n",
       "        [ 8.95863548e-02],\n",
       "        [-1.50607675e-01],\n",
       "        [-7.24039674e-02],\n",
       "        [ 1.49001375e-01],\n",
       "        [ 1.61926791e-01],\n",
       "        [-5.12878671e-02],\n",
       "        [ 9.21329707e-02],\n",
       "        [ 3.87384966e-02],\n",
       "        [ 1.34020120e-01],\n",
       "        [-4.48516123e-02],\n",
       "        [ 8.93616769e-03],\n",
       "        [-8.65443498e-02],\n",
       "        [ 8.69182963e-03],\n",
       "        [-2.29907501e-02],\n",
       "        [ 4.52838317e-02],\n",
       "        [-1.03498317e-01],\n",
       "        [-7.38420188e-02],\n",
       "        [-1.05392337e-01],\n",
       "        [-8.71607959e-02],\n",
       "        [-1.29405066e-01],\n",
       "        [-6.22687163e-04],\n",
       "        [-6.15527406e-02],\n",
       "        [-1.60848033e-02],\n",
       "        [-1.85059816e-01],\n",
       "        [ 8.76245871e-02],\n",
       "        [ 3.24902590e-03],\n",
       "        [ 3.89152467e-02],\n",
       "        [-4.65949364e-02],\n",
       "        [ 2.89901346e-02],\n",
       "        [ 1.12154074e-01],\n",
       "        [ 8.04428849e-03],\n",
       "        [ 1.06879473e-01],\n",
       "        [ 1.22096203e-01],\n",
       "        [ 1.14465535e-01],\n",
       "        [-8.97411108e-02],\n",
       "        [-1.64673496e-02],\n",
       "        [-1.21778389e-02],\n",
       "        [-7.99962413e-03],\n",
       "        [-7.13739768e-02],\n",
       "        [ 1.43412411e-01],\n",
       "        [-1.15511611e-01],\n",
       "        [ 9.38293859e-02],\n",
       "        [ 1.31361008e-01],\n",
       "        [ 1.71429515e-01],\n",
       "        [ 5.76935224e-02],\n",
       "        [-4.19502631e-02],\n",
       "        [-1.00140972e-03],\n",
       "        [-8.76636580e-02],\n",
       "        [-7.01169716e-04],\n",
       "        [-1.31890355e-02],\n",
       "        [-4.91886698e-02],\n",
       "        [ 1.91561922e-01],\n",
       "        [-1.06010683e-01],\n",
       "        [-1.91037744e-01],\n",
       "        [-1.35131210e-01],\n",
       "        [-1.10110253e-04],\n",
       "        [ 3.67756374e-02],\n",
       "        [-1.95506677e-01],\n",
       "        [ 1.17719762e-01],\n",
       "        [ 3.41542838e-09],\n",
       "        [ 3.96404006e-02],\n",
       "        [ 1.57728337e-08],\n",
       "        [-1.95929743e-02],\n",
       "        [-4.16693391e-09],\n",
       "        [-5.26794456e-02],\n",
       "        [-1.67643908e-08],\n",
       "        [ 1.46674201e-01],\n",
       "        [ 1.73045875e-04],\n",
       "        [-2.32114956e-01],\n",
       "        [ 4.92477259e-09],\n",
       "        [ 1.42844573e-01],\n",
       "        [-2.44983193e-03],\n",
       "        [-8.66675377e-02],\n",
       "        [ 1.48491585e-03],\n",
       "        [-1.69914335e-01],\n",
       "        [ 1.74436465e-08],\n",
       "        [-2.96116687e-09],\n",
       "        [-6.88543427e-04],\n",
       "        [-5.39601974e-10],\n",
       "        [ 8.70251260e-10],\n",
       "        [ 1.10177777e-03],\n",
       "        [-2.23759837e-08],\n",
       "        [ 5.69516256e-09],\n",
       "        [-2.07367558e-02],\n",
       "        [-1.36672296e-09],\n",
       "        [ 5.27007803e-02],\n",
       "        [-1.79660802e-07],\n",
       "        [-8.14630240e-02],\n",
       "        [-1.05777499e-03],\n",
       "        [ 1.15063593e-01],\n",
       "        [ 1.10638132e-09],\n",
       "        [ 4.24930751e-02],\n",
       "        [-1.67647549e-08]], dtype=float32), array([0.00897142], dtype=float32)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='models/lstm_plus4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_class = Metrics(threshold=hyperparams['threshold'])\n",
    "dependencies = {\n",
    "    'f1_m': metrics_class.f1_m,\n",
    "    'precision_m': metrics_class.precision_m,\n",
    "    'recall_m': metrics_class.recall_m,\n",
    "    'binary_crossentropy_custom': binary_crossentropy_custom\n",
    "}\n",
    "model = load_model('models/mlp_user_anorex_best', custom_objects=dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 1s 7ms/sample - loss: 8.3864 - f1_m: 0.3464 - precision_m: 0.3250 - recall_m: 0.4000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8.386428258951428, 0.3464285, 0.325, 0.4]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179 179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('himself', 'stopword'), -0.7284296154975891),\n",
       " (('against', 'stopword'), -0.677932858467102),\n",
       " (('themselves', 'stopword'), -0.6269885897636414),\n",
       " (('myself', 'stopword'), 0.5944074988365173),\n",
       " (('me', 'stopword'), 0.5808129906654358),\n",
       " (('which', 'stopword'), -0.5802603363990784),\n",
       " (('y', 'stopword'), -0.4890809953212738),\n",
       " (('yours', 'stopword'), -0.45802047848701477),\n",
       " (('shouldn', 'stopword'), -0.44473156332969666),\n",
       " (('herself', 'stopword'), -0.44048914313316345),\n",
       " (('ain', 'stopword'), 0.43934911489486694),\n",
       " (('as', 'stopword'), -0.42869824171066284),\n",
       " (('or', 'stopword'), 0.42340654134750366),\n",
       " (('by', 'stopword'), -0.4231763184070587),\n",
       " (('hasn', 'stopword'), -0.4093189239501953),\n",
       " (('from', 'stopword'), -0.40467941761016846),\n",
       " (('during', 'stopword'), -0.40312299132347107),\n",
       " (('no', 'stopword'), 0.39973184466362),\n",
       " (('am', 'stopword'), 0.39801648259162903),\n",
       " (('re', 'stopword'), -0.39346882700920105),\n",
       " (('once', 'stopword'), -0.3780168890953064),\n",
       " (('below', 'stopword'), -0.3677508234977722),\n",
       " (('i', 'stopword'), 0.36311987042427063),\n",
       " (('don', 'stopword'), 0.3617483377456665),\n",
       " (('haven', 'stopword'), 0.3546900451183319),\n",
       " (('my', 'stopword'), 0.33312052488327026),\n",
       " (('itself', 'stopword'), -0.32148054242134094),\n",
       " (('ma', 'stopword'), -0.3166116774082184),\n",
       " (('where', 'stopword'), -0.3089277744293213),\n",
       " (('isn', 'stopword'), 0.2894366383552551),\n",
       " (('our', 'stopword'), -0.27598702907562256),\n",
       " (('but', 'stopword'), 0.2721453905105591),\n",
       " (('their', 'stopword'), -0.2711140513420105),\n",
       " (('will', 'stopword'), -0.267701119184494),\n",
       " (('couldn', 'stopword'), -0.26473215222358704),\n",
       " (('above', 'stopword'), -0.26369601488113403),\n",
       " (('has', 'stopword'), -0.26124489307403564),\n",
       " (('too', 'stopword'), 0.26051756739616394),\n",
       " (('he', 'stopword'), -0.2553723156452179),\n",
       " (('doesn', 'stopword'), -0.24496737122535706),\n",
       " (('being', 'stopword'), 0.2437523603439331),\n",
       " (('those', 'stopword'), -0.23140552639961243),\n",
       " (('had', 'stopword'), 0.2304789274930954),\n",
       " (('few', 'stopword'), 0.22835423052310944),\n",
       " (('in', 'stopword'), -0.22744475305080414),\n",
       " (('such', 'stopword'), 0.22520944476127625),\n",
       " (('been', 'stopword'), 0.21875424683094025),\n",
       " (('having', 'stopword'), 0.21869756281375885),\n",
       " (('wouldn', 'stopword'), -0.21862567961215973),\n",
       " (('before', 'stopword'), 0.21012906730175018),\n",
       " (('we', 'stopword'), -0.20108430087566376),\n",
       " (('ourselves', 'stopword'), -0.19850486516952515),\n",
       " (('there', 'stopword'), -0.19686132669448853),\n",
       " (('other', 'stopword'), 0.19673803448677063),\n",
       " (('some', 'stopword'), 0.18504102528095245),\n",
       " (('so', 'stopword'), 0.18480679392814636),\n",
       " (('here', 'stopword'), -0.18447595834732056),\n",
       " (('what', 'stopword'), -0.1844455450773239),\n",
       " (('does', 'stopword'), 0.17731575667858124),\n",
       " (('do', 'stopword'), 0.17503418028354645),\n",
       " (('won', 'stopword'), 0.17335145175457),\n",
       " (('whom', 'stopword'), -0.1720760464668274),\n",
       " (('are', 'stopword'), -0.16836398839950562),\n",
       " (('that', 'stopword'), -0.1671752631664276),\n",
       " (('to', 'stopword'), -0.16634982824325562),\n",
       " (('how', 'stopword'), -0.16420386731624603),\n",
       " (('out', 'stopword'), 0.16412632167339325),\n",
       " (('theirs', 'stopword'), -0.16323165595531464),\n",
       " (('s', 'stopword'), -0.16315224766731262),\n",
       " (('both', 'stopword'), -0.16060587763786316),\n",
       " (('who', 'stopword'), -0.1551540493965149),\n",
       " (('and', 'stopword'), 0.15454809367656708),\n",
       " (('not', 'stopword'), -0.14919698238372803),\n",
       " (('same', 'stopword'), 0.1489904373884201),\n",
       " (('him', 'stopword'), 0.14691443741321564),\n",
       " (('weren', 'stopword'), -0.14680826663970947),\n",
       " (('an', 'stopword'), -0.14651371538639069),\n",
       " (('have', 'stopword'), 0.14613273739814758),\n",
       " (('between', 'stopword'), -0.14189879596233368),\n",
       " (('on', 'stopword'), -0.13969027996063232),\n",
       " (('yourself', 'stopword'), 0.13830257952213287),\n",
       " (('didn', 'stopword'), -0.13764648139476776),\n",
       " (('ll', 'stopword'), 0.1372218132019043),\n",
       " (('they', 'stopword'), 0.13642309606075287),\n",
       " (('because', 'stopword'), 0.13503842055797577),\n",
       " (('hers', 'stopword'), -0.13339081406593323),\n",
       " (('into', 'stopword'), 0.12987807393074036),\n",
       " (('very', 'stopword'), 0.12931254506111145),\n",
       " (('m', 'stopword'), 0.12383969873189926),\n",
       " (('you', 'stopword'), -0.1210186555981636),\n",
       " (('them', 'stopword'), 0.11946829408407211),\n",
       " (('each', 'stopword'), -0.11706925928592682),\n",
       " (('just', 'stopword'), 0.1131720021367073),\n",
       " (('this', 'stopword'), -0.11145927011966705),\n",
       " (('the', 'stopword'), -0.10931143164634705),\n",
       " (('more', 'stopword'), -0.10882273316383362),\n",
       " (('over', 'stopword'), -0.10315610468387604),\n",
       " (('should', 'stopword'), 0.10081365704536438),\n",
       " (('yourselves', 'stopword'), -0.0988498404622078),\n",
       " (('doing', 'stopword'), -0.09709588438272476),\n",
       " (('she', 'stopword'), -0.0968303382396698),\n",
       " (('ours', 'stopword'), -0.09657665342092514),\n",
       " (('only', 'stopword'), 0.09469287842512131),\n",
       " (('now', 'stopword'), 0.08953463286161423),\n",
       " (('nor', 'stopword'), -0.08595557510852814),\n",
       " (('hadn', 'stopword'), 0.08587336540222168),\n",
       " (('through', 'stopword'), -0.08433729410171509),\n",
       " (('under', 'stopword'), -0.08411584794521332),\n",
       " (('aren', 'stopword'), 0.08257634937763214),\n",
       " (('own', 'stopword'), -0.07760537415742874),\n",
       " (('was', 'stopword'), -0.07583646476268768),\n",
       " (('at', 'stopword'), -0.07479090988636017),\n",
       " (('these', 'stopword'), -0.07382287830114365),\n",
       " (('most', 'stopword'), -0.07372624427080154),\n",
       " (('than', 'stopword'), 0.0693732500076294),\n",
       " (('why', 'stopword'), 0.06750459969043732),\n",
       " (('it', 'stopword'), 0.06707317382097244),\n",
       " (('ve', 'stopword'), 0.06469342112541199),\n",
       " (('wasn', 'stopword'), -0.06450213491916656),\n",
       " (('while', 'stopword'), 0.06333070993423462),\n",
       " (('can', 'stopword'), 0.058743420988321304),\n",
       " (('until', 'stopword'), -0.058417029678821564),\n",
       " (('o', 'stopword'), 0.05738965421915054),\n",
       " (('of', 'stopword'), -0.05538037419319153),\n",
       " (('further', 'stopword'), -0.051090940833091736),\n",
       " (('t', 'stopword'), 0.04609139263629913),\n",
       " (('its', 'stopword'), -0.042768996208906174),\n",
       " (('your', 'stopword'), 0.03739121928811073),\n",
       " (('his', 'stopword'), -0.034853242337703705),\n",
       " (('down', 'stopword'), 0.03455885127186775),\n",
       " (('did', 'stopword'), 0.03330613672733307),\n",
       " (('is', 'stopword'), 0.032258592545986176),\n",
       " (('about', 'stopword'), 0.02921261638402939),\n",
       " (('if', 'stopword'), -0.028307614848017693),\n",
       " (('again', 'stopword'), -0.022284070029854774),\n",
       " (('d', 'stopword'), -0.022185973823070526),\n",
       " (('then', 'stopword'), 0.021076273173093796),\n",
       " (('when', 'stopword'), 0.02044433355331421),\n",
       " (('up', 'stopword'), -0.019743449985980988),\n",
       " (('after', 'stopword'), -0.019444486126303673),\n",
       " (('be', 'stopword'), 0.01701318472623825),\n",
       " (('with', 'stopword'), 0.013119727373123169),\n",
       " (('were', 'stopword'), 0.012461096048355103),\n",
       " (('all', 'stopword'), -0.01203831098973751),\n",
       " (('needn', 'stopword'), -0.008970396593213081),\n",
       " (('her', 'stopword'), 0.0054072244092822075),\n",
       " (('any', 'stopword'), -0.002541568363085389),\n",
       " (('a', 'stopword'), -0.002423858502879739),\n",
       " (('off', 'stopword'), -0.001514786621555686),\n",
       " (('shan', 'stopword'), -0.0011344060767441988),\n",
       " (('for', 'stopword'), 0.00031475667492486537),\n",
       " ((\"wouldn't\", 'stopword'), 5.176468636734443e-32),\n",
       " ((\"shan't\", 'stopword'), -5.095613433307483e-32),\n",
       " ((\"should've\", 'stopword'), -4.964383594970385e-32),\n",
       " ((\"weren't\", 'stopword'), 4.772837965998244e-32),\n",
       " ((\"won't\", 'stopword'), -4.63614354829776e-32),\n",
       " ((\"that'll\", 'stopword'), 4.584724780530504e-32),\n",
       " ((\"it's\", 'stopword'), -4.3887857414221647e-32),\n",
       " ((\"you'd\", 'stopword'), -4.3158663003576057e-32),\n",
       " ((\"couldn't\", 'stopword'), -4.2802335401011297e-32),\n",
       " ((\"hasn't\", 'stopword'), -4.041534424613691e-32),\n",
       " (('mightn', 'stopword'), 3.896663855956063e-32),\n",
       " ((\"hadn't\", 'stopword'), 3.5740990960092086e-32),\n",
       " ((\"don't\", 'stopword'), -3.12946688844274e-32),\n",
       " ((\"didn't\", 'stopword'), 3.1185759332823714e-32),\n",
       " ((\"shouldn't\", 'stopword'), -3.057398799161764e-32),\n",
       " ((\"wasn't\", 'stopword'), 3.0061598820301835e-32),\n",
       " ((\"mightn't\", 'stopword'), 2.937466930904006e-32),\n",
       " ((\"mustn't\", 'stopword'), 2.8985042890252383e-32),\n",
       " ((\"doesn't\", 'stopword'), 2.7697121320981575e-32),\n",
       " ((\"you're\", 'stopword'), 2.1849133903924637e-32),\n",
       " ((\"needn't\", 'stopword'), 1.6041142682154428e-32),\n",
       " ((\"you've\", 'stopword'), -1.4975576154917225e-32),\n",
       " ((\"you'll\", 'stopword'), 1.1830921585588274e-32),\n",
       " ((\"aren't\", 'stopword'), 1.0727144879793623e-32),\n",
       " ((\"isn't\", 'stopword'), 5.661518013502029e-33),\n",
       " (('mustn', 'stopword'), -3.381039136439621e-33),\n",
       " ((\"haven't\", 'stopword'), 1.1777872994903602e-33),\n",
       " ((\"she's\", 'stopword'), -2.1108939804891228e-35)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [\n",
    "#     (e, 'nrc') for e in emotions] + ['pers_pronouns'] + [\n",
    "#     (c, 'liwc') for c in list(categories) if c in writings_df.columns] + [\n",
    "(st, 'stopword') for st in stopword_list]\n",
    "weights = model.get_layer('output_layer').get_weights()[0].tolist()[-(len(features)):]\n",
    "\n",
    "print(len(weights), len(features))\n",
    "feature_importance = {}\n",
    "for (i, f) in enumerate(features):\n",
    "    feature_importance[f] = weights[i][0]\n",
    "\n",
    "sorted(feature_importance.items(), key=lambda t: abs(t[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_layer('output_layer').get_weights()[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/lstm_plus_ablated_user_anorexia')#, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5631"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5644"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "voc2=pickle.load(open('vocabulary20K_selfharm.pkl', 'rb'))\n",
    "voc2['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(predictions.flatten()).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([327])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(predictions>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([488])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(predictions<0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_point(subject, voc, hyperparams_features=hyperparams_features, nrc_lexicon=nrc_lexicon,\n",
    "                      emotions=emotions):\n",
    "    eval_writings_df = writings_df[writings_df['subject']==subject]\n",
    "    correct_label = eval_writings_df.label.values[0]\n",
    "    (x_train, y_train), (x_valid, y_valid), (x_test, y_test), voc = load_erisk_data(eval_writings_df,\n",
    "                        seq_len=hyperparams_features['maxlen'],\n",
    "                        voc_size=hyperparams_features['max_features'],\n",
    "                        emotion_lexicon=nrc_lexicon,\n",
    "                        emotions=emotions, user_level=False,\n",
    "                        train_prop=0.0, vocabulary=voc)\n",
    "    return x_test, y_test, correct_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_per_user(writings_df, majority_prop=0.2, train_prop=0.7, majority_nr=0, validate=False, voc=None,\n",
    "                    test_slice=2, nr_slices=5, random=False):\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    thresh=0.5\n",
    "    majority_proportion=majority_prop\n",
    "    valid_prop = 0.3\n",
    "    \n",
    "    if 'subset' in writings_df.columns:\n",
    "        training_subjects = list(set(writings_df[writings_df['subset']=='train'].subject))\n",
    "        test_subjects = list(set(writings_df[writings_df['subset']=='test'].subject))\n",
    "    else:\n",
    "        all_subjects = sorted(list(set(writings_df.subject)))\n",
    "        training_subjects_size = int(len(all_subjects) * train_prop)\n",
    "        test_subjects_size = len(all_subjects) - training_subjects_size\n",
    "        # Cross-validation, with fixed slice as input\n",
    "        test_prop = 1-train_prop\n",
    "        test_slice = min(test_slice, nr_slices)\n",
    "        logger.debug(\"start index: %f, from %f\\n\" % (\n",
    "            len(all_subjects)*(1/nr_slices)*test_slice, test_prop*test_slice))\n",
    "        start_slice = int(len(all_subjects)*(1/nr_slices)*test_slice)\n",
    "        test_subjects = all_subjects[start_slice: start_slice+test_subjects_size]\n",
    "        training_subjects = [s for s in all_subjects if s not in test_subjects]\n",
    "    training_subjects = sorted(training_subjects) # ensuring reproducibility\n",
    "    valid_subjects_size = int(len(training_subjects) * valid_prop)\n",
    "    valid_subjects = training_subjects[:valid_subjects_size]\n",
    "    training_subjects = training_subjects[valid_subjects_size:]\n",
    "    \n",
    "    if validate:\n",
    "        subjects = valid_subjects\n",
    "    else:\n",
    "        subjects = test_subjects\n",
    "    for subject in subjects:\n",
    "        x_test_user, y_test_user, label = get_data_for_point(subject, voc=voc)\n",
    "        outputs = model.predict(x_test_user)\n",
    "        if random:\n",
    "            sigma = np.std(outputs)\n",
    "            mu = np.mean(outputs)\n",
    "            print(\"generating random outputs with sigma\", sigma, \"and mu\", mu)\n",
    "            outputs = sigma*np.random.randn(len(outputs))+mu\n",
    "        positive_pred = sum(outputs>=thresh)\n",
    "        negative_pred = sum(outputs<thresh)\n",
    "        majority_pred = 0\n",
    "        if majority_proportion and positive_pred >= majority_proportion*negative_pred:\n",
    "            majority_pred = 1\n",
    "        if majority_nr and positive_pred>=majority_nr:\n",
    "            majority_pred = 1\n",
    "        if label == 1:\n",
    "            if majority_pred == 1:\n",
    "                tp+=1\n",
    "            else:\n",
    "                fn+=1\n",
    "        else:\n",
    "            if majority_pred == 0:\n",
    "                tn+=1\n",
    "            else:\n",
    "                fp+=1\n",
    "        print(negative_pred, positive_pred, majority_pred)\n",
    "        all_predictions.append(majority_pred)\n",
    "        all_labels.append(label)\n",
    "    def prec_recall_f1(tp, fp, tn, fn):\n",
    "        recall = tp/(tp+fn+0.0000001)\n",
    "        precision = tp/(tp+fp+0.0000001)\n",
    "        f1 = 2*precision*recall/(precision+recall+0.0000001)\n",
    "        print(\"Recall\", recall, \"Precision\", precision, \"F1\", f1)\n",
    "    if majority_prop:\n",
    "        print(\"Vote proportion\", majority_prop)\n",
    "    if majority_nr:\n",
    "        print(\"Vote points\", majority_nr)\n",
    "    prec_recall_f1(tp, fp, tn, fn)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[417] [22] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[78] [7] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[496] [7] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1285] [62] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[172] [14] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[112] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1231] [27] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1866] [10] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[282] [10] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[113] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[478] [2] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[9] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[155] [3] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[197] [2] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[989] [33] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[872] [12] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[105] [2] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[70] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[42] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[688] [37] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[965] [14] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[168] [4] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[321] [7] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[100] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[379] [21] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[31] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1099] [16] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[363] [3] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[87] [9] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[19] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[160] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[62] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1181] [5] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[25] [2] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[160] [6] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[562] [23] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1188] [50] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[285] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[14] [2] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[962] [8] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[935] [13] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[391] [3] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[22] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[848] [42] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1356] [24] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1165] [7] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[25] [4] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[19] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[46] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[14] [2] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[52] [4] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1124] [29] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[289] [20] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[119] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[237] [41] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[261] [7] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[41] [14] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[27] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1132] [16] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[569] [13] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[440] [31] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[48] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[51] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[97] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1079] [12] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[239] [5] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[33] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[314] [2] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[918] [7] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1375] [31] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[322] [9] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1586] [13] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[25] [4] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[59] [46] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[407] [13] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[51] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[481] [8] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[36] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[131] [4] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[143] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[34] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[37] [2] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[491] [39] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1539] [23] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[32] [1] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[22] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[93] [2] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[34] [12] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[139] [2] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[12] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[53] [2] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[35] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[120] [7] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[800] [18] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[98] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[243] [7] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[122] [0] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[90] [4] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[72] [76] 1\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[282] [7] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[1039] [24] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[587] [4] 0\n",
      "Loading data...\n",
      "0 training users,  0 validation users,  1  test users.\n",
      "[95] [5] 0\n",
      "Vote proportion 0.2\n",
      "Recall 0.23529411626297578 Precision 0.9999999750000006 F1 0.3809523464852629\n"
     ]
    }
   ],
   "source": [
    "predict_per_user(writings_df=writings_df, voc=voc, majority_prop=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_per_slice = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "103/103 [==============================] - 0s 48us/sample - loss: 0.4739 - f1_m: 0.3333 - precision_m: 0.3000 - recall_m: 0.5000     \n",
      "Results for slice 0: [0.47392676351139845, 0.33333328, 0.29999998, 0.49999994]\n",
      "\n",
      "Train...\n",
      "103/103 [==============================] - 0s 54us/sample - loss: 0.4942 - f1_m: 0.3576 - precision_m: 0.3806 - recall_m: 0.5250     \n",
      "Results for slice 1: [0.49420061707496643, 0.3575757, 0.3805555, 0.525]\n",
      "\n",
      "Train...\n",
      "103/103 [==============================] - 0s 216us/sample - loss: 0.4024 - f1_m: 0.6373 - precision_m: 0.5214 - recall_m: 0.9500\n",
      "Results for slice 2: [0.4023708821211046, 0.6373015, 0.5214286, 0.9499999]\n",
      "\n",
      "Train...\n",
      "103/103 [==============================] - 0s 148us/sample - loss: 0.3093 - f1_m: 0.8264 - precision_m: 0.7250 - recall_m: 1.0000\n",
      "Results for slice 3: [0.3093455121355149, 0.82638884, 0.725, 1.0]\n",
      "\n",
      "Train...\n",
      "68/68 [==============================] - 0s 130us/sample - loss: 0.3460 - f1_m: 0.8611 - precision_m: 0.7714 - recall_m: 1.0000\n",
      "Results for slice 4: [0.34599672871477466, 0.8611111, 0.7714286, 1.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nr_slices=5\n",
    "logger.setLevel(logging.INFO)\n",
    "for tslice in range(nr_slices): \n",
    "    (x_train, y_train), (x_valid, y_valid), (x_test, y_test), voc = load_erisk_data(writings_df, \n",
    "                                                                seq_len=hyperparams_features['maxlen'],\n",
    "                                                                voc_size=hyperparams_features['max_features'],\n",
    "                                                               emotion_lexicon=nrc_lexicon,\n",
    "                                                               emotions=emotions,\n",
    "                                                               user_level=hyperparams_features['user_level'],\n",
    "                                                                                    test_slice=tslice,\n",
    "                                                                                    nr_slices=nr_slices,\n",
    "    #                                                            vocabulary=pickle.load(open('vocabulary20K_selfharm.pkl', 'rb'))\n",
    "                                                                                   logger=logger)\n",
    "    model, history = train_model(model, x_train, y_train, x_valid, y_valid,\n",
    "           epochs=200, batch_size=hyperparams['batch_size'],\n",
    "                      class_weight={0:0.5, 1:5}, start_epoch=0,\n",
    "                      callback_list = [freeze_layer, weights_history, reduce_lr],\n",
    "                      workers=2, verbose=0)\n",
    "    results_per_slice[tslice] = model.evaluate(x_test, y_test)\n",
    "    logger.info(\"Results for slice %d: %s\\n\" % (tslice, results_per_slice[tslice]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 score:  0.60314214 all F1 scores:  {0: 0.33333328, 1: 0.3575757, 2: 0.6373015, 3: 0.82638884, 4: 0.8611111}\n"
     ]
    }
   ],
   "source": [
    "print(\"Average F1 score: \", np.array([results_per_slice[s][1] for s in results_per_slice.keys()]).mean(),\n",
    "     \"all F1 scores: \", {s: v[1] for (s,v) in results_per_slice.items()} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tokens(row):\n",
    "    tokens = []\n",
    "    if row.tokenized_text:\n",
    "        tokens += row.tokenized_text\n",
    "    if row.tokenized_title:\n",
    "        tokens += row.tokenized_title\n",
    "    return tokens\n",
    "writings_df['all_tokens'] = writings_df.apply (lambda row: merge_tokens(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: include the title\n",
    "def extract_emotions(tokens, emotion, relative=True):\n",
    "    if not tokens:\n",
    "        return None\n",
    "    emotion_words = [t for t in tokens \n",
    "                     if t in nrc_lexicon[emotion]]\n",
    "    if relative:\n",
    "        return len(emotion_words) / len(tokens)\n",
    "    else:\n",
    "        return len(emotion_words)\n",
    "    \n",
    "    return encoded_emotions\n",
    "\n",
    "from functools import partial\n",
    "for emotion in emotions:\n",
    "    writings_df[emotion] = writings_df['all_tokens'].apply(partial(extract_emotions, emotion=emotion, \n",
    "                                                                   relative=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['pronouns'] = writings_df['all_tokens'].apply(partial(encode_pronouns, relative=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>text_len</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.104269</td>\n",
       "      <td>0.011986</td>\n",
       "      <td>0.020197</td>\n",
       "      <td>0.031982</td>\n",
       "      <td>0.031271</td>\n",
       "      <td>0.019335</td>\n",
       "      <td>0.040782</td>\n",
       "      <td>0.023853</td>\n",
       "      <td>0.023621</td>\n",
       "      <td>0.032969</td>\n",
       "      <td>0.020421</td>\n",
       "      <td>0.023590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pronouns</th>\n",
       "      <td>0.104269</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.636745</td>\n",
       "      <td>0.449384</td>\n",
       "      <td>0.567496</td>\n",
       "      <td>0.452098</td>\n",
       "      <td>0.464899</td>\n",
       "      <td>0.548570</td>\n",
       "      <td>0.513029</td>\n",
       "      <td>0.571303</td>\n",
       "      <td>0.524614</td>\n",
       "      <td>0.461328</td>\n",
       "      <td>0.538335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_len</th>\n",
       "      <td>0.011986</td>\n",
       "      <td>0.636745</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.708853</td>\n",
       "      <td>0.791715</td>\n",
       "      <td>0.642980</td>\n",
       "      <td>0.738146</td>\n",
       "      <td>0.728836</td>\n",
       "      <td>0.823974</td>\n",
       "      <td>0.867609</td>\n",
       "      <td>0.723653</td>\n",
       "      <td>0.650420</td>\n",
       "      <td>0.834939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.020197</td>\n",
       "      <td>0.449384</td>\n",
       "      <td>0.708853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.643459</td>\n",
       "      <td>0.762591</td>\n",
       "      <td>0.858442</td>\n",
       "      <td>0.564162</td>\n",
       "      <td>0.835345</td>\n",
       "      <td>0.681573</td>\n",
       "      <td>0.774846</td>\n",
       "      <td>0.583704</td>\n",
       "      <td>0.671042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anticipation</th>\n",
       "      <td>0.031982</td>\n",
       "      <td>0.567496</td>\n",
       "      <td>0.791715</td>\n",
       "      <td>0.643459</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.573916</td>\n",
       "      <td>0.668326</td>\n",
       "      <td>0.834784</td>\n",
       "      <td>0.684882</td>\n",
       "      <td>0.849864</td>\n",
       "      <td>0.668269</td>\n",
       "      <td>0.727331</td>\n",
       "      <td>0.818885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.031271</td>\n",
       "      <td>0.452098</td>\n",
       "      <td>0.642980</td>\n",
       "      <td>0.762591</td>\n",
       "      <td>0.573916</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.729799</td>\n",
       "      <td>0.526733</td>\n",
       "      <td>0.765865</td>\n",
       "      <td>0.603013</td>\n",
       "      <td>0.737717</td>\n",
       "      <td>0.540439</td>\n",
       "      <td>0.589641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.019335</td>\n",
       "      <td>0.464899</td>\n",
       "      <td>0.738146</td>\n",
       "      <td>0.858442</td>\n",
       "      <td>0.668326</td>\n",
       "      <td>0.729799</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.570632</td>\n",
       "      <td>0.862778</td>\n",
       "      <td>0.706676</td>\n",
       "      <td>0.824782</td>\n",
       "      <td>0.569688</td>\n",
       "      <td>0.687232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.040782</td>\n",
       "      <td>0.548570</td>\n",
       "      <td>0.728836</td>\n",
       "      <td>0.564162</td>\n",
       "      <td>0.834784</td>\n",
       "      <td>0.526733</td>\n",
       "      <td>0.570632</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.604964</td>\n",
       "      <td>0.850961</td>\n",
       "      <td>0.603296</td>\n",
       "      <td>0.722710</td>\n",
       "      <td>0.811529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0.023853</td>\n",
       "      <td>0.513029</td>\n",
       "      <td>0.823974</td>\n",
       "      <td>0.835345</td>\n",
       "      <td>0.684882</td>\n",
       "      <td>0.765865</td>\n",
       "      <td>0.862778</td>\n",
       "      <td>0.604964</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.735431</td>\n",
       "      <td>0.840379</td>\n",
       "      <td>0.597634</td>\n",
       "      <td>0.706808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>0.023621</td>\n",
       "      <td>0.571303</td>\n",
       "      <td>0.867609</td>\n",
       "      <td>0.681573</td>\n",
       "      <td>0.849864</td>\n",
       "      <td>0.603013</td>\n",
       "      <td>0.706676</td>\n",
       "      <td>0.850961</td>\n",
       "      <td>0.735431</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.702751</td>\n",
       "      <td>0.678778</td>\n",
       "      <td>0.916526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.032969</td>\n",
       "      <td>0.524614</td>\n",
       "      <td>0.723653</td>\n",
       "      <td>0.774846</td>\n",
       "      <td>0.668269</td>\n",
       "      <td>0.737717</td>\n",
       "      <td>0.824782</td>\n",
       "      <td>0.603296</td>\n",
       "      <td>0.840379</td>\n",
       "      <td>0.702751</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.584816</td>\n",
       "      <td>0.665750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.020421</td>\n",
       "      <td>0.461328</td>\n",
       "      <td>0.650420</td>\n",
       "      <td>0.583704</td>\n",
       "      <td>0.727331</td>\n",
       "      <td>0.540439</td>\n",
       "      <td>0.569688</td>\n",
       "      <td>0.722710</td>\n",
       "      <td>0.597634</td>\n",
       "      <td>0.678778</td>\n",
       "      <td>0.584816</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.660681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust</th>\n",
       "      <td>0.023590</td>\n",
       "      <td>0.538335</td>\n",
       "      <td>0.834939</td>\n",
       "      <td>0.671042</td>\n",
       "      <td>0.818885</td>\n",
       "      <td>0.589641</td>\n",
       "      <td>0.687232</td>\n",
       "      <td>0.811529</td>\n",
       "      <td>0.706808</td>\n",
       "      <td>0.916526</td>\n",
       "      <td>0.665750</td>\n",
       "      <td>0.660681</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 label  pronouns  text_len     anger  anticipation   disgust  \\\n",
       "label         1.000000  0.104269  0.011986  0.020197      0.031982  0.031271   \n",
       "pronouns      0.104269  1.000000  0.636745  0.449384      0.567496  0.452098   \n",
       "text_len      0.011986  0.636745  1.000000  0.708853      0.791715  0.642980   \n",
       "anger         0.020197  0.449384  0.708853  1.000000      0.643459  0.762591   \n",
       "anticipation  0.031982  0.567496  0.791715  0.643459      1.000000  0.573916   \n",
       "disgust       0.031271  0.452098  0.642980  0.762591      0.573916  1.000000   \n",
       "fear          0.019335  0.464899  0.738146  0.858442      0.668326  0.729799   \n",
       "joy           0.040782  0.548570  0.728836  0.564162      0.834784  0.526733   \n",
       "negative      0.023853  0.513029  0.823974  0.835345      0.684882  0.765865   \n",
       "positive      0.023621  0.571303  0.867609  0.681573      0.849864  0.603013   \n",
       "sadness       0.032969  0.524614  0.723653  0.774846      0.668269  0.737717   \n",
       "surprise      0.020421  0.461328  0.650420  0.583704      0.727331  0.540439   \n",
       "trust         0.023590  0.538335  0.834939  0.671042      0.818885  0.589641   \n",
       "\n",
       "                  fear       joy  negative  positive   sadness  surprise  \\\n",
       "label         0.019335  0.040782  0.023853  0.023621  0.032969  0.020421   \n",
       "pronouns      0.464899  0.548570  0.513029  0.571303  0.524614  0.461328   \n",
       "text_len      0.738146  0.728836  0.823974  0.867609  0.723653  0.650420   \n",
       "anger         0.858442  0.564162  0.835345  0.681573  0.774846  0.583704   \n",
       "anticipation  0.668326  0.834784  0.684882  0.849864  0.668269  0.727331   \n",
       "disgust       0.729799  0.526733  0.765865  0.603013  0.737717  0.540439   \n",
       "fear          1.000000  0.570632  0.862778  0.706676  0.824782  0.569688   \n",
       "joy           0.570632  1.000000  0.604964  0.850961  0.603296  0.722710   \n",
       "negative      0.862778  0.604964  1.000000  0.735431  0.840379  0.597634   \n",
       "positive      0.706676  0.850961  0.735431  1.000000  0.702751  0.678778   \n",
       "sadness       0.824782  0.603296  0.840379  0.702751  1.000000  0.584816   \n",
       "surprise      0.569688  0.722710  0.597634  0.678778  0.584816  1.000000   \n",
       "trust         0.687232  0.811529  0.706808  0.916526  0.665750  0.660681   \n",
       "\n",
       "                 trust  \n",
       "label         0.023590  \n",
       "pronouns      0.538335  \n",
       "text_len      0.834939  \n",
       "anger         0.671042  \n",
       "anticipation  0.818885  \n",
       "disgust       0.589641  \n",
       "fear          0.687232  \n",
       "joy           0.811529  \n",
       "negative      0.706808  \n",
       "positive      0.916526  \n",
       "sadness       0.665750  \n",
       "surprise      0.660681  \n",
       "trust         1.000000  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len'] + emotions].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pronouns</th>\n",
       "      <th>text_len</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.868213</td>\n",
       "      <td>32.031615</td>\n",
       "      <td>0.386069</td>\n",
       "      <td>0.58984</td>\n",
       "      <td>0.263683</td>\n",
       "      <td>0.478014</td>\n",
       "      <td>0.479908</td>\n",
       "      <td>0.818800</td>\n",
       "      <td>1.280788</td>\n",
       "      <td>0.385315</td>\n",
       "      <td>0.284790</td>\n",
       "      <td>0.830560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.484271</td>\n",
       "      <td>36.398389</td>\n",
       "      <td>0.529232</td>\n",
       "      <td>0.86985</td>\n",
       "      <td>0.416203</td>\n",
       "      <td>0.654371</td>\n",
       "      <td>0.769766</td>\n",
       "      <td>1.152422</td>\n",
       "      <td>1.717428</td>\n",
       "      <td>0.627088</td>\n",
       "      <td>0.375418</td>\n",
       "      <td>1.128341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pronouns   text_len     anger  anticipation   disgust      fear  \\\n",
       "label                                                                    \n",
       "0      0.868213  32.031615  0.386069       0.58984  0.263683  0.478014   \n",
       "1      2.484271  36.398389  0.529232       0.86985  0.416203  0.654371   \n",
       "\n",
       "            joy  negative  positive   sadness  surprise     trust  \n",
       "label                                                              \n",
       "0      0.479908  0.818800  1.280788  0.385315  0.284790  0.830560  \n",
       "1      0.769766  1.152422  1.717428  0.627088  0.375418  1.128341  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len'] + emotions].groupby('label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentAnalyzer, SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.66, 'pos': 0.34, 'compound': 0.5574}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid.polarity_scores(\"We are here today happiness is all around\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['neg_vader'] = writings_df.text.apply(lambda t: sid.polarity_scores(t)['neg']\n",
    "                                                 if type(t)==str else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>title_len</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>text_len</th>\n",
       "      <th>emotions</th>\n",
       "      <th>...</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>all_tokens</th>\n",
       "      <th>neg_vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>If anyone could help with which sub to put thi...</td>\n",
       "      <td>2016-08-02 09:22:12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[if, anyone, could, help, with, which, sub, to...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[if, anyone, could, help, with, which, sub, to...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>I'm literally never gonna stop waiting...</td>\n",
       "      <td>2016-08-05 09:35:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, m, literally, never, gonna, stop, waiting]</td>\n",
       "      <td>7.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[i, m, literally, never, gonna, stop, waiting]</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>This is a really interesting study! Makes sens...</td>\n",
       "      <td>2016-08-05 21:36:24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[this, is, a, really, interesting, study, make...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[this, is, a, really, interesting, study, make...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>The only thing Frank is building ...</td>\n",
       "      <td>2016-08-07 23:35:23</td>\n",
       "      <td>... Is hype. Think about it, every time he wor...</td>\n",
       "      <td>0</td>\n",
       "      <td>[the, only, thing, frank, is, building]</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[is, hype, think, about, it, every, time, he, ...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[is, hype, think, about, it, every, time, he, ...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>Mostly always me during this whole charade</td>\n",
       "      <td>2016-08-09 08:39:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[mostly, always, me, during, this, whole, char...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[mostly, always, me, during, this, whole, char...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170693</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 11:29:21</td>\n",
       "      <td>this is my personal experience ,it may not ref...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[this, is, my, personal, experience, it, may, ...</td>\n",
       "      <td>153.0</td>\n",
       "      <td>0.026144</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[this, is, my, personal, experience, it, may, ...</td>\n",
       "      <td>0.089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170694</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 16:17:34</td>\n",
       "      <td>stop looking at 20 million saudis as one entit...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[stop, looking, at, 20, million, saudis, as, o...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[stop, looking, at, 20, million, saudis, as, o...</td>\n",
       "      <td>0.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170695</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 20:00:31</td>\n",
       "      <td>i am aware of stats now and then. i was just s...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[i, am, aware, of, stats, now, and, then, i, w...</td>\n",
       "      <td>198.0</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>[i, am, aware, of, stats, now, and, then, i, w...</td>\n",
       "      <td>0.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170696</th>\n",
       "      <td>subject217</td>\n",
       "      <td>WHAT DID YOU SAY TO ME?</td>\n",
       "      <td>2018-08-20 10:54:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[what, did, you, say, to, me]</td>\n",
       "      <td>6.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[what, did, you, say, to, me]</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170697</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-20 12:07:44</td>\n",
       "      <td>me smellz fish,me find no fish!...what the fuc...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[me, smellz, fish, me, find, no, fish, what, t...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[me, smellz, fish, me, find, no, fish, what, t...</td>\n",
       "      <td>0.484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170698 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            subject                                              title  \\\n",
       "0       subject8292  If anyone could help with which sub to put thi...   \n",
       "1       subject8292          I'm literally never gonna stop waiting...   \n",
       "2       subject8292  This is a really interesting study! Makes sens...   \n",
       "3       subject8292               The only thing Frank is building ...   \n",
       "4       subject8292         Mostly always me during this whole charade   \n",
       "...             ...                                                ...   \n",
       "170693   subject217                                                NaN   \n",
       "170694   subject217                                                NaN   \n",
       "170695   subject217                                                NaN   \n",
       "170696   subject217                            WHAT DID YOU SAY TO ME?   \n",
       "170697   subject217                                                NaN   \n",
       "\n",
       "                       date  \\\n",
       "0       2016-08-02 09:22:12   \n",
       "1       2016-08-05 09:35:55   \n",
       "2       2016-08-05 21:36:24   \n",
       "3       2016-08-07 23:35:23   \n",
       "4       2016-08-09 08:39:41   \n",
       "...                     ...   \n",
       "170693  2018-08-19 11:29:21   \n",
       "170694  2018-08-19 16:17:34   \n",
       "170695  2018-08-19 20:00:31   \n",
       "170696  2018-08-20 10:54:11   \n",
       "170697  2018-08-20 12:07:44   \n",
       "\n",
       "                                                     text  label  \\\n",
       "0                                                     NaN      0   \n",
       "1                                                     NaN      0   \n",
       "2                                                     NaN      0   \n",
       "3       ... Is hype. Think about it, every time he wor...      0   \n",
       "4                                                     NaN      0   \n",
       "...                                                   ...    ...   \n",
       "170693  this is my personal experience ,it may not ref...      0   \n",
       "170694  stop looking at 20 million saudis as one entit...      0   \n",
       "170695  i am aware of stats now and then. i was just s...      0   \n",
       "170696                                                NaN      0   \n",
       "170697  me smellz fish,me find no fish!...what the fuc...      0   \n",
       "\n",
       "                                          tokenized_title  title_len  \\\n",
       "0       [if, anyone, could, help, with, which, sub, to...       11.0   \n",
       "1          [i, m, literally, never, gonna, stop, waiting]        7.0   \n",
       "2       [this, is, a, really, interesting, study, make...        9.0   \n",
       "3                 [the, only, thing, frank, is, building]        6.0   \n",
       "4       [mostly, always, me, during, this, whole, char...        7.0   \n",
       "...                                                   ...        ...   \n",
       "170693                                               None        NaN   \n",
       "170694                                               None        NaN   \n",
       "170695                                               None        NaN   \n",
       "170696                      [what, did, you, say, to, me]        6.0   \n",
       "170697                                               None        NaN   \n",
       "\n",
       "                                           tokenized_text  text_len  emotions  \\\n",
       "0                                                    None       NaN       NaN   \n",
       "1                                                    None       NaN       NaN   \n",
       "2                                                    None       NaN       NaN   \n",
       "3       [is, hype, think, about, it, every, time, he, ...      26.0  0.000000   \n",
       "4                                                    None       NaN       NaN   \n",
       "...                                                   ...       ...       ...   \n",
       "170693  [this, is, my, personal, experience, it, may, ...     153.0  0.026144   \n",
       "170694  [stop, looking, at, 20, million, saudis, as, o...      15.0  0.000000   \n",
       "170695  [i, am, aware, of, stats, now, and, then, i, w...     198.0  0.030303   \n",
       "170696                                               None       NaN       NaN   \n",
       "170697  [me, smellz, fish, me, find, no, fish, what, t...      11.0  0.000000   \n",
       "\n",
       "        ...  fear  joy  negative  positive  sadness  surprise  trust  \\\n",
       "0       ...   0.0  0.0       0.0       0.0      0.0       0.0    0.0   \n",
       "1       ...   0.0  0.0       0.0       0.0      0.0       0.0    0.0   \n",
       "2       ...   0.0  0.0       0.0       3.0      0.0       0.0    0.0   \n",
       "3       ...   0.0  0.0       3.0       3.0      0.0       0.0    1.0   \n",
       "4       ...   0.0  0.0       1.0       0.0      0.0       0.0    0.0   \n",
       "...     ...   ...  ...       ...       ...      ...       ...    ...   \n",
       "170693  ...   1.0  1.0       1.0       7.0      0.0       1.0    4.0   \n",
       "170694  ...   1.0  0.0       1.0       0.0      0.0       0.0    0.0   \n",
       "170695  ...   2.0  3.0       4.0      11.0      3.0       0.0    6.0   \n",
       "170696  ...   0.0  0.0       0.0       0.0      0.0       0.0    0.0   \n",
       "170697  ...   0.0  0.0       0.0       0.0      0.0       0.0    0.0   \n",
       "\n",
       "        pronouns                                         all_tokens  neg_vader  \n",
       "0            0.0  [if, anyone, could, help, with, which, sub, to...      0.000  \n",
       "1            1.0     [i, m, literally, never, gonna, stop, waiting]      0.000  \n",
       "2            0.0  [this, is, a, really, interesting, study, make...      0.000  \n",
       "3            0.0  [is, hype, think, about, it, every, time, he, ...      0.000  \n",
       "4            1.0  [mostly, always, me, during, this, whole, char...      0.000  \n",
       "...          ...                                                ...        ...  \n",
       "170693       4.0  [this, is, my, personal, experience, it, may, ...      0.089  \n",
       "170694       0.0  [stop, looking, at, 20, million, saudis, as, o...      0.145  \n",
       "170695      16.0  [i, am, aware, of, stats, now, and, then, i, w...      0.070  \n",
       "170696       1.0                      [what, did, you, say, to, me]      0.000  \n",
       "170697       2.0  [me, smellz, fish, me, find, no, fish, what, t...      0.484  \n",
       "\n",
       "[170698 rows x 23 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['pos_vader'] = writings_df.text.apply(lambda t: sid.polarity_scores(t)['pos']\n",
    "                                                 if type(t)==str else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pronouns</th>\n",
       "      <th>text_len</th>\n",
       "      <th>neg_vader</th>\n",
       "      <th>pos_vader</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.868213</td>\n",
       "      <td>32.031615</td>\n",
       "      <td>0.054259</td>\n",
       "      <td>0.109981</td>\n",
       "      <td>0.386069</td>\n",
       "      <td>0.58984</td>\n",
       "      <td>0.263683</td>\n",
       "      <td>0.478014</td>\n",
       "      <td>0.479908</td>\n",
       "      <td>0.818800</td>\n",
       "      <td>1.280788</td>\n",
       "      <td>0.385315</td>\n",
       "      <td>0.284790</td>\n",
       "      <td>0.830560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.484271</td>\n",
       "      <td>36.398389</td>\n",
       "      <td>0.079191</td>\n",
       "      <td>0.148154</td>\n",
       "      <td>0.529232</td>\n",
       "      <td>0.86985</td>\n",
       "      <td>0.416203</td>\n",
       "      <td>0.654371</td>\n",
       "      <td>0.769766</td>\n",
       "      <td>1.152422</td>\n",
       "      <td>1.717428</td>\n",
       "      <td>0.627088</td>\n",
       "      <td>0.375418</td>\n",
       "      <td>1.128341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pronouns   text_len  neg_vader  pos_vader     anger  anticipation  \\\n",
       "label                                                                      \n",
       "0      0.868213  32.031615   0.054259   0.109981  0.386069       0.58984   \n",
       "1      2.484271  36.398389   0.079191   0.148154  0.529232       0.86985   \n",
       "\n",
       "        disgust      fear       joy  negative  positive   sadness  surprise  \\\n",
       "label                                                                         \n",
       "0      0.263683  0.478014  0.479908  0.818800  1.280788  0.385315  0.284790   \n",
       "1      0.416203  0.654371  0.769766  1.152422  1.717428  0.627088  0.375418   \n",
       "\n",
       "          trust  \n",
       "label            \n",
       "0      0.830560  \n",
       "1      1.128341  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len', 'neg_vader', 'pos_vader'] + emotions].groupby('label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>text_len</th>\n",
       "      <th>neg_vader</th>\n",
       "      <th>pos_vader</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.097800</td>\n",
       "      <td>0.033477</td>\n",
       "      <td>0.067170</td>\n",
       "      <td>0.065211</td>\n",
       "      <td>0.022057</td>\n",
       "      <td>0.025666</td>\n",
       "      <td>0.030664</td>\n",
       "      <td>0.019114</td>\n",
       "      <td>0.033977</td>\n",
       "      <td>0.022934</td>\n",
       "      <td>0.019590</td>\n",
       "      <td>0.032641</td>\n",
       "      <td>0.018109</td>\n",
       "      <td>0.024014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pronouns</th>\n",
       "      <td>0.097800</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.332071</td>\n",
       "      <td>0.193938</td>\n",
       "      <td>0.221419</td>\n",
       "      <td>0.076345</td>\n",
       "      <td>0.128030</td>\n",
       "      <td>0.094069</td>\n",
       "      <td>0.063176</td>\n",
       "      <td>0.144011</td>\n",
       "      <td>0.076670</td>\n",
       "      <td>0.106055</td>\n",
       "      <td>0.100827</td>\n",
       "      <td>0.106790</td>\n",
       "      <td>0.122914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_len</th>\n",
       "      <td>0.033477</td>\n",
       "      <td>0.332071</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.343154</td>\n",
       "      <td>0.159673</td>\n",
       "      <td>0.360460</td>\n",
       "      <td>0.386351</td>\n",
       "      <td>0.312393</td>\n",
       "      <td>0.381410</td>\n",
       "      <td>0.339398</td>\n",
       "      <td>0.370250</td>\n",
       "      <td>0.330075</td>\n",
       "      <td>0.384031</td>\n",
       "      <td>0.349498</td>\n",
       "      <td>0.389620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_vader</th>\n",
       "      <td>0.067170</td>\n",
       "      <td>0.193938</td>\n",
       "      <td>0.343154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.169624</td>\n",
       "      <td>0.384510</td>\n",
       "      <td>0.141868</td>\n",
       "      <td>0.362582</td>\n",
       "      <td>0.339245</td>\n",
       "      <td>0.126042</td>\n",
       "      <td>0.431111</td>\n",
       "      <td>0.099767</td>\n",
       "      <td>0.374256</td>\n",
       "      <td>0.159302</td>\n",
       "      <td>0.143060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_vader</th>\n",
       "      <td>0.065211</td>\n",
       "      <td>0.221419</td>\n",
       "      <td>0.159673</td>\n",
       "      <td>0.169624</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.079693</td>\n",
       "      <td>0.225925</td>\n",
       "      <td>0.087309</td>\n",
       "      <td>0.071450</td>\n",
       "      <td>0.323148</td>\n",
       "      <td>0.058266</td>\n",
       "      <td>0.270687</td>\n",
       "      <td>0.095040</td>\n",
       "      <td>0.186243</td>\n",
       "      <td>0.231954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.022057</td>\n",
       "      <td>0.076345</td>\n",
       "      <td>0.360460</td>\n",
       "      <td>0.384510</td>\n",
       "      <td>0.079693</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.196795</td>\n",
       "      <td>0.583864</td>\n",
       "      <td>0.587460</td>\n",
       "      <td>0.157202</td>\n",
       "      <td>0.631708</td>\n",
       "      <td>0.128169</td>\n",
       "      <td>0.528980</td>\n",
       "      <td>0.273195</td>\n",
       "      <td>0.169261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anticipation</th>\n",
       "      <td>0.025666</td>\n",
       "      <td>0.128030</td>\n",
       "      <td>0.386351</td>\n",
       "      <td>0.141868</td>\n",
       "      <td>0.225925</td>\n",
       "      <td>0.196795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.164649</td>\n",
       "      <td>0.241958</td>\n",
       "      <td>0.583107</td>\n",
       "      <td>0.178827</td>\n",
       "      <td>0.452457</td>\n",
       "      <td>0.198972</td>\n",
       "      <td>0.460851</td>\n",
       "      <td>0.469028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.030664</td>\n",
       "      <td>0.094069</td>\n",
       "      <td>0.312393</td>\n",
       "      <td>0.362582</td>\n",
       "      <td>0.087309</td>\n",
       "      <td>0.583864</td>\n",
       "      <td>0.164649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.440376</td>\n",
       "      <td>0.152731</td>\n",
       "      <td>0.552021</td>\n",
       "      <td>0.116588</td>\n",
       "      <td>0.490181</td>\n",
       "      <td>0.232166</td>\n",
       "      <td>0.153723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.019114</td>\n",
       "      <td>0.063176</td>\n",
       "      <td>0.381410</td>\n",
       "      <td>0.339245</td>\n",
       "      <td>0.071450</td>\n",
       "      <td>0.587460</td>\n",
       "      <td>0.241958</td>\n",
       "      <td>0.440376</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.159907</td>\n",
       "      <td>0.576962</td>\n",
       "      <td>0.141985</td>\n",
       "      <td>0.583703</td>\n",
       "      <td>0.248160</td>\n",
       "      <td>0.184240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.033977</td>\n",
       "      <td>0.144011</td>\n",
       "      <td>0.339398</td>\n",
       "      <td>0.126042</td>\n",
       "      <td>0.323148</td>\n",
       "      <td>0.157202</td>\n",
       "      <td>0.583107</td>\n",
       "      <td>0.152731</td>\n",
       "      <td>0.159907</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.113400</td>\n",
       "      <td>0.645827</td>\n",
       "      <td>0.176440</td>\n",
       "      <td>0.477317</td>\n",
       "      <td>0.582920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0.022934</td>\n",
       "      <td>0.076670</td>\n",
       "      <td>0.370250</td>\n",
       "      <td>0.431111</td>\n",
       "      <td>0.058266</td>\n",
       "      <td>0.631708</td>\n",
       "      <td>0.178827</td>\n",
       "      <td>0.552021</td>\n",
       "      <td>0.576962</td>\n",
       "      <td>0.113400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.105821</td>\n",
       "      <td>0.612781</td>\n",
       "      <td>0.226230</td>\n",
       "      <td>0.145220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>0.019590</td>\n",
       "      <td>0.106055</td>\n",
       "      <td>0.330075</td>\n",
       "      <td>0.099767</td>\n",
       "      <td>0.270687</td>\n",
       "      <td>0.128169</td>\n",
       "      <td>0.452457</td>\n",
       "      <td>0.116588</td>\n",
       "      <td>0.141985</td>\n",
       "      <td>0.645827</td>\n",
       "      <td>0.105821</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.139827</td>\n",
       "      <td>0.333998</td>\n",
       "      <td>0.648163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.032641</td>\n",
       "      <td>0.100827</td>\n",
       "      <td>0.384031</td>\n",
       "      <td>0.374256</td>\n",
       "      <td>0.095040</td>\n",
       "      <td>0.528980</td>\n",
       "      <td>0.198972</td>\n",
       "      <td>0.490181</td>\n",
       "      <td>0.583703</td>\n",
       "      <td>0.176440</td>\n",
       "      <td>0.612781</td>\n",
       "      <td>0.139827</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.265026</td>\n",
       "      <td>0.171245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.018109</td>\n",
       "      <td>0.106790</td>\n",
       "      <td>0.349498</td>\n",
       "      <td>0.159302</td>\n",
       "      <td>0.186243</td>\n",
       "      <td>0.273195</td>\n",
       "      <td>0.460851</td>\n",
       "      <td>0.232166</td>\n",
       "      <td>0.248160</td>\n",
       "      <td>0.477317</td>\n",
       "      <td>0.226230</td>\n",
       "      <td>0.333998</td>\n",
       "      <td>0.265026</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.354746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust</th>\n",
       "      <td>0.024014</td>\n",
       "      <td>0.122914</td>\n",
       "      <td>0.389620</td>\n",
       "      <td>0.143060</td>\n",
       "      <td>0.231954</td>\n",
       "      <td>0.169261</td>\n",
       "      <td>0.469028</td>\n",
       "      <td>0.153723</td>\n",
       "      <td>0.184240</td>\n",
       "      <td>0.582920</td>\n",
       "      <td>0.145220</td>\n",
       "      <td>0.648163</td>\n",
       "      <td>0.171245</td>\n",
       "      <td>0.354746</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 label  pronouns  text_len  neg_vader  pos_vader     anger  \\\n",
       "label         1.000000  0.097800  0.033477   0.067170   0.065211  0.022057   \n",
       "pronouns      0.097800  1.000000  0.332071   0.193938   0.221419  0.076345   \n",
       "text_len      0.033477  0.332071  1.000000   0.343154   0.159673  0.360460   \n",
       "neg_vader     0.067170  0.193938  0.343154   1.000000   0.169624  0.384510   \n",
       "pos_vader     0.065211  0.221419  0.159673   0.169624   1.000000  0.079693   \n",
       "anger         0.022057  0.076345  0.360460   0.384510   0.079693  1.000000   \n",
       "anticipation  0.025666  0.128030  0.386351   0.141868   0.225925  0.196795   \n",
       "disgust       0.030664  0.094069  0.312393   0.362582   0.087309  0.583864   \n",
       "fear          0.019114  0.063176  0.381410   0.339245   0.071450  0.587460   \n",
       "joy           0.033977  0.144011  0.339398   0.126042   0.323148  0.157202   \n",
       "negative      0.022934  0.076670  0.370250   0.431111   0.058266  0.631708   \n",
       "positive      0.019590  0.106055  0.330075   0.099767   0.270687  0.128169   \n",
       "sadness       0.032641  0.100827  0.384031   0.374256   0.095040  0.528980   \n",
       "surprise      0.018109  0.106790  0.349498   0.159302   0.186243  0.273195   \n",
       "trust         0.024014  0.122914  0.389620   0.143060   0.231954  0.169261   \n",
       "\n",
       "              anticipation   disgust      fear       joy  negative  positive  \\\n",
       "label             0.025666  0.030664  0.019114  0.033977  0.022934  0.019590   \n",
       "pronouns          0.128030  0.094069  0.063176  0.144011  0.076670  0.106055   \n",
       "text_len          0.386351  0.312393  0.381410  0.339398  0.370250  0.330075   \n",
       "neg_vader         0.141868  0.362582  0.339245  0.126042  0.431111  0.099767   \n",
       "pos_vader         0.225925  0.087309  0.071450  0.323148  0.058266  0.270687   \n",
       "anger             0.196795  0.583864  0.587460  0.157202  0.631708  0.128169   \n",
       "anticipation      1.000000  0.164649  0.241958  0.583107  0.178827  0.452457   \n",
       "disgust           0.164649  1.000000  0.440376  0.152731  0.552021  0.116588   \n",
       "fear              0.241958  0.440376  1.000000  0.159907  0.576962  0.141985   \n",
       "joy               0.583107  0.152731  0.159907  1.000000  0.113400  0.645827   \n",
       "negative          0.178827  0.552021  0.576962  0.113400  1.000000  0.105821   \n",
       "positive          0.452457  0.116588  0.141985  0.645827  0.105821  1.000000   \n",
       "sadness           0.198972  0.490181  0.583703  0.176440  0.612781  0.139827   \n",
       "surprise          0.460851  0.232166  0.248160  0.477317  0.226230  0.333998   \n",
       "trust             0.469028  0.153723  0.184240  0.582920  0.145220  0.648163   \n",
       "\n",
       "               sadness  surprise     trust  \n",
       "label         0.032641  0.018109  0.024014  \n",
       "pronouns      0.100827  0.106790  0.122914  \n",
       "text_len      0.384031  0.349498  0.389620  \n",
       "neg_vader     0.374256  0.159302  0.143060  \n",
       "pos_vader     0.095040  0.186243  0.231954  \n",
       "anger         0.528980  0.273195  0.169261  \n",
       "anticipation  0.198972  0.460851  0.469028  \n",
       "disgust       0.490181  0.232166  0.153723  \n",
       "fear          0.583703  0.248160  0.184240  \n",
       "joy           0.176440  0.477317  0.582920  \n",
       "negative      0.612781  0.226230  0.145220  \n",
       "positive      0.139827  0.333998  0.648163  \n",
       "sadness       1.000000  0.265026  0.171245  \n",
       "surprise      0.265026  1.000000  0.354746  \n",
       "trust         0.171245  0.354746  1.000000  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len', 'neg_vader', 'pos_vader'] + emotions].corr('spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from liwc_readDict import readDict\n",
    "\n",
    "liwc = readDict('/home/ana/resources/FakeOrFact/features/LIWC/LIWC/liwc.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'achieve',\n",
       " 'adverb',\n",
       " 'affect',\n",
       " 'anger',\n",
       " 'anx',\n",
       " 'article',\n",
       " 'assent',\n",
       " 'auxverb',\n",
       " 'bio',\n",
       " 'body',\n",
       " 'cause',\n",
       " 'certain',\n",
       " 'cogmech',\n",
       " 'conj',\n",
       " 'death',\n",
       " 'discrep',\n",
       " 'excl',\n",
       " 'family',\n",
       " 'feel',\n",
       " 'filler',\n",
       " 'friend',\n",
       " 'funct',\n",
       " 'future',\n",
       " 'health',\n",
       " 'hear',\n",
       " 'home',\n",
       " 'humans',\n",
       " 'i',\n",
       " 'incl',\n",
       " 'ingest',\n",
       " 'inhib',\n",
       " 'insight',\n",
       " 'ipron',\n",
       " 'leisure',\n",
       " 'money',\n",
       " 'motion',\n",
       " 'negate',\n",
       " 'negemo',\n",
       " 'nonfl',\n",
       " 'number',\n",
       " 'past',\n",
       " 'percept',\n",
       " 'posemo',\n",
       " 'ppron',\n",
       " 'preps',\n",
       " 'present',\n",
       " 'pronoun',\n",
       " 'quant',\n",
       " 'relativ',\n",
       " 'relig',\n",
       " 'sad',\n",
       " 'see',\n",
       " 'sexual',\n",
       " 'shehe',\n",
       " 'social',\n",
       " 'space',\n",
       " 'swear',\n",
       " 'tentat',\n",
       " 'they',\n",
       " 'time',\n",
       " 'verb',\n",
       " 'we',\n",
       " 'work',\n",
       " 'you'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = [c for (w,c) in liwc]\n",
    "set(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 'funct'],\n",
       " ['a', 'article'],\n",
       " ['abandon*', 'affect'],\n",
       " ['abandon*', 'negemo'],\n",
       " ['abandon*', 'sad'],\n",
       " ['abandon*', 'cogmech'],\n",
       " ['abandon*', 'inhib'],\n",
       " ['abdomen*', 'bio'],\n",
       " ['abdomen*', 'body'],\n",
       " ['abilit*', 'achieve'],\n",
       " ['able*', 'achieve'],\n",
       " ['abortion*', 'bio'],\n",
       " ['abortion*', 'health'],\n",
       " ['abortion*', 'sexual'],\n",
       " ['about', 'funct'],\n",
       " ['about', 'adverb'],\n",
       " ['about', 'preps'],\n",
       " ['above', 'funct'],\n",
       " ['above', 'preps'],\n",
       " ['above', 'space'],\n",
       " ['above', 'relativ'],\n",
       " ['abrupt*', 'time'],\n",
       " ['abrupt*', 'relativ'],\n",
       " ['abs', 'bio'],\n",
       " ['abs', 'body'],\n",
       " ['absent*', 'work'],\n",
       " ['absolute', 'cogmech'],\n",
       " ['absolute', 'certain'],\n",
       " ['absolutely', 'funct'],\n",
       " ['absolutely', 'adverb'],\n",
       " ['absolutely', 'cogmech'],\n",
       " ['absolutely', 'certain'],\n",
       " ['absolutely', 'assent'],\n",
       " ['abstain*', 'cogmech'],\n",
       " ['abstain*', 'inhib'],\n",
       " ['abuse*', 'affect'],\n",
       " ['abuse*', 'negemo'],\n",
       " ['abuse*', 'anger'],\n",
       " ['abusi*', 'affect'],\n",
       " ['abusi*', 'negemo'],\n",
       " ['abusi*', 'anger'],\n",
       " ['academ*', 'work'],\n",
       " ['accept', 'affect'],\n",
       " ['accept', 'posemo'],\n",
       " ['accept', 'cogmech'],\n",
       " ['accept', 'insight'],\n",
       " ['accepta*', 'affect'],\n",
       " ['accepta*', 'posemo'],\n",
       " ['accepta*', 'cogmech'],\n",
       " ['accepta*', 'insight'],\n",
       " ['accepted', 'verb'],\n",
       " ['accepted', 'past'],\n",
       " ['accepted', 'affect'],\n",
       " ['accepted', 'posemo'],\n",
       " ['accepted', 'cogmech'],\n",
       " ['accepted', 'insight'],\n",
       " ['accepting', 'affect'],\n",
       " ['accepting', 'posemo'],\n",
       " ['accepting', 'cogmech'],\n",
       " ['accepting', 'insight'],\n",
       " ['accepts', 'affect'],\n",
       " ['accepts', 'posemo'],\n",
       " ['accepts', 'cogmech'],\n",
       " ['accepts', 'insight'],\n",
       " ['accomplish*', 'work'],\n",
       " ['accomplish*', 'achieve'],\n",
       " ['account*', 'money'],\n",
       " ['accura*', 'cogmech'],\n",
       " ['accura*', 'certain'],\n",
       " ['ace', 'achieve'],\n",
       " ['ache*', 'affect'],\n",
       " ['ache*', 'negemo'],\n",
       " ['ache*', 'sad'],\n",
       " ['ache*', 'bio'],\n",
       " ['ache*', 'health'],\n",
       " ['achiev*', 'work'],\n",
       " ['achiev*', 'achieve'],\n",
       " ['aching', 'affect'],\n",
       " ['aching', 'negemo'],\n",
       " ['aching', 'sad'],\n",
       " ['aching', 'bio'],\n",
       " ['aching', 'health'],\n",
       " ['acid*', 'percept'],\n",
       " ['acknowledg*', 'cogmech'],\n",
       " ['acknowledg*', 'insight'],\n",
       " ['acne', 'bio'],\n",
       " ['acne', 'health'],\n",
       " ['acquainta*', 'social'],\n",
       " ['acquainta*', 'friend'],\n",
       " ['acquir*', 'achieve'],\n",
       " ['acquisition*', 'achieve'],\n",
       " ['acrid*', 'percept'],\n",
       " ['across', 'funct'],\n",
       " ['across', 'preps'],\n",
       " ['across', 'space'],\n",
       " ['across', 'relativ'],\n",
       " ['act', 'relativ'],\n",
       " ['act', 'motion'],\n",
       " ['action*', 'motion'],\n",
       " ['action*', 'relativ'],\n",
       " ['activat*', 'cogmech'],\n",
       " ['activat*', 'cause'],\n",
       " ['active*', 'affect'],\n",
       " ['active*', 'posemo'],\n",
       " ['actor*', 'leisure'],\n",
       " ['actress*', 'leisure'],\n",
       " ['actually', 'funct'],\n",
       " ['actually', 'adverb'],\n",
       " ['add', 'cogmech'],\n",
       " ['add', 'incl'],\n",
       " ['addict*', 'bio'],\n",
       " ['addict*', 'health'],\n",
       " ['addit*', 'cogmech'],\n",
       " ['addit*', 'incl'],\n",
       " ['address', 'home'],\n",
       " ['adequa*', 'achieve'],\n",
       " ['adjust*', 'cogmech'],\n",
       " ['adjust*', 'insight'],\n",
       " ['administrat*', 'work'],\n",
       " ['admir*', 'affect'],\n",
       " ['admir*', 'posemo'],\n",
       " ['admit', 'verb'],\n",
       " ['admit', 'present'],\n",
       " ['admit', 'social'],\n",
       " ['admit', 'cogmech'],\n",
       " ['admit', 'insight'],\n",
       " ['admits', 'verb'],\n",
       " ['admits', 'present'],\n",
       " ['admits', 'social'],\n",
       " ['admits', 'cogmech'],\n",
       " ['admits', 'insight'],\n",
       " ['admitted', 'verb'],\n",
       " ['admitted', 'past'],\n",
       " ['admitted', 'social'],\n",
       " ['admitted', 'cogmech'],\n",
       " ['admitted', 'insight'],\n",
       " ['admitting', 'social'],\n",
       " ['admitting', 'cogmech'],\n",
       " ['admitting', 'insight'],\n",
       " ['ador*', 'affect'],\n",
       " ['ador*', 'posemo'],\n",
       " ['adult', 'social'],\n",
       " ['adult', 'humans'],\n",
       " ['adults', 'social'],\n",
       " ['adults', 'humans'],\n",
       " ['advanc*', 'motion'],\n",
       " ['advanc*', 'relativ'],\n",
       " ['advanc*', 'achieve'],\n",
       " ['advantag*', 'affect'],\n",
       " ['advantag*', 'posemo'],\n",
       " ['advantag*', 'achieve'],\n",
       " ['adventur*', 'affect'],\n",
       " ['adventur*', 'posemo'],\n",
       " ['advers*', 'affect'],\n",
       " ['advers*', 'negemo'],\n",
       " ['advertising', 'work'],\n",
       " ['advice', 'social'],\n",
       " ['advil', 'bio'],\n",
       " ['advil', 'health'],\n",
       " ['advis*', 'social'],\n",
       " ['advis*', 'work'],\n",
       " ['aerobic*', 'leisure'],\n",
       " ['affair*', 'social'],\n",
       " ['affect', 'cogmech'],\n",
       " ['affect', 'cause'],\n",
       " ['affected', 'verb'],\n",
       " ['affected', 'past'],\n",
       " ['affected', 'cogmech'],\n",
       " ['affected', 'cause'],\n",
       " ['affecting', 'cogmech'],\n",
       " ['affecting', 'cause'],\n",
       " ['affection*', 'affect'],\n",
       " ['affection*', 'posemo'],\n",
       " ['affects', 'cogmech'],\n",
       " ['affects', 'cause'],\n",
       " ['afraid', 'affect'],\n",
       " ['afraid', 'negemo'],\n",
       " ['afraid', 'anx'],\n",
       " ['after', 'funct'],\n",
       " ['after', 'preps'],\n",
       " ['after', 'time'],\n",
       " ['after', 'relativ'],\n",
       " ['afterlife*', 'time'],\n",
       " ['afterlife*', 'relativ'],\n",
       " ['afterlife*', 'relig'],\n",
       " ['aftermath*', 'time'],\n",
       " ['aftermath*', 'relativ'],\n",
       " ['afternoon*', 'time'],\n",
       " ['afternoon*', 'relativ'],\n",
       " ['afterthought*', 'cogmech'],\n",
       " ['afterthought*', 'insight'],\n",
       " ['afterthought*', 'time'],\n",
       " ['afterthought*', 'relativ'],\n",
       " ['afterward*', 'time'],\n",
       " ['afterward*', 'relativ'],\n",
       " ['again', 'funct'],\n",
       " ['again', 'adverb'],\n",
       " ['again', 'time'],\n",
       " ['again', 'relativ'],\n",
       " ['against', 'funct'],\n",
       " ['against', 'preps'],\n",
       " ['age', 'time'],\n",
       " ['age', 'relativ'],\n",
       " ['aged', 'time'],\n",
       " ['aged', 'relativ'],\n",
       " ['agent', 'work'],\n",
       " ['agents', 'work'],\n",
       " ['ages', 'time'],\n",
       " ['ages', 'relativ'],\n",
       " ['aggravat*', 'affect'],\n",
       " ['aggravat*', 'negemo'],\n",
       " ['aggravat*', 'anger'],\n",
       " ['aggravat*', 'cogmech'],\n",
       " ['aggravat*', 'cause'],\n",
       " ['aggress*', 'affect'],\n",
       " ['aggress*', 'negemo'],\n",
       " ['aggress*', 'anger'],\n",
       " ['aging', 'time'],\n",
       " ['aging', 'relativ'],\n",
       " ['agitat*', 'affect'],\n",
       " ['agitat*', 'negemo'],\n",
       " ['agitat*', 'anger'],\n",
       " ['agnost*', 'relig'],\n",
       " ['ago', 'time'],\n",
       " ['ago', 'relativ'],\n",
       " ['agoniz*', 'affect'],\n",
       " ['agoniz*', 'negemo'],\n",
       " ['agoniz*', 'sad'],\n",
       " ['agony', 'affect'],\n",
       " ['agony', 'negemo'],\n",
       " ['agony', 'sad'],\n",
       " ['agree', 'affect'],\n",
       " ['agree', 'posemo'],\n",
       " ['agree', 'assent'],\n",
       " ['agreeab*', 'affect'],\n",
       " ['agreeab*', 'posemo'],\n",
       " ['agreed', 'affect'],\n",
       " ['agreed', 'posemo'],\n",
       " ['agreeing', 'affect'],\n",
       " ['agreeing', 'posemo'],\n",
       " ['agreement*', 'affect'],\n",
       " ['agreement*', 'posemo'],\n",
       " ['agrees', 'affect'],\n",
       " ['agrees', 'posemo'],\n",
       " ['ah', 'assent'],\n",
       " ['ahead', 'funct'],\n",
       " ['ahead', 'preps'],\n",
       " ['ahead', 'time'],\n",
       " ['ahead', 'relativ'],\n",
       " ['ahead', 'achieve'],\n",
       " ['aids', 'bio'],\n",
       " ['aids', 'health'],\n",
       " ['aids', 'sexual'],\n",
       " [\"ain't\", 'verb'],\n",
       " [\"ain't\", 'funct'],\n",
       " [\"ain't\", 'auxverb'],\n",
       " [\"ain't\", 'present'],\n",
       " [\"ain't\", 'negate'],\n",
       " ['aint', 'verb'],\n",
       " ['aint', 'funct'],\n",
       " ['aint', 'auxverb'],\n",
       " ['aint', 'present'],\n",
       " ['aint', 'negate'],\n",
       " ['air', 'relativ'],\n",
       " ['air', 'space'],\n",
       " ['alarm*', 'affect'],\n",
       " ['alarm*', 'negemo'],\n",
       " ['alarm*', 'anx'],\n",
       " ['alcohol*', 'bio'],\n",
       " ['alcohol*', 'health'],\n",
       " ['alcohol*', 'ingest'],\n",
       " ['alive', 'bio'],\n",
       " ['alive', 'health'],\n",
       " ['alive', 'death'],\n",
       " ['all', 'funct'],\n",
       " ['all', 'quant'],\n",
       " ['all', 'cogmech'],\n",
       " ['all', 'certain'],\n",
       " ['alla', 'relig'],\n",
       " ['allah*', 'relig'],\n",
       " ['allerg*', 'bio'],\n",
       " ['allerg*', 'health'],\n",
       " ['allot', 'funct'],\n",
       " ['allot', 'quant'],\n",
       " ['allot', 'cogmech'],\n",
       " ['allot', 'tentat'],\n",
       " ['allow*', 'cogmech'],\n",
       " ['allow*', 'cause'],\n",
       " ['almost', 'cogmech'],\n",
       " ['almost', 'tentat'],\n",
       " ['alone', 'affect'],\n",
       " ['alone', 'negemo'],\n",
       " ['alone', 'sad'],\n",
       " ['along', 'funct'],\n",
       " ['along', 'preps'],\n",
       " ['along', 'cogmech'],\n",
       " ['along', 'incl'],\n",
       " ['alot', 'funct'],\n",
       " ['alot', 'article'],\n",
       " ['alot', 'quant'],\n",
       " ['alot', 'cogmech'],\n",
       " ['alot', 'tentat'],\n",
       " ['already', 'time'],\n",
       " ['already', 'relativ'],\n",
       " ['alright*', 'affect'],\n",
       " ['alright*', 'posemo'],\n",
       " ['alright*', 'assent'],\n",
       " ['also', 'funct'],\n",
       " ['also', 'adverb'],\n",
       " ['also', 'conj'],\n",
       " ['altar*', 'relig'],\n",
       " ['although', 'funct'],\n",
       " ['although', 'conj'],\n",
       " ['altogether', 'cogmech'],\n",
       " ['altogether', 'certain'],\n",
       " ['always', 'cogmech'],\n",
       " ['always', 'certain'],\n",
       " ['always', 'time'],\n",
       " ['always', 'relativ'],\n",
       " ['am', 'verb'],\n",
       " ['am', 'funct'],\n",
       " ['am', 'auxverb'],\n",
       " ['am', 'present'],\n",
       " ['amaz*', 'affect'],\n",
       " ['amaz*', 'posemo'],\n",
       " ['ambigu*', 'cogmech'],\n",
       " ['ambigu*', 'tentat'],\n",
       " ['ambiti*', 'work'],\n",
       " ['ambiti*', 'achieve'],\n",
       " ['amen', 'relig'],\n",
       " ['amigo*', 'social'],\n",
       " ['amigo*', 'friend'],\n",
       " ['amish', 'relig'],\n",
       " ['among*', 'funct'],\n",
       " ['among*', 'preps'],\n",
       " ['among*', 'space'],\n",
       " ['among*', 'relativ'],\n",
       " ['amor*', 'affect'],\n",
       " ['amor*', 'posemo'],\n",
       " ['amount*', 'quant'],\n",
       " ['amput*', 'bio'],\n",
       " ['amput*', 'health'],\n",
       " ['amus*', 'affect'],\n",
       " ['amus*', 'posemo'],\n",
       " ['amus*', 'leisure'],\n",
       " ['an', 'funct'],\n",
       " ['an', 'article'],\n",
       " ['anal', 'cogmech'],\n",
       " ['anal', 'inhib'],\n",
       " ['anal', 'bio'],\n",
       " ['anal', 'body'],\n",
       " ['analy*', 'cogmech'],\n",
       " ['analy*', 'insight'],\n",
       " ['ancient*', 'time'],\n",
       " ['ancient*', 'relativ'],\n",
       " ['and', 'funct'],\n",
       " ['and', 'conj'],\n",
       " ['and', 'cogmech'],\n",
       " ['and', 'incl'],\n",
       " ['angel', 'relig'],\n",
       " ['angelic*', 'relig'],\n",
       " ['angels', 'relig'],\n",
       " ['anger*', 'affect'],\n",
       " ['anger*', 'negemo'],\n",
       " ['anger*', 'anger'],\n",
       " ['angr*', 'affect'],\n",
       " ['angr*', 'negemo'],\n",
       " ['angr*', 'anger'],\n",
       " ['anguish*', 'affect'],\n",
       " ['anguish*', 'negemo'],\n",
       " ['anguish*', 'anx'],\n",
       " ['ankle*', 'bio'],\n",
       " ['ankle*', 'body'],\n",
       " ['annoy*', 'affect'],\n",
       " ['annoy*', 'negemo'],\n",
       " ['annoy*', 'anger'],\n",
       " ['annual*', 'time'],\n",
       " ['annual*', 'relativ'],\n",
       " ['anorexi*', 'bio'],\n",
       " ['anorexi*', 'health'],\n",
       " ['anorexi*', 'ingest'],\n",
       " ['another', 'funct'],\n",
       " ['another', 'quant'],\n",
       " ['answer*', 'cogmech'],\n",
       " ['answer*', 'insight'],\n",
       " ['antacid*', 'bio'],\n",
       " ['antacid*', 'health'],\n",
       " ['antagoni*', 'affect'],\n",
       " ['antagoni*', 'negemo'],\n",
       " ['antagoni*', 'anger'],\n",
       " ['antidepressant*', 'bio'],\n",
       " ['antidepressant*', 'health'],\n",
       " ['anus*', 'bio'],\n",
       " ['anus*', 'body'],\n",
       " ['anxi*', 'affect'],\n",
       " ['anxi*', 'negemo'],\n",
       " ['anxi*', 'anx'],\n",
       " ['any', 'funct'],\n",
       " ['any', 'quant'],\n",
       " ['any', 'cogmech'],\n",
       " ['any', 'tentat'],\n",
       " ['anybod*', 'funct'],\n",
       " ['anybod*', 'pronoun'],\n",
       " ['anybod*', 'ipron'],\n",
       " ['anybod*', 'social'],\n",
       " ['anybod*', 'cogmech'],\n",
       " ['anybod*', 'tentat'],\n",
       " ['anyhow', 'cogmech'],\n",
       " ['anyhow', 'tentat'],\n",
       " ['anymore', 'funct'],\n",
       " ['anymore', 'quant'],\n",
       " ['anymore', 'relativ'],\n",
       " ['anymore', 'time'],\n",
       " ['anyone*', 'funct'],\n",
       " ['anyone*', 'pronoun'],\n",
       " ['anyone*', 'ipron'],\n",
       " ['anyone*', 'social'],\n",
       " ['anyone*', 'cogmech'],\n",
       " ['anyone*', 'tentat'],\n",
       " ['anything', 'funct'],\n",
       " ['anything', 'pronoun'],\n",
       " ['anything', 'ipron'],\n",
       " ['anything', 'cogmech'],\n",
       " ['anything', 'tentat'],\n",
       " ['anytime', 'cogmech'],\n",
       " ['anytime', 'tentat'],\n",
       " ['anytime', 'time'],\n",
       " ['anytime', 'relativ'],\n",
       " ['anyway*', 'funct'],\n",
       " ['anyway*', 'adverb'],\n",
       " ['anywhere', 'funct'],\n",
       " ['anywhere', 'adverb'],\n",
       " ['anywhere', 'cogmech'],\n",
       " ['anywhere', 'tentat'],\n",
       " ['anywhere', 'space'],\n",
       " ['anywhere', 'relativ'],\n",
       " ['aok', 'affect'],\n",
       " ['aok', 'posemo'],\n",
       " ['aok', 'assent'],\n",
       " ['apart', 'space'],\n",
       " ['apart', 'relativ'],\n",
       " ['apartment*', 'leisure'],\n",
       " ['apartment*', 'home'],\n",
       " ['apath*', 'affect'],\n",
       " ['apath*', 'negemo'],\n",
       " ['apolog*', 'social'],\n",
       " ['appall*', 'affect'],\n",
       " ['appall*', 'negemo'],\n",
       " ['apparent', 'cogmech'],\n",
       " ['apparent', 'certain'],\n",
       " ['apparently', 'funct'],\n",
       " ['apparently', 'adverb'],\n",
       " ['apparently', 'cogmech'],\n",
       " ['apparently', 'tentat'],\n",
       " ['appear', 'verb'],\n",
       " ['appear', 'present'],\n",
       " ['appear', 'cogmech'],\n",
       " ['appear', 'tentat'],\n",
       " ['appear', 'motion'],\n",
       " ['appear', 'relativ'],\n",
       " ['appeared', 'verb'],\n",
       " ['appeared', 'past'],\n",
       " ['appeared', 'cogmech'],\n",
       " ['appeared', 'tentat'],\n",
       " ['appeared', 'motion'],\n",
       " ['appeared', 'relativ'],\n",
       " ['appearing', 'cogmech'],\n",
       " ['appearing', 'tentat'],\n",
       " ['appearing', 'motion'],\n",
       " ['appearing', 'relativ'],\n",
       " ['appears', 'verb'],\n",
       " ['appears', 'present'],\n",
       " ['appears', 'cogmech'],\n",
       " ['appears', 'tentat'],\n",
       " ['appears', 'motion'],\n",
       " ['appears', 'relativ'],\n",
       " ['appendic*', 'bio'],\n",
       " ['appendic*', 'health'],\n",
       " ['appendix', 'bio'],\n",
       " ['appendix', 'body'],\n",
       " ['appeti*', 'bio'],\n",
       " ['appeti*', 'ingest'],\n",
       " ['applicant*', 'work'],\n",
       " ['applicat*', 'work'],\n",
       " ['appreciat*', 'affect'],\n",
       " ['appreciat*', 'posemo'],\n",
       " ['appreciat*', 'cogmech'],\n",
       " ['appreciat*', 'insight'],\n",
       " ['apprehens*', 'affect'],\n",
       " ['apprehens*', 'negemo'],\n",
       " ['apprehens*', 'anx'],\n",
       " ['apprentic*', 'work'],\n",
       " ['approach*', 'motion'],\n",
       " ['approach*', 'relativ'],\n",
       " ['approv*', 'achieve'],\n",
       " ['approximat*', 'cogmech'],\n",
       " ['approximat*', 'tentat'],\n",
       " ['april', 'time'],\n",
       " ['april', 'relativ'],\n",
       " ['arbitrar*', 'cogmech'],\n",
       " ['arbitrar*', 'tentat'],\n",
       " ['arch', 'bio'],\n",
       " ['arch', 'body'],\n",
       " ['are', 'verb'],\n",
       " ['are', 'funct'],\n",
       " ['are', 'auxverb'],\n",
       " ['are', 'present'],\n",
       " ['area*', 'space'],\n",
       " ['area*', 'relativ'],\n",
       " [\"aren't\", 'verb'],\n",
       " [\"aren't\", 'funct'],\n",
       " [\"aren't\", 'auxverb'],\n",
       " [\"aren't\", 'present'],\n",
       " [\"aren't\", 'negate'],\n",
       " ['arent', 'verb'],\n",
       " ['arent', 'funct'],\n",
       " ['arent', 'auxverb'],\n",
       " ['arent', 'present'],\n",
       " ['arent', 'negate'],\n",
       " ['argh*', 'affect'],\n",
       " ['argh*', 'negemo'],\n",
       " ['argh*', 'anger'],\n",
       " ['argu*', 'social'],\n",
       " ['argu*', 'affect'],\n",
       " ['argu*', 'negemo'],\n",
       " ['argu*', 'anger'],\n",
       " ['arm', 'bio'],\n",
       " ['arm', 'body'],\n",
       " ['armies', 'social'],\n",
       " ['armpit*', 'bio'],\n",
       " ['armpit*', 'body'],\n",
       " ['arms*', 'bio'],\n",
       " ['arms*', 'body'],\n",
       " ['army', 'social'],\n",
       " ['aroma*', 'percept'],\n",
       " ['around', 'funct'],\n",
       " ['around', 'adverb'],\n",
       " ['around', 'preps'],\n",
       " ['around', 'cogmech'],\n",
       " ['around', 'incl'],\n",
       " ['around', 'space'],\n",
       " ['around', 'relativ'],\n",
       " ['arous*', 'bio'],\n",
       " ['arous*', 'body'],\n",
       " ['arous*', 'sexual'],\n",
       " ['arrival*', 'motion'],\n",
       " ['arrival*', 'relativ'],\n",
       " ['arrive', 'verb'],\n",
       " ['arrive', 'present'],\n",
       " ['arrive', 'motion'],\n",
       " ['arrive', 'relativ'],\n",
       " ['arrived', 'verb'],\n",
       " ['arrived', 'past'],\n",
       " ['arrived', 'motion'],\n",
       " ['arrived', 'relativ'],\n",
       " ['arrives', 'verb'],\n",
       " ['arrives', 'present'],\n",
       " ['arrives', 'motion'],\n",
       " ['arrives', 'relativ'],\n",
       " ['arriving', 'motion'],\n",
       " ['arriving', 'relativ'],\n",
       " ['arrogan*', 'affect'],\n",
       " ['arrogan*', 'negemo'],\n",
       " ['arrogan*', 'anger'],\n",
       " ['arse', 'bio'],\n",
       " ['arse', 'body'],\n",
       " ['arse', 'swear'],\n",
       " ['arsehole*', 'swear'],\n",
       " ['arses', 'bio'],\n",
       " ['arses', 'body'],\n",
       " ['arses', 'swear'],\n",
       " ['art', 'leisure'],\n",
       " ['arter*', 'bio'],\n",
       " ['arter*', 'body'],\n",
       " ['arthr*', 'bio'],\n",
       " ['arthr*', 'health'],\n",
       " ['artist*', 'leisure'],\n",
       " ['arts', 'leisure'],\n",
       " ['as', 'funct'],\n",
       " ['as', 'preps'],\n",
       " ['as', 'conj'],\n",
       " ['asham*', 'affect'],\n",
       " ['asham*', 'negemo'],\n",
       " ['asham*', 'anx'],\n",
       " ['ask', 'verb'],\n",
       " ['ask', 'present'],\n",
       " ['ask', 'social'],\n",
       " ['asked', 'verb'],\n",
       " ['asked', 'past'],\n",
       " ['asked', 'social'],\n",
       " ['asking', 'social'],\n",
       " ['asks', 'verb'],\n",
       " ['asks', 'present'],\n",
       " ['asks', 'social'],\n",
       " ['asleep', 'bio'],\n",
       " ['asleep', 'body'],\n",
       " ['aspirin*', 'bio'],\n",
       " ['aspirin*', 'health'],\n",
       " ['ass', 'bio'],\n",
       " ['ass', 'body'],\n",
       " ['ass', 'sexual'],\n",
       " ['ass', 'swear'],\n",
       " ['assault*', 'affect'],\n",
       " ['assault*', 'negemo'],\n",
       " ['assault*', 'anger'],\n",
       " ['assembl*', 'social'],\n",
       " ['asses', 'bio'],\n",
       " ['asses', 'body'],\n",
       " ['asses', 'sexual'],\n",
       " ['asses', 'swear'],\n",
       " ['asshole*', 'affect'],\n",
       " ['asshole*', 'negemo'],\n",
       " ['asshole*', 'anger'],\n",
       " ['asshole*', 'swear'],\n",
       " ['assign*', 'work'],\n",
       " ['assistan*', 'work'],\n",
       " ['associat*', 'work'],\n",
       " ['assum*', 'cogmech'],\n",
       " ['assum*', 'insight'],\n",
       " ['assum*', 'tentat'],\n",
       " ['assur*', 'affect'],\n",
       " ['assur*', 'posemo'],\n",
       " ['assur*', 'cogmech'],\n",
       " ['assur*', 'certain'],\n",
       " ['asthma*', 'bio'],\n",
       " ['asthma*', 'health'],\n",
       " ['at', 'funct'],\n",
       " ['at', 'preps'],\n",
       " ['at', 'space'],\n",
       " ['at', 'relativ'],\n",
       " ['ate', 'verb'],\n",
       " ['ate', 'past'],\n",
       " ['ate', 'bio'],\n",
       " ['ate', 'ingest'],\n",
       " ['athletic*', 'leisure'],\n",
       " ['atho', 'funct'],\n",
       " ['atho', 'conj'],\n",
       " ['atm', 'money'],\n",
       " ['atms', 'money'],\n",
       " ['atop', 'funct'],\n",
       " ['atop', 'preps'],\n",
       " ['atop', 'space'],\n",
       " ['atop', 'relativ'],\n",
       " ['attachment*', 'affect'],\n",
       " ['attachment*', 'posemo'],\n",
       " ['attack*', 'affect'],\n",
       " ['attack*', 'negemo'],\n",
       " ['attack*', 'anger'],\n",
       " ['attain*', 'achieve'],\n",
       " ['attempt*', 'achieve'],\n",
       " ['attend', 'motion'],\n",
       " ['attend', 'relativ'],\n",
       " ['attended', 'motion'],\n",
       " ['attended', 'relativ'],\n",
       " ['attending', 'motion'],\n",
       " ['attending', 'relativ'],\n",
       " ['attends', 'motion'],\n",
       " ['attends', 'relativ'],\n",
       " ['attent*', 'cogmech'],\n",
       " ['attent*', 'insight'],\n",
       " ['attract*', 'affect'],\n",
       " ['attract*', 'posemo'],\n",
       " ['attribut*', 'cogmech'],\n",
       " ['attribut*', 'cause'],\n",
       " ['auction*', 'money'],\n",
       " ['audibl*', 'percept'],\n",
       " ['audibl*', 'hear'],\n",
       " ['audio*', 'percept'],\n",
       " ['audio*', 'hear'],\n",
       " ['audit', 'money'],\n",
       " ['audited', 'money'],\n",
       " ['auditing', 'money'],\n",
       " ['auditor', 'money'],\n",
       " ['auditorium*', 'work'],\n",
       " ['auditors', 'money'],\n",
       " ['audits', 'money'],\n",
       " ['august', 'time'],\n",
       " ['august', 'relativ'],\n",
       " ['aunt*', 'social'],\n",
       " ['aunt*', 'family'],\n",
       " ['authorit*', 'achieve'],\n",
       " ['autops*', 'death'],\n",
       " ['autumn', 'time'],\n",
       " ['autumn', 'relativ'],\n",
       " ['aversi*', 'affect'],\n",
       " ['aversi*', 'negemo'],\n",
       " ['aversi*', 'anx'],\n",
       " ['avert*', 'cogmech'],\n",
       " ['avert*', 'inhib'],\n",
       " ['avoid*', 'affect'],\n",
       " ['avoid*', 'negemo'],\n",
       " ['avoid*', 'anx'],\n",
       " ['avoid*', 'cogmech'],\n",
       " ['avoid*', 'inhib'],\n",
       " ['aw', 'assent'],\n",
       " ['award*', 'affect'],\n",
       " ['award*', 'posemo'],\n",
       " ['award*', 'work'],\n",
       " ['award*', 'achieve'],\n",
       " ['aware*', 'cogmech'],\n",
       " ['aware*', 'insight'],\n",
       " ['away', 'funct'],\n",
       " ['away', 'preps'],\n",
       " ['away', 'space'],\n",
       " ['away', 'relativ'],\n",
       " ['awesome', 'affect'],\n",
       " ['awesome', 'posemo'],\n",
       " ['awesome', 'assent'],\n",
       " ['awful', 'affect'],\n",
       " ['awful', 'negemo'],\n",
       " ['awhile', 'time'],\n",
       " ['awhile', 'relativ'],\n",
       " ['awkward*', 'affect'],\n",
       " ['awkward*', 'negemo'],\n",
       " ['awkward*', 'anx'],\n",
       " ['babe*', 'social'],\n",
       " ['babe*', 'humans'],\n",
       " ['babies', 'social'],\n",
       " ['babies', 'humans'],\n",
       " ['baby*', 'social'],\n",
       " ['baby*', 'humans'],\n",
       " ['back', 'funct'],\n",
       " ['back', 'adverb'],\n",
       " ['back', 'time'],\n",
       " ['back', 'relativ'],\n",
       " ['backward*', 'space'],\n",
       " ['backward*', 'relativ'],\n",
       " ['backyard', 'home'],\n",
       " ['bad', 'affect'],\n",
       " ['bad', 'negemo'],\n",
       " ['bake*', 'bio'],\n",
       " ['bake*', 'ingest'],\n",
       " ['bake*', 'home'],\n",
       " ['baking', 'bio'],\n",
       " ['baking', 'ingest'],\n",
       " ['baking', 'home'],\n",
       " ['balcon*', 'home'],\n",
       " ['bald', 'bio'],\n",
       " ['bald', 'body'],\n",
       " ['ball', 'leisure'],\n",
       " ['ballet*', 'leisure'],\n",
       " ['bambino*', 'social'],\n",
       " ['bambino*', 'humans'],\n",
       " ['ban', 'cogmech'],\n",
       " ['ban', 'inhib'],\n",
       " ['band', 'social'],\n",
       " ['band', 'leisure'],\n",
       " ['bandage*', 'bio'],\n",
       " ['bandage*', 'health'],\n",
       " ['bandaid', 'bio'],\n",
       " ['bandaid', 'health'],\n",
       " ['bands', 'social'],\n",
       " ['bands', 'leisure'],\n",
       " ['bank*', 'money'],\n",
       " ['banned', 'cogmech'],\n",
       " ['banned', 'inhib'],\n",
       " ['banning', 'cogmech'],\n",
       " ['banning', 'inhib'],\n",
       " ['bans', 'cogmech'],\n",
       " ['bans', 'inhib'],\n",
       " ['baptis*', 'relig'],\n",
       " ['baptiz*', 'relig'],\n",
       " ['bar', 'bio'],\n",
       " ['bar', 'ingest'],\n",
       " ['bar', 'leisure'],\n",
       " ['barely', 'cogmech'],\n",
       " ['barely', 'tentat'],\n",
       " ['bargain*', 'money'],\n",
       " ['barrier*', 'cogmech'],\n",
       " ['barrier*', 'inhib'],\n",
       " ['bars', 'bio'],\n",
       " ['bars', 'ingest'],\n",
       " ['bars', 'leisure'],\n",
       " ['baseball*', 'leisure'],\n",
       " ['based', 'cogmech'],\n",
       " ['based', 'cause'],\n",
       " ['bases', 'cogmech'],\n",
       " ['bases', 'cause'],\n",
       " ['bashful*', 'affect'],\n",
       " ['bashful*', 'negemo'],\n",
       " ['basically', 'funct'],\n",
       " ['basically', 'adverb'],\n",
       " ['basis', 'cogmech'],\n",
       " ['basis', 'cause'],\n",
       " ['basketball*', 'leisure'],\n",
       " ['bastard*', 'affect'],\n",
       " ['bastard*', 'negemo'],\n",
       " ['bastard*', 'anger'],\n",
       " ['bastard*', 'swear'],\n",
       " ['bath*', 'leisure'],\n",
       " ['bath*', 'home'],\n",
       " ['battl*', 'affect'],\n",
       " ['battl*', 'negemo'],\n",
       " ['battl*', 'anger'],\n",
       " ['be', 'verb'],\n",
       " ['be', 'funct'],\n",
       " ['be', 'auxverb'],\n",
       " ['beach*', 'leisure'],\n",
       " ['beat', 'achieve'],\n",
       " ['beaten', 'affect'],\n",
       " ['beaten', 'negemo'],\n",
       " ['beaten', 'anger'],\n",
       " ['beaten', 'work'],\n",
       " ['beaten', 'achieve'],\n",
       " ['beaut*', 'affect'],\n",
       " ['beaut*', 'posemo'],\n",
       " ['beaut*', 'percept'],\n",
       " ['beaut*', 'see'],\n",
       " ['became', 'verb'],\n",
       " ['became', 'funct'],\n",
       " ['became', 'auxverb'],\n",
       " ['became', 'past'],\n",
       " ['became', 'cogmech'],\n",
       " ['became', 'insight'],\n",
       " ['because', 'funct'],\n",
       " ['because', 'conj'],\n",
       " ['because', 'cogmech'],\n",
       " ['because', 'cause'],\n",
       " ['become', 'verb'],\n",
       " ['become', 'funct'],\n",
       " ['become', 'auxverb'],\n",
       " ['become', 'present'],\n",
       " ['become', 'cogmech'],\n",
       " ['become', 'insight'],\n",
       " ['becomes', 'verb'],\n",
       " ['becomes', 'funct'],\n",
       " ['becomes', 'auxverb'],\n",
       " ['becomes', 'present'],\n",
       " ['becomes', 'cogmech'],\n",
       " ['becomes', 'insight'],\n",
       " ['becoming', 'verb'],\n",
       " ['becoming', 'funct'],\n",
       " ['becoming', 'auxverb'],\n",
       " ['becoming', 'cogmech'],\n",
       " ['becoming', 'insight'],\n",
       " ['bed', 'home'],\n",
       " ['bedding', 'home'],\n",
       " ['bedroom*', 'home'],\n",
       " ['beds', 'home'],\n",
       " ['been', 'verb'],\n",
       " ['been', 'funct'],\n",
       " ['been', 'auxverb'],\n",
       " ['been', 'past'],\n",
       " ['beer*', 'bio'],\n",
       " ['beer*', 'ingest'],\n",
       " ['beer*', 'leisure'],\n",
       " ['before', 'funct'],\n",
       " ['before', 'preps'],\n",
       " ['before', 'time'],\n",
       " ['before', 'relativ'],\n",
       " ['began', 'verb'],\n",
       " ['began', 'past'],\n",
       " ['began', 'time'],\n",
       " ['began', 'relativ'],\n",
       " ['beggar*', 'money'],\n",
       " ['begging', 'money'],\n",
       " ['begin', 'verb'],\n",
       " ['begin', 'present'],\n",
       " ['begin', 'time'],\n",
       " ['begin', 'relativ'],\n",
       " ['beginn*', 'time'],\n",
       " ['beginn*', 'relativ'],\n",
       " ['begins', 'verb'],\n",
       " ['begins', 'present'],\n",
       " ['begins', 'time'],\n",
       " ['begins', 'relativ'],\n",
       " ['begun', 'time'],\n",
       " ['begun', 'relativ'],\n",
       " ['behavio*', 'relativ'],\n",
       " ['behavio*', 'motion'],\n",
       " ['behind', 'funct'],\n",
       " ['behind', 'preps'],\n",
       " ['being', 'verb'],\n",
       " ['being', 'funct'],\n",
       " ['being', 'auxverb'],\n",
       " ['belief*', 'cogmech'],\n",
       " ['belief*', 'insight'],\n",
       " ['belief*', 'relig'],\n",
       " ['believe', 'verb'],\n",
       " ['believe', 'present'],\n",
       " ['believe', 'cogmech'],\n",
       " ['believe', 'insight'],\n",
       " ['believed', 'verb'],\n",
       " ['believed', 'past'],\n",
       " ['believed', 'cogmech'],\n",
       " ['believed', 'insight'],\n",
       " ['believes', 'verb'],\n",
       " ['believes', 'present'],\n",
       " ['believes', 'cogmech'],\n",
       " ['believes', 'insight'],\n",
       " ['believing', 'cogmech'],\n",
       " ['believing', 'insight'],\n",
       " ['bellies', 'bio'],\n",
       " ['bellies', 'body'],\n",
       " ['belly', 'bio'],\n",
       " ['belly', 'body'],\n",
       " ['beloved', 'affect'],\n",
       " ['beloved', 'posemo'],\n",
       " ['below', 'funct'],\n",
       " ['below', 'preps'],\n",
       " ['below', 'space'],\n",
       " ['below', 'relativ'],\n",
       " ['bend', 'space'],\n",
       " ['bend', 'relativ'],\n",
       " ['bending', 'space'],\n",
       " ['bending', 'relativ'],\n",
       " ['bends', 'space'],\n",
       " ['bends', 'relativ'],\n",
       " ['beneath', 'funct'],\n",
       " ['beneath', 'preps'],\n",
       " ['beneath', 'space'],\n",
       " ['beneath', 'relativ'],\n",
       " ['benefic*', 'affect'],\n",
       " ['benefic*', 'posemo'],\n",
       " ['benefit', 'affect'],\n",
       " ['benefit', 'posemo'],\n",
       " ['benefits', 'affect'],\n",
       " ['benefits', 'posemo'],\n",
       " ['benefits', 'work'],\n",
       " ['benefitt*', 'affect'],\n",
       " ['benefitt*', 'posemo'],\n",
       " ['benevolen*', 'affect'],\n",
       " ['benevolen*', 'posemo'],\n",
       " ['benign*', 'affect'],\n",
       " ['benign*', 'posemo'],\n",
       " ['bent', 'space'],\n",
       " ['bent', 'relativ'],\n",
       " ['bereave*', 'death'],\n",
       " ['beside', 'funct'],\n",
       " ['beside', 'preps'],\n",
       " ['beside', 'space'],\n",
       " ['beside', 'relativ'],\n",
       " ['besides', 'funct'],\n",
       " ['besides', 'preps'],\n",
       " ['besides', 'quant'],\n",
       " ['besides', 'cogmech'],\n",
       " ['besides', 'discrep'],\n",
       " ['best', 'affect'],\n",
       " ['best', 'posemo'],\n",
       " ['best', 'achieve'],\n",
       " ['best', 'funct'],\n",
       " ['best', 'quant'],\n",
       " ['bet', 'cogmech'],\n",
       " ['bet', 'tentat'],\n",
       " ['bet', 'money'],\n",
       " ['bets', 'cogmech'],\n",
       " ['bets', 'tentat'],\n",
       " ['bets', 'money'],\n",
       " ['better', 'affect'],\n",
       " ['better', 'posemo'],\n",
       " ['better', 'achieve'],\n",
       " ['betting', 'cogmech'],\n",
       " ['betting', 'tentat'],\n",
       " ['betting', 'money'],\n",
       " ['between', 'funct'],\n",
       " ['between', 'preps'],\n",
       " ['beyond', 'funct'],\n",
       " ['beyond', 'adverb'],\n",
       " ['beyond', 'preps'],\n",
       " ['beyond', 'space'],\n",
       " ['beyond', 'relativ'],\n",
       " ['bf*', 'social'],\n",
       " ['bf*', 'friend'],\n",
       " ['bi', 'bio'],\n",
       " ['bi', 'sexual'],\n",
       " ['biannu*', 'time'],\n",
       " ['biannu*', 'relativ'],\n",
       " ['bible*', 'relig'],\n",
       " ['biblic*', 'relig'],\n",
       " ['bicep*', 'bio'],\n",
       " ['bicep*', 'body'],\n",
       " ['bicyc*', 'leisure'],\n",
       " ['big', 'space'],\n",
       " ['big', 'relativ'],\n",
       " ['bigger', 'space'],\n",
       " ['bigger', 'relativ'],\n",
       " ['biggest', 'space'],\n",
       " ['biggest', 'relativ'],\n",
       " ['bike*', 'leisure'],\n",
       " ['bill', 'money'],\n",
       " ['billed', 'money'],\n",
       " ['billing*', 'money'],\n",
       " ['billion*', 'funct'],\n",
       " ['billion*', 'number'],\n",
       " ['bills', 'money'],\n",
       " ['bimonth*', 'time'],\n",
       " ['bimonth*', 'relativ'],\n",
       " ['binding', 'cogmech'],\n",
       " ['binding', 'inhib'],\n",
       " ['binge*', 'bio'],\n",
       " ['binge*', 'health'],\n",
       " ['binge*', 'ingest'],\n",
       " ['binging', 'bio'],\n",
       " ['binging', 'health'],\n",
       " ['binging', 'ingest'],\n",
       " ['biolog*', 'work'],\n",
       " ['bipolar', 'bio'],\n",
       " ['bipolar', 'health'],\n",
       " ['birdie*', 'leisure'],\n",
       " ['birth*', 'time'],\n",
       " ...]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liwc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_dict = {}\n",
    "for (w, c) in liwc:\n",
    "    if c not in liwc_dict:\n",
    "        liwc_dict[c] = []\n",
    "    liwc_dict[c].append(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anybod*',\n",
       " 'anyone*',\n",
       " 'anything',\n",
       " 'everybod*',\n",
       " 'everyone*',\n",
       " 'everything*',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he's\",\n",
       " 'hed',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'hes',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'id',\n",
       " 'im',\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'itd',\n",
       " 'itll',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'ive',\n",
       " \"let's\",\n",
       " 'lets',\n",
       " 'me',\n",
       " 'mine',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'nobod*',\n",
       " 'oneself',\n",
       " 'other',\n",
       " 'others',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'shes',\n",
       " 'somebod*',\n",
       " 'someone*',\n",
       " 'something*',\n",
       " 'somewhere',\n",
       " 'stuff',\n",
       " 'that',\n",
       " \"that'd\",\n",
       " \"that'll\",\n",
       " \"that's\",\n",
       " 'thatd',\n",
       " 'thatll',\n",
       " 'thats',\n",
       " 'thee',\n",
       " 'their*',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they've\",\n",
       " 'theyd',\n",
       " 'theyll',\n",
       " 'theyve',\n",
       " 'thine',\n",
       " 'thing*',\n",
       " 'this',\n",
       " 'those',\n",
       " 'thou',\n",
       " 'thoust',\n",
       " 'thy',\n",
       " 'us',\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'weve',\n",
       " 'what',\n",
       " \"what's\",\n",
       " 'whatever',\n",
       " 'whats',\n",
       " 'which',\n",
       " 'whichever',\n",
       " 'who',\n",
       " \"who'd\",\n",
       " \"who'll\",\n",
       " 'whod',\n",
       " 'wholl',\n",
       " 'whom',\n",
       " 'whose',\n",
       " \"y'all\",\n",
       " 'ya',\n",
       " 'yall',\n",
       " 'ye',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'youd',\n",
       " 'youll',\n",
       " 'your',\n",
       " 'youre',\n",
       " 'yours',\n",
       " 'youve']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liwc_dict['pronoun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_liwc_categories(tokens, category_words, relative=True):\n",
    "    category_cnt = 0\n",
    "    if not tokens:\n",
    "        return None\n",
    "    text_len = len(tokens)\n",
    "    for t in tokens:\n",
    "        for word in category_words:\n",
    "            if t==word or (word[-1]=='*' and t.startswith(word[:-1])) \\\n",
    "            or (t==word.split(\"'\")[0]):\n",
    "                category_cnt += 1\n",
    "                break # one token cannot belong to more than one word in the category\n",
    "    if relative:\n",
    "        return category_cnt/text_len\n",
    "    else:\n",
    "        return category_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing for category pronoun...\n",
      "CPU times: user 1min 13s, sys: 37.9 ms, total: 1min 13s\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from functools import partial\n",
    "# for categ in ['negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']:#liwc_dict.keys():\n",
    "for categ in liwc_dict.keys():\n",
    "    if categ in writings_df.columns:\n",
    "        continue\n",
    "    print(\"Encoding for category %s...\" % categ)\n",
    "    writings_df[categ] = writings_df['all_tokens'].apply(partial(encode_liwc_categories, \n",
    "                                                                   category_words=liwc_dict[categ], \n",
    "                                                                   relative=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>negemo</th>\n",
       "      <th>posemo</th>\n",
       "      <th>affect</th>\n",
       "      <th>sad</th>\n",
       "      <th>anx</th>\n",
       "      <th>pronoun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.110183</td>\n",
       "      <td>0.036761</td>\n",
       "      <td>0.079594</td>\n",
       "      <td>0.165661</td>\n",
       "      <td>0.253118</td>\n",
       "      <td>0.404901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negemo</th>\n",
       "      <td>0.110183</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.169785</td>\n",
       "      <td>0.212622</td>\n",
       "      <td>0.415533</td>\n",
       "      <td>0.402437</td>\n",
       "      <td>0.143275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>posemo</th>\n",
       "      <td>0.036761</td>\n",
       "      <td>-0.169785</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.926702</td>\n",
       "      <td>-0.115436</td>\n",
       "      <td>-0.075223</td>\n",
       "      <td>0.239505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>affect</th>\n",
       "      <td>0.079594</td>\n",
       "      <td>0.212622</td>\n",
       "      <td>0.926702</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.043988</td>\n",
       "      <td>0.079093</td>\n",
       "      <td>0.292047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sad</th>\n",
       "      <td>0.165661</td>\n",
       "      <td>0.415533</td>\n",
       "      <td>-0.115436</td>\n",
       "      <td>0.043988</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.117133</td>\n",
       "      <td>0.104218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anx</th>\n",
       "      <td>0.253118</td>\n",
       "      <td>0.402437</td>\n",
       "      <td>-0.075223</td>\n",
       "      <td>0.079093</td>\n",
       "      <td>0.117133</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.156770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pronoun</th>\n",
       "      <td>0.404901</td>\n",
       "      <td>0.143275</td>\n",
       "      <td>0.239505</td>\n",
       "      <td>0.292047</td>\n",
       "      <td>0.104218</td>\n",
       "      <td>0.156770</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            label    negemo    posemo    affect       sad       anx   pronoun\n",
       "label    1.000000  0.110183  0.036761  0.079594  0.165661  0.253118  0.404901\n",
       "negemo   0.110183  1.000000 -0.169785  0.212622  0.415533  0.402437  0.143275\n",
       "posemo   0.036761 -0.169785  1.000000  0.926702 -0.115436 -0.075223  0.239505\n",
       "affect   0.079594  0.212622  0.926702  1.000000  0.043988  0.079093  0.292047\n",
       "sad      0.165661  0.415533 -0.115436  0.043988  1.000000  0.117133  0.104218\n",
       "anx      0.253118  0.402437 -0.075223  0.079093  0.117133  1.000000  0.156770\n",
       "pronoun  0.404901  0.143275  0.239505  0.292047  0.104218  0.156770  1.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.groupby('subject').mean()[['label', 'negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negemo</th>\n",
       "      <th>posemo</th>\n",
       "      <th>affect</th>\n",
       "      <th>sad</th>\n",
       "      <th>anx</th>\n",
       "      <th>pronoun</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.023493</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>0.074548</td>\n",
       "      <td>0.003242</td>\n",
       "      <td>0.002606</td>\n",
       "      <td>0.120154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.026116</td>\n",
       "      <td>0.056145</td>\n",
       "      <td>0.082611</td>\n",
       "      <td>0.003706</td>\n",
       "      <td>0.003591</td>\n",
       "      <td>0.162310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         negemo    posemo    affect       sad       anx   pronoun\n",
       "label                                                            \n",
       "0      0.023493  0.050800  0.074548  0.003242  0.002606  0.120154\n",
       "1      0.026116  0.056145  0.082611  0.003706  0.003591  0.162310"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['label', 'negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']].groupby('label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-bf3a999e8d68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwritings_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'subject'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mcorr\u001b[0;34m(self, method, min_periods)\u001b[0m\n\u001b[1;32m   7513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7514\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pearson\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7515\u001b[0;31m             \u001b[0mcorrel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibalgos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnancorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_float64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_periods\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7516\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"spearman\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7517\u001b[0m             \u001b[0mcorrel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibalgos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnancorr_spearman\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_float64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_periods\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "writings_df.groupby('subject').mean()[['label'] + categories].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: COMET_OPTIMIZER_ID=786fc2b3654047e69f492db122f55b95\n",
      "COMET INFO: Using optimizer config: {'algorithm': 'random', 'configSpaceSize': 600000000000, 'endTime': None, 'id': '786fc2b3654047e69f492db122f55b95', 'lastUpdateTime': None, 'maxCombo': 0, 'name': '786fc2b3654047e69f492db122f55b95', 'parameters': {'batch_size': {'max': 512, 'min': 10, 'scalingType': 'loguniform', 'type': 'integer'}, 'decay': {'max': 0.5, 'min': 1e-08, 'scalingType': 'loguniform', 'type': 'float'}, 'dense_bow_units': {'max': 20, 'min': 1, 'scalingType': 'uniform', 'type': 'integer'}, 'dropout': {'max': 0.7, 'min': 0, 'scalingType': 'uniform', 'type': 'float'}, 'freeze_patience': {'max': 16, 'min': 2, 'scalingType': 'uniform', 'type': 'integer'}, 'l2_dense': {'max': 0.5, 'min': 1e-05, 'scalingType': 'loguniform', 'type': 'float'}, 'lr': {'max': 0.05, 'min': 1e-05, 'scalingType': 'loguniform', 'type': 'float'}, 'lr_reduce_factor': {'max': 0.8, 'min': 0.0001, 'scalingType': 'uniform', 'type': 'float'}, 'lr_reduce_patience': {'max': 16, 'min': 2, 'scalingType': 'uniform', 'type': 'integer'}, 'lstm_units': {'max': 100, 'min': 10, 'scalingType': 'uniform', 'type': 'integer'}, 'optimizer': {'type': 'categorical', 'values': ['adam', 'adagrad', '']}, 'positive_class_weight': {'max': 25, 'min': 1, 'scalingType': 'uniform', 'type': 'integer'}, 'trainable_embeddings': {'type': 'discrete', 'values': [True, False]}}, 'predictor': None, 'spec': {'gridSize': 10, 'maxCombo': 0, 'metric': 'loss', 'minSampleSize': 100, 'objective': 'minimize', 'retryAssignLimit': 0, 'retryLimit': 20, 'seed': 3968493229}, 'startTime': 15879681523, 'state': {'sequence_i': 0, 'sequence_pid': None, 'sequence_retry': 0}, 'status': 'running', 'suggestion_count': 0, 'trials': 1, 'version': '1.0.24'}\n",
      "COMET INFO: Optimizer metrics is 'loss' but no logged values found. Experiment ignored in sweep.\n",
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary:\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     url: https://www.comet.ml/ananana/mental/1ebbfc14aa9440e1948aabac5664d837\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     batch_binary_accuracy [29]: (0.5306122303009033, 0.9572663307189941)\n",
      "COMET INFO:     batch_f1_m [29]           : (0.0030426757875829935, 0.0336134247481823)\n",
      "COMET INFO:     batch_loss [29]           : (0.30054065585136414, 1.4031785726547241)\n",
      "COMET INFO:     batch_precision_m [29]    : (0.011335793882608414, 0.09260831028223038)\n",
      "COMET INFO:     batch_recall_m [29]       : (0.002346971072256565, 0.1666666716337204)\n",
      "COMET INFO:     sys.cpu.percent.01 [20]   : (4.8, 82.9)\n",
      "COMET INFO:     sys.cpu.percent.02 [20]   : (5.2, 73.9)\n",
      "COMET INFO:     sys.cpu.percent.03 [20]   : (4.6, 60.6)\n",
      "COMET INFO:     sys.cpu.percent.04 [20]   : (3.3, 57.8)\n",
      "COMET INFO:     sys.cpu.percent.avg [20]  : (4.925, 66.2)\n",
      "COMET INFO:     sys.gpu.0.total_memory    : (1073414144.0, 1073414144.0)\n",
      "COMET INFO:     sys.load.avg [20]         : (0.16, 4.02)\n",
      "COMET INFO:     sys.ram.total [20]        : (8277307392.0, 8277307392.0)\n",
      "COMET INFO:     sys.ram.used [20]         : (7586635776.0, 7714193408.0)\n",
      "COMET INFO:   Other [count]:\n",
      "COMET INFO:     optimizer_count       : 3\n",
      "COMET INFO:     optimizer_id          : 57eb178250e2401aa14cfc6860a4217a\n",
      "COMET INFO:     optimizer_metric      : loss\n",
      "COMET INFO:     optimizer_metric_value: None\n",
      "COMET INFO:     optimizer_parameters  : {'batch_size': 245, 'decay': 0.1402033634855056, 'dense_bow_units': 4, 'dropout': 0.46002110523805456, 'freeze_patience': 15, 'l2_dense': 0.0814165473266369, 'lr': 1.6244588122197995e-05, 'lr_reduce_factor': 0.6638163734300414, 'lr_reduce_patience': 10, 'lstm_units': 92, 'optimizer': 'adagrad', 'positive_class_weight': 5, 'set_trainable': True, 'trainable_embeddings': False}\n",
      "COMET INFO:     optimizer_pid         : 681f4971a3f98a865eb3100a742e6621e62a3420\n",
      "COMET INFO:     optimizer_process     : 2985\n",
      "COMET INFO:     optimizer_trial       : 1\n",
      "COMET INFO:     optimizer_version     : 1.0.24\n",
      "COMET INFO:     trainable_params      : 2071854\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     git-patch  : 1\n",
      "COMET INFO:     histogram3d: 1\n",
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/ananana/mental/6925a9644c034cbf9cd7f12c0a96e0f3\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 111375 samples, validate on 31863 samples\n",
      "Epoch 1/15\n",
      "111320/111375 [============================>.] - ETA: 0s - loss: 0.6696 - binary_accuracy: 0.8707 - f1_m: 0.1200 - precision_m: 0.0902 - recall_m: 0.2582\n",
      "Epoch 00001: val_loss improved from inf to 0.48204, saving model to models/experiment_best\n",
      "111375/111375 [==============================] - 317s 3ms/sample - loss: 0.6695 - binary_accuracy: 0.8707 - f1_m: 0.1201 - precision_m: 0.0902 - recall_m: 0.2584 - val_loss: 0.4820 - val_binary_accuracy: 0.7968 - val_f1_m: 0.2121 - val_precision_m: 0.1454 - val_recall_m: 0.4314\n",
      "Epoch 2/15\n",
      " 47190/111375 [===========>..................] - ETA: 2:52 - loss: 0.5882 - binary_accuracy: 0.8703 - f1_m: 0.1490 - precision_m: 0.1088 - recall_m: 0.2949"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-f94476829a7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m                           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                           \u001b[0mcallback_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfreeze_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_lr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                       model_path='models/experiment')\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-124-48d6d530785b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, x_train, y_train, x_test, y_test, batch_size, epochs, class_weight, start_epoch, workers, callback_list, model_path)\u001b[0m\n\u001b[1;32m     17\u001b[0m             callbacks = [\n\u001b[1;32m     18\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%s_best'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcallback_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             ])\n\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/comet_ml/monkey_patching.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m                     )\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Call after callbacks once we have the return value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Declare your hyperparameters search:\n",
    "tune_epochs=15\n",
    "config = {\n",
    "      \"algorithm\": \"random\",\n",
    "      \"parameters\": {\n",
    "          \"lstm_units\": {\"type\": \"integer\", \"min\": 10, \"max\": 100},\n",
    "          \"dense_bow_units\": {\"type\": \"integer\", \"min\": 1, \"max\": 20},\n",
    "          \"lr\": {\"type\": \"float\", \"min\": 0.00001, \"max\": 0.05, \"scalingType\": \"loguniform\"},\n",
    "          \"l2_dense\": {\"type\": \"float\", \"min\": 0.00001, \"max\": 0.5, \"scalingType\": \"loguniform\"},\n",
    "          \"dropout\": {\"type\": \"float\", \"min\": 0, \"max\": 0.7, \"scalingType\": \"uniform\"},\n",
    "          \"optimizer\": {\"type\": \"categorical\", \"values\": [\"adam\", \"adagrad\", \"\"]},\n",
    "          \"batch_size\": {\"type\": \"integer\", \"min\": 10, \"max\": 512, \"scalingType\": \"loguniform\"},\n",
    "          \"positive_class_weight\": {\"type\": \"integer\", \"min\": 1, \"max\": 25},\n",
    "          \"trainable_embeddings\": {\"type\": \"discrete\", \"values\": [True, False]},\n",
    "          \"freeze_patience\": {\"type\": \"integer\", \"min\": 2, \"max\": tune_epochs+1},\n",
    "          \"lr_reduce_factor\": {\"type\": \"float\", \"min\": 0.0001, \"max\": 0.8},\n",
    "          \"lr_reduce_patience\": {\"type\": \"integer\", \"min\": 2, \"max\": tune_epochs+1},\n",
    "          \"decay\": {\"type\": \"float\", \"min\": 0.00000001, \"max\": 0.5, \"scalingType\": \"loguniform\"},          \n",
    "      },\n",
    "      \"spec\": {\n",
    "          \"metric\": \"loss\",\n",
    "          \"objective\": \"minimize\",\n",
    "      },\n",
    "  }\n",
    "optimizer = Optimizer(config, api_key=\"eoBdVyznAhfg3bK9pZ58ZSXfv\")\n",
    "\n",
    "for experiment in optimizer.get_experiments(project_name=\"mental\"):\n",
    "    experiment.add_tag(\"tune\")\n",
    "    \n",
    "    # Test the model\n",
    "    hyperparams_config = {\n",
    "        param: experiment.get_parameter(param) for param in config['parameters'].keys()}\n",
    "    if not hyperparams_config['optimizer']:\n",
    "        hyperparams_config['optimizer'] = optimizers.Adam(lr=hyperparams_config['lr'], \n",
    "                                   decay=hyperparams_config['decay'])\n",
    "    model = build_model(hyperparams=hyperparams_config,\n",
    "                        hyperparams_features=hyperparams_features, \n",
    "                        embedding_matrix=embedding_matrix, emotions=emotions,\n",
    "                       stopwords_list=stopword_list)\n",
    "    freeze_layer = FreezeLayer(patience=experiment.get_parameter('freeze_patience'),\n",
    "                              set_to=not experiment.get_parameter('trainable_embeddings'))\n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            factor=experiment.get_parameter('lr_reduce_factor'),\n",
    "                                            patience=experiment.get_parameter('lr_reduce_patience'), \n",
    "                                            min_lr=0.000001, verbose=1)\n",
    "    history = train_model(model, \n",
    "            x_train, y_train, x_test, y_test,\n",
    "            epochs=tune_epochs, batch_size=experiment.get_parameter('batch_size'),\n",
    "                      class_weight={0:1, 1:experiment.get_parameter('positive_class_weight')}, \n",
    "                          workers=2,\n",
    "                          callback_list = [freeze_layer, reduce_lr],\n",
    "                      model_path='models/experiment')\n",
    "    loss = history.history['loss'][-1]\n",
    "    \n",
    "    # Report the loss, if not auto-logged:\n",
    "    experiment.log_metric(\"loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
