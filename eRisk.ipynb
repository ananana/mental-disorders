{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob, os\n",
    "import numpy as np\n",
    "from comet_ml import Experiment, Optimizer\n",
    "import pickle\n",
    "import logging\n",
    "import sys\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # When cudnn implementation not found, run this\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Note: when starting kernel, for gpu_available to be true, this needs to be run\n",
    "# only reserve 1 GPU\n",
    "os.environ['TFHUB_CACHE_DIR'] = '/home/anasab/tf_cache'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "assert 'GPU' in str(device_lib.list_local_devices())\n",
    "\n",
    "# # confirm Keras sees the GPU (for TensorFlow 1.X + Keras)\n",
    "# from keras import backend\n",
    "# assert len(backend.tensorflow_backend._get_available_gpus()) > 0\n",
    "\n",
    "# # confirm PyTorch sees the GPU\n",
    "# from torch import cuda\n",
    "# assert cuda.is_available()\n",
    "# assert cuda.device_count() > 0\n",
    "# print(cuda.get_device_name(cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 11305799109130634554,\n",
       " name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 14461666012902459528\n",
       " physical_device_desc: \"device: XLA_CPU device\",\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 7831709287\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 882650986324403272\n",
       " physical_device_desc: \"device: 0, name: GeForce RTX 2080, pci bus id: 0000:17:00.0, compute capability: 7.5\",\n",
       " name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 33926571032134383\n",
       " physical_device_desc: \"device: XLA_GPU device\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_KERAS'] = '1'\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Lambda, BatchNormalization, TimeDistributed, \\\n",
    "    Bidirectional, Input, concatenate, Flatten, RepeatVector, Activation, Multiply, Permute, \\\n",
    "    Conv1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import callbacks, optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model, Sequence\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer, TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = \"depression\"\n",
    "transfer_type = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('training')\n",
    "ch = logging.StreamHandler(sys.stdout)\n",
    "# create formatter\n",
    "formatter = logging.Formatter(\"%(asctime)s;%(levelname)s;%(message)s\")\n",
    "# add formatter to ch\n",
    "ch.setFormatter(formatter)\n",
    "# add ch to logger\n",
    "logger.addHandler(ch)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.enable_eager_execution()\n",
    "my_seed = 1234\n",
    "# tf.set_random_seed(my_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.backend import manual_variable_initialization \n",
    "# manual_variable_initialization(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_subject_writings(subject_file):\n",
    "    writings = []\n",
    "    with open(subject_file) as sf:\n",
    "        contents = sf.read()\n",
    "        root = ET.fromstring(contents)\n",
    "        try:\n",
    "            subject = root.findall('ID')[0].text.strip()\n",
    "        except Exception:\n",
    "            print('Cannot extract ID', contents[:500], '\\n-------\\n')        \n",
    "        for w in root.iter('WRITING'):\n",
    "            subject_writings = {'subject': subject}\n",
    "            for title in w.findall('TITLE'):\n",
    "                subject_writings['title'] = title.text\n",
    "            for text in w.findall('TEXT'):\n",
    "                subject_writings['text'] = text.text\n",
    "            for date in w.findall('DATE'):\n",
    "                subject_writings['date'] = date.text\n",
    "            writings.append(subject_writings)\n",
    "    return writings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/anasab/' \n",
    "# root_dir = '/home/ana/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eRisk 2020 T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datadir_T1 = root_dir + '/eRisk/data/eRisk2020_T1_train/eRISK2020_T1_training_data/eRISK2020_training_data/data/'\n",
    "# labels_file_T1 = root_dir + '/eRisk/data//eRisk2020_T1_train/eRISK2020_T1_training_data/eRISK2020_training_data/golden_truth.txt'\n",
    "\n",
    "datadirs_T1_2020 = {\n",
    "    'train': ['./data/'],\n",
    "    'test': ['./DATA/']\n",
    "}\n",
    "datadir_root_T1_2020 = {\n",
    "    'train': root_dir + '/eRisk/data/eRisk2020_T1_train/eRISK2020_T1_training_data/eRISK2020_training_data/',\n",
    "    'test': root_dir + '/eRisk/data/2020/T1/'\n",
    "}\n",
    "    \n",
    "labels_files_T1_2020 = {\n",
    "    'train': ['golden_truth.txt'],\n",
    "    'test': ['T1_erisk_golden_truth.txt']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_2020(datadir_root_T1_2020,\n",
    "                   datadirs_T1_2020,\n",
    "                   labels_files_T1_2020,\n",
    "                   test_suffix='0000',\n",
    "                    chunked_subsets=None):\n",
    "\n",
    "\n",
    "    writings = {'train': [], 'test': []}\n",
    "    writings_df = pd.DataFrame()\n",
    "    labels_df = pd.DataFrame()\n",
    "#     for subset in ('train', 'test'):\n",
    "    for subset in ('test',):\n",
    "        for subdir in [os.path.join(datadir_root_T1_2020[subset], subp) for subp in datadirs_T1_2020[subset]]:\n",
    "\n",
    "            for subject_file in os.listdir(subdir):\n",
    "                writings[subset].extend(read_subject_writings(os.path.join(subdir, subject_file)))\n",
    "        writings_df_part = pd.DataFrame(writings[subset])\n",
    "        # add a suffix for users in the test -- the numbers are duplicated with the ones in train\n",
    "        if subset=='test':\n",
    "            writings_df_part['subject'] = writings_df_part['subject'].apply(lambda s: s+test_suffix)\n",
    "            print(subset, writings_df_part.subject)\n",
    "        writings_df_part['subset'] = subset\n",
    "        writings_df = pd.concat([writings_df, writings_df_part])\n",
    "        writings_df.reindex()\n",
    "\n",
    "        for label_file in labels_files_T1_2020[subset]:\n",
    "            labels = pd.read_csv(os.path.join(datadir_root_T1_2020[subset], label_file), \n",
    "                                 delimiter='\\s+', names=['subject', 'label'])\n",
    "            # add a suffix for users in the test -- the numbers are duplicated with the ones in train\n",
    "            if subset=='test':\n",
    "                labels['subject'] = labels['subject'].apply(lambda s: s+test_suffix)\n",
    "            labels_df = pd.concat([labels_df, labels])\n",
    "    labels_df = labels_df.drop_duplicates()\n",
    "    labels_df = labels_df.set_index('subject')\n",
    "\n",
    "    writings_df = writings_df.drop_duplicates()\n",
    "    \n",
    "    writings_df = writings_df.join(labels_df, on='subject')\n",
    "    \n",
    "    return writings_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eRisk 2019 T1 (Anorexia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadirs_T1_2019 = {\n",
    "    'train': ['2018 test/', '2018 train/positive_examples/', '2018 train/negative_examples/'],\n",
    "    'test': ['data/']\n",
    "}\n",
    "datadir_root_T1_2019 = {\n",
    "    'train': root_dir + '/eRisk/data/past/eRisk2019_T1/training data - t1/',\n",
    "    'test': root_dir + '/eRisk/data/past/eRisk2019_T1/test data - T1/'\n",
    "}\n",
    "    \n",
    "labels_files_T1_2019 = {\n",
    "    'train': ['2018 train/risk_golden_truth.txt', '2018 test/risk-golden-truth-test.txt'],\n",
    "    'test': ['T1_erisk_golden_truth.txt']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_2019(datadir_root_T1_2019,\n",
    "                   datadirs_T1_2019,\n",
    "                   labels_files_T1_2019,\n",
    "                   test_suffix='0000',\n",
    "                    chunked_subsets='train'):\n",
    "    writings = {'train': [], 'test': []}\n",
    "    writings_df = pd.DataFrame()\n",
    "    labels_df = pd.DataFrame()\n",
    "\n",
    "    for subset in ('train', 'test'):\n",
    "        for subdir in [os.path.join(datadir_root_T1_2019[subset], subp) for subp in datadirs_T1_2019[subset]]:\n",
    "            if subset in chunked_subsets:\n",
    "                chunkdirs = [os.path.join(datadir_root_T1_2019[subset], subdir, chunkdir) \n",
    "                             for chunkdir in os.listdir(subdir)]\n",
    "            else:\n",
    "                chunkdirs = [os.path.join(datadir_root_T1_2019[subset], subdir)]\n",
    "                \n",
    "            for chunkdir in chunkdirs:\n",
    "                print(chunkdir)\n",
    "                if not os.path.isdir(chunkdir):\n",
    "                    continue\n",
    "                for subject_file in os.listdir(chunkdir):\n",
    "                    writings[subset].extend(read_subject_writings(os.path.join(chunkdir, subject_file)))\n",
    "        writings_df_part = pd.DataFrame(writings[subset])\n",
    "        # add a suffix for users in the test -- the numbers are duplicated with the ones in train\n",
    "        if subset=='test':\n",
    "            writings_df_part['subject'] = writings_df_part['subject'].apply(lambda s: s+test_suffix)\n",
    "            print(subset, writings_df_part.subject)\n",
    "        writings_df_part['subset'] = subset\n",
    "        writings_df = pd.concat([writings_df, writings_df_part])\n",
    "        writings_df.reindex()\n",
    "\n",
    "        for label_file in labels_files_T1_2019[subset]:\n",
    "            labels = pd.read_csv(os.path.join(datadir_root_T1_2019[subset], label_file), \n",
    "                                 delimiter='\\s+', names=['subject', 'label'])\n",
    "            # add a suffix for users in the test -- the numbers are duplicated with the ones in train\n",
    "            if subset=='test':\n",
    "                labels['subject'] = labels['subject'].apply(lambda s: s+test_suffix)\n",
    "            labels_df = pd.concat([labels_df, labels])\n",
    "    labels_df = labels_df.drop_duplicates()\n",
    "    labels_df = labels_df.set_index('subject')\n",
    "\n",
    "    writings_df = writings_df.drop_duplicates()\n",
    "    \n",
    "    writings_df = writings_df.join(labels_df, on='subject')\n",
    "    \n",
    "    return writings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eRisk 2018 (Depression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadirs_T1_2018 = {\n",
    "    'train': ['train/positive_examples_anonymous_chunks/', 'train/positive_examples_anonymous_chunks/', 'test/'],\n",
    "    'test': ['task 1 - depression (test split, train split is 2017 data)/']\n",
    "}\n",
    "datadir_root_T1_2018 = {\n",
    "    'train': root_dir + '/eRisk/data/2017/',\n",
    "    'test': root_dir + '/eRisk/data/2018/'\n",
    "}\n",
    "    \n",
    "labels_files_T1_2018 = {\n",
    "    'train': ['train/risk_golden_truth.txt', 'test/test_golden_truth.txt'],\n",
    "    'test': ['task 1 - depression (test split, train split is 2017 data)/risk-golden-truth-test.txt']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLPsych\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadirs_clpsych = {\n",
    "    'train': [''],\n",
    "    'test': ['']\n",
    "}\n",
    "datadir_root_clpsych = {\n",
    "    'train': root_dir + '/eRisk/data/clpsych/final_training_data/',\n",
    "    'test': root_dir + '/eRisk/data/clpsych/final_testing_data/'\n",
    "}\n",
    "    \n",
    "labels_files_clpsych = [root_dir + '/eRisk/data/clpsych/anonymized_user_info_by_chunk.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_subject_data_clpsych(subject_file):\n",
    "    writings = []\n",
    "    with open(subject_file, \"rt\", encoding=\"utf-8\") as sf:\n",
    "        user = subject_file.split(\"/\")[-1].split(\".\")[0]\n",
    "        print(subject_file)\n",
    "\n",
    "        for line in sf:\n",
    "            data = json.loads(line)#.encode('utf-16','surrogatepass').decode('utf-16'))\n",
    "            data['subject'] = user\n",
    "            writings.append(data)\n",
    "    return writings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_clpsych(datadir_root_clpsych,\n",
    "                   datadirs_clpsych,\n",
    "                   labels_files_T1_2019,\n",
    "                      label_by=['depression']):\n",
    "    writings = {'train': [], 'test': []}\n",
    "    writings_df = pd.DataFrame()\n",
    "    labels_df = pd.DataFrame()\n",
    "\n",
    "    for subset in ('test',):#, 'test'):\n",
    "        for subdir in [os.path.join(datadir_root_clpsych[subset], subp) for subp in datadirs_clpsych[subset]]:\n",
    "            for subject_file in glob.glob(subdir + \"/*.tweets\"):\n",
    "#                 if subject_file.split(\"/\")[-1] != 'sZVVktDN8qqjA.tweets':\n",
    "#                     continue\n",
    "                writings[subset].extend(read_subject_data_clpsych(os.path.join(subdir, subject_file)))\n",
    "        writings_df_part = pd.DataFrame(writings[subset])\n",
    "        writings_df_part['subset'] = subset\n",
    "        writings_df = pd.concat([writings_df, writings_df_part])\n",
    "        writings_df.reindex()\n",
    "\n",
    "    for label_file in labels_files_clpsych:\n",
    "        labels = pd.read_csv(label_file, \n",
    "                    names=['subject','age','num_tweets','gender','condition','chunk_index'])\n",
    "        labels['label'] = labels['condition'].apply(lambda c: 1 if c in label_by else 0)\n",
    "        \n",
    "        labels_df = pd.concat([labels_df, labels])\n",
    "        labels_df = labels_df.drop_duplicates()\n",
    "        labels_df = labels_df.set_index('subject')\n",
    "\n",
    "        # TODO: this deduplication throws some unicode, surrogates not allowed, exception\n",
    "#     writings_df = writings_df.drop_duplicates(subset=['id', 'subject', 'subset', 'created_at', 'text'])\n",
    "    \n",
    "    writings_df = writings_df.join(labels_df, on='subject')\n",
    "    writings_df['date'] = writings_df['created_at']\n",
    "    \n",
    "    return writings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symanto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_symanto(tsv_path=\"/eRisk/data/symanto/clean_dataset_with_timeline_balancedUsers.tsv\"):\n",
    "    label_key = {'REAL_LABEL_IS_DEPRESSED': 1,\n",
    "             'REAL_LABEL_IS_NON_DEPRESSED': 0}\n",
    "    prediction_key = {'predicted_as_depressed': 1,\n",
    "                     'predicted_as_non_depressed': 0,\n",
    "                     'DEPRESS_STRING_WAS_MENTIONED': -1}\n",
    "    \n",
    "    writings_df = pd.read_csv(root_dir + tsv_path, \n",
    "                              sep='\\t', names=['subject', 'date', 'text', 'prediction_text', 'real_label_text'])\n",
    "\n",
    "    writings_df = writings_df.dropna()\n",
    "    writings_df['label'] = writings_df['real_label_text'].apply(lambda l: label_key[l])\n",
    "    writings_df['prediction'] = writings_df['prediction_text'].apply(lambda l: prediction_key[l])\n",
    "#     By ommitting -1s we are excluding tweets where \"DEPRESSED\" was mentioned, and all after\n",
    "    writings_df = writings_df.drop(writings_df[writings_df['prediction']==-1].index)\n",
    "\n",
    "    \n",
    "    return writings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df = read_texts_2020(datadir_T1, labels_file_T1)\n",
    "# writings_df = read_texts_2019(datadir_root_T1_2019,\n",
    "#                    datadirs_T1_2019,\n",
    "#                    labels_files_T1_2019)\n",
    "# writings_df_depression = read_texts_2019(datadir_root_T1_2018,\n",
    "#                    datadirs_T1_2018,\n",
    "#                    labels_files_T1_2018,\n",
    "#                              chunked_subsets=['train', 'test'])\n",
    "\n",
    "if dataset_type == \"combined\":\n",
    "    writings_df_selfharm = pickle.load(open('data/writings_df_selfharm_liwc_subsets', 'rb'))\n",
    "    writings_df_anorexia = pickle.load(open('data/writings_df_anorexia_liwc', 'rb'))\n",
    "    writings_df_depression = pickle.load(open('data/writings_df_depression_liwc', 'rb'))\n",
    "    writings_df = pd.DataFrame()\n",
    "    writings_df = pd.concat([writings_df, writings_df_depression])\n",
    "    writings_df = pd.concat([writings_df, writings_df_selfharm])\n",
    "    writings_df = pd.concat([writings_df, writings_df_anorexia])\n",
    "elif dataset_type == \"combined_depr\":\n",
    "    writings_df = pd.DataFrame.from_dict(json.load(open('data/writings_df_depression_all.json')))\n",
    "elif dataset_type == \"clpsych\":\n",
    "    writings_df = pd.DataFrame.from_dict(json.load(open('data/writings_df_%s_liwc_affect.json' % dataset_type)))#read_texts_clpsych(datadir_root_clpsych, datadirs_clpsych, labels_files_clpsych)\n",
    "#     writings_df_test = pd.DataFrame.from_dict(json.load(open('writings_df_%s_test.json' % dataset_type)))#read_texts_clpsych(datadir_root_clpsych, datadirs_clpsych, labels_files_clpsych)\n",
    "#     writings_df_test = read_texts_clpsych(datadir_root_clpsych, datadirs_clpsych, labels_files_clpsych)\n",
    "    label_by = ['depression', 'ptsd']\n",
    "    writings_df = writings_df.drop(writings_df[writings_df['condition']=='depression'].index)\n",
    "    writings_df['label'] = writings_df['condition'].apply(lambda c: 1 if c in label_by else 0)\n",
    "#     writings_df['date'] = writings_df['created_at']\n",
    "elif dataset_type == \"symanto\":\n",
    "    writings_df = read_texts_symanto()\n",
    "elif dataset_type == 'selfharm':\n",
    "    writings_df = pickle.load(open('data/writings_df_%s_all' % dataset_type, 'rb'))\n",
    "elif dataset_type in [\"depression\", \"anorexia\", \"selfharm\"]:\n",
    "    writings_df = pickle.load(open('data/writings_df_%s_liwc' % dataset_type, 'rb'))\n",
    "else:\n",
    "    logger.error(\"Unknown dataset %s\" % dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df[writings_df['condition']!='control'].head(150)[['label', 'condition', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb6ff12c510>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "writings_df.label.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>subject</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>subset</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>title_len</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>text_len</th>\n",
       "      <th>...</th>\n",
       "      <th>joy</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>fear</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>negative</th>\n",
       "      <th>pronouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-06-23 14:42:09</td>\n",
       "      <td>train_subject2219</td>\n",
       "      <td>Thing is these are shown on Irish TV before w...</td>\n",
       "      <td></td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[thing, is, these, are, shown, on, irish, tv, ...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-06-23 14:39:55</td>\n",
       "      <td>train_subject2219</td>\n",
       "      <td>Spot on</td>\n",
       "      <td></td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[spot, on]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-06-23 11:32:11</td>\n",
       "      <td>train_subject2219</td>\n",
       "      <td>All 3 of them...</td>\n",
       "      <td></td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[all, 3, of, them]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-06-23 10:42:56</td>\n",
       "      <td>train_subject2219</td>\n",
       "      <td>Oh man, I love Rottweilers! They're awesome w...</td>\n",
       "      <td></td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[oh, man, i, love, rottweilers, they, re, awes...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-06-22 21:31:52</td>\n",
       "      <td>train_subject2219</td>\n",
       "      <td>Another reason why I love the Yogscast. Good,...</td>\n",
       "      <td></td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[another, reason, why, i, love, the, yogscast,...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    date            subject  \\\n",
       "0   2014-06-23 14:42:09   train_subject2219   \n",
       "1   2014-06-23 14:39:55   train_subject2219   \n",
       "2   2014-06-23 11:32:11   train_subject2219   \n",
       "3   2014-06-23 10:42:56   train_subject2219   \n",
       "4   2014-06-22 21:31:52   train_subject2219   \n",
       "\n",
       "                                                text title subset  label  \\\n",
       "0   Thing is these are shown on Irish TV before w...        train      1   \n",
       "1                                           Spot on         train      1   \n",
       "2                                  All 3 of them...         train      1   \n",
       "3   Oh man, I love Rottweilers! They're awesome w...        train      1   \n",
       "4   Another reason why I love the Yogscast. Good,...        train      1   \n",
       "\n",
       "  tokenized_title  title_len  \\\n",
       "0              []        NaN   \n",
       "1              []        NaN   \n",
       "2              []        NaN   \n",
       "3              []        NaN   \n",
       "4              []        NaN   \n",
       "\n",
       "                                      tokenized_text  text_len  ...       joy  \\\n",
       "0  [thing, is, these, are, shown, on, irish, tv, ...      10.0  ...  0.000000   \n",
       "1                                         [spot, on]       2.0  ...  0.000000   \n",
       "2                                 [all, 3, of, them]       4.0  ...  0.000000   \n",
       "3  [oh, man, i, love, rottweilers, they, re, awes...      22.0  ...  0.090909   \n",
       "4  [another, reason, why, i, love, the, yogscast,...      12.0  ...  0.333333   \n",
       "\n",
       "      trust  surprise  anticipation      fear  positive   sadness   disgust  \\\n",
       "0  0.000000  0.000000      0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000      0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000      0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000      0.000000  0.000000  0.090909  0.000000  0.000000   \n",
       "4  0.166667  0.083333      0.166667  0.083333  0.416667  0.083333  0.083333   \n",
       "\n",
       "   negative  pronouns  \n",
       "0       0.0  0.000000  \n",
       "1       0.0  0.000000  \n",
       "2       0.0  0.000000  \n",
       "3       0.0  0.090909  \n",
       "4       0.0  0.083333  \n",
       "\n",
       "[5 rows x 85 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "regtokenizer = RegexpTokenizer(r'\\w+')\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "def tokenize(t, tokenizer=regtokenizer):\n",
    "    return regtokenizer.tokenize(t.lower())\n",
    "\n",
    "def tokenize_tweets(t, stop=True):\n",
    "    tokens = tweet_tokenizer.tokenize(t.lower())\n",
    "    tokens_clean = [re.sub(\"^#\", \"\", token) for token in tokens]\n",
    "    tokens_clean = [token for token in tokens_clean \n",
    "                            if re.match(\"^[a-z]*$\", token)]\n",
    "    if not stop:\n",
    "        tokens_clean = [token for token in tokens\n",
    "                       if (token not in sw)]\n",
    "    return tokens_clean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.sub(\"^#\", \"\", \"#h#ash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fields(writings_df, tokenize_fct=tokenize, columns=['title', 'text']):\n",
    "    for c in columns:\n",
    "        writings_df['tokenized_%s' % c] = writings_df['%s' % c].apply(lambda t: tokenize_fct(t) \n",
    "                                                                if type(t)==str and t else None)\n",
    "        writings_df['%s_len' % c] = writings_df['tokenized_%s' % c].apply(lambda t: len(t) \n",
    "                                                                    if type(t)==list and t else None)\n",
    "    return writings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df = tokenize_fields(writings_df, tokenize_fct=tokenize_tweets, columns=['title'])\n",
    "# writings_df = tokenize_fields(writings_df, tokenize_fct=tokenize_tweets, columns=['text'])\n",
    "writings_df = tokenize_fields(writings_df, tokenize_fct=tokenize, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    603403.000000\n",
       "mean         42.364488\n",
       "std         131.248600\n",
       "min           1.000000\n",
       "25%           8.000000\n",
       "50%          18.000000\n",
       "75%          40.000000\n",
       "max        7471.000000\n",
       "Name: text_len, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.text_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df.title_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title_len</th>\n",
       "      <th>text_len</th>\n",
       "      <th>ingest</th>\n",
       "      <th>death</th>\n",
       "      <th>adverb</th>\n",
       "      <th>body</th>\n",
       "      <th>number</th>\n",
       "      <th>money</th>\n",
       "      <th>work</th>\n",
       "      <th>...</th>\n",
       "      <th>joy</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>fear</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>negative</th>\n",
       "      <th>pronouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1304.000000</td>\n",
       "      <td>1284.000000</td>\n",
       "      <td>1302.000000</td>\n",
       "      <td>1304.000000</td>\n",
       "      <td>1304.000000</td>\n",
       "      <td>1304.000000</td>\n",
       "      <td>1304.000000</td>\n",
       "      <td>1304.000000</td>\n",
       "      <td>1304.000000</td>\n",
       "      <td>1304.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1304.000000</td>\n",
       "      <td>1304.000000</td>\n",
       "      <td>1304.000000</td>\n",
       "      <td>1304.000000</td>\n",
       "      <td>1304.000000</td>\n",
       "      <td>1304.000000</td>\n",
       "      <td>1304.000000</td>\n",
       "      <td>1304.000000</td>\n",
       "      <td>1304.000000</td>\n",
       "      <td>1304.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.164110</td>\n",
       "      <td>10.636258</td>\n",
       "      <td>49.916872</td>\n",
       "      <td>0.005033</td>\n",
       "      <td>0.002726</td>\n",
       "      <td>0.043196</td>\n",
       "      <td>0.007766</td>\n",
       "      <td>0.006980</td>\n",
       "      <td>0.007891</td>\n",
       "      <td>0.021144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024421</td>\n",
       "      <td>0.033824</td>\n",
       "      <td>0.014099</td>\n",
       "      <td>0.025814</td>\n",
       "      <td>0.020818</td>\n",
       "      <td>0.053492</td>\n",
       "      <td>0.016796</td>\n",
       "      <td>0.012884</td>\n",
       "      <td>0.034817</td>\n",
       "      <td>0.037269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.370518</td>\n",
       "      <td>4.121411</td>\n",
       "      <td>88.935839</td>\n",
       "      <td>0.006444</td>\n",
       "      <td>0.002959</td>\n",
       "      <td>0.013896</td>\n",
       "      <td>0.006061</td>\n",
       "      <td>0.003241</td>\n",
       "      <td>0.005979</td>\n",
       "      <td>0.011770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012403</td>\n",
       "      <td>0.010008</td>\n",
       "      <td>0.008916</td>\n",
       "      <td>0.009352</td>\n",
       "      <td>0.010577</td>\n",
       "      <td>0.015097</td>\n",
       "      <td>0.008358</td>\n",
       "      <td>0.007585</td>\n",
       "      <td>0.013235</td>\n",
       "      <td>0.021059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.001826</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.034498</td>\n",
       "      <td>0.003972</td>\n",
       "      <td>0.005205</td>\n",
       "      <td>0.004127</td>\n",
       "      <td>0.013316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018009</td>\n",
       "      <td>0.028591</td>\n",
       "      <td>0.009820</td>\n",
       "      <td>0.021554</td>\n",
       "      <td>0.014658</td>\n",
       "      <td>0.045869</td>\n",
       "      <td>0.012533</td>\n",
       "      <td>0.008682</td>\n",
       "      <td>0.026887</td>\n",
       "      <td>0.021593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.082790</td>\n",
       "      <td>32.745133</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>0.002078</td>\n",
       "      <td>0.043924</td>\n",
       "      <td>0.006481</td>\n",
       "      <td>0.006781</td>\n",
       "      <td>0.006602</td>\n",
       "      <td>0.018267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022175</td>\n",
       "      <td>0.032890</td>\n",
       "      <td>0.012408</td>\n",
       "      <td>0.024915</td>\n",
       "      <td>0.019090</td>\n",
       "      <td>0.051711</td>\n",
       "      <td>0.015823</td>\n",
       "      <td>0.011801</td>\n",
       "      <td>0.033296</td>\n",
       "      <td>0.035563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.400000</td>\n",
       "      <td>52.056639</td>\n",
       "      <td>0.005974</td>\n",
       "      <td>0.003680</td>\n",
       "      <td>0.051979</td>\n",
       "      <td>0.009905</td>\n",
       "      <td>0.008615</td>\n",
       "      <td>0.010333</td>\n",
       "      <td>0.026224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027605</td>\n",
       "      <td>0.037781</td>\n",
       "      <td>0.015503</td>\n",
       "      <td>0.028641</td>\n",
       "      <td>0.024670</td>\n",
       "      <td>0.058294</td>\n",
       "      <td>0.019557</td>\n",
       "      <td>0.015536</td>\n",
       "      <td>0.040746</td>\n",
       "      <td>0.050339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>2016.800000</td>\n",
       "      <td>0.084664</td>\n",
       "      <td>0.031507</td>\n",
       "      <td>0.102871</td>\n",
       "      <td>0.063539</td>\n",
       "      <td>0.028346</td>\n",
       "      <td>0.059196</td>\n",
       "      <td>0.111261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.124099</td>\n",
       "      <td>0.146667</td>\n",
       "      <td>0.146667</td>\n",
       "      <td>0.125675</td>\n",
       "      <td>0.172621</td>\n",
       "      <td>0.116558</td>\n",
       "      <td>0.100405</td>\n",
       "      <td>0.151969</td>\n",
       "      <td>0.137130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             label    title_len     text_len       ingest        death  \\\n",
       "count  1304.000000  1284.000000  1302.000000  1304.000000  1304.000000   \n",
       "mean      0.164110    10.636258    49.916872     0.005033     0.002726   \n",
       "std       0.370518     4.121411    88.935839     0.006444     0.002959   \n",
       "min       0.000000     1.400000     1.181818     0.000000     0.000000   \n",
       "25%       0.000000     8.000000    21.000000     0.001826     0.000955   \n",
       "50%       0.000000    10.082790    32.745133     0.003467     0.002078   \n",
       "75%       0.000000    12.400000    52.056639     0.005974     0.003680   \n",
       "max       1.000000    41.000000  2016.800000     0.084664     0.031507   \n",
       "\n",
       "            adverb         body       number        money         work  ...  \\\n",
       "count  1304.000000  1304.000000  1304.000000  1304.000000  1304.000000  ...   \n",
       "mean      0.043196     0.007766     0.006980     0.007891     0.021144  ...   \n",
       "std       0.013896     0.006061     0.003241     0.005979     0.011770  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.034498     0.003972     0.005205     0.004127     0.013316  ...   \n",
       "50%       0.043924     0.006481     0.006781     0.006602     0.018267  ...   \n",
       "75%       0.051979     0.009905     0.008615     0.010333     0.026224  ...   \n",
       "max       0.102871     0.063539     0.028346     0.059196     0.111261  ...   \n",
       "\n",
       "               joy        trust     surprise  anticipation         fear  \\\n",
       "count  1304.000000  1304.000000  1304.000000   1304.000000  1304.000000   \n",
       "mean      0.024421     0.033824     0.014099      0.025814     0.020818   \n",
       "std       0.012403     0.010008     0.008916      0.009352     0.010577   \n",
       "min       0.001472     0.001472     0.000000      0.001786     0.000000   \n",
       "25%       0.018009     0.028591     0.009820      0.021554     0.014658   \n",
       "50%       0.022175     0.032890     0.012408      0.024915     0.019090   \n",
       "75%       0.027605     0.037781     0.015503      0.028641     0.024670   \n",
       "max       0.160000     0.124099     0.146667      0.146667     0.125675   \n",
       "\n",
       "          positive      sadness      disgust     negative     pronouns  \n",
       "count  1304.000000  1304.000000  1304.000000  1304.000000  1304.000000  \n",
       "mean      0.053492     0.016796     0.012884     0.034817     0.037269  \n",
       "std       0.015097     0.008358     0.007585     0.013235     0.021059  \n",
       "min       0.007294     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.045869     0.012533     0.008682     0.026887     0.021593  \n",
       "50%       0.051711     0.015823     0.011801     0.033296     0.035563  \n",
       "75%       0.058294     0.019557     0.015536     0.040746     0.050339  \n",
       "max       0.172621     0.116558     0.100405     0.151969     0.137130  \n",
       "\n",
       "[8 rows x 77 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.groupby('subject').mean().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>subset</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>title_len</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>text_len</th>\n",
       "      <th>all_tokens</th>\n",
       "      <th>ingest</th>\n",
       "      <th>...</th>\n",
       "      <th>joy</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>fear</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>negative</th>\n",
       "      <th>pronouns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1090</td>\n",
       "      <td>1090</td>\n",
       "      <td>1090</td>\n",
       "      <td>1090</td>\n",
       "      <td>1090</td>\n",
       "      <td>1088</td>\n",
       "      <td>1090</td>\n",
       "      <td>1088</td>\n",
       "      <td>1090</td>\n",
       "      <td>1090</td>\n",
       "      <td>...</td>\n",
       "      <td>1090</td>\n",
       "      <td>1090</td>\n",
       "      <td>1090</td>\n",
       "      <td>1090</td>\n",
       "      <td>1090</td>\n",
       "      <td>1090</td>\n",
       "      <td>1090</td>\n",
       "      <td>1090</td>\n",
       "      <td>1090</td>\n",
       "      <td>1090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>196</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>...</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       date  text  title  subset  tokenized_title  title_len  tokenized_text  \\\n",
       "label                                                                          \n",
       "0      1090  1090   1090    1090             1090       1088            1090   \n",
       "1       214   214    214     214              214        196             214   \n",
       "\n",
       "       text_len  all_tokens  ingest  ...   joy  trust  surprise  anticipation  \\\n",
       "label                                ...                                        \n",
       "0          1088        1090    1090  ...  1090   1090      1090          1090   \n",
       "1           214         214     214  ...   214    214       214           214   \n",
       "\n",
       "       fear  positive  sadness  disgust  negative  pronouns  \n",
       "label                                                        \n",
       "0      1090      1090     1090     1090      1090      1090  \n",
       "1       214       214      214      214       214       214  \n",
       "\n",
       "[2 rows x 83 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.groupby('subject').max().groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of comments per user 622.3819018404907\n"
     ]
    }
   ],
   "source": [
    "# print(\"Average number of posts per user\", writings_df.groupby('subject').count().title.mean())\n",
    "print(\"Average number of comments per user\", writings_df.groupby('subject').count().text.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df.groupby('subject').count().title.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1304.000000\n",
       "mean      622.381902\n",
       "std       620.157450\n",
       "min         9.000000\n",
       "25%        81.750000\n",
       "50%       365.500000\n",
       "75%      1097.500000\n",
       "max      2000.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.groupby('subject').count().text.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [thing, is, these, are, shown, on, irish, tv, ...\n",
       "1                                           [spot, on]\n",
       "2                                   [all, 3, of, them]\n",
       "3    [oh, man, i, love, rottweilers, they, re, awes...\n",
       "4    [another, reason, why, i, love, the, yogscast,...\n",
       "Name: tokenized_text, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.tokenized_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_dict = writings_df.to_dict()\n",
    "# json.dump(writings_dict, open(\"writings_df_clpsych_all.json\", \"w+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Total examples\", writings_df[['subject', 'subset', 'label'\n",
    "#                                     ]].groupby('subject').min().groupby('subset').count())\n",
    "# print(\"Positive examples\", writings_df[['subject', 'subset', 'label'\n",
    "#                                     ]].groupby('subject').min().groupby('subset').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df.groupby('subject').sort_values('date').date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features and encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_params(model, model_path, hyperparams, hyperparams_features):\n",
    "    model.save_weights(model_path + \"_weights.h5\", save_format='h5')\n",
    "    model.save_weights(model_path + \"_weights\")\n",
    "    model.save(model_path + \"_model.model\")\n",
    "    model.save(model_path + \"_model.h5\")\n",
    "    with open(model_path + '.hp.json', 'w+') as hpf:\n",
    "        hpf.write(json.dumps({k:v for (k,v) in hyperparams.items() if k!='optimizer'}))\n",
    "    with open(model_path + '.hpf.json', 'w+') as hpff:\n",
    "        hpff.write(json.dumps(hyperparams_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_params(model_path):\n",
    "    with open(model_path + '.hp.json', 'r') as hpf:\n",
    "        hyperparams = json.loads(hpf.read())\n",
    "    with open(model_path + '.hpf.json', 'r') as hpff:\n",
    "        hyperparams_features = json.loads(hpff.read())\n",
    "    return hyperparams, hyperparams_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_features = {\n",
    "    \"max_features\": 20002,\n",
    "    # cut texts after this number of words\n",
    "    # (among top max_features most common words)\n",
    "    \"embedding_dim\": 100,\n",
    "    'vocabulary_path': 'data/all_vocab_clpsych_erisk_stop_20000.pkl',\n",
    "#     'embeddings_path': root_dir + '/eRisk/embeddings/finetuned_glove_clpsych_erisk_stop_normalized_20000.pkl',\n",
    "    'embeddings_path': root_dir + '/resources/glove.840B/glove.840B.300d.txt',\n",
    "    'liwc_words_cached': 'data/liwc_categories_for_vocabulary_erisk_clpsych_stop_20K.pkl',\n",
    "    \"user_level\": True, # deprecated\n",
    "    \"transfer\": transfer_type,\n",
    "    \"pretrained_model_path\": 'models/lstm_depression_seq30',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if transfer_type:\n",
    "    pretrained_model_path = hyperparams_features['pretrained_model_path']\n",
    "    hyperparams, hyperparams_features = load_params(hyperparams_features['pretrained_model_path'])\n",
    "    hyperparams_features['pretrained_model_path'] = pretrained_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_NRC(nrc_path):\n",
    "    word_emotions = {}\n",
    "    emotion_words = {}\n",
    "    with open(nrc_path) as in_f:\n",
    "        for line in in_f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            word, emotion, label = line.split()\n",
    "            if word not in word_emotions:\n",
    "                word_emotions[word] = set()\n",
    "            if emotion not in emotion_words:\n",
    "                emotion_words[emotion] = set()\n",
    "            label = int(label)\n",
    "            if label:\n",
    "                word_emotions[word].add(emotion)\n",
    "                emotion_words[emotion].add(word)\n",
    "    return emotion_words\n",
    "\n",
    "nrc_lexicon_path = root_dir + '/resources/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'\n",
    "nrc_lexicon = load_NRC(nrc_lexicon_path)\n",
    "emotions = list(nrc_lexicon.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_emotions(tokens, emotion_lexicon, emotions, relative=True):\n",
    "    text_len = len(tokens)\n",
    "    encoded_emotions = [0 for e in emotions]\n",
    "    for i, emotion in enumerate(emotions):\n",
    "        try:\n",
    "            emotion_words = [t for t in tokens if t in emotion_lexicon[emotion]]\n",
    "            if relative and len(tokens):\n",
    "                encoded_emotions[i] = len(emotion_words) / len(tokens)\n",
    "            else:\n",
    "                encoded_emotions[i] = len(emotion_words)\n",
    "        except ValueError:\n",
    "            print(\"Emotion not found.\")\n",
    "    return encoded_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from liwc_readDict import readDict\n",
    "\n",
    "liwc_dict = {}\n",
    "for (w, c) in readDict(root_dir + '/resources/liwc.dic'):\n",
    "    if c not in liwc_dict:\n",
    "        liwc_dict[c] = []\n",
    "    liwc_dict[c].append(w)\n",
    "\n",
    "categories = set(liwc_dict.keys())\n",
    "len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_categories=[c for c in categories]# if c in writings_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/liwc_categories_for_vocabulary_erisk_clpsych_stop_20K.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-8e740437ec83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mliwc_words_for_categories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/liwc_categories_for_vocabulary_erisk_clpsych_stop_20K.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/liwc_categories_for_vocabulary_erisk_clpsych_stop_20K.pkl'"
     ]
    }
   ],
   "source": [
    "liwc_words_for_categories = pickle.load(open(\"data/liwc_categories_for_vocabulary_erisk_clpsych_stop_20K.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Char n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(tokens):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Personal pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_person_pronouns = {\"i\", \"me\", \"my\", \"mine\", \"myself\"}\n",
    "def encode_pronouns(tokens, pronouns={\"i\", \"me\", \"my\", \"mine\", \"myself\"}, relative=True):\n",
    "    if not tokens:\n",
    "        return 0\n",
    "    text_len = len(tokens)\n",
    "    nr_pronouns = len([t for t in tokens if t in pronouns])\n",
    "    if relative and text_len:\n",
    "        return nr_pronouns/text_len\n",
    "    else:\n",
    "        return nr_pronouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words(\"english\")\n",
    "def encode_stopwords(tokens, stopwords=stopword_list):\n",
    "    encoded_stopwords = [0 for s in stopword_list]\n",
    "    if not tokens:\n",
    "        return encoded_stopwords\n",
    "    for i, stopword in enumerate(stopwords):\n",
    "        if stopword in tokens:\n",
    "            encoded_stopwords[i] += 1\n",
    "    return encoded_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_hub'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-5fb51e9c55d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbert_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFullTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_hub'"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "from bert.tokenization import FullTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "    Args:\n",
    "      guid: Unique id for the example.\n",
    "      text_a: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "        Only must be specified for sequence pair tasks.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text_for_bert(tokenizer, example, max_seq_length=256):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "#     if isinstance(example, PaddingInputExample):\n",
    "#         input_ids = [0] * max_seq_length\n",
    "#         input_mask = [0] * max_seq_length\n",
    "#         segment_ids = [0] * max_seq_length\n",
    "#         label = 0\n",
    "#         return input_ids, input_mask, segment_ids, label\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    if len(tokens_a) > max_seq_length - 2:\n",
    "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
    "\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    return input_ids, input_mask, segment_ids, example.label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer_from_hub_module(sess):\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    bert_module =  hub.Module(bert_path)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    vocab_file, do_lower_case = sess.run(\n",
    "        [\n",
    "            tokenization_info[\"vocab_file\"],\n",
    "            tokenization_info[\"do_lower_case\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_sess(sess=None):\n",
    "    if not sess:\n",
    "        sess_config = tf.ConfigProto(\n",
    "            device_count={ 'GPU' : 1, 'CPU': 4 },\n",
    "            intra_op_parallelism_threads = 0,\n",
    "            inter_op_parallelism_threads = 4,\n",
    "            allow_soft_placement=False,\n",
    "            log_device_placement=True,\n",
    "        )\n",
    "        sess_config.gpu_options.allow_growth = True\n",
    "        sess_config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "        sess = tf.Session(config=sess_config)\n",
    "\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_model(model_path, hyperparams):\n",
    "    metrics_class = Metrics(threshold=hyperparams['threshold'])\n",
    "    dependencies = {\n",
    "    'f1_m': metrics_class.f1_m,\n",
    "    'auc': metrics_class.auc,\n",
    "    'precision_m': metrics_class.precision_m,\n",
    "    'recall_m': metrics_class.recall_m,\n",
    "    'binary_crossentropy_custom': binary_crossentropy_custom,\n",
    "    'BertLayer': BertLayer\n",
    "    }\n",
    "#     loaded_model = load_model(model_path + \"_model.h5\", custom_objects=dependencies)\n",
    "    loaded_model = load_model(model_path + \"_model.model\", custom_objects=dependencies)\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def load_erisk_data(writings_df, voc_size, emotion_lexicon, emotions =  \n",
    "                    ['anger', 'anticipation', 'disgust', 'fear', 'joy', \n",
    "                     'negative', 'positive', 'sadness', 'surprise', 'trust'],\n",
    "                    liwc_categories = categories, by_subset=True,\n",
    "                    pronouns = [\"i\", \"me\", \"my\", \"mine\", \"myself\"],\n",
    "                    train_prop=0.7, valid_prop=0.3, test_slice=2,\n",
    "                    nr_slices=5,\n",
    "                    min_post_len=3, min_word_len=1, \n",
    "                    user_level=True, vocabulary=None,\n",
    "                   logger=logger):\n",
    "    logger.debug(\"Loading data...\\n\")\n",
    "    \n",
    "    ## Build vocabulary\n",
    "    vocabulary_all = {}\n",
    "    word_freqs = Counter()\n",
    "    for words in writings_df.tokenized_text:\n",
    "        word_freqs.update(words)\n",
    "    if 'tokenized_title' in writings_df.columns:\n",
    "        for words in writings_df.tokenized_title:\n",
    "            word_freqs.update(words)\n",
    "    i = 1\n",
    "    for w, f in word_freqs.most_common(voc_size-2): # keeping voc_size-1 for unk\n",
    "        if len(w) < min_word_len:\n",
    "            continue\n",
    "        vocabulary_all[w] = i\n",
    "        i += 1\n",
    "    if not vocabulary:\n",
    "        vocabulary = vocabulary_all\n",
    "    else:\n",
    "        logger.info(\"Words not found in the vocabulary: %d\\n\" % len(set(vocabulary_all.keys()).difference(\n",
    "            set(vocabulary.keys()))))\n",
    "\n",
    "   \n",
    "    if by_subset and 'subset' in writings_df.columns:\n",
    "        training_subjects = list(set(writings_df[writings_df['subset']=='train'].subject))\n",
    "        test_subjects = list(set(writings_df[writings_df['subset']=='test'].subject))\n",
    "    else:\n",
    "        all_subjects = sorted(list(set(writings_df.subject)))\n",
    "        training_subjects_size = int(len(all_subjects) * train_prop)\n",
    "        test_subjects_size = len(all_subjects) - training_subjects_size\n",
    "        logger.info(\"%d training subjects, %d test subjects\\n\" % (training_subjects_size, test_subjects_size))\n",
    "        # Cross-validation, with fixed slice as input\n",
    "        test_prop = 1-train_prop\n",
    "        test_slice = min(test_slice, nr_slices)\n",
    "        logger.debug(\"start index: %f, from %f\\n\" % (\n",
    "            len(all_subjects)*(1/nr_slices)*test_slice, test_prop*test_slice))\n",
    "        start_slice = int(len(all_subjects)*(1/nr_slices)*test_slice)\n",
    "        test_subjects = all_subjects[start_slice: start_slice+test_subjects_size]\n",
    "        training_subjects = [s for s in all_subjects if s not in test_subjects]\n",
    "    training_subjects = sorted(training_subjects) # ensuring reproducibility\n",
    "    valid_subjects_size = int(len(training_subjects) * valid_prop)\n",
    "    valid_subjects = training_subjects[:valid_subjects_size]\n",
    "    training_subjects = training_subjects[valid_subjects_size:]\n",
    "    categories = [c for c in liwc_categories if c in writings_df.columns]\n",
    "    logger.debug(\"%d training users, %d validation users, %d test users.\" % (\n",
    "        len(training_subjects), \n",
    "          len(valid_subjects),\n",
    "          len(test_subjects)))\n",
    "    subjects_split = {'train': training_subjects, \n",
    "                      'valid': valid_subjects, \n",
    "                      'test': test_subjects}\n",
    "\n",
    "    user_level_texts = {}\n",
    "    for row in writings_df.sort_values(by='date').itertuples():\n",
    "        words = []\n",
    "        raw_text = \"\"\n",
    "        if hasattr(row, 'tokenized_title'):\n",
    "            if row.tokenized_title:\n",
    "                words.extend(row.tokenized_title)\n",
    "                raw_text += row.title\n",
    "        if hasattr(row, 'tokenized_text'):\n",
    "            if row.tokenized_text:\n",
    "                words.extend(row.tokenized_text)\n",
    "                raw_text += row.text\n",
    "        if not words or len(words)<min_post_len:\n",
    "#             logger.debug(row.subject)\n",
    "            continue\n",
    "        label = row.label\n",
    "        liwc_categs = [getattr(row, categ) for categ in categories]\n",
    "        if row.subject not in user_level_texts.keys():\n",
    "            user_level_texts[row.subject] = {}\n",
    "            user_level_texts[row.subject]['texts'] = [words]\n",
    "            user_level_texts[row.subject]['label'] = label\n",
    "            user_level_texts[row.subject]['liwc'] = [liwc_categs]\n",
    "            user_level_texts[row.subject]['raw'] = [raw_text]\n",
    "        else:\n",
    "            user_level_texts[row.subject]['texts'].append(words)\n",
    "            user_level_texts[row.subject]['liwc'].append(liwc_categs)\n",
    "            user_level_texts[row.subject]['raw'].append(raw_text)\n",
    "            \n",
    "    return user_level_texts, subjects_split, vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/all_vocab_clpsych_erisk_stop_20000.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-7109bf33ee0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocabulary_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparams_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vocabulary_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvocabulary_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvocabulary_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m user_level_data, subjects_split, vocabulary = load_erisk_data(writings_df, \n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/all_vocab_clpsych_erisk_stop_20000.pkl'"
     ]
    }
   ],
   "source": [
    "vocabulary_list = pickle.load(open(hyperparams_features['vocabulary_path'], 'rb'))\n",
    "vocabulary_dict={}\n",
    "for i,w in enumerate(vocabulary_list):\n",
    "    vocabulary_dict[w] = i\n",
    "user_level_data, subjects_split, vocabulary = load_erisk_data(writings_df, \n",
    "                                                            voc_size=hyperparams_features['max_features'],\n",
    "                                                           emotion_lexicon=nrc_lexicon,\n",
    "                                                           emotions=emotions,\n",
    "                                                           user_level=hyperparams_features['user_level'],\n",
    "                                                                                logger=logger,\n",
    "#                                                            vocabulary=pickle.load(open('vocabulary_40K_all.pkl', 'rb')),\n",
    "#                                                            vocabulary=pickle.load(open('vocab_clpsych_10000.pkl', 'rb')),\n",
    "#                                                               vocabulary=vocabulary_dict,\n",
    "                                                              by_subset=False\n",
    "                                                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df['subset'] = writings_df['subject'].apply(lambda s: 'test' if s in subjects_split['test'] else 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocabulary_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-f425a31c2fad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vocabulary_dict' is not defined"
     ]
    }
   ],
   "source": [
    "len(vocabulary_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [thing, is, these, are, shown, on, irish, tv, ...\n",
       "1                                           [spot, on]\n",
       "2                                   [all, 3, of, them]\n",
       "3    [oh, man, i, love, rottweilers, they, re, awes...\n",
       "4    [another, reason, why, i, love, the, yogscast,...\n",
       "Name: tokenized_text, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df['tokenized_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df.text_len.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(writings_df, open('writings_df_selfharm_liwc_subsets', 'wb+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(vocabulary.items(), key=lambda t:t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_collection = {}\n",
    "models_collection = {}\n",
    "hyperparams_collection = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, user_level_data, subjects_split, session=None, use_bert=False, set_type='train',\n",
    "                 batch_size=32, seq_len=512, vocabulary=vocabulary,\n",
    "                 voc_size=hyperparams_features['max_features'], emotion_lexicon=nrc_lexicon,\n",
    "                 hierarchical=False, pad_value=0, padding='pre',\n",
    "                 post_groups_per_user=None, posts_per_group=10,\n",
    "                 sampling_distr_alfa=0.1, sampling_distr='exp', # 'exp', 'uniform'\n",
    "                 emotions=emotions, pronouns=[\"i\", \"me\", \"my\", \"mine\", \"myself\"], liwc_categories=liwc_categories,\n",
    "                 liwc_dict=liwc_dict, compute_liwc=False, liwc_words_for_categories=None,\n",
    "                 pad_with_duplication=False,\n",
    "                 max_posts_per_user=None, sample_seqs=True,\n",
    "                 shuffle=True, return_subjects=False):\n",
    "        'Initialization'\n",
    "        self.seq_len = seq_len\n",
    "        # Instantiate tokenizer\n",
    "        if session:\n",
    "            self.bert_tokenizer = create_tokenizer_from_hub_module(session)\n",
    "            session.run(tf.local_variables_initializer())\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            session.run(tf.tables_initializer())\n",
    "        else:\n",
    "            if use_bert:\n",
    "                logger.error(\"Need a session to use bert in data generation\")\n",
    "            self.bert_tokenizer = None\n",
    "        self.use_bert = use_bert\n",
    "        self.subjects_split = subjects_split\n",
    "        self.set = set_type\n",
    "        self.emotion_lexicon = emotion_lexicon\n",
    "        self.batch_size = batch_size\n",
    "        self.hierarchical = hierarchical\n",
    "        self.data = user_level_data\n",
    "        self.pad_value = pad_value\n",
    "        self.return_subjects = return_subjects\n",
    "        self.sampling_distr_alfa = sampling_distr_alfa\n",
    "        self.sampling_distr = sampling_distr\n",
    "        self.emotions = emotions\n",
    "        self.pronouns = pronouns\n",
    "        self.liwc_categories = liwc_categories\n",
    "        self.liwc_dict = liwc_dict\n",
    "        self.liwc_words_for_categories = liwc_words_for_categories\n",
    "        self.compute_liwc = compute_liwc\n",
    "        self.sample_seqs = sample_seqs\n",
    "        self.pad_with_duplication = pad_with_duplication\n",
    "        self.padding = padding\n",
    "        self.shuffle = shuffle\n",
    "        self.voc_size = voc_size\n",
    "        self.vocabulary = vocabulary\n",
    "        self.max_posts_per_user = max_posts_per_user\n",
    "        self.post_groups_per_user = post_groups_per_user\n",
    "        self.posts_per_group = posts_per_group\n",
    "        self.__post_indexes_per_user()\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    @staticmethod\n",
    "    def _random_sample(population_size, sample_size, sampling_distr, alfa=0.1, replacement=False):\n",
    "        if sampling_distr == 'exp':\n",
    "            # Exponential sampling\n",
    "            sample = sorted(np.random.choice(population_size, \n",
    "                            min(sample_size, population_size),\n",
    "                            p = DataGenerator.__generate_reverse_exponential_indices(population_size, alfa),\n",
    "                            replace=replacement))\n",
    "                                                                # if pad_with_duplication, \n",
    "                                                                # pad by adding the same post multiple times\n",
    "                                                                # if there are not enough posts\n",
    "        elif sampling_distr == 'uniform':\n",
    "            # Uniform sampling\n",
    "            sample = sorted(np.random.choice(population_size,\n",
    "                            min(sample_size, population_size),\n",
    "                            replace=replacement))\n",
    "        return sample\n",
    "    \n",
    "    @staticmethod\n",
    "    def __generate_reverse_exponential_indices(max_index, alfa=1):\n",
    "        probabilities = []\n",
    "        for x in range(max_index):\n",
    "            probabilities.append(alfa * (np.exp(alfa*x)))\n",
    "        reverse_probabilities = [p for p in probabilities]\n",
    "        sump = sum(reverse_probabilities)\n",
    "        normalized_probabilities = [p/sump for p in reverse_probabilities]\n",
    "        return normalized_probabilities\n",
    "    \n",
    "    def __post_indexes_per_user(self):\n",
    "        self.indexes_per_user = {u: [] for u in range(len(self.subjects_split[self.set]))}\n",
    "        self.indexes_with_user = []\n",
    "        for u in range(len(self.subjects_split[self.set])):\n",
    "            if self.subjects_split[self.set][u] not in self.data:\n",
    "                logger.warning(\"User %s has no posts in %s set. Ignoring.\\n\" % (\n",
    "                    self.subjects_split[self.set][u], self.set))\n",
    "                continue\n",
    "            user_posts = self.data[self.subjects_split[self.set][u]]['texts']\n",
    "            if self.max_posts_per_user:\n",
    "                user_posts = user_posts[:self.max_posts_per_user]\n",
    "            nr_post_groups = int(np.ceil(len(user_posts) / self.posts_per_group))\n",
    "            \n",
    "            if self.post_groups_per_user:\n",
    "                nr_post_groups = min(self.post_groups_per_user, nr_post_groups)\n",
    "            for i in range(nr_post_groups):\n",
    "                # Generate random ordered samples of the posts\n",
    "                if self.sample_seqs:\n",
    "                    indexes_sample = DataGenerator._random_sample(population_size=len(user_posts),\n",
    "                                                         sample_size=self.posts_per_group,\n",
    "                                                         sampling_distr=self.sampling_distr,\n",
    "                                                         alfa=self.sampling_distr_alfa,\n",
    "                                                         replacement=self.pad_with_duplication)\n",
    "                    self.indexes_per_user[u].append(indexes_sample)\n",
    "                    self.indexes_with_user.append((u, indexes_sample))\n",
    "                    # break # just generate one?\n",
    "                # Generate all subsets of the posts in order\n",
    "                # TODO: Change here if you want a sliding window\n",
    "                else:\n",
    "                    self.indexes_per_user[u].append(range(i*self.posts_per_group ,\n",
    "                                                        min((i+1)*self.posts_per_group, len(user_posts))))\n",
    "                    self.indexes_with_user.append((u, range(i*self.posts_per_group ,\n",
    "                                                        min((i+1)*self.posts_per_group, len(user_posts)))))\n",
    "\n",
    "\n",
    "    def __encode_text(self, tokens, raw_text):\n",
    "        # Using voc_size-1 value for OOV token\n",
    "        encoded_tokens = [self.vocabulary.get(w, self.voc_size-1) for w in tokens]\n",
    "        encoded_emotions = encode_emotions(tokens, self.emotion_lexicon, self.emotions)\n",
    "        encoded_pronouns = encode_pronouns(tokens, self.pronouns)\n",
    "        encoded_stopwords = encode_stopwords(tokens)\n",
    "        if not self.compute_liwc:\n",
    "            encoded_liwc = None\n",
    "        else:\n",
    "            encoded_liwc = self.__encode_liwc_categories(tokens)\n",
    "        if self.bert_tokenizer:\n",
    "            bert_ids, bert_masks, bert_segments, label = encode_text_for_bert(self.bert_tokenizer, InputExample(None, \n",
    "                                               raw_text), self.seq_len)\n",
    "        else:\n",
    "            bert_ids, bert_masks, bert_segments = [[0]*self.seq_len, [0]*self.seq_len, [0]*self.seq_len]\n",
    "        return (encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords, encoded_liwc,\n",
    "               bert_ids, bert_masks, bert_segments)\n",
    "    \n",
    "    def __encode_liwc_categories_full(self, tokens, relative=True):\n",
    "        categories_cnt = [0 for c in self.liwc_categories]\n",
    "        if not tokens:\n",
    "            return categories_cnt\n",
    "        text_len = len(tokens)\n",
    "        for i, category in enumerate(self.liwc_categories):\n",
    "            category_words = self.liwc_dict[category]\n",
    "            for t in tokens:\n",
    "                for word in category_words:\n",
    "                    if t==word or (word[-1]=='*' and t.startswith(word[:-1])) \\\n",
    "                    or (t==word.split(\"'\")[0]):\n",
    "                        categories_cnt[i] += 1\n",
    "                        break # one token cannot belong to more than one word in the category\n",
    "            if relative and text_len:\n",
    "                categories_cnt[i] = categories_cnt[i]/text_len\n",
    "        return categories_cnt\n",
    "        \n",
    "        \n",
    "    def __encode_liwc_categories(self, tokens, relative=True):\n",
    "        categories_cnt = [0 for c in self.liwc_categories]\n",
    "        if not tokens:\n",
    "            return categories_cnt\n",
    "        text_len = len(tokens)\n",
    "        for i, category in enumerate(self.liwc_categories):\n",
    "            for t in tokens:\n",
    "                if t in self.liwc_words_for_categories[category]:\n",
    "                    categories_cnt[i] += 1\n",
    "            if relative and text_len:\n",
    "                categories_cnt[i] = categories_cnt[i]/text_len\n",
    "        return categories_cnt\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(len(self.indexes) / self.batch_size)) # + 1 to not discard last batch\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Find users\n",
    "        user_indexes = [t[0] for t in indexes]\n",
    "        users = set([self.subjects_split[self.set][i] for i in user_indexes\n",
    "                    if self.subjects_split[self.set][i] in self.data.keys()]) # TODO: maybe needs a warning that user is missing\n",
    "        post_indexes_per_user = {u: [] for u in users}\n",
    "        # Sample post ids\n",
    "        for u, post_indexes in indexes:\n",
    "            user = self.subjects_split[self.set][u]\n",
    "            # Note: was bug here - changed it into a list\n",
    "            post_indexes_per_user[user].append(post_indexes)\n",
    "\n",
    "        # Generate data\n",
    "        if self.hierarchical:\n",
    "            X, s, y = self.__data_generation_hierarchical(users, post_indexes_per_user)\n",
    "        else:\n",
    "            X, s, y = self.__data_generation(users, post_indexes_per_user)\n",
    "        if self.return_subjects:\n",
    "            return X, s, y\n",
    "        else:\n",
    "            return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = self.indexes_with_user\n",
    "#         np.arange(len(self.subjects_split[self.set]))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, users, post_indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        tokens_data = []\n",
    "        categ_data = []\n",
    "        sparse_data = []\n",
    "        subjects = []\n",
    "        bert_ids_data = []\n",
    "        bert_masks_data = []\n",
    "        bert_segments_data = []\n",
    "        labels = []\n",
    "        for subject in users:\n",
    "\n",
    "            if 'label' in self.data[subject]:\n",
    "                label = self.data[subject]['label']\n",
    "            else:\n",
    "                label = None\n",
    "\n",
    "            \n",
    "            all_words = []\n",
    "            all_raw_texts = []\n",
    "            liwc_aggreg = []\n",
    "\n",
    "            for post_index_range in post_indexes[subject]:\n",
    "                # Sample\n",
    "                texts = [self.data[subject]['texts'][i] for i in post_index_range]\n",
    "                if 'liwc' in self.data[subject] and not self.compute_liwc:\n",
    "                    liwc_selection = [self.data[subject]['liwc'][i] for i in post_index_range]\n",
    "                raw_texts = [self.data[subject]['raw'][i] for i in post_index_range]\n",
    "\n",
    "                all_words.append(sum(texts, [])) # merge all texts in group in one list\n",
    "                if 'liwc' in self.data[subject] and not self.compute_liwc:\n",
    "                    liwc_aggreg.append(np.array(liwc_selection).mean(axis=0).tolist())\n",
    "                all_raw_texts.append(\" \".join(raw_texts))\n",
    "            for i, words in enumerate(all_words):\n",
    "                encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords, encoded_liwc, \\\n",
    "                    bert_ids, bert_masks, bert_segments = self.__encode_text(words, all_raw_texts[i])\n",
    "                try:\n",
    "                    subject_id = int(re.findall('[0-9]+', subject)[0])\n",
    "                except IndexError:\n",
    "                    subject_id = subject\n",
    "                tokens_data.append(encoded_tokens)\n",
    "                # TODO: what will be the difference between these?\n",
    "                # I think instead of averaging for the post group, it just does it correctly\n",
    "                # for the whole post group (when computing, non-lazily)\n",
    "                if 'liwc' in self.data[subject] and not self.compute_liwc:  \n",
    "                    categ_data.append(encoded_emotions + [encoded_pronouns] + liwc_aggreg[i])\n",
    "                   \n",
    "                else:\n",
    "                    categ_data.append(encoded_emotions + [encoded_pronouns] + encoded_liwc)\n",
    "                    \n",
    "                sparse_data.append(encoded_stopwords)\n",
    "                bert_ids_data.append(bert_ids)\n",
    "                bert_masks_data.append(bert_masks)\n",
    "                bert_segments_data.append(bert_segments)\n",
    "                \n",
    "                labels.append(label)\n",
    "                subjects.append(subject_id)\n",
    "\n",
    "        \n",
    "        # using zeros for padding\n",
    "        tokens_data_padded = sequence.pad_sequences(tokens_data, maxlen=self.seq_len, \n",
    "                                                    padding=self.padding,\n",
    "                                                   truncating=self.padding)\n",
    "\n",
    "        if self.use_bert:\n",
    "            return ([np.array(tokens_data_padded), np.array(categ_data), np.array(sparse_data),\n",
    "                 np.array(bert_ids_data), np.array(bert_masks_data), np.array(bert_segments_data),\n",
    "                ],\n",
    "                np.array(subjects),\n",
    "                np.array(labels))\n",
    "        else:\n",
    "            return ([np.array(tokens_data_padded), np.array(categ_data), np.array(sparse_data),\n",
    "                ],\n",
    "                np.array(subjects),\n",
    "                np.array(labels))\n",
    "    \n",
    "    def __data_generation_hierarchical(self, users, post_indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        user_tokens = []\n",
    "        user_categ_data = []\n",
    "        user_sparse_data = []\n",
    "        user_bert_ids_data = []\n",
    "        user_bert_masks_data = []\n",
    "        user_bert_segments_data = []\n",
    "        \n",
    "        labels = []\n",
    "        subjects = []\n",
    "        for subject in users:\n",
    "             \n",
    "            all_words = []\n",
    "            all_raw_texts = []\n",
    "            liwc_scores = []\n",
    "            \n",
    "            if 'label' in self.data[subject]:\n",
    "                label = self.data[subject]['label']\n",
    "            else:\n",
    "                label = None\n",
    "\n",
    "            for post_index_range in post_indexes[subject]:\n",
    "                # Sample\n",
    "                texts = [self.data[subject]['texts'][i] for i in post_index_range]\n",
    "                if 'liwc' in self.data[subject] and not self.compute_liwc:\n",
    "                    liwc_selection = [self.data[subject]['liwc'][i] for i in post_index_range]\n",
    "                raw_texts = [self.data[subject]['raw'][i] for i in post_index_range]\n",
    "\n",
    "                all_words.append(texts)\n",
    "                if 'liwc' in self.data[subject] and not self.compute_liwc:\n",
    "                    liwc_scores.append(liwc_selection)\n",
    "                all_raw_texts.append(raw_texts)\n",
    "            \n",
    "#             if len(texts) < self.max_posts_per_user:\n",
    "#                 # TODO: pad with zeros\n",
    "#                 pass\n",
    "\n",
    "            for i, words in enumerate(all_words):\n",
    "                tokens_data = []\n",
    "                categ_data = []\n",
    "                sparse_data = []\n",
    "                bert_ids_data = []\n",
    "                bert_masks_data = []\n",
    "                bert_segments_data = []\n",
    "                \n",
    "                raw_text = all_raw_texts[i]\n",
    "                words = all_words[i]\n",
    "                \n",
    "                for p, posting in enumerate(words): \n",
    "                    encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords, encoded_liwc, \\\n",
    "                        bert_ids, bert_masks, bert_segments = self.__encode_text(words[p], raw_text[p])\n",
    "                    if 'liwc' in self.data[subject] and not self.compute_liwc:\n",
    "                        liwc = liwc_scores[i][p]\n",
    "                    else:\n",
    "                        liwc = encoded_liwc\n",
    "                    try:\n",
    "                        subject_id = int(re.findall('[0-9]+', subject)[0])\n",
    "                    except IndexError:\n",
    "                        subject_id = subject\n",
    "                    tokens_data.append(encoded_tokens)\n",
    "                    # using zeros for padding\n",
    "                    # TODO: there is something wrong with this\n",
    "                    categ_data.append(encoded_emotions + [encoded_pronouns] + liwc)\n",
    "                    sparse_data.append(encoded_stopwords)\n",
    "                    bert_ids_data.append(bert_ids)\n",
    "                    bert_masks_data.append(bert_masks)\n",
    "                    bert_segments_data.append(bert_segments)\n",
    "                \n",
    "                # For each range\n",
    "                tokens_data_padded = np.array(sequence.pad_sequences(tokens_data, maxlen=self.seq_len,\n",
    "                                              padding=self.padding,\n",
    "                                            truncating=self.padding))\n",
    "                user_tokens.append(tokens_data_padded)\n",
    "\n",
    "                user_categ_data.append(categ_data)\n",
    "                user_sparse_data.append(sparse_data)\n",
    "\n",
    "                user_bert_ids_data.append(bert_ids_data)\n",
    "                user_bert_masks_data.append(bert_masks_data)\n",
    "                user_bert_segments_data.append(bert_segments_data)\n",
    "\n",
    "                labels.append(label)\n",
    "                subjects.append(subject_id)\n",
    "\n",
    "        user_tokens = sequence.pad_sequences(user_tokens, \n",
    "                                             maxlen=self.posts_per_group, \n",
    "                                             value=self.pad_value)\n",
    "        user_tokens = np.rollaxis(np.dstack(user_tokens), -1)\n",
    "        user_categ_data = sequence.pad_sequences(user_categ_data,  \n",
    "                                                 maxlen=self.posts_per_group, \n",
    "                                                 value=self.pad_value, dtype='float32')\n",
    "        user_categ_data = np.rollaxis(np.dstack(user_categ_data), -1)\n",
    "        \n",
    "        user_sparse_data = sequence.pad_sequences(user_sparse_data, \n",
    "                                                  maxlen=self.posts_per_group, \n",
    "                                                  value=self.pad_value)\n",
    "        user_sparse_data = np.rollaxis(np.dstack(user_sparse_data), -1)\n",
    "        \n",
    "        user_bert_ids_data = sequence.pad_sequences(user_bert_ids_data, \n",
    "                                                    maxlen=self.posts_per_group, \n",
    "                                                    value=self.pad_value)\n",
    "        user_bert_ids_data = np.rollaxis(np.dstack(user_bert_ids_data), -1)\n",
    "        \n",
    "        user_bert_masks_data = sequence.pad_sequences(user_bert_masks_data, \n",
    "                                                      maxlen=self.posts_per_group, \n",
    "                                                      value=self.pad_value)\n",
    "        user_bert_masks_data = np.rollaxis(np.dstack(user_bert_masks_data), -1)\n",
    "        \n",
    "        user_bert_segments_data = sequence.pad_sequences(user_bert_segments_data, \n",
    "                                                         maxlen=self.posts_per_group, \n",
    "                                                         value=self.pad_value)\n",
    "        user_bert_segments_data = np.rollaxis(np.dstack(user_bert_segments_data), -1)\n",
    "        \n",
    "        if self.use_bert:\n",
    "            return ((user_tokens, user_categ_data, user_sparse_data, \n",
    "                 user_bert_ids_data, user_bert_masks_data, user_bert_segments_data),\n",
    "                np.array(subjects),\n",
    "                np.array(labels))\n",
    "        else:\n",
    "            return ((user_tokens, user_categ_data, user_sparse_data), \n",
    "                np.array(subjects),\n",
    "                np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# TODO: it is skipping the last batch\n",
    "x_data = {'train': [], 'valid': [], 'test': []}\n",
    "y_data = {'train': [], 'valid': [], 'test': []}\n",
    "for set_type in ['train', 'valid', 'test']:\n",
    "    total_positive = 0\n",
    "    for x, y in DataGenerator(user_level_data, subjects_split, sample_seqs=False, max_posts_per_user=None,\n",
    "                                          set_type=set_type, hierarchical=True, post_groups_per_user=1,\n",
    "                              posts_per_group=50, shuffle=False,\n",
    "                             sampling_distr='exp', liwc_words_for_categories=liwc_words_for_categories,\n",
    "                             compute_liwc=True):\n",
    "        total_positive += pd.Series(y).sum()\n",
    "        x_data[set_type].append(x)\n",
    "        y_data[set_type].append(y)\n",
    "        logger.info(\"%d %s positive examples\\n\" % (total_positive, set_type))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# TODO: it is skipping the last batch\n",
    "x_data2 = {'train': [], 'valid': [], 'test': []}\n",
    "y_data2 = {'train': [], 'valid': [], 'test': []}\n",
    "for set_type in ['train', 'valid', 'test']:\n",
    "    total_positive = 0\n",
    "    for x, y in DataGenerator(user_level_data, subjects_split, sample_seqs=False, max_posts_per_user=None,\n",
    "                                          set_type=set_type, hierarchical=False, post_groups_per_user=1,\n",
    "                              posts_per_group=50, shuffle=False,\n",
    "                             sampling_distr='exp', liwc_words_for_categories=liwc_words_for_categories,\n",
    "                             compute_liwc=False):\n",
    "        total_positive += pd.Series(y).sum()\n",
    "        x_data2[set_type].append(x)\n",
    "        y_data2[set_type].append(y)\n",
    "        logger.info(\"%d %s positive examples\\n\" % (total_positive, set_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data['train'][0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data2['train'][0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #x_data['train'][]\n",
    "# featureindex = 1\n",
    "\n",
    "# from scipy.stats import pearsonr, spearmanr\n",
    "# for i in range(len(x_data['train'][0][featureindex])):\n",
    "#     print(spearmanr(x_data['train'][0][featureindex][i], x_data2['train'][0][featureindex][i]))    \n",
    "#     plt.scatter(x_data['train'][0][featureindex][i], x_data2['train'][0][featureindex][i])\n",
    "# #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(liwc_words_for_categories[c]) for c in categories])\n",
    "len(categories)\n",
    "len(set(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x_data['valid'][0][0].shape, x_data['valid'][0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_for_bert = encode_text_for_bert(bert_tokenizer, InputExample(None, \n",
    "#                                                \"Ana are mere\"), 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids, masks, segments, label = encoded_for_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weights = class_weight.compute_class_weight('balanced',\n",
    "#                                                  np.unique(y_data['train']),\n",
    "#                                                  y_data['train'])\n",
    "# class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path, embedding_dim, voc):\n",
    "    # random matrix with mean value = 0\n",
    "    embedding_matrix = np.random.random((len(voc)+2, embedding_dim)) - 0.5 # voc + unk + pad value(0)\n",
    "    cnt_inv = 0\n",
    "    f = open(path, encoding='utf8')\n",
    "    for i, line in enumerate(f):\n",
    "#         print(i)\n",
    "        values = line.split()\n",
    "        word = ''.join(values[:-hyperparams_features['embedding_dim']])\n",
    "        coefs = np.asarray(values[-hyperparams_features['embedding_dim']:], dtype='float32')\n",
    "        word_i = voc.get(word)\n",
    "        if word_i is not None:\n",
    "            embedding_matrix[word_i] = coefs\n",
    "            cnt_inv += 1\n",
    "    f.close()\n",
    "\n",
    "    print('Total %s word vectors.' % len(embedding_matrix))\n",
    "    print('Words not found in embedding space %d' % (len(embedding_matrix)-cnt_inv))\n",
    " \n",
    "    return embedding_matrix\n",
    "\n",
    "def load_embeddings2(path, embedding_dim, voc):\n",
    "    # random matrix with mean value = 0\n",
    "    embedding_matrix = np.random.random((len(voc)+2, embedding_dim)) #- 0.5 # voc + unk + pad value(0)\n",
    "    cnt_inv = 0\n",
    "    with open(path, \"rb\") as f:\n",
    "        embedding_dict = pickle.load(f)\n",
    "    for word, coefs in embedding_dict.items():\n",
    "        word_i = voc.get(word)\n",
    "        if word_i is not None:\n",
    "            embedding_matrix[word_i] = coefs\n",
    "            cnt_inv += 1\n",
    "    print('Total %s word vectors.' % len(embedding_matrix))\n",
    "    print('Words not found in embedding space %d' % (len(embedding_matrix)-cnt_inv))\n",
    " \n",
    "    return embedding_matrix\n",
    "# \n",
    "# pretrained_embeddings_path = root_dir + '/resources/glove.twitter.27B/glove.twitter.27B.%dd.txt' % hyperparams_features['embedding_dim']\n",
    "# pretrained_embeddings_path = root_dir + '/resources/glove.840B/glove.840B.%dd.txt' % hyperparams_features['embedding_dim']\n",
    "pretrained_embeddings_path = hyperparams_features['embeddings_path']#root_dir + '/eRisk/finetuned_glove_clpsych_erisk_normalized_2_20000.pkl'\n",
    "embedding_matrix = load_embeddings(pretrained_embeddings_path, hyperparams_features['embedding_dim'], \n",
    "                                    voc=vocabulary_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.mean()\n",
    "# hyperparams_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics():\n",
    "    def __init__(self, threshold=0.5):\n",
    "        self.threshold=threshold\n",
    "        \n",
    "    def recall_m(self, y_true, y_pred):\n",
    "            y_labels = y_true\n",
    "            y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), self.threshold), K.floatx())        \n",
    "            possible_positives = K.sum(K.round(K.clip(y_labels, 0, 1)))\n",
    "            true_positives = K.sum(K.round(K.clip(y_labels * y_pred, 0, 1)))\n",
    "            recall = true_positives / (possible_positives + K.epsilon())\n",
    "            return recall\n",
    "\n",
    "    def precision_m(self, y_true, y_pred):\n",
    "            y_labels = y_true\n",
    "            y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), self.threshold), K.floatx())        \n",
    "            true_positives = K.sum(K.round(K.clip(y_labels * y_pred, 0, 1)))\n",
    "            predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "            precision = true_positives / (predicted_positives + K.epsilon())\n",
    "            return precision\n",
    "\n",
    "    def f1_m(self, y_true, y_pred):\n",
    "        precision = self.precision_m(y_true, y_pred)\n",
    "        recall = self.recall_m(y_true, y_pred)\n",
    "        return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "    \n",
    "    def auc2(self, y_true, y_pred):\n",
    "#         auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "#         K.get_session().run(tf.local_variables_initializer())\n",
    "        return y_true\n",
    "\n",
    "    def auc(self, y_true, y_pred):\n",
    "        has_true_examples = K.greater(K.cast(K.sum(y_true), K.floatx()),0)\n",
    "        has_false_examples = K.less(K.cast(K.mean(y_true), K.floatx()),1)\n",
    "        score = tf.cond(tf.logical_and(has_true_examples, has_false_examples), \n",
    "                        lambda:tf.py_function(roc_auc_score, (y_true, y_pred), tf.float32), \n",
    "                        lambda:0.0)\n",
    "        return score\n",
    "        \n",
    "def binary_crossentropy_custom(y_true, y_pred):\n",
    "    y_labels = y_true\n",
    "    return K.binary_crossentropy(y_labels, \n",
    "                                 y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fine_tune_layers=10,\n",
    "        pooling=\"first\",\n",
    "        trainable=True,\n",
    "        bert_path=\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\", \n",
    "        **kwargs\n",
    "    ):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = trainable\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_path = bert_path\n",
    "        if self.pooling not in [\"first\", \"mean\"]:\n",
    "            raise NameError(\n",
    "               \"Undefined pooling type (must be either first or mean, but is %s)\" % self.pooling\n",
    "            )\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def get_config(self):\n",
    "\n",
    "        config = super(BertLayer, self).get_config().copy()\n",
    "        config.update({\n",
    "            'n_fine_tune_layers': self.n_fine_tune_layers,\n",
    "            'trainable': self.trainable,\n",
    "            'output_size': self.output_size,\n",
    "            'pooling': self.pooling,\n",
    "            'bert_path': self.bert_path,\n",
    "        })\n",
    "\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_path, trainable=self.trainable, name=\"%s_module\" % self.name\n",
    "        )\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        if self.pooling == \"first\":\n",
    "            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "            trainable_layers = [\"pooler/dense\"]\n",
    "\n",
    "        elif self.pooling == \"mean\":\n",
    "            trainable_vars = [\n",
    "                var\n",
    "                for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "            ]\n",
    "            trainable_layers = []\n",
    "        else:\n",
    "            raise NameError(\n",
    "                \"Undefined pooling type (must be either first or mean, but is %s)\" % self.pooling\n",
    "            )\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(\"encoder/layer_%s\" % str(11 - i))\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        if self.pooling == \"first\":\n",
    "            pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"pooled_output\"\n",
    "            ]\n",
    "        elif self.pooling == \"mean\":\n",
    "            result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"sequence_output\"\n",
    "            ]\n",
    "\n",
    "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
    "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
    "            input_mask = tf.cast(input_mask, tf.float32)\n",
    "            pooled = masked_reduce_mean(result, input_mask)\n",
    "        else:\n",
    "            raise NameError(\"Undefined pooling type (must be either first or mean, but is %s)\" % self.pooling)\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopwords_list,\n",
    "                liwc_categories,\n",
    "               ignore_layer=[]):\n",
    "    def attention(xin):\n",
    "        return K.sum(xin, axis=1) \n",
    "    tokens_features = Input(shape=(hyperparams['maxlen'],), name='word_seq')\n",
    "    embedding_layer = Embedding(hyperparams_features['max_features'], \n",
    "                                hyperparams_features['embedding_dim'], \n",
    "                                input_length=hyperparams['maxlen'],\n",
    "                                embeddings_regularizer=regularizers.l2(hyperparams['l2_embeddings']),\n",
    "                                weights=[embedding_matrix], \n",
    "                                trainable=hyperparams['trainable_embeddings'],\n",
    "                               name='embeddings_layer')(\n",
    "        tokens_features)\n",
    "#     if 'batchnorm' not in ignore_layer:\n",
    "#         embedding_layer_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "#                                                      name='embeddings_layer_norm')(embedding_layer)\n",
    "    embedding_layer = Dropout(hyperparams['dropout'], name='embedding_dropout')(embedding_layer)\n",
    "\n",
    "    if 'lstm' not in hyperparams['ignore_layer']:\n",
    "        if False: #tf.test.is_gpu_available():\n",
    "            lstm_layers = Bidirectional(CuDNNLSTM(\n",
    "                            hyperparams['lstm_units'], \n",
    "                                    return_sequences='attention' not in hyperparams['ignore_layer'], # only True if using attention\n",
    "                          name='LSTM_layer'), name='bilstm')(embedding_layer)\n",
    "        else:\n",
    "#             lstm_layers = Bidirectional(LSTM(hyperparams['lstm_units'], \n",
    "#                                return_sequences='attention' not in ignore_layer,\n",
    "#                           name='LSTM_layer'), name='bilstm')(embedding_layer)\n",
    "            lstm_layers = LSTM(hyperparams['lstm_units'], \n",
    "                               return_sequences='attention' not in hyperparams['ignore_layer'],\n",
    "                          name='LSTM_layer')(embedding_layer)\n",
    "            \n",
    "    elif 'cnn' not in hyperparams['ignore_layer']:\n",
    "        cnn_layers = Conv1D(hyperparams['filters'],\n",
    "                             hyperparams['kernel_size'],\n",
    "                             padding='valid',\n",
    "                             activation='relu',\n",
    "                             strides=1, name='convolution')(embedding_layer)\n",
    "        # we use max pooling:\n",
    "        cnn_layers = GlobalMaxPooling1D()(cnn_layers)\n",
    "    \n",
    "    # Attention\n",
    "    if 'attention' not in hyperparams['ignore_layer']:\n",
    "        attention = Dense(1, activation='tanh', name='attention')(lstm_layers)\n",
    "        attention = Flatten()(attention)\n",
    "        attention = Activation('softmax')(attention)\n",
    "        attention = RepeatVector(hyperparams['lstm_units']*2)(attention)\n",
    "        attention = Permute([2, 1])(attention)\n",
    "\n",
    "        sent_representation = Multiply()([lstm_layers, attention])\n",
    "        sent_representation = Lambda(lambda xin: K.sum(xin, axis=1), \n",
    "                                     output_shape=(hyperparams['lstm_units'],)\n",
    "                                    )(sent_representation)\n",
    "#         sent_representation = Lambda(attention, \n",
    "#                                          output_shape=(hyperparams['lstm_units'],\n",
    "#                                         ))(sent_representation)\n",
    "\n",
    "        \n",
    "    elif 'lstm' not in hyperparams['ignore_layer']:\n",
    "        sent_representation = lstm_layers\n",
    "    elif 'cnn' not in hyperparams['ignore_layer']:\n",
    "        sent_representation = cnn_layers\n",
    "    else:\n",
    "        sent_representation = None\n",
    "        \n",
    "    if sent_representation is not None:\n",
    "        sent_representation = Dropout(hyperparams['dropout'], name='lstm_att_dropout')(sent_representation)\n",
    "        if hyperparams['dense_sentence_units']:\n",
    "            sent_representation = Dense(units=hyperparams['dense_sentence_units'], activation='relu',\n",
    "                                       name='dense_sent_representation')(sent_representation)\n",
    "    \n",
    "    # Other features\n",
    "    numerical_features = Input(shape=(len(emotions) + 1 + len(liwc_categories),), name='numeric_input') # emotions and pronouns\n",
    "    sparse_features = Input(shape=(len(stopwords_list),), name='sparse_input') # stopwords\n",
    "\n",
    "    dense_layer_sparse = Dense(units=hyperparams['dense_bow_units'],\n",
    "                              name='sparse_feat_dense_layer', activation='relu',\n",
    "                                kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                              )(sparse_features)\n",
    "    \n",
    "    # BERT encoder\n",
    "    if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "        in_id_bert = Input(shape=(hyperparams['maxlen'],), name=\"input_ids_bert\")\n",
    "        in_mask_bert = Input(shape=(hyperparams['maxlen'],), name=\"input_masks_bert\")\n",
    "        in_segment_bert = Input(shape=(hyperparams['maxlen'],), name=\"segment_ids_bert\")\n",
    "        bert_inputs = [in_id_bert, in_mask_bert, in_segment_bert]\n",
    "\n",
    "        bert_output = BertLayer(n_fine_tune_layers=hyperparams['bert_finetune_layers'], \n",
    "                                pooling=hyperparams['bert_pooling'],\n",
    "                               trainable=hyperparams['bert_trainable'],\n",
    "                               name='bert_layer')(bert_inputs)\n",
    "        dense_bert = Dense(hyperparams['bert_dense_units'], activation='relu',\n",
    "                           kernel_regularizer=regularizers.l2(hyperparams['l2_bert']),\n",
    "                          name='bert_dense_layer')(bert_output)\n",
    "    else:\n",
    "        dense_bert = None\n",
    "\n",
    "\n",
    "    \n",
    "    # Batch normalization\n",
    "    if 'batchnorm' not in hyperparams['ignore_layer']:\n",
    "        numerical_features_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                     name='numerical_features_norm')(numerical_features)\n",
    "        sent_representation_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                      name='sent_repr_norm')(sent_representation)\n",
    "        dense_layer_sparse_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                     name='sparse_features_norm')(dense_layer_sparse)\n",
    "        if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "            if hyperparams['bert_dense_units']:\n",
    "                dense_bert_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                         name='bert_layer_norm')(dense_bert)\n",
    "            else:\n",
    "                dense_bert_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                         name='bert_layer_norm')(bert_output)\n",
    "        else:\n",
    "            dense_bert_norm = None\n",
    "        \n",
    "#     subjects = Input(shape=(1,), name='subjects')\n",
    "    \n",
    "\n",
    "    all_layers = {\n",
    "        'user_encoded': sent_representation,\n",
    "        'numerical_dense_layer': numerical_features,\n",
    "        'sparse_feat_dense_layer': dense_layer_sparse,\n",
    "        'bert_layer': dense_bert if hyperparams['bert_dense_units'] else bert_output,\n",
    "    }\n",
    "    if 'batchnorm' not in hyperparams['ignore_layer']:\n",
    "        all_layers = {\n",
    "            'user_encoded': sent_representation_norm,\n",
    "            'numerical_dense_layer': numerical_features_norm,\n",
    "            'sparse_feat_dense_layer': dense_layer_sparse_norm,\n",
    "            'bert_layer': dense_bert_norm\n",
    "        }\n",
    "    layers_to_merge = []\n",
    "    for n, l in all_layers.items():\n",
    "        if n in hyperparams['ignore_layer'] or l is None:\n",
    "            continue\n",
    "        layers_to_merge.append(l)\n",
    "        \n",
    "    if len(layers_to_merge) == 1:\n",
    "        merged_layers = layers_to_merge[0]\n",
    "    else:\n",
    "        merged_layers = concatenate(layers_to_merge)\n",
    "    output_layer = Dense(1, activation='sigmoid',\n",
    "                         name='output_layer',\n",
    "                        kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                        )(merged_layers)\n",
    "\n",
    "    # Compile model\n",
    "    if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "        inputs=[tokens_features, numerical_features, sparse_features, \n",
    "                              in_id_bert, in_mask_bert, in_segment_bert,\n",
    "    #                           subjects\n",
    "                             ]\n",
    "    else:\n",
    "        inputs=[tokens_features, numerical_features, sparse_features]\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "        \n",
    "\n",
    "    metrics_class = Metrics(threshold=hyperparams['threshold'])\n",
    "    model.compile(hyperparams['optimizer'], binary_crossentropy_custom,\n",
    "                  metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m,\n",
    "                          metrics_class.auc, \n",
    "                           tf.keras.metrics.AUC(name=\"AUC_keras\"), \n",
    "                           tf.keras.metrics.FalsePositives(), \n",
    "                           tf.keras.metrics.FalseNegatives()])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tl_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopwords_list,\n",
    "                liwc_categories,\n",
    "               ignore_layer=[]):\n",
    "    def attention(xin):\n",
    "        return K.sum(xin, axis=1) \n",
    "    tokens_features = Input(shape=(hyperparams['maxlen'],), name='word_seq')\n",
    "    embedding_layer = Embedding(hyperparams_features['max_features'], \n",
    "                                hyperparams_features['embedding_dim'], \n",
    "                                input_length=hyperparams['maxlen'],\n",
    "                                embeddings_regularizer=regularizers.l2(hyperparams['l2_embeddings']),\n",
    "                                weights=[embedding_matrix], \n",
    "                                trainable=False,\n",
    "                               name='embeddings_layer')(\n",
    "        tokens_features)\n",
    "#     if 'batchnorm' not in ignore_layer:\n",
    "#         embedding_layer_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "#                                                      name='embeddings_layer_norm')(embedding_layer)\n",
    "    embedding_layer = Dropout(hyperparams['dropout'], name='embedding_dropout')(embedding_layer)\n",
    "\n",
    "    if 'lstm' not in ignore_layer:\n",
    "        if False: #tf.test.is_gpu_available():\n",
    "            lstm_layers = Bidirectional(CuDNNLSTM(hyperparams['lstm_units'], trainable=False,\n",
    "                                    return_sequences='attention' not in ignore_layer, # only True if using attention\n",
    "                          name='LSTM_layer'), name='bilstm')(embedding_layer)\n",
    "        else:\n",
    "            lstm_layers = Bidirectional(LSTM(hyperparams['lstm_units'], trainable=False,\n",
    "                               return_sequences='attention' not in ignore_layer,\n",
    "                          name='LSTM_layer'), name='bilstm')(embedding_layer)\n",
    "            \n",
    "    elif 'cnn' not in ignore_layer:\n",
    "        cnn_layers = Conv1D(hyperparams['filters'],\n",
    "                             hyperparams['kernel_size'],\n",
    "                             padding='valid',\n",
    "                             activation='relu',\n",
    "                             strides=1,\n",
    "                           trainable=False,\n",
    "                           name='convolution')(embedding_layer)\n",
    "        # we use max pooling:\n",
    "        cnn_layers = GlobalMaxPooling1D(trainable=False)(cnn_layers)\n",
    "    \n",
    "    # Attention\n",
    "    if 'attention' not in ignore_layer:\n",
    "        attention = Dense(1, activation='tanh', name='attention', trainable=False)(lstm_layers)\n",
    "        attention = Flatten()(attention)\n",
    "        attention = Activation('softmax')(attention)\n",
    "        attention = RepeatVector(hyperparams['lstm_units']*2)(attention)\n",
    "        attention = Permute([2, 1])(attention)\n",
    "\n",
    "        sent_representation = Multiply()([lstm_layers, attention])\n",
    "        sent_representation = Lambda(lambda xin: K.sum(xin, axis=1), \n",
    "                                     output_shape=(hyperparams['lstm_units'],)\n",
    "                                    )(sent_representation)\n",
    "#         sent_representation = Lambda(attention, \n",
    "#                                          output_shape=(hyperparams['lstm_units'],\n",
    "#                                         ))(sent_representation)\n",
    "\n",
    "        \n",
    "    elif 'lstm' not in ignore_layer:\n",
    "        sent_representation = lstm_layers\n",
    "    elif 'cnn' not in ignore_layer:\n",
    "        sent_representation = cnn_layers\n",
    "        \n",
    "    \n",
    "    sent_representation = Dropout(hyperparams['dropout'], name='lstm_att_dropout')(sent_representation)\n",
    "    if hyperparams['dense_sentence_units']:\n",
    "        sent_representation = Dense(units=hyperparams['dense_sentence_units'], activation='relu',\n",
    "                                   name='dense_sent_representation',\n",
    "                                   trainable=False)(sent_representation)\n",
    "    \n",
    "    # Other features\n",
    "    numerical_features = Input(shape=(len(emotions) + 1 + len(liwc_categories),), name='numeric_input') # emotions and pronouns\n",
    "    sparse_features = Input(shape=(len(stopwords_list),), name='sparse_input') # stopwords\n",
    "\n",
    "    dense_layer_sparse = Dense(units=hyperparams['dense_bow_units'],\n",
    "                              name='sparse_feat_dense_layer', activation='relu',\n",
    "                                kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                               trainable=False,\n",
    "                              )(sparse_features)\n",
    "    \n",
    "    # BERT encoder\n",
    "    if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "        in_id_bert = Input(shape=(hyperparams['maxlen'],), name=\"input_ids_bert\")\n",
    "        in_mask_bert = Input(shape=(hyperparams['maxlen'],), name=\"input_masks_bert\")\n",
    "        in_segment_bert = Input(shape=(hyperparams['maxlen'],), name=\"segment_ids_bert\")\n",
    "        bert_inputs = [in_id_bert, in_mask_bert, in_segment_bert]\n",
    "\n",
    "        bert_output = BertLayer(n_fine_tune_layers=hyperparams['bert_finetune_layers'], \n",
    "                                pooling=hyperparams['bert_pooling'],\n",
    "                               trainable=False,\n",
    "                               name='bert_layer')(bert_inputs)\n",
    "        dense_bert = Dense(hyperparams['bert_dense_units'], activation='relu',\n",
    "                           kernel_regularizer=regularizers.l2(hyperparams['l2_bert']),\n",
    "                          name='bert_dense_layer',\n",
    "                          trainable=False)(bert_output)\n",
    "    else:\n",
    "        dense_bert = None\n",
    "\n",
    "\n",
    "    \n",
    "    # Batch normalization\n",
    "    if 'batchnorm' not in ignore_layer:\n",
    "        numerical_features_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                     name='numerical_features_norm')(numerical_features)\n",
    "        sent_representation_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                      name='sent_repr_norm')(sent_representation)\n",
    "        dense_layer_sparse_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                     name='sparse_features_norm')(dense_layer_sparse)\n",
    "        if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "            if hyperparams['bert_dense_units']:\n",
    "                dense_bert_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                         name='bert_layer_norm')(dense_bert)\n",
    "            else:\n",
    "                dense_bert_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                         name='bert_layer_norm')(bert_output)\n",
    "        else:\n",
    "            dense_bert_norm = None\n",
    "        \n",
    "#     subjects = Input(shape=(1,), name='subjects')\n",
    "    \n",
    "\n",
    "    all_layers = {\n",
    "        'user_encoded': sent_representation,\n",
    "        'numerical_dense_layer': numerical_features,\n",
    "        'sparse_feat_dense_layer': dense_layer_sparse,\n",
    "        'bert_layer': dense_bert if hyperparams['bert_dense_units'] else bert_output,\n",
    "    }\n",
    "    if 'batchnorm' not in ignore_layer:\n",
    "        all_layers = {\n",
    "            'user_encoded': sent_representation_norm,\n",
    "            'numerical_dense_layer': numerical_features_norm,\n",
    "            'sparse_feat_dense_layer': dense_layer_sparse_norm,\n",
    "            'bert_layer': dense_bert_norm\n",
    "        }\n",
    "    layers_to_merge = []\n",
    "    for n, l in all_layers.items():\n",
    "        if n in ignore_layer:\n",
    "            continue\n",
    "        layers_to_merge.append(l)\n",
    "        \n",
    "    if len(layers_to_merge) == 1:\n",
    "        merged_layers = layers_to_merge[0]\n",
    "    else:\n",
    "        merged_layers = concatenate(layers_to_merge)\n",
    "        \n",
    "    TL_layer = Dense(hyperparams['transfer_units'], activation='sigmoid',\n",
    "                         name='tl_layer',\n",
    "                        kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                         trainable=True\n",
    "                        )(merged_layers)\n",
    "    output_layer = Dense(1, activation='sigmoid',\n",
    "                         name='output_layer_tl',\n",
    "                        kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                         trainable=True,\n",
    "                        )(TL_layer)\n",
    "\n",
    "    # Compile model\n",
    "    if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "        inputs=[tokens_features, numerical_features, sparse_features, \n",
    "                              in_id_bert, in_mask_bert, in_segment_bert,\n",
    "    #                           subjects\n",
    "                             ]\n",
    "    else:\n",
    "        inputs=[tokens_features, numerical_features, sparse_features]\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "        \n",
    "\n",
    "    metrics_class = Metrics(threshold=hyperparams['threshold'])\n",
    "    model.summary()\n",
    "    model.compile(hyperparams['optimizer'], binary_crossentropy_custom,\n",
    "                  metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m,\n",
    "                          metrics_class.auc, \n",
    "                           tf.keras.metrics.AUC(name=\"AUC_keras\"), \n",
    "                           tf.keras.metrics.FalsePositives(), \n",
    "                           tf.keras.metrics.FalseNegatives()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hierarchical_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopwords_list,\n",
    "                liwc_categories,\n",
    "               ignore_layer=[]):\n",
    "    def attention(xin):\n",
    "        return K.sum(xin, axis=1) \n",
    "\n",
    "\n",
    "    # Post/sentence representation - word sequence\n",
    "    tokens_features = Input(shape=(hyperparams['maxlen'],), name='word_seq')\n",
    "    embedding_layer = Embedding(hyperparams_features['max_features'], \n",
    "                                hyperparams_features['embedding_dim'], \n",
    "                                input_length=hyperparams['maxlen'],\n",
    "                                embeddings_regularizer=regularizers.l2(hyperparams['l2_embeddings']),\n",
    "                                weights=[embedding_matrix], \n",
    "                                trainable=hyperparams['trainable_embeddings'],\n",
    "                               name='embeddings_layer')(\n",
    "        tokens_features)\n",
    "    embedding_layer = Dropout(hyperparams['dropout'], name='embedding_dropout')(embedding_layer)\n",
    "\n",
    "    \n",
    "    if 'lstm' not in ignore_layer:\n",
    "        if False: #tf.test.is_gpu_available():\n",
    "            lstm_layers = CuDNNLSTM(hyperparams['lstm_units'], \n",
    "                                    return_sequences='attention' not in ignore_layer, # only True if using attention\n",
    "                          name='LSTM_layer')(embedding_layer)\n",
    "        else:\n",
    "            lstm_layers = LSTM(hyperparams['lstm_units'], \n",
    "                               return_sequences='attention' not in ignore_layer,\n",
    "                          name='LSTM_layer')(embedding_layer)\n",
    "\n",
    "        # Attention\n",
    "        if 'attention' not in ignore_layer:\n",
    "            attention = Dense(1, activation='tanh', name='attention')(lstm_layers)\n",
    "            attention = Flatten()(attention)\n",
    "            attention = Activation('softmax')(attention)\n",
    "            attention = RepeatVector(hyperparams['lstm_units'])(attention)\n",
    "            attention = Permute([2, 1])(attention)\n",
    "\n",
    "            sent_representation = Multiply()([lstm_layers, attention])\n",
    "            sent_representation = Lambda(lambda xin: K.sum(xin, axis=1), \n",
    "                                     output_shape=(hyperparams['lstm_units'],)\n",
    "                                    )(sent_representation)\n",
    "\n",
    "#             sent_representation = Lambda(attention, \n",
    "#                                          output_shape=(hyperparams['lstm_units'],\n",
    "#                                         ))(sent_representation)\n",
    "        else:\n",
    "            sent_representation = lstm_layers\n",
    "\n",
    "    elif 'cnn' not in ignore_layer:\n",
    "        cnn_layers = Conv1D(hyperparams['filters'],\n",
    "                             hyperparams['kernel_size'],\n",
    "                             padding='valid',\n",
    "                             activation='relu',\n",
    "                             strides=1)(embedding_layer)\n",
    "        # we use max pooling:\n",
    "        sent_representation = GlobalMaxPooling1D()(cnn_layers)\n",
    "    \n",
    "    if 'batchnorm' not in ignore_layer:\n",
    "        sent_representation = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                          name='sent_repr_norm')(sent_representation)\n",
    "    sent_representation = Dropout(hyperparams['dropout'], name='sent_repr_dropout')(sent_representation)\n",
    "\n",
    "\n",
    "    # Hierarchy\n",
    "    sentEncoder = Model(inputs=tokens_features, \n",
    "                        outputs=sent_representation)\n",
    "    sentEncoder.summary()\n",
    "\n",
    "    posts_history_input = Input(shape=(hyperparams['posts_per_group'], \n",
    "                                 hyperparams['maxlen']\n",
    "                                      ), name='hierarchical_word_seq_input')\n",
    "\n",
    "    user_encoder = TimeDistributed(sentEncoder, name='user_encoder')(posts_history_input)    \n",
    "        \n",
    "    # BERT encoder\n",
    "    if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "        in_id_bert = Input(shape=(hyperparams['maxlen'],), name=\"input_ids_bert\")\n",
    "        in_mask_bert = Input(shape=(hyperparams['maxlen'],), name=\"input_masks_bert\")\n",
    "        in_segment_bert = Input(shape=(hyperparams['maxlen'],), name=\"segment_ids_bert\")\n",
    "        bert_inputs = [in_id_bert, in_mask_bert, in_segment_bert]\n",
    "\n",
    "        bert_output = BertLayer(n_fine_tune_layers=hyperparams['bert_finetune_layers'], \n",
    "                                pooling=hyperparams['bert_pooling'],\n",
    "                               trainable=hyperparams['bert_trainable'],\n",
    "                               name='bert_layer')(bert_inputs)\n",
    "        dense_bert = Dense(hyperparams['bert_dense_units'], \n",
    "                           activation='relu',\n",
    "                          kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                          name='bert_dense_layer')(bert_output)\n",
    "    \n",
    "        bertSentEncoder = Model(bert_inputs, dense_bert)\n",
    "\n",
    "\n",
    "        in_id_bert_history = Input(shape=(hyperparams['posts_per_group'],\n",
    "                                                          hyperparams['maxlen'],), name=\"input_ids_bert_hist\")\n",
    "        in_mask_bert_history = Input(shape=(hyperparams['posts_per_group'],\n",
    "                                                            hyperparams['maxlen'],), name=\"input_masks_bert_hist\")\n",
    "        in_segment_bert_history = Input(shape=(hyperparams['posts_per_group'],\n",
    "                                                               hyperparams['maxlen'],), name=\"segment_ids_bert_hist\")\n",
    "        bert_inputs_history = [in_id_bert_history, in_mask_bert_history, in_segment_bert_history]\n",
    "        bert_inputs_concatenated = concatenate(bert_inputs_history)\n",
    "        inputs_indices = [hyperparams['maxlen']*i for i in range(3)]\n",
    "        # slice the input in equal slices on the last dimension\n",
    "        bert_encoder_layer = TimeDistributed(Lambda(lambda x: bertSentEncoder([x[:,inputs_indices[0]:inputs_indices[1]], \n",
    "                                                                      x[:,inputs_indices[1]:inputs_indices[2]],\n",
    "                                                                              x[:,inputs_indices[2]:]])),\n",
    "                                            name='bert_distributed_layer')(\n",
    "                            bert_inputs_concatenated)\n",
    "        bertUserEncoder = Model(bert_inputs_history, bert_encoder_layer)\n",
    "        bertUserEncoder.summary()\n",
    "\n",
    "        bert_user_encoder = bertUserEncoder(bert_inputs_history)\n",
    "    else:\n",
    "        bert_user_encoder = None\n",
    "    \n",
    "    # Other features \n",
    "    numerical_features_history = Input(shape=(\n",
    "            hyperparams['posts_per_group'],\n",
    "            len(emotions) + 1 + len(liwc_categories)\n",
    "        ), name='numeric_input_hist') # emotions and pronouns\n",
    "    sparse_features_history = Input(shape=(\n",
    "            hyperparams['posts_per_group'],\n",
    "            len(stopwords_list)\n",
    "        ), name='sparse_input_hist') # stopwords\n",
    "    \n",
    "    \n",
    "    dense_layer_sparse = Dense(units=hyperparams['dense_bow_units'],\n",
    "                              name='sparse_feat_dense_layer', activation='relu',\n",
    "                                kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                              )\n",
    "    dense_layer_sparse_user = TimeDistributed(dense_layer_sparse,\n",
    "                                             name='sparse_dense_layer_user')(sparse_features_history)\n",
    "\n",
    "    \n",
    "    # Concatenate features\n",
    "    if 'batchnorm' not in ignore_layer:\n",
    "        numerical_features_history_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                     name='numerical_features_norm')(numerical_features_history)\n",
    "        dense_layer_sparse_user = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                     name='sparse_features_norm')(dense_layer_sparse_user)\n",
    "    all_layers = {\n",
    "        'user_encoded': user_encoder,\n",
    "        'bert_layer': bert_user_encoder,\n",
    "        'numerical_dense_layer': numerical_features_history if 'batchnorm' in ignore_layer \\\n",
    "                    else numerical_features_history_norm,\n",
    "        'sparse_feat_dense_layer': dense_layer_sparse_user,\n",
    "    }\n",
    "    \n",
    "    layers_to_merge = [l for n,l in all_layers.items() if n not in ignore_layer]\n",
    "    if len(layers_to_merge) == 1:\n",
    "        merged_layers = layers_to_merge[0]\n",
    "    else:\n",
    "        merged_layers = concatenate(layers_to_merge)\n",
    "    \n",
    "    if 'lstm_user' not in ignore_layer:\n",
    "\n",
    "        if False:#tf.test.is_gpu_available():\n",
    "            lstm_user_layers = CuDNNLSTM(hyperparams['lstm_units_user'], \n",
    "                                    return_sequences='attention_user' not in ignore_layer, # only True if using attention\n",
    "                          name='LSTM_layer_user')(merged_layers)\n",
    "        else:\n",
    "            lstm_user_layers = LSTM(hyperparams['lstm_units_user'], \n",
    "                               return_sequences='attention_user' not in ignore_layer,\n",
    "                          name='LSTM_layer_user')(merged_layers)\n",
    "\n",
    "        # Attention\n",
    "        if 'attention_user' not in ignore_layer:\n",
    "            attention_user = Dense(1, activation='tanh', name='attention_user')(lstm_user_layers)\n",
    "            attention_user = Flatten()(attention_user)\n",
    "            attention_user = Activation('softmax')(attention_user)\n",
    "            attention_user = RepeatVector(hyperparams['lstm_units_user'])(attention_user)\n",
    "            attention_user = Permute([2, 1])(attention_user)\n",
    "\n",
    "            user_representation = Multiply()([lstm_user_layers, attention_user])\n",
    "            user_representation = Lambda(lambda xin: K.sum(xin, axis=1), \n",
    "                                         output_shape=(hyperparams['lstm_units_user'],))(user_representation)\n",
    "#             user_representation = Lambda(attention, \n",
    "#                                          output_shape=(hyperparams['lstm_units_user'],\n",
    "#                                         ))(user_representation)\n",
    "        else:\n",
    "            user_representation = lstm_user_layers\n",
    "    \n",
    "    \n",
    "    elif 'cnn_user' not in ignore_layer:\n",
    "        cnn_layers_user = Conv1D(hyperparams['filters_user'],\n",
    "                             hyperparams['kernel_size_user'],\n",
    "                             padding='valid',\n",
    "                             activation='relu',\n",
    "                             strides=1)(merged_layers)\n",
    "        # we use max pooling:\n",
    "        user_representation = GlobalMaxPooling1D()(cnn_layers_user)\n",
    "#         user_representation = Flatten()(user_representation)\n",
    "    \n",
    "    \n",
    "    user_representation = Dropout(hyperparams['dropout'], name='user_repr_dropout')(user_representation)\n",
    "    \n",
    "    \n",
    "    if hyperparams['dense_user_units']:\n",
    "        user_representation = Dense(units=hyperparams['dense_user_units'], activation='relu',\n",
    "                                   name='dense_user_representation')(user_representation)\n",
    "        \n",
    "    output_layer = Dense(1, activation='sigmoid',\n",
    "                         name='output_layer',\n",
    "                        kernel_regularizer=regularizers.l2(hyperparams['l2_dense'])\n",
    "                        )(user_representation)\n",
    "\n",
    "    # Compile model\n",
    "    if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "        \n",
    "        hierarchical_model = Model(inputs=[posts_history_input, \n",
    "                                       numerical_features_history, sparse_features_history,\n",
    "                                      in_id_bert_history, in_mask_bert_history, in_segment_bert_history], \n",
    "                  outputs=output_layer)\n",
    "    else:\n",
    "        hierarchical_model = Model(inputs=[posts_history_input, \n",
    "                                       numerical_features_history, sparse_features_history,\n",
    "                                      ], \n",
    "                  outputs=output_layer)\n",
    "    hierarchical_model.summary()\n",
    "    \n",
    "    metrics_class = Metrics(threshold=hyperparams['threshold'])\n",
    "    hierarchical_model.compile(hyperparams['optimizer'], binary_crossentropy_custom,\n",
    "                  metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m,\n",
    "                          metrics_class.auc])\n",
    "    return hierarchical_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_tl_model(pretrained_model, hyperparams, units=100,  emotions=emotions, stopwords_list=stopword_list,\n",
    "#                 liwc_categories=liwc_categories):\n",
    "#     tokens_features = Input(shape=(hyperparams['maxlen'],), name='word_seq_tl')\n",
    "#     numerical_features = Input(shape=(len(emotions) + 1 + len(liwc_categories),), name='numeric_input_tl') # emotions and pronouns\n",
    "#     sparse_features = Input(shape=(len(stopwords_list),), name='sparse_input_tl') # stopwords\n",
    "#     pretrained_model.summary()\n",
    "#     print(\"inputs\", tokens_features, numerical_features, sparse_features)\n",
    "#     pretrained_output = pretrained_model(inputs=[tokens_features, numerical_features, sparse_features])\n",
    "#     # TODO: set trainable to false\n",
    "#     tl_layer = Dense(units=units, activation='relu',\n",
    "#                                    name='tl_layer', trainable=False,\n",
    "#                         kernel_regularizer=regularizers.l2(hyperparams['l2_dense'])\n",
    "#                     )(pretrained_output)\n",
    "#     output_layer = Dense(1, activation='sigmoid',\n",
    "#                          name='tl_output_layer',\n",
    "#                         kernel_regularizer=regularizers.l2(hyperparams['l2_dense'])\n",
    "#                         )(tl_layer)\n",
    "# #     pretrained_model.outputs = output_layer ## I DUNNO\n",
    "#     tl_model = Model(inputs=[tokens_features, numerical_features, sparse_features], \n",
    "#           outputs=output_layer)\n",
    "#     metrics_class = Metrics(threshold=hyperparams['threshold'])\n",
    "#     tl_model.compile(hyperparams['optimizer'], binary_crossentropy_custom,\n",
    "#                   metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m,\n",
    "#                           metrics_class.auc, \n",
    "#                            tf.keras.metrics.AUC(name=\"AUC_keras\"), \n",
    "#                            tf.keras.metrics.FalsePositives(), \n",
    "#                            tf.keras.metrics.FalseNegatives()])\n",
    "\n",
    "#     return tl_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopword_list,\n",
    "#                     liwc_categories=[c for c in categories if c in writings_df.columns]\n",
    "# ,\n",
    "#                    ignore_layer=hyperparams['ignore_layer'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchical_model = build_hierarchical_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopword_list,\n",
    "#                     liwc_categories=[c for c in categories if c in writings_df.columns]\n",
    "# ,\n",
    "#                    ignore_layer=hyperparams['ignore_layer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(model, 'models/sequential_bert_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize_vars(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightsHistory(callbacks.Callback):\n",
    "    def __init__(self, logs={}):\n",
    "        super(WeightsHistory, self).__init__()\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.log_weights(0)\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % 10 == 0:\n",
    "            self.log_weights(epoch)\n",
    "    def log_weights(self, step):\n",
    "        for layer in self.model.layers:\n",
    "            try:\n",
    "                experiment.log_histogram_3d(layer.get_weights()[0], \n",
    "                                            name=layer.name + \"_weight\", step=step)\n",
    "            except Exception as e:\n",
    "#                 logger.debug(\"Logging weights error: \" + layer.name + \"; \" + str(e) + \"\\n\")\n",
    "                # Layer probably does not exist\n",
    "                pass\n",
    "\n",
    "class OutputsHistory(callbacks.Callback):\n",
    "    def __init__(self, logs={}, generator=None, generator_type=\"\"):\n",
    "        super(OutputsHistory, self).__init__()\n",
    "        self.generator_type = generator_type\n",
    "        if generator:\n",
    "            self.generator = generator\n",
    "        elif generator_type:\n",
    "            self.generator = DataGenerator(user_level_data, subjects_split, \n",
    "                                     set_type=generator_type, \n",
    "                                   hierarchical=hyperparams['hierarchical'],\n",
    "                                seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
    "                                     max_posts_per_user=None,\n",
    "                                   pad_with_duplication=False,\n",
    "                                    posts_per_group=hyperparams['posts_per_group'],\n",
    "                                    post_groups_per_user=None, \n",
    "                                     sample_seqs=False, shuffle=False)\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.log_outputs(0)\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % 10 == 0:\n",
    "            self.log_outputs(epoch)\n",
    "    def log_outputs(self, step):\n",
    "        try:\n",
    "            experiment.log_histogram_3d(self.model.predict(self.generator,  verbose=1, steps=2),\n",
    "                                        name='output_%s' % self.generator_type, step=step)\n",
    "        except Exception as e:\n",
    "            logger.debug(\"Logging outputs error: \" + str(e) + \"\\n\")\n",
    "#                 Layer probably does not exist\n",
    "            pass\n",
    "\n",
    "class LRHistory(callbacks.Callback):\n",
    "    def __init__(self, logs={}):\n",
    "        super(LRHistory, self).__init__()\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.log_lr()\n",
    "        \n",
    "    def log_lr(self):\n",
    "        lr = K.eval(self.model.optimizer.lr)\n",
    "        logger.debug(\"Learning rate is %f...\\n\" % lr)\n",
    "        experiment.log_parameter('lr', lr)\n",
    "\n",
    "class FreezeLayer(callbacks.Callback):\n",
    "    def __init__(self, logs={}, patience=5, layer={'user_encoder':'embeddings_layer'}, verbose=1, set_to=False):\n",
    "        super(FreezeLayer, self).__init__()\n",
    "        self.freeze_epoch = patience\n",
    "        self.freeze_layer = layer\n",
    "        self.verbose = verbose\n",
    "        self.set_to = set_to\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        if type(self.freeze_layer)==dict:\n",
    "            submodel = self.model.get_layer(list(self.freeze_layer.keys())[0])\n",
    "        else:\n",
    "            submodel = self.model\n",
    "        logging.debug(\"Trainable embeddings\", submodel.get_layer(self.freeze_layer).trainable)\n",
    "        if epoch == self.freeze_epoch:\n",
    "            try:\n",
    "                layer = submodel.get_layer(self.freeze_layer)\n",
    "                old_value = layer.trainable\n",
    "                layer.trainable = self.set_to\n",
    "                # TODO: does this reset the optimizer? should I also compile the top-level model?\n",
    "                self.model.compile(hyperparams['optimizer'], binary_crossentropy_custom,\n",
    "                  metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m])\n",
    "                if self.verbose:\n",
    "                    logging.debug(\"Setting %s layer from %s to trainable=%s...\\n\" % (layer.name, old_value,\n",
    "                                                                   submodel.get_layer(self.freeze_layer).trainable))\n",
    "            except Exception as e:\n",
    "                # layer probably does not exist\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, hyperparams,\n",
    "                data_generator_train, data_generator_valid,\n",
    "                epochs, class_weight, start_epoch=0, workers=4,\n",
    "                callback_list = [],\n",
    "                \n",
    "                model_path='/tmp/model',\n",
    "                validation_set='valid',\n",
    "               verbose=1):\n",
    "    \n",
    "    logger.info(\"Initializing callbacks...\\n\")\n",
    "    # Initialize callbacks\n",
    "    freeze_layer = FreezeLayer(patience=hyperparams['freeze_patience'], set_to=not hyperparams['trainable_embeddings'])\n",
    "    weights_history = WeightsHistory()\n",
    "    outputs_history_valid = OutputsHistory(generator_type=validation_set)\n",
    "    outputs_history_train = OutputsHistory(generator_type='train')\n",
    "    lr_history = LRHistory()\n",
    "\n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=hyperparams['reduce_lr_factor'],\n",
    "                              patience=hyperparams['reduce_lr_patience'], min_lr=0.000001, verbose=1)\n",
    "    lr_schedule = callbacks.LearningRateScheduler(lambda epoch, lr: \n",
    "                                                  lr if (epoch+1)%hyperparams['scheduled_reduce_lr_freq']!=0 else\n",
    "                                                  lr*hyperparams['scheduled_reduce_lr_factor'], verbose=1)\n",
    "    callbacks_dict = {'freeze_layer': freeze_layer, 'weights_history': weights_history,\n",
    "           'outputs_history_valid': outputs_history_valid, 'outputs_history_train': outputs_history_train,\n",
    "           'lr_history': lr_history,\n",
    "           'reduce_lr_plateau': reduce_lr,\n",
    "            'lr_schedule': lr_schedule}\n",
    "\n",
    "    \n",
    "    logging.info('Train...')\n",
    "\n",
    "\n",
    "    history = model.fit_generator(data_generator_train,\n",
    "                steps_per_epoch=100,\n",
    "              epochs=epochs, initial_epoch=start_epoch, \n",
    "              class_weight=class_weight,\n",
    "              validation_data=data_generator_valid,\n",
    "                        verbose=verbose,\n",
    "#               validation_split=0.3,\n",
    "                       workers=workers,\n",
    "            callbacks = [\n",
    "                callbacks.ModelCheckpoint(filepath='%s_best.h5' % model_path, verbose=1, \n",
    "                                          save_best_only=True, save_weights_only=True),\n",
    "                callbacks.EarlyStopping(patience=hyperparams['early_stopping_patience'],\n",
    "                                       restore_best_weights=True)] + [\n",
    "                callbacks_dict[c] for c in [\n",
    "#                     'weights_history', \n",
    "#                     'outputs_history_valid', 'outputs_history_train', \n",
    "                    'reduce_lr_plateau', 'lr_schedule'\n",
    "                ]])\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network_type(hyperparams):\n",
    "    if 'lstm' in hyperparams['ignore_layer']:\n",
    "        network_type = 'cnn'\n",
    "    else:\n",
    "        network_type = 'lstm'\n",
    "    if 'user_encoded' in hyperparams['ignore_layer']:\n",
    "        if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "            network_type = 'bert'\n",
    "        else:\n",
    "            network_type = 'extfeatures'\n",
    "    if hyperparams['hierarchical']:\n",
    "        hierarch_type = 'hierarchical'\n",
    "    else:\n",
    "        hierarch_type = 'seq'\n",
    "    return network_type, hierarch_type\n",
    "\n",
    "def initialize_experiment(hyperparams, nrc_lexicon_path, emotions, pretrained_embeddings_path, \n",
    "                          dataset_type, transfer_type):\n",
    "\n",
    "    experiment = Experiment(api_key=\"eoBdVyznAhfg3bK9pZ58ZSXfv\",\n",
    "                            project_name=\"mental\", workspace=\"ananana\", disabled=False)\n",
    "\n",
    "    experiment.log_parameters(hyperparams_features)\n",
    "\n",
    "    experiment.log_parameter('emotion_lexicon', nrc_lexicon_path)\n",
    "    experiment.log_parameter('emotions', emotions)\n",
    "    experiment.log_parameter('embeddings_path', pretrained_embeddings_path)\n",
    "    experiment.log_parameter('dataset_type', dataset_type)\n",
    "    experiment.log_parameter('transfer_type', transfer_type)\n",
    "    experiment.add_tag(dataset_type)\n",
    "    experiment.log_parameters(hyperparams)\n",
    "    network_type, hierarch_type = get_network_type(hyperparams)\n",
    "    experiment.add_tag(network_type)\n",
    "    experiment.add_tag(hierarch_type)\n",
    "    \n",
    "    return experiment\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_datasets(user_level_data, subjects_split, hyperparams, hyperparams_features, \n",
    "                        validation_set, session=None):\n",
    "    liwc_words_for_categories = pickle.load(open(hyperparams_features['liwc_words_cached'], 'rb'))\n",
    "    data_generator_train = DataGenerator(user_level_data, subjects_split, set_type='train',\n",
    "                                        seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
    "                                        sample_seqs=hyperparams['sample_seqs'], sampling_distr=hyperparams['sampling_distr'],\n",
    "                                        posts_per_group=hyperparams['posts_per_group'], post_groups_per_user=hyperparams['post_groups_per_user'],\n",
    "                                        max_posts_per_user=hyperparams['posts_per_user'], \n",
    "                                         hierarchical=hyperparams['hierarchical'], liwc_categories=liwc_categories,\n",
    "                                         use_bert='bert_layer' not in hyperparams['ignore_layer'],\n",
    "                                         compute_liwc=True, liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                        session=session)\n",
    "    data_generator_valid = DataGenerator(user_level_data, subjects_split, set_type=validation_set,\n",
    "                                        seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
    "                                        posts_per_group=hyperparams['posts_per_group'], \n",
    "                                         post_groups_per_user=1,#hyperparams['post_groups_per_user'],\n",
    "                                        max_posts_per_user=None, liwc_categories=liwc_categories,\n",
    "                                        sample_seqs=False, shuffle=False, hierarchical=hyperparams['hierarchical'],\n",
    "                                         use_bert='bert_layer' not in hyperparams['ignore_layer'],\n",
    "                                         compute_liwc=True, liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                        session=session)\n",
    "    return data_generator_train, data_generator_valid\n",
    "\n",
    "def initialize_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopword_list,\n",
    "                    liwc_categories, session=None, transfer=False):\n",
    "\n",
    "    logger.info(\"Initializing model...\\n\")\n",
    "    # Initialize model\n",
    "    if hyperparams['hierarchical']:\n",
    "        model = build_hierarchical_model(hyperparams, hyperparams_features, embedding_matrix, \n",
    "                                         emotions, stopword_list, liwc_categories,\n",
    "                       ignore_layer=hyperparams['ignore_layer'])\n",
    "    else:\n",
    "        model = build_model(hyperparams, hyperparams_features, embedding_matrix, \n",
    "                                        emotions, stopword_list, liwc_categories,\n",
    "                       ignore_layer=hyperparams['ignore_layer'])\n",
    "    if transfer:\n",
    "        model = build_tl_model(hyperparams, hyperparams_features, embedding_matrix, \n",
    "                                        emotions, stopword_list, liwc_categories,\n",
    "                       ignore_layer=hyperparams['ignore_layer'])\n",
    "        model.load_weights(hyperparams_features['pretrained_model_path'] + '_weights.h5', by_name=True)\n",
    "    # Needed just for bert\n",
    "    if 'bert_layer' not in hyperparams['ignore_layer']:\n",
    "        initialize_sess(session)                  \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(user_level_data, subjects_split, \n",
    "          hyperparams, hyperparams_features, \n",
    "          embedding_matrix, emotions, stopword_list, liwc_categories,\n",
    "          experiment, validation_set='valid',\n",
    "          version=0, epochs=50, start_epoch=0,\n",
    "         session=None, model=None, transfer=False):\n",
    "    network_type, hierarch_type = get_network_type(hyperparams)\n",
    "    model_path='models/%s_%s_%s%d' % (network_type, dataset_type, hierarch_type, version)\n",
    "\n",
    "    logger.info(\"Initializing datasets...\\n\")\n",
    "    data_generator_train, data_generator_valid = initialize_datasets(user_level_data, subjects_split, \n",
    "                                                                     hyperparams,hyperparams_features,\n",
    "                                                                     validation_set=validation_set,\n",
    "                                                                    session=session)\n",
    "    if not model:\n",
    "        if transfer:\n",
    "            logger.info(\"Initializing pretrained model...\\n\")\n",
    "        else:\n",
    "            logger.info(\"Initializing model...\\n\")\n",
    "        model = initialize_model(hyperparams, hyperparams_features, embedding_matrix, \n",
    "                                 emotions, stopword_list, liwc_categories, session=session, transfer=transfer)\n",
    "\n",
    "       \n",
    "    print(model_path)\n",
    "    logger.info(\"Training model...\\n\")\n",
    "    model, history = train_model(model, hyperparams,\n",
    "                                 data_generator_train, data_generator_valid,\n",
    "                       epochs=epochs, start_epoch=start_epoch,\n",
    "                      class_weight={0:1, 1:hyperparams['positive_class_weight']},\n",
    "                      callback_list = [\n",
    "#                           'weights_history',\n",
    "                          'lr_history',\n",
    "#                           'outputs_history_valid',\n",
    "#                           'outputs_history_train',\n",
    "                          'reduce_lr_plateau',\n",
    "                          'lr_schedule'\n",
    "                                      ],\n",
    "                      model_path=model_path, workers=4,\n",
    "                                validation_set=validation_set)\n",
    "    logger.info(\"Saving model...\\n\")\n",
    "    save_model_and_params(model, model_path, hyperparams, hyperparams_features)\n",
    "    experiment.log_parameter(\"model_path\", model_path)\n",
    "\n",
    "    return model, history\n",
    "# except Exception as e:# tf.errors.ResourceExhaustedError:\n",
    "#     print(e)\n",
    "#     sess.close()\n",
    "#     sess = tf.Session(config=sess_config)\n",
    "#     initialize_vars(sess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'lstm_seq'\n",
    "# # non-duplicate key\n",
    "# while key in models_collection.keys():\n",
    "#     key=key + \"1\"\n",
    "# # models_collection[key] = cur_model\n",
    "# # session.close()\n",
    "# session = initialize_sess()\n",
    "# # all_sessions.append(session)\n",
    "# session_collection[key] = session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(liwc_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    # Network parmeters\n",
    "    \n",
    "    # Sequential + hierarchical layers\n",
    "    'trainable_embeddings': True,\n",
    "\n",
    "    'lstm_units': 128,\n",
    "    \n",
    "    'dense_bow_units': 20,\n",
    "    'dense_sentence_units': 0,\n",
    "    \n",
    "    # CNN\n",
    "    'filters': 100,\n",
    "    'kernel_size': 5,\n",
    "    \n",
    "    # Just hierarchical layers\n",
    "    'lstm_units_user': 64,\n",
    "    'dense_user_units': 0,\n",
    "    \n",
    "    'filters_user': 10,\n",
    "    'kernel_size_user': 3,\n",
    "        \n",
    "    # BERT layers\n",
    "    'bert_dense_units': 256,\n",
    "    'bert_finetune_layers': 0,\n",
    "    'bert_trainable': False ,\n",
    "    'bert_pooling': 'first', # mean, first\n",
    "    \n",
    "    'transfer_units': 20,\n",
    "\n",
    "    # Regularization etc\n",
    "#     'dropout': 0.2,\n",
    "    'dropout': 0,\n",
    "    'l2_dense': 0.00011,\n",
    "    'l2_embeddings': 0.0000001,\n",
    "    'l2_bert': 0.0001,\n",
    "    'norm_momentum': 0.1,\n",
    "    \n",
    "    'ignore_layer': [\n",
    "        'lstm', 'attention', \n",
    "#         'cnn',\n",
    "#         'lstm_user', 'attention_user', \n",
    "                     'batchnorm',\n",
    "#                      'user_encoded', # remove LSTM/CNN\n",
    "                     'bert_layer',\n",
    "#                      'numerical_dense_layer', \n",
    "#         'sparse_feat_dense_layer' # remove extracted features\n",
    "                    ],\n",
    "\n",
    "    # Learning parameters\n",
    "    'optimizer': None,#'adam',\n",
    "    'decay': 0.001,\n",
    "#     'lr': 0.01,\n",
    "#     'lr': 0.00005,#     'lr': 0.01,\n",
    "    'lr': 0.01,#     'lr': 0.01,\n",
    "    \"reduce_lr_factor\": 0.5,\n",
    "    \"reduce_lr_patience\": 55,\n",
    "    'scheduled_reduce_lr_freq': 95,\n",
    "    'scheduled_reduce_lr_factor': 0.5,\n",
    "    \"freeze_patience\": 2000,\n",
    "    'threshold': 0.5,\n",
    "    'early_stopping_patience': 20,\n",
    "    'positive_class_weight': 5,\n",
    "#     'positive_class_weight': 2,\n",
    "    \n",
    "    # Generator parameters\n",
    "    \n",
    "    # Note: average text length in eRisk: 300\n",
    "    #       average text length in CLPsych: 13\n",
    "#     \"maxlen\": 512,\n",
    "    \"maxlen\": 256,\n",
    "    \"posts_per_user\": None, # if you want to limit total nr of posts considered per user\n",
    "    \"post_groups_per_user\": None, # if you want a fixed number of post groups per user\n",
    "                                  # to even out user weights\n",
    "    \"posts_per_group\": 100, # how long are the \"batches\" of posts. maxlen/avglen~=posts_per_group\n",
    "    \"batch_size\": 32,\n",
    "    \"padding\": \"pre\",\n",
    "    \"hierarchical\": False,\n",
    "    'sample_seqs': False,\n",
    "    'sampling_distr': 'exp',\n",
    "\n",
    "}\n",
    "if not hyperparams['optimizer']:\n",
    "    hyperparams['optimizer'] = optimizers.Adam(lr=hyperparams['lr'], #beta_1=0.9, beta_2=0.999, epsilon=0.0001,\n",
    "                                   decay=hyperparams['decay'])\n",
    "    \n",
    "if transfer_type:\n",
    "    hyperparams, _ = load_params(hyperparams_features['pretrained_model_path'])\n",
    "    if 'optimizer' not in hyperparams:\n",
    "        hyperparams['optimizer'] = optimizers.Adam(lr=hyperparams['lr'], #beta_1=0.9, beta_2=0.999, epsilon=0.0001,\n",
    "                                       decay=hyperparams['decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hyperparams_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-a2bc3bfb0c33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# K.clear_session()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# transfer_type=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhyperparams_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'hyperparams_features' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# session_collection['cnn_hierarch'].close()\n",
    "# K.clear_session()\n",
    "# transfer_type=None\n",
    "hyperparams_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "transfer_type=None\n",
    "# with session_collection[key].as_default():\n",
    "#     with session_collection[key].graph.as_default():\n",
    "hyperparams_collection[key] = hyperparams\n",
    "\n",
    "\n",
    "if transfer_type:\n",
    "    # TODO: try to compile? with proper optimizer?\n",
    "    hyperparams_collection[key]['trainable_embeddings'] = False\n",
    "    hyperparams_collection[key]['transfer_units'] = 20\n",
    "    hyperparams_collection[key]['positive_class_weight'] = 10\n",
    "    hyperparams_collection[key]['lr'] = 0.0001\n",
    "    hyperparams_collection[key]['optimizer'] = optimizers.Adam(lr=hyperparams_collection[key]['lr'], #beta_1=0.9, beta_2=0.999, epsilon=0.0001,\n",
    "                                   decay=hyperparams_collection[key]['decay'])\n",
    "#     hyperparams_collection[key]['lr'] = 0.01\n",
    "\n",
    "#     models_collection[key] = load_saved_model(hyperparams_features['pretrained_model_path'], \n",
    "#                                               hyperparams_collection[key])\n",
    "#     metrics_class = Metrics(threshold=hyperparams_collection[key]['threshold'])\n",
    "#     models_collection[key].compile(hyperparams_collection[key]['optimizer'], binary_crossentropy_custom,\n",
    "#                   metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m,\n",
    "#                           metrics_class.auc, \n",
    "#                            tf.keras.metrics.AUC(name=\"AUC_keras\"), \n",
    "#                            tf.keras.metrics.FalsePositives(), \n",
    "#                            tf.keras.metrics.FalseNegatives()])\n",
    "\n",
    "\n",
    "\n",
    "    experiment = initialize_experiment(hyperparams_collection[key], nrc_lexicon_path, emotions, pretrained_embeddings_path, \n",
    "                                  dataset_type, transfer_type)\n",
    "    models_collection[key], history = train(user_level_data, subjects_split, \n",
    "              hyperparams_collection[key], hyperparams_features, \n",
    "              embedding_matrix, emotions, stopword_list, liwc_categories,\n",
    "                  experiment,\n",
    "                  validation_set='valid',\n",
    "              version=52, epochs=90, start_epoch=80, transfer=True,\n",
    "#                     session=session_collection[key],\n",
    "#                 model=models_collection[key]\n",
    "                                           )\n",
    "else:\n",
    "    experiment = initialize_experiment(hyperparams, nrc_lexicon_path, emotions, pretrained_embeddings_path, \n",
    "                          dataset_type, transfer_type)\n",
    "    models_collection[key], history = train(user_level_data, subjects_split, \n",
    "              hyperparams_collection[key], hyperparams_features, \n",
    "              embedding_matrix, emotions, stopword_list, liwc_categories,\n",
    "                  experiment,\n",
    "                  validation_set='valid',\n",
    "              version=52, epochs=25, start_epoch=0,\n",
    "#                     session=session_collection[key],\n",
    "#                     model=models_collection[key]\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_collection[key] = initialize_model(hyperparams_collection[key], \n",
    "#                                           hyperparams_features, embedding_matrix, emotions, stopword_list,\n",
    "#                     liwc_categories)\n",
    "# cur_model.load_weights(model_path + \"_weights.h5\")\n",
    "# embedding_matrix.shape\n",
    "# models_collection[key].load_weights(hyperparams_features['pretrained_model_path'] + \"_weights.h5\", by_name=True)\n",
    "# metrics_class = Metrics(threshold=hyperparams_collection[key]['threshold'])\n",
    "# models_collection[key].compile(hyperparams_collection[key]['optimizer'], binary_crossentropy_custom,\n",
    "#               metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m,\n",
    "#                       metrics_class.auc, \n",
    "#                        tf.keras.metrics.AUC(name=\"AUC_keras\"), \n",
    "#                        tf.keras.metrics.FalsePositives(), \n",
    "#                        tf.keras.metrics.FalseNegatives()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_collection[key].get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_collection[key].get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_collection[key].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 0\n",
    "\n",
    "# Evaluate on entire posts history, final F1-score\n",
    "# print(\"Evaluating on same nr of groups as train (%d)...\" % hyperparams['post_groups_per_user'] if \n",
    "#       hyperparams['post_groups_per_user'] else 0)\n",
    "# cur_model.evaluate_generator(DataGenerator(user_level_data, subjects_split, \n",
    "#                                          set_type='test', \n",
    "#                                     seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
    "#                                        hierarchical=hyperparams['hierarchical'],\n",
    "#                                          max_posts_per_user=None,\n",
    "#                                        pad_with_duplication=False,\n",
    "#                                         posts_per_group=hyperparams['posts_per_group'],\n",
    "#                                         post_groups_per_user=hyperparams['post_groups_per_user'], \n",
    "#                                          sample_seqs=False, shuffle=False), verbose=1)\n",
    "# print(\"Evaluating on entire posts history...\")\n",
    "# cur_model.evaluate_generator(DataGenerator(user_level_data, subjects_split, \n",
    "#                                          set_type='test', \n",
    "#                                     seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
    "#                                        hierarchical=hyperparams['hierarchical'],\n",
    "#                                          max_posts_per_user=None,\n",
    "#                                        pad_with_duplication=False,\n",
    "#                                         posts_per_group=hyperparams['posts_per_group'],\n",
    "#                                         post_groups_per_user=hyperparams['post_groups_per_user'], \n",
    "#                                          sample_seqs=False, shuffle=False), verbose=1)\n",
    "liwc_words_for_categories = pickle.load(open(hyperparams_features['liwc_words_cached'], 'rb'))\n",
    "# # \n",
    "# with session_collection[key].as_default():\n",
    "#     with session_collection[key].graph.as_default():\n",
    "print(\"Evaluating only on last group (1)...\")\n",
    "metrics_class = Metrics(threshold=0.5)\n",
    "models_collection[key].compile(hyperparams_collection[key]['optimizer'], binary_crossentropy_custom,\n",
    "                  metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m,\n",
    "                          metrics_class.auc, \n",
    "                           tf.keras.metrics.AUC(name=\"AUC_keras\"), \n",
    "                           tf.keras.metrics.FalsePositives(), \n",
    "                           tf.keras.metrics.FalseNegatives()])\n",
    "results1 = models_collection[key].evaluate_generator(DataGenerator(user_level_data, subjects_split, \n",
    "                                         set_type='test', \n",
    "                                    seq_len=hyperparams_collection[key]['maxlen'],   \n",
    "#                                         batch_size=len(subjects_split['test']), # on all data at once\n",
    "                                        batch_size=hyperparams_collection[key]['batch_size'], # on all data at once\n",
    "                                       hierarchical=hyperparams_collection[key]['hierarchical'],\n",
    "                                         max_posts_per_user=None,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams_collection[key]['posts_per_group'],\n",
    "                                        post_groups_per_user=1,  compute_liwc=True,\n",
    "                                        liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                         sample_seqs=False, shuffle=False), verbose=1)\n",
    "experiment.log_metric(\"test_f1\", results1[1])\n",
    "experiment.log_metric(\"test_prec\", results1[2])\n",
    "experiment.log_metric(\"test_recall\", results1[3])\n",
    "experiment.log_metric(\"test_auc\", results1[4])\n",
    "\n",
    "# Evaluate on partial post history, simulating stream\n",
    "print(\"Evaluating on partial posts history...\")\n",
    "scores_per_iteration = []\n",
    "for iteration in range(0, iterations, 10):\n",
    "    print(\"Iteration\", iteration)\n",
    "    results = models_collection[key].evaluate_generator(DataGenerator(user_level_data, subjects_split, \n",
    "                                         set_type='test', \n",
    "                                       hierarchical=hyperparams_collection[key]['hierarchical'],\n",
    "                                        seq_len=hyperparams['maxlen'], \n",
    "#                                         batch_size=len(subjects_split['test'])//2, # on all data at once\n",
    "                                        batch_size=hyperparams_collection[key]['batch_size'], # on all data at once\n",
    "                                         max_posts_per_user=iteration,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams_collection[key]['posts_per_group'],\n",
    "                                        post_groups_per_user=None, compute_liwc=True,\n",
    "                                        liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                         sample_seqs=False, shuffle=False), verbose=1)\n",
    "    scores_per_iteration.append(results[1])\n",
    "\n",
    "#     Optimal thresh\n",
    "print(\"Finding optimal threshold...\")\n",
    "f1s = {}\n",
    "for thresh in range(0, 10, 2):\n",
    "    print(thresh)\n",
    "    # model1.load_weights(model_path + \"_weights\", by_name=True)\n",
    "    metrics_class = Metrics(threshold=thresh/10)\n",
    "    models_collection[key].compile(hyperparams_collection[key]['optimizer'], binary_crossentropy_custom,\n",
    "                      metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m,\n",
    "                            metrics_class.auc, \n",
    "                           tf.keras.metrics.AUC(name=\"AUC_keras\"), \n",
    "                           tf.keras.metrics.FalsePositives(), \n",
    "                           tf.keras.metrics.FalseNegatives()])\n",
    "    results = models_collection[key].evaluate_generator(DataGenerator(user_level_data, subjects_split, \n",
    "                                         set_type='valid', compute_liwc=True,\n",
    "                                        liwc_words_for_categories=liwc_words_for_categories,\n",
    "\n",
    "                                    seq_len=hyperparams_collection[key]['maxlen'], \n",
    "                                        batch_size=hyperparams_collection[key]['batch_size'],\n",
    "                                       hierarchical=hyperparams_collection[key]['hierarchical'],\n",
    "                                         max_posts_per_user=None,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams_collection[key]['posts_per_group'],\n",
    "                                        post_groups_per_user=1, \n",
    "                                                 sample_seqs=False, shuffle=False), verbose=1)\n",
    "    f1s[thresh] = results[1]\n",
    "best_f1 = 0\n",
    "best_thresh = 0.5\n",
    "for t, f1 in f1s.items():\n",
    "    if f1 >= best_f1:\n",
    "        best_thresh = t\n",
    "        best_f1 = f1\n",
    "print(f1s, best_thresh)\n",
    "hyperparams_features['best_thresh'] = best_thresh/10\n",
    "experiment.log_metric(\"best_thresh\", best_thresh/10)\n",
    "\n",
    "print(\"Computing with best threshold...\")\n",
    "metrics_class = Metrics(best_thresh/10)\n",
    "models_collection[key].compile(hyperparams_collection[key]['optimizer'], binary_crossentropy_custom,\n",
    "                      metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m,\n",
    "                            metrics_class.auc, \n",
    "                           tf.keras.metrics.AUC(name=\"AUC_keras\"), \n",
    "                           tf.keras.metrics.FalsePositives(), \n",
    "                           tf.keras.metrics.FalseNegatives()])\n",
    "results = models_collection[key].evaluate_generator(DataGenerator(user_level_data, subjects_split, \n",
    "                                         set_type='test', compute_liwc=True,\n",
    "                                        liwc_words_for_categories=liwc_words_for_categories,\n",
    "\n",
    "                                    seq_len=hyperparams_collection[key]['maxlen'], \n",
    "                                        batch_size=hyperparams_collection[key]['batch_size'],\n",
    "                                       hierarchical=hyperparams_collection[key]['hierarchical'],\n",
    "                                         max_posts_per_user=None,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams_collection[key]['posts_per_group'],\n",
    "                                        post_groups_per_user=1, \n",
    "                                                 sample_seqs=False, shuffle=False), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_level_data2, subjects_split2, vocabulary = load_erisk_data(writings_df[writings_df['source']=='clpsych'], \n",
    "                                                            voc_size=hyperparams_features['max_features'],\n",
    "                                                           emotion_lexicon=nrc_lexicon,\n",
    "                                                           emotions=emotions,\n",
    "                                                           user_level=hyperparams_features['user_level'],\n",
    "                                                                                logger=logger,\n",
    "#                                                            vocabulary=pickle.load(open('vocabulary_40K_all.pkl', 'rb')),\n",
    "#                                                            vocabulary=pickle.load(open('vocab_clpsych_10000.pkl', 'rb')),\n",
    "                                                              vocabulary=vocabulary_dict,\n",
    "                                                              by_subset=False\n",
    "                                                                               )\n",
    "results2 = models_collection[key].evaluate_generator(DataGenerator(user_level_data2, subjects_split2, \n",
    "                                         set_type='test', \n",
    "                                    seq_len=hyperparams_collection[key]['maxlen'],   \n",
    "#                                         batch_size=len(subjects_split['test']), # on all data at once\n",
    "                                        batch_size=hyperparams_collection[key]['batch_size'], # on all data at once\n",
    "                                       hierarchical=hyperparams_collection[key]['hierarchical'],\n",
    "                                         max_posts_per_user=None,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams_collection[key]['posts_per_group'],\n",
    "                                        post_groups_per_user=1,  compute_liwc=True,\n",
    "                                        liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                         sample_seqs=False, shuffle=False), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1, d2 = initialize_datasets(user_level_data, subjects_split, hyperparams, hyperparams_features, \n",
    "                        validation_set='valid', session=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in d1:\n",
    "    print(len(d[0][1][0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0, iterations, 10), scores_per_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model_and_params(cur_model, 'models/lstm_selfharm_seq16_2', hyperparams, hyperparams_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_collection = {}\n",
    "# hyperparams_collection = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop here\n",
    "tf.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([v for v in model.get_layer('attention_user').get_weights()[0].flatten()])#.rolling(3).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([abs(v) for v in model.get_layer('output_layer').get_weights()[0].flatten()]).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = initialize_experiment(hyperparams, nrc_lexicon_path, emotions, pretrained_embeddings_path, \n",
    "                          dataset_type, transfer_type)\n",
    "experiment.add_tag(\"cross-validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 5\n",
    "\n",
    "f1_scores = []\n",
    "for test_slice in range(folds): \n",
    "    logger.info(\"Testing on slice %d, training on the rest...\\n\" % test_slice)\n",
    "    user_level_data, subjects_split, vocabulary = load_erisk_data(writings_df, \n",
    "                                                            voc_size=hyperparams_features['max_features'],\n",
    "                                                           emotion_lexicon=nrc_lexicon,\n",
    "                                                           emotions=emotions,\n",
    "                                                           user_level=hyperparams_features['user_level'],\n",
    "                                                                                logger=logger,\n",
    "                                                           vocabulary=pickle.load(open('all_vocab_clpsych_erisk_2000_dict.pkl', 'rb')),\n",
    "                                                              by_subset=False,\n",
    "                                                              nr_slices=folds,\n",
    "                                                                valid_prop=0.1,\n",
    "                                                                  test_slice=test_slice\n",
    "                                                                 )\n",
    "    model, history = train(user_level_data, subjects_split, \n",
    "              hyperparams, hyperparams_features, \n",
    "              embedding_matrix, emotions, stopword_list, liwc_categories,\n",
    "                  experiment,\n",
    "                validation_set='test',\n",
    "              version=8, epochs=50,\n",
    "    )\n",
    "    f1_scores.append(sum(history.history['val_f1_m'][-6:-1])/5)\n",
    "experiment.add_metric(\"average_f1\", sum(f1_scores)/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on server data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize_vars(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"models/lstm_selfharm_seq16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams1, hyperparams_features1 = load_params(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams_features1['embeddings_path']=[\n",
    "#     '/home/anasab//eRisk/embeddings/finetuned_glove_clpsych_erisk_stop_20000_2.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_features1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_saved_model(model_path, hyperparams1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_list = pickle.load(open(hyperparams_features1['vocabulary_path'], 'rb'))\n",
    "vocabulary_dict={}\n",
    "for i,w in enumerate(vocabulary_list):\n",
    "    vocabulary_dict[w] = i\n",
    "embedding_matrix = load_embeddings2(hyperparams_features1['embeddings_path'], \n",
    "                                    hyperparams_features1['embedding_dim'], vocabulary_dict)\n",
    "liwc_words_for_categories = pickle.load(open(hyperparams_features1['liwc_words_cached'], 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'optimizer' not in hyperparams1:\n",
    "#         hyperparams1['optimizer'] = optimizers.Adam(lr=hyperparams1['lr'], #beta_1=0.9, beta_2=0.999, epsilon=0.0001,\n",
    "#                                        decay=hyperparams1['decay'])\n",
    "# loaded_model = initialize_model(hyperparams1, hyperparams_features1, embedding_matrix, emotions, stopword_list,\n",
    "#                     liwc_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_weights(model_path + \"_weights.h5\", by_name=True)\n",
    "# metrics_class = Metrics(threshold=0.5)\n",
    "# loaded_model.compile(hyperparams1['optimizer'], binary_crossentropy_custom,\n",
    "#                   metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_runs = {\n",
    "    'bert': 0,\n",
    "    'lstm_seq': 1,\n",
    "    'cnn_hierarch': 2,\n",
    "    'transfer': 3,\n",
    "    'ensemble': 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_key = 'lstm_seq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def load_erisk_server_data(datarounds_json, voc_size, emotion_lexicon, tokenizer,\n",
    "                           liwc_words_for_categories,\n",
    "                           emotions =  \n",
    "                    ['anger', 'anticipation', 'disgust', 'fear', 'joy', \n",
    "                     'negative', 'positive', 'sadness', 'surprise', 'trust'],\n",
    "                    liwc_categories = liwc_categories, \n",
    "                    pronouns = [\"i\", \"me\", \"my\", \"mine\", \"myself\"],\n",
    "                    user_level=True, vocabulary=vocabulary_dict,\n",
    "                   logger=logger):\n",
    "    def __encode_liwc_categories(tokens, liwc_words_for_categories, relative=True):\n",
    "        categories_cnt = [0 for c in liwc_categories]\n",
    "        if not tokens:\n",
    "            return categories_cnt\n",
    "        text_len = len(tokens)\n",
    "        for i, category in enumerate(liwc_categories):\n",
    "            for t in tokens:\n",
    "                if t in liwc_words_for_categories[category]:\n",
    "                    categories_cnt[i] += 1\n",
    "            if relative and text_len:\n",
    "                categories_cnt[i] = categories_cnt[i]/text_len\n",
    "        return categories_cnt\n",
    "    logger.debug(\"Loading data...\\n\")\n",
    "     \n",
    "    subjects_split = {'test': []}\n",
    " \n",
    "    user_level_texts = {}\n",
    "    for datapoints_json in datarounds_json:\n",
    "        for datapoint in datapoints_json:\n",
    "            words = []\n",
    "            raw_text = \"\"\n",
    "            if \"title\" in datapoint:\n",
    "                tokenized_title = tokenizer.tokenize(datapoint[\"title\"])\n",
    "                words.extend(tokenized_title)\n",
    "                raw_text += datapoint[\"title\"]\n",
    "            if \"content\" in datapoint:\n",
    "                tokenized_text = tokenizer.tokenize(datapoint[\"content\"])\n",
    "                words.extend(tokenized_text)\n",
    "                raw_text += datapoint[\"content\"]\n",
    "            \n",
    "            liwc_categs = __encode_liwc_categories(words, liwc_words_for_categories)\n",
    "            if datapoint[\"nick\"] not in user_level_texts.keys():\n",
    "                user_level_texts[datapoint[\"nick\"]] = {}\n",
    "                user_level_texts[datapoint[\"nick\"]]['texts'] = [words]\n",
    "                user_level_texts[datapoint[\"nick\"]]['liwc'] = [liwc_categs]\n",
    "                user_level_texts[datapoint[\"nick\"]]['raw'] = [raw_text]\n",
    "                subjects_split['test'].append(datapoint['nick'])\n",
    "            else:\n",
    "                user_level_texts[datapoint[\"nick\"]]['texts'].append(words)\n",
    "                user_level_texts[datapoint[\"nick\"]]['liwc'].append(liwc_categs)\n",
    "                user_level_texts[datapoint[\"nick\"]]['raw'].append(raw_text)\n",
    "            \n",
    "    return user_level_texts, subjects_split, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_datapoint(jlpath):\n",
    "    datapoints = []\n",
    "    with open(jlpath) as f:\n",
    "        for line in f:\n",
    "            datapoints.append(json.loads(line))\n",
    "    return datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# erisk_server_data, erisk_server_subjects_split, vocabulary = load_erisk_server_data(data,\n",
    "#                        tokenizer=regtokenizer,\n",
    "#                        liwc_words_for_categories=liwc_words_for_categories,\n",
    "#                     voc_size=hyperparams_features['max_features'],\n",
    "#                     emotion_lexicon=nrc_lexicon,\n",
    "#                     emotions=emotions,\n",
    "#                     user_level=hyperparams_features['user_level'],\n",
    "#                        vocabulary=vocabulary_dict,\n",
    "#     #                                                            vocabulary=pickle.load(open('vocabulary20K_selfharm.pkl', 'rb'))\n",
    "#                     logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erisk_server_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#TODO: predict only on last 50 posts\n",
    "\n",
    "with session_collection[model_key].as_default():\n",
    "    with session_collection[model_key].graph.as_default():\n",
    "        server_erisk_predictions = models_collection[model_key].predict(DataGenerator(erisk_server_data, erisk_server_subjects_split, \n",
    "                                             set_type='test', vocabulary=vocabulary_dict, \n",
    "                                           hierarchical=hyperparams_collection[model_key]['hierarchical'],\n",
    "                                        seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
    "                                             max_posts_per_user=None,\n",
    "                                           pad_with_duplication=False,\n",
    "                                            posts_per_group=hyperparams['posts_per_group'],\n",
    "                                            post_groups_per_user=None, \n",
    "                                             sample_seqs=False, shuffle=False,\n",
    "                                                    compute_liwc=False)\n",
    "                                                                       )\n",
    "        pd.Series(server_erisk_predictions.flatten()).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(server_erisk_predictions.flatten()).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seding results to server!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects = [d['nick'] for d in read_json_datapoint(\"client/data0.jl\")]\n",
    "# results = {s: 0 for s in subjects}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_next_data(rnd, results):\n",
    "#     # TODO: send results to get data\n",
    "#     response = build_response(results)\n",
    "#     # Send response\n",
    "#     data = {\"...\"}\n",
    "#     # Make sure it's the correct round\n",
    "#     assert data['number'] == rnd\n",
    "#     serialize_data(data)\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_next_data_dummy(rnd, results):\n",
    "#     return read_json_datapoint(\"client/data0.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for round and model\n",
    "results = {key: {} for key in models_runs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_chunk(rnds):\n",
    "    # Send same results for a chunk of rounds to get new posts\n",
    "    data = [read_json_datapoint(\"data_server/data%d.jl\" % i) for i in rnds]\n",
    "#     data_chunks = []\n",
    "#     for rnd in rnds:\n",
    "#         # TODO: REPLACE THIS WITH THE CORRECT ONE\n",
    "#         data = get_next_data_dummy(rnd, results)\n",
    "#         data_chunks.append(data)\n",
    "#         all_data[rnd] = data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def predict_for_round_chunk(model, hyperparams, hyperparams_features, vocabulary, data_chunk, subjects=[],\n",
    "                           model_key='', cache_round=None): \n",
    "    # preload for subjects not occurring in the round with results from previous round\n",
    "#     results = {s: 0 for s in subjects}\n",
    "    if cache_round:\n",
    "        results = load_results(model_key, cache_round)\n",
    "    else:\n",
    "        results = {s: 0 for s in subjects}\n",
    "        \n",
    "    \n",
    "    erisk_server_data, erisk_server_subjects_split, vocabulary = load_erisk_server_data(data_chunk,\n",
    "                       tokenizer=regtokenizer,\n",
    "                       liwc_words_for_categories=liwc_words_for_categories,\n",
    "                    voc_size=hyperparams_features['max_features'],\n",
    "                    emotion_lexicon=nrc_lexicon,\n",
    "                    emotions=emotions,\n",
    "                    user_level=1,\n",
    "                       vocabulary=vocabulary,\n",
    "                    logger=logger)\n",
    "\n",
    "    for features, subjects, _ in DataGenerator(erisk_server_data, erisk_server_subjects_split, \n",
    "                                         set_type='test', vocabulary=vocabulary, \n",
    "                                       hierarchical=hyperparams['hierarchical'],\n",
    "                                    seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
    "                                         max_posts_per_user=None,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams['posts_per_group'],\n",
    "                                        post_groups_per_user=None, \n",
    "                                         sample_seqs=False, shuffle=False,\n",
    "                                      return_subjects=True):\n",
    "        predictions = model.predict_on_batch(features)\n",
    "        print(len(features[0]), len(subjects), len(predictions), len(results))\n",
    "        for i,s in enumerate(subjects):\n",
    "            results[\"subject\" + str(s)] = predictions[i].item()\n",
    "    return(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_response(results, decision_thresh=0.5, model_name='', rnd=0):\n",
    "    response = []\n",
    "    for subject, score in results.items():\n",
    "        prediction = 1 if score >= decision_thresh else 0\n",
    "        response.append({'nick': subject, 'score': score, 'decision': prediction})\n",
    "    json.dump(response, open(\"data_server/response_run%s_rnd%d.json\" % (model_name, rnd), 'w+'))\n",
    "    return response\n",
    "# build_response(results, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_ensemble_results(rnd, all_results, model_keys_to_average=['lstm_seq', 'cnn_hierarch']):\n",
    "#     subjects = [s for s in all_results[model_keys_to_average[0]][rnd]]\n",
    "#     results_ensemble = {}\n",
    "#     for sub in subjects:\n",
    "#         s = 0\n",
    "#         for k in model_keys_to_average:\n",
    "#             s += all_results[k][rnd][sub]\n",
    "#         results_ensemble[sub] = s/len(model_keys_to_average)\n",
    "#     return results_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble_results(results_to_average):\n",
    "    subjects = [s for s in results_to_average[0]]\n",
    "    results_ensemble = {}\n",
    "    for sub in subjects:\n",
    "        s = 0\n",
    "        for res in results_to_average:\n",
    "            s += res[sub]\n",
    "        results_ensemble[sub] = s/len(results_to_average)\n",
    "    return results_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transfer_results(rnd, all_results, model_to_average='lstm_seq', rounds_back=100):\n",
    "    subjects = [s for s in all_results[model_to_average][rnd]]\n",
    "    results_ensemble = {}\n",
    "    for sub in subjects:\n",
    "        s = 0\n",
    "        existing_rounds = 0\n",
    "        for prev_rnd in range(rnd-rounds_back, rnd+1):\n",
    "#             print(\"rolling rnds\", prev_rnd)\n",
    "            if prev_rnd in all_results[model_to_average]:\n",
    "                s += all_results[model_to_average][prev_rnd][sub]\n",
    "                existing_rounds += 1\n",
    "        results_ensemble[sub] = s/existing_rounds\n",
    "#         print(\"Have found a rolling window of %d for the transfer model\" % existing_rounds)\n",
    "    return results_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(model_key, rnd):\n",
    "    results = {}\n",
    "    with open(\"data_server/response_run%s_rnd%d.json\" % (models_runs[model_key], rnd)) as f:\n",
    "        response = json.loads(f.read())\n",
    "        for line in response:\n",
    "            results[line['nick']] = line['score']\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results['lstm_seq'][20] = load_results('lstm_seq', 20)\n",
    "# results['lstm_seq'][40] = \n",
    "results# results['bert'][40] = load_results('bert', 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnds = range(500,600)\n",
    "decision_thresh = 0.5\n",
    "data_chunks = get_data_chunk(rnds)\n",
    "subjects = [d['nick'] for d in read_json_datapoint(\"data_server/data0.jl\")]\n",
    "# for key in ['transfer', 'ensemble']:\n",
    "for model_key in [\n",
    "                  'lstm_seq',\n",
    "                    'bert',\n",
    "                  'cnn_hierarch', \n",
    "                  'transfer', \n",
    "                  'ensemble',\n",
    "                ]:\n",
    "    print(model_key)\n",
    "    end_rnd = rnds[-1]\n",
    "#     if model_key=='lstm_seq':\n",
    "#         results[model_key][end_rnd]=load_results('lstm_seq', 20)\n",
    "    if model_key=='cnn_hierarch':\n",
    "        results[model_key][end_rnd]=load_results('cnn_hierarch', 40)\n",
    "    elif model_key=='bert':\n",
    "        results[model_key][end_rnd]=load_results('bert', end_rnd)\n",
    "    elif model_key=='ensemble':\n",
    "        model_keys_to_average=['bert', 'cnn_hierarch', 'lstm_seq']\n",
    "        missing_models = [m for m in model_keys_to_average if not results[m]]\n",
    "        if len(missing_models)!=0:\n",
    "            print(\"Missing models! cannot compute ensemble results\", missing_models)\n",
    "            continue\n",
    "        results_to_average = [results[m][end_rnd] for m in model_keys_to_average]\n",
    "#         results[model_key][end_rnd] = get_ensemble_results(rnd, results, \n",
    "#                                                 model_keys_to_average)\n",
    "        results[model_key][end_rnd] = get_ensemble_results(results_to_average)\n",
    "    ## For now\n",
    "    elif model_key=='transfer':\n",
    "        results[model_key][end_rnd]=get_transfer_results(\n",
    "            end_rnd, results, model_to_average='lstm_seq', rounds_back=60)\n",
    "#         results[model_key][end_rnd]=results['lstm_seq'][end_rnd]\n",
    "    ##\n",
    "    else:\n",
    "        with session_collection[model_key].as_default():\n",
    "            with session_collection[model_key].graph.as_default():\n",
    "                results[model_key][end_rnd] = predict_for_round_chunk(models_collection[model_key], \n",
    "                                              hyperparams_collection[model_key], hyperparams_features, \n",
    "                                              vocabulary_dict, \n",
    "                                          data_chunks, subjects=subjects, model_key=model_key, cache_round=499)\n",
    "\n",
    "    \n",
    "    print(len(results[model_key][end_rnd].values()), \"positive:\", \n",
    "      len([r for r in results[model_key][end_rnd].values() if r >=0.5]))\n",
    "    response1 = build_response(results[model_key][end_rnd], rnd=end_rnd, \n",
    "                               model_name=models_runs[model_key], decision_thresh=decision_thresh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['lstm_seq'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(list(results['transfer'][180].values())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "pearsonr(list(results['bert'][220].values()), list(results['bert'][180].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(list(results['lstm_seq'][160].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on eRisk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = {}\n",
    "# labels = {}\n",
    "# featuresall = {}\n",
    "# with session_collection['lstm_seq2'].as_default():\n",
    "#     with session_collection['lstm_seq2'].graph.as_default():\n",
    "        # for features, subjects, lbls in DataGenerator(user_level_data, subjects_split, \n",
    "        #                                          set_type='train', vocabulary=vocabulary_dict,\n",
    "        #                                        hierarchical=hyperparams1['hierarchical'],\n",
    "        #                                     seq_len=hyperparams1['maxlen'], batch_size=hyperparams1['batch_size'],\n",
    "        #                                          max_posts_per_user=None,\n",
    "        #                                        pad_with_duplication=False,\n",
    "        #                                         posts_per_group=hyperparams1['posts_per_group'],\n",
    "        #                                         post_groups_per_user=None, \n",
    "        #                                          sample_seqs=False, shuffle=False,\n",
    "        #                                                return_subjects=True):\n",
    "\n",
    "        #     predictions = loaded_model.predict_on_batch(features)\n",
    "        #     print(len(features[0]), len(subjects), len(predictions), len(labels), len(results))\n",
    "        #     for i,s in enumerate(subjects):\n",
    "        #         if s not in results:\n",
    "        #             results[s] = []\n",
    "        #             featuresall[s] = []\n",
    "        #         results[s].append(predictions[i].item())\n",
    "        #         featuresall[s].append([features[j][i] for j in range(len(features))])\n",
    "        #         labels[s] = lbls[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in results:\n",
    "    if not labels[subject]:\n",
    "        if np.std(results[subject])>0.0:\n",
    "            print(subject), print(results[subject][0], results[subject][-1]-results[subject][0])\n",
    "            pd.Series(results[subject]).rolling(window=5).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[featuresall[4278][i][0].sum() for i in range(len(featuresall[4278]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(user_level_data['subject4278']['raw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, s, y in DataGenerator(user_level_data, subjects_split, \n",
    "                                         set_type='test', vocabulary=vocabulary_dict,\n",
    "                                       hierarchical=hyperparams1['hierarchical'],\n",
    "                                    seq_len=hyperparams1['maxlen'], batch_size=hyperparams1['batch_size'],\n",
    "                                         max_posts_per_user=None,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams1['posts_per_group'],\n",
    "                                        post_groups_per_user=None, \n",
    "                                         sample_seqs=False, shuffle=False,\n",
    "                                               return_subjects=True):\n",
    "    print(\"subject\", s, \"features\", x[0].sum(axis=1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_key='lstm_seq'\n",
    "with session_collection[model_key].as_default():\n",
    "    with session_collection[model_key].graph.as_default():\n",
    "        res = models_collection[model_key].evaluate_generator(DataGenerator(user_level_data, subjects_split, \n",
    "                                              liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                         set_type='test', vocabulary=vocabulary_dict,\n",
    "                                       hierarchical=hyperparams_collection[model_key]['hierarchical'],\n",
    "                                    seq_len=hyperparams_collection[model_key]['maxlen'], \n",
    "                                    batch_size=hyperparams['batch_size'],\n",
    "                                         max_posts_per_user=None,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams_collection[model_key]['posts_per_group'],\n",
    "                                        post_groups_per_user=1,#None, \n",
    "                                         sample_seqs=False, shuffle=False,\n",
    "                                             compute_liwc=False))\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams_collection['cnn_bert'] = {'hierarchical': False, 'maxlen': 512, 'posts_per_group': 50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    (e, 'nrc') for e in emotions] + ['pers_pronouns'] + [\n",
    "    (c, 'liwc') for c in list(categories) if c in writings_df.columns] + [\n",
    "(st, 'stopword') for st in stopword_list]\n",
    "weights = model.get_layer('output_layer').get_weights()[0].tolist()[-(len(features)):]\n",
    "\n",
    "print(len(weights), len(features))\n",
    "feature_importance = {}\n",
    "for (i, f) in enumerate(features):\n",
    "    feature_importance[f] = weights[i][0]\n",
    "\n",
    "sorted(feature_importance.items(), key=lambda t: abs(t[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_point(subject, voc, hyperparams_features=hyperparams_features, nrc_lexicon=nrc_lexicon,\n",
    "                      emotions=emotions):\n",
    "    eval_writings_df = writings_df[writings_df['subject']==subject]\n",
    "    correct_label = eval_writings_df.label.values[0]\n",
    "    (x_train, y_train), (x_valid, y_valid), (x_test, y_test), voc = load_erisk_data(eval_writings_df,\n",
    "                        seq_len=hyperparams_features['maxlen'],\n",
    "                        voc_size=hyperparams_features['max_features'],\n",
    "                        emotion_lexicon=nrc_lexicon,\n",
    "                        emotions=emotions, user_level=False,\n",
    "                        train_prop=0.0, vocabulary=voc)\n",
    "    return x_test, y_test, correct_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_per_user(writings_df, majority_prop=0.2, train_prop=0.7, majority_nr=0, validate=False, voc=None,\n",
    "                    random=False, nr_slices=5, test_slice=2):\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    thresh=0.5\n",
    "    majority_proportion=majority_prop\n",
    "    valid_prop = 0.3\n",
    "    \n",
    "    if 'subset' in writings_df.columns:\n",
    "        training_subjects = list(set(writings_df[writings_df['subset']=='train'].subject))\n",
    "        test_subjects = list(set(writings_df[writings_df['subset']=='test'].subject))\n",
    "    else:\n",
    "        all_subjects = sorted(list(set(writings_df.subject)))\n",
    "        training_subjects_size = int(len(all_subjects) * train_prop)\n",
    "        test_subjects_size = len(all_subjects) - training_subjects_size\n",
    "        # Cross-validation, with fixed slice as input\n",
    "        test_prop = 1-train_prop\n",
    "        test_slice = min(test_slice, nr_slices)\n",
    "        logger.debug(\"start index: %f, from %f\\n\" % (\n",
    "            len(all_subjects)*(1/nr_slices)*test_slice, test_prop*test_slice))\n",
    "        start_slice = int(len(all_subjects)*(1/nr_slices)*test_slice)\n",
    "        test_subjects = all_subjects[start_slice: start_slice+test_subjects_size]\n",
    "        training_subjects = [s for s in all_subjects if s not in test_subjects]\n",
    "    training_subjects = sorted(training_subjects) # ensuring reproducibility\n",
    "    valid_subjects_size = int(len(training_subjects) * valid_prop)\n",
    "    valid_subjects = training_subjects[:valid_subjects_size]\n",
    "    training_subjects = training_subjects[valid_subjects_size:]\n",
    "    \n",
    "    if validate:\n",
    "        subjects = valid_subjects\n",
    "    else:\n",
    "        subjects = test_subjects\n",
    "    for subject in subjects:\n",
    "        x_test_user, y_test_user, label = get_data_for_point(subject, voc=voc)\n",
    "        outputs = model.predict(x_test_user)\n",
    "        if random:\n",
    "            sigma = np.std(outputs)\n",
    "            mu = np.mean(outputs)\n",
    "            print(\"generating random outputs with sigma\", sigma, \"and mu\", mu)\n",
    "            outputs = sigma*np.random.randn(len(outputs))+mu\n",
    "        positive_pred = sum(outputs>=thresh)\n",
    "        negative_pred = sum(outputs<thresh)\n",
    "        majority_pred = 0\n",
    "        if majority_proportion and positive_pred >= majority_proportion*negative_pred:\n",
    "            majority_pred = 1\n",
    "        if majority_nr and positive_pred>=majority_nr:\n",
    "            majority_pred = 1\n",
    "        if label == 1:\n",
    "            if majority_pred == 1:\n",
    "                tp+=1\n",
    "            else:\n",
    "                fn+=1\n",
    "        else:\n",
    "            if majority_pred == 0:\n",
    "                tn+=1\n",
    "            else:\n",
    "                fp+=1\n",
    "        print(negative_pred, positive_pred, majority_pred)\n",
    "        all_predictions.append(majority_pred)\n",
    "        all_labels.append(label)\n",
    "    def prec_recall_f1(tp, fp, tn, fn):\n",
    "        recall = tp/(tp+fn+0.0000001)\n",
    "        precision = tp/(tp+fp+0.0000001)\n",
    "        f1 = 2*precision*recall/(precision+recall+0.0000001)\n",
    "        print(\"Recall\", recall, \"Precision\", precision, \"F1\", f1)\n",
    "    if majority_prop:\n",
    "        print(\"Vote proportion\", majority_prop)\n",
    "    if majority_nr:\n",
    "        print(\"Vote points\", majority_nr)\n",
    "    prec_recall_f1(tp, fp, tn, fn)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_per_user(writings_df=writings_df, voc=voc, majority_prop=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_per_slice = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_slices=5\n",
    "logger.setLevel(logging.INFO)\n",
    "for tslice in range(nr_slices): \n",
    "    (x_train, y_train), (x_valid, y_valid), (x_test, y_test), voc = load_erisk_data(writings_df, \n",
    "                                                                seq_len=hyperparams_features['maxlen'],\n",
    "                                                                voc_size=hyperparams_features['max_features'],\n",
    "                                                               emotion_lexicon=nrc_lexicon,\n",
    "                                                               emotions=emotions,\n",
    "                                                               user_level=hyperparams_features['user_level'],\n",
    "                                                                                    test_slice=tslice,\n",
    "                                                                                    nr_slices=nr_slices,\n",
    "    #                                                            vocabulary=pickle.load(open('vocabulary20K_selfharm.pkl', 'rb'))\n",
    "                                                                                   logger=logger)\n",
    "    model, history = train_model(model, x_train, y_train, x_valid, y_valid,\n",
    "           epochs=200, batch_size=hyperparams['batch_size'],\n",
    "                      class_weight={0:0.5, 1:5}, start_epoch=0,\n",
    "                      callback_list = [freeze_layer, weights_history, reduce_lr],\n",
    "                      workers=2, verbose=0)\n",
    "    results_per_slice[tslice] = model.evaluate(x_test, y_test)\n",
    "    logger.info(\"Results for slice %d: %s\\n\" % (tslice, results_per_slice[tslice]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average F1 score: \", np.array([results_per_slice[s][1] for s in results_per_slice.keys()]).mean(),\n",
    "     \"all F1 scores: \", {s: v[1] for (s,v) in results_per_slice.items()} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tokens(row):\n",
    "    tokens = []\n",
    "    if row.tokenized_text:\n",
    "        tokens += row.tokenized_text\n",
    "    if row.tokenized_title:\n",
    "        tokens += row.tokenized_title\n",
    "    return tokens\n",
    "writings_df['all_tokens'] = writings_df.apply (lambda row: merge_tokens(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: include the title\n",
    "def extract_emotions(tokens, emotion, relative=True):\n",
    "    if not tokens:\n",
    "        return None\n",
    "    emotion_words = [t for t in tokens \n",
    "                     if t in nrc_lexicon[emotion]]\n",
    "    if relative and len(tokens):\n",
    "        return len(emotion_words) / len(tokens)\n",
    "    else:\n",
    "        return len(emotion_words)\n",
    "    \n",
    "    return encoded_emotions\n",
    "\n",
    "from functools import partial\n",
    "for emotion in emotions:\n",
    "    writings_df[emotion] = writings_df['all_tokens'].apply(partial(extract_emotions, emotion=emotion, \n",
    "                                                                   relative=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['pronouns'] = writings_df['all_tokens'].apply(partial(encode_pronouns, relative=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>text_len</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.141548</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>-0.018706</td>\n",
       "      <td>-0.001991</td>\n",
       "      <td>-0.000641</td>\n",
       "      <td>-0.029771</td>\n",
       "      <td>0.015376</td>\n",
       "      <td>-0.020250</td>\n",
       "      <td>-0.004770</td>\n",
       "      <td>-0.002814</td>\n",
       "      <td>-0.017061</td>\n",
       "      <td>-0.008363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pronouns</th>\n",
       "      <td>0.141548</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009578</td>\n",
       "      <td>-0.046744</td>\n",
       "      <td>-0.023335</td>\n",
       "      <td>-0.023392</td>\n",
       "      <td>-0.063543</td>\n",
       "      <td>0.006837</td>\n",
       "      <td>-0.063402</td>\n",
       "      <td>-0.028198</td>\n",
       "      <td>-0.030865</td>\n",
       "      <td>-0.027233</td>\n",
       "      <td>-0.029973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_len</th>\n",
       "      <td>0.001767</td>\n",
       "      <td>0.009578</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.009253</td>\n",
       "      <td>-0.011282</td>\n",
       "      <td>-0.014849</td>\n",
       "      <td>-0.002235</td>\n",
       "      <td>-0.024045</td>\n",
       "      <td>-0.007351</td>\n",
       "      <td>-0.016108</td>\n",
       "      <td>-0.001069</td>\n",
       "      <td>-0.013683</td>\n",
       "      <td>-0.011317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>-0.018706</td>\n",
       "      <td>-0.046744</td>\n",
       "      <td>-0.009253</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.079270</td>\n",
       "      <td>0.538836</td>\n",
       "      <td>0.573363</td>\n",
       "      <td>0.027015</td>\n",
       "      <td>0.632087</td>\n",
       "      <td>0.022286</td>\n",
       "      <td>0.464580</td>\n",
       "      <td>0.163645</td>\n",
       "      <td>0.036400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anticipation</th>\n",
       "      <td>-0.001991</td>\n",
       "      <td>-0.023335</td>\n",
       "      <td>-0.011282</td>\n",
       "      <td>0.079270</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.046367</td>\n",
       "      <td>0.118551</td>\n",
       "      <td>0.593373</td>\n",
       "      <td>0.059835</td>\n",
       "      <td>0.456859</td>\n",
       "      <td>0.080516</td>\n",
       "      <td>0.475230</td>\n",
       "      <td>0.456102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>-0.000641</td>\n",
       "      <td>-0.023392</td>\n",
       "      <td>-0.014849</td>\n",
       "      <td>0.538836</td>\n",
       "      <td>0.046367</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.383289</td>\n",
       "      <td>0.023484</td>\n",
       "      <td>0.570610</td>\n",
       "      <td>0.014141</td>\n",
       "      <td>0.423732</td>\n",
       "      <td>0.110992</td>\n",
       "      <td>0.023565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>-0.029771</td>\n",
       "      <td>-0.063543</td>\n",
       "      <td>-0.002235</td>\n",
       "      <td>0.573363</td>\n",
       "      <td>0.118551</td>\n",
       "      <td>0.383289</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027865</td>\n",
       "      <td>0.577988</td>\n",
       "      <td>0.036232</td>\n",
       "      <td>0.527490</td>\n",
       "      <td>0.140559</td>\n",
       "      <td>0.063541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.015376</td>\n",
       "      <td>0.006837</td>\n",
       "      <td>-0.024045</td>\n",
       "      <td>0.027015</td>\n",
       "      <td>0.593373</td>\n",
       "      <td>0.023484</td>\n",
       "      <td>0.027865</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.012048</td>\n",
       "      <td>0.685398</td>\n",
       "      <td>0.049961</td>\n",
       "      <td>0.462737</td>\n",
       "      <td>0.589400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>-0.020250</td>\n",
       "      <td>-0.063402</td>\n",
       "      <td>-0.007351</td>\n",
       "      <td>0.632087</td>\n",
       "      <td>0.059835</td>\n",
       "      <td>0.570610</td>\n",
       "      <td>0.577988</td>\n",
       "      <td>-0.012048</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.013037</td>\n",
       "      <td>0.581562</td>\n",
       "      <td>0.127099</td>\n",
       "      <td>0.007170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>-0.004770</td>\n",
       "      <td>-0.028198</td>\n",
       "      <td>-0.016108</td>\n",
       "      <td>0.022286</td>\n",
       "      <td>0.456859</td>\n",
       "      <td>0.014141</td>\n",
       "      <td>0.036232</td>\n",
       "      <td>0.685398</td>\n",
       "      <td>-0.013037</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.029600</td>\n",
       "      <td>0.330997</td>\n",
       "      <td>0.647792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>-0.002814</td>\n",
       "      <td>-0.030865</td>\n",
       "      <td>-0.001069</td>\n",
       "      <td>0.464580</td>\n",
       "      <td>0.080516</td>\n",
       "      <td>0.423732</td>\n",
       "      <td>0.527490</td>\n",
       "      <td>0.049961</td>\n",
       "      <td>0.581562</td>\n",
       "      <td>0.029600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.161691</td>\n",
       "      <td>0.029850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>-0.017061</td>\n",
       "      <td>-0.027233</td>\n",
       "      <td>-0.013683</td>\n",
       "      <td>0.163645</td>\n",
       "      <td>0.475230</td>\n",
       "      <td>0.110992</td>\n",
       "      <td>0.140559</td>\n",
       "      <td>0.462737</td>\n",
       "      <td>0.127099</td>\n",
       "      <td>0.330997</td>\n",
       "      <td>0.161691</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.314850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust</th>\n",
       "      <td>-0.008363</td>\n",
       "      <td>-0.029973</td>\n",
       "      <td>-0.011317</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.456102</td>\n",
       "      <td>0.023565</td>\n",
       "      <td>0.063541</td>\n",
       "      <td>0.589400</td>\n",
       "      <td>0.007170</td>\n",
       "      <td>0.647792</td>\n",
       "      <td>0.029850</td>\n",
       "      <td>0.314850</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 label  pronouns  text_len     anger  anticipation   disgust  \\\n",
       "label         1.000000  0.141548  0.001767 -0.018706     -0.001991 -0.000641   \n",
       "pronouns      0.141548  1.000000  0.009578 -0.046744     -0.023335 -0.023392   \n",
       "text_len      0.001767  0.009578  1.000000 -0.009253     -0.011282 -0.014849   \n",
       "anger        -0.018706 -0.046744 -0.009253  1.000000      0.079270  0.538836   \n",
       "anticipation -0.001991 -0.023335 -0.011282  0.079270      1.000000  0.046367   \n",
       "disgust      -0.000641 -0.023392 -0.014849  0.538836      0.046367  1.000000   \n",
       "fear         -0.029771 -0.063543 -0.002235  0.573363      0.118551  0.383289   \n",
       "joy           0.015376  0.006837 -0.024045  0.027015      0.593373  0.023484   \n",
       "negative     -0.020250 -0.063402 -0.007351  0.632087      0.059835  0.570610   \n",
       "positive     -0.004770 -0.028198 -0.016108  0.022286      0.456859  0.014141   \n",
       "sadness      -0.002814 -0.030865 -0.001069  0.464580      0.080516  0.423732   \n",
       "surprise     -0.017061 -0.027233 -0.013683  0.163645      0.475230  0.110992   \n",
       "trust        -0.008363 -0.029973 -0.011317  0.036400      0.456102  0.023565   \n",
       "\n",
       "                  fear       joy  negative  positive   sadness  surprise  \\\n",
       "label        -0.029771  0.015376 -0.020250 -0.004770 -0.002814 -0.017061   \n",
       "pronouns     -0.063543  0.006837 -0.063402 -0.028198 -0.030865 -0.027233   \n",
       "text_len     -0.002235 -0.024045 -0.007351 -0.016108 -0.001069 -0.013683   \n",
       "anger         0.573363  0.027015  0.632087  0.022286  0.464580  0.163645   \n",
       "anticipation  0.118551  0.593373  0.059835  0.456859  0.080516  0.475230   \n",
       "disgust       0.383289  0.023484  0.570610  0.014141  0.423732  0.110992   \n",
       "fear          1.000000  0.027865  0.577988  0.036232  0.527490  0.140559   \n",
       "joy           0.027865  1.000000 -0.012048  0.685398  0.049961  0.462737   \n",
       "negative      0.577988 -0.012048  1.000000 -0.013037  0.581562  0.127099   \n",
       "positive      0.036232  0.685398 -0.013037  1.000000  0.029600  0.330997   \n",
       "sadness       0.527490  0.049961  0.581562  0.029600  1.000000  0.161691   \n",
       "surprise      0.140559  0.462737  0.127099  0.330997  0.161691  1.000000   \n",
       "trust         0.063541  0.589400  0.007170  0.647792  0.029850  0.314850   \n",
       "\n",
       "                 trust  \n",
       "label        -0.008363  \n",
       "pronouns     -0.029973  \n",
       "text_len     -0.011317  \n",
       "anger         0.036400  \n",
       "anticipation  0.456102  \n",
       "disgust       0.023565  \n",
       "fear          0.063541  \n",
       "joy           0.589400  \n",
       "negative      0.007170  \n",
       "positive      0.647792  \n",
       "sadness       0.029850  \n",
       "surprise      0.314850  \n",
       "trust         1.000000  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len'] + emotions].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>text_len</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.466827</td>\n",
       "      <td>0.017641</td>\n",
       "      <td>-0.104687</td>\n",
       "      <td>-0.014279</td>\n",
       "      <td>-0.045834</td>\n",
       "      <td>-0.139488</td>\n",
       "      <td>0.029806</td>\n",
       "      <td>-0.102315</td>\n",
       "      <td>-0.056676</td>\n",
       "      <td>-0.002080</td>\n",
       "      <td>-0.109270</td>\n",
       "      <td>-0.020131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pronouns</th>\n",
       "      <td>0.466827</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.143079</td>\n",
       "      <td>-0.241525</td>\n",
       "      <td>-0.064173</td>\n",
       "      <td>-0.063054</td>\n",
       "      <td>-0.338177</td>\n",
       "      <td>0.073155</td>\n",
       "      <td>-0.295912</td>\n",
       "      <td>-0.138638</td>\n",
       "      <td>-0.102741</td>\n",
       "      <td>-0.187297</td>\n",
       "      <td>-0.120777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_len</th>\n",
       "      <td>0.017641</td>\n",
       "      <td>0.143079</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.034923</td>\n",
       "      <td>-0.067237</td>\n",
       "      <td>-0.022241</td>\n",
       "      <td>-0.066312</td>\n",
       "      <td>-0.104094</td>\n",
       "      <td>-0.068714</td>\n",
       "      <td>-0.106308</td>\n",
       "      <td>-0.041583</td>\n",
       "      <td>-0.070508</td>\n",
       "      <td>-0.103088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>-0.104687</td>\n",
       "      <td>-0.241525</td>\n",
       "      <td>-0.034923</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.063304</td>\n",
       "      <td>0.559546</td>\n",
       "      <td>0.782360</td>\n",
       "      <td>-0.007202</td>\n",
       "      <td>0.730543</td>\n",
       "      <td>0.069692</td>\n",
       "      <td>0.513875</td>\n",
       "      <td>0.243803</td>\n",
       "      <td>0.208935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anticipation</th>\n",
       "      <td>-0.014279</td>\n",
       "      <td>-0.064173</td>\n",
       "      <td>-0.067237</td>\n",
       "      <td>0.063304</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.079474</td>\n",
       "      <td>0.053420</td>\n",
       "      <td>0.657685</td>\n",
       "      <td>0.070433</td>\n",
       "      <td>0.634664</td>\n",
       "      <td>0.176743</td>\n",
       "      <td>0.591110</td>\n",
       "      <td>0.601632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>-0.045834</td>\n",
       "      <td>-0.063054</td>\n",
       "      <td>-0.022241</td>\n",
       "      <td>0.559546</td>\n",
       "      <td>0.079474</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.439077</td>\n",
       "      <td>0.113387</td>\n",
       "      <td>0.665141</td>\n",
       "      <td>0.057294</td>\n",
       "      <td>0.432380</td>\n",
       "      <td>0.196867</td>\n",
       "      <td>0.095206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>-0.139488</td>\n",
       "      <td>-0.338177</td>\n",
       "      <td>-0.066312</td>\n",
       "      <td>0.782360</td>\n",
       "      <td>0.053420</td>\n",
       "      <td>0.439077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.074427</td>\n",
       "      <td>0.690689</td>\n",
       "      <td>0.039081</td>\n",
       "      <td>0.522041</td>\n",
       "      <td>0.187633</td>\n",
       "      <td>0.178849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.029806</td>\n",
       "      <td>0.073155</td>\n",
       "      <td>-0.104094</td>\n",
       "      <td>-0.007202</td>\n",
       "      <td>0.657685</td>\n",
       "      <td>0.113387</td>\n",
       "      <td>-0.074427</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.017230</td>\n",
       "      <td>0.792108</td>\n",
       "      <td>0.098208</td>\n",
       "      <td>0.551726</td>\n",
       "      <td>0.496315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>-0.102315</td>\n",
       "      <td>-0.295912</td>\n",
       "      <td>-0.068714</td>\n",
       "      <td>0.730543</td>\n",
       "      <td>0.070433</td>\n",
       "      <td>0.665141</td>\n",
       "      <td>0.690689</td>\n",
       "      <td>-0.017230</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.081112</td>\n",
       "      <td>0.698466</td>\n",
       "      <td>0.226197</td>\n",
       "      <td>0.203600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>-0.056676</td>\n",
       "      <td>-0.138638</td>\n",
       "      <td>-0.106308</td>\n",
       "      <td>0.069692</td>\n",
       "      <td>0.634664</td>\n",
       "      <td>0.057294</td>\n",
       "      <td>0.039081</td>\n",
       "      <td>0.792108</td>\n",
       "      <td>0.081112</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.119927</td>\n",
       "      <td>0.518270</td>\n",
       "      <td>0.662197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>-0.002080</td>\n",
       "      <td>-0.102741</td>\n",
       "      <td>-0.041583</td>\n",
       "      <td>0.513875</td>\n",
       "      <td>0.176743</td>\n",
       "      <td>0.432380</td>\n",
       "      <td>0.522041</td>\n",
       "      <td>0.098208</td>\n",
       "      <td>0.698466</td>\n",
       "      <td>0.119927</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.246272</td>\n",
       "      <td>0.152635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>-0.109270</td>\n",
       "      <td>-0.187297</td>\n",
       "      <td>-0.070508</td>\n",
       "      <td>0.243803</td>\n",
       "      <td>0.591110</td>\n",
       "      <td>0.196867</td>\n",
       "      <td>0.187633</td>\n",
       "      <td>0.551726</td>\n",
       "      <td>0.226197</td>\n",
       "      <td>0.518270</td>\n",
       "      <td>0.246272</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.472995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust</th>\n",
       "      <td>-0.020131</td>\n",
       "      <td>-0.120777</td>\n",
       "      <td>-0.103088</td>\n",
       "      <td>0.208935</td>\n",
       "      <td>0.601632</td>\n",
       "      <td>0.095206</td>\n",
       "      <td>0.178849</td>\n",
       "      <td>0.496315</td>\n",
       "      <td>0.203600</td>\n",
       "      <td>0.662197</td>\n",
       "      <td>0.152635</td>\n",
       "      <td>0.472995</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 label  pronouns  text_len     anger  anticipation   disgust  \\\n",
       "label         1.000000  0.466827  0.017641 -0.104687     -0.014279 -0.045834   \n",
       "pronouns      0.466827  1.000000  0.143079 -0.241525     -0.064173 -0.063054   \n",
       "text_len      0.017641  0.143079  1.000000 -0.034923     -0.067237 -0.022241   \n",
       "anger        -0.104687 -0.241525 -0.034923  1.000000      0.063304  0.559546   \n",
       "anticipation -0.014279 -0.064173 -0.067237  0.063304      1.000000  0.079474   \n",
       "disgust      -0.045834 -0.063054 -0.022241  0.559546      0.079474  1.000000   \n",
       "fear         -0.139488 -0.338177 -0.066312  0.782360      0.053420  0.439077   \n",
       "joy           0.029806  0.073155 -0.104094 -0.007202      0.657685  0.113387   \n",
       "negative     -0.102315 -0.295912 -0.068714  0.730543      0.070433  0.665141   \n",
       "positive     -0.056676 -0.138638 -0.106308  0.069692      0.634664  0.057294   \n",
       "sadness      -0.002080 -0.102741 -0.041583  0.513875      0.176743  0.432380   \n",
       "surprise     -0.109270 -0.187297 -0.070508  0.243803      0.591110  0.196867   \n",
       "trust        -0.020131 -0.120777 -0.103088  0.208935      0.601632  0.095206   \n",
       "\n",
       "                  fear       joy  negative  positive   sadness  surprise  \\\n",
       "label        -0.139488  0.029806 -0.102315 -0.056676 -0.002080 -0.109270   \n",
       "pronouns     -0.338177  0.073155 -0.295912 -0.138638 -0.102741 -0.187297   \n",
       "text_len     -0.066312 -0.104094 -0.068714 -0.106308 -0.041583 -0.070508   \n",
       "anger         0.782360 -0.007202  0.730543  0.069692  0.513875  0.243803   \n",
       "anticipation  0.053420  0.657685  0.070433  0.634664  0.176743  0.591110   \n",
       "disgust       0.439077  0.113387  0.665141  0.057294  0.432380  0.196867   \n",
       "fear          1.000000 -0.074427  0.690689  0.039081  0.522041  0.187633   \n",
       "joy          -0.074427  1.000000 -0.017230  0.792108  0.098208  0.551726   \n",
       "negative      0.690689 -0.017230  1.000000  0.081112  0.698466  0.226197   \n",
       "positive      0.039081  0.792108  0.081112  1.000000  0.119927  0.518270   \n",
       "sadness       0.522041  0.098208  0.698466  0.119927  1.000000  0.246272   \n",
       "surprise      0.187633  0.551726  0.226197  0.518270  0.246272  1.000000   \n",
       "trust         0.178849  0.496315  0.203600  0.662197  0.152635  0.472995   \n",
       "\n",
       "                 trust  \n",
       "label        -0.020131  \n",
       "pronouns     -0.120777  \n",
       "text_len     -0.103088  \n",
       "anger         0.208935  \n",
       "anticipation  0.601632  \n",
       "disgust       0.095206  \n",
       "fear          0.178849  \n",
       "joy           0.496315  \n",
       "negative      0.203600  \n",
       "positive      0.662197  \n",
       "sadness       0.152635  \n",
       "surprise      0.472995  \n",
       "trust         1.000000  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len', 'subject'] + emotions].groupby('subject').mean().corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pronouns</th>\n",
       "      <th>text_len</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.027455</td>\n",
       "      <td>42.271740</td>\n",
       "      <td>0.018265</td>\n",
       "      <td>0.025429</td>\n",
       "      <td>0.012754</td>\n",
       "      <td>0.022802</td>\n",
       "      <td>0.02193</td>\n",
       "      <td>0.036955</td>\n",
       "      <td>0.052819</td>\n",
       "      <td>0.017002</td>\n",
       "      <td>0.014242</td>\n",
       "      <td>0.034492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.052482</td>\n",
       "      <td>42.944565</td>\n",
       "      <td>0.015202</td>\n",
       "      <td>0.025060</td>\n",
       "      <td>0.012661</td>\n",
       "      <td>0.017507</td>\n",
       "      <td>0.02493</td>\n",
       "      <td>0.032309</td>\n",
       "      <td>0.051519</td>\n",
       "      <td>0.016573</td>\n",
       "      <td>0.011768</td>\n",
       "      <td>0.032667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pronouns   text_len     anger  anticipation   disgust      fear  \\\n",
       "label                                                                    \n",
       "0      0.027455  42.271740  0.018265      0.025429  0.012754  0.022802   \n",
       "1      0.052482  42.944565  0.015202      0.025060  0.012661  0.017507   \n",
       "\n",
       "           joy  negative  positive   sadness  surprise     trust  \n",
       "label                                                             \n",
       "0      0.02193  0.036955  0.052819  0.017002  0.014242  0.034492  \n",
       "1      0.02493  0.032309  0.051519  0.016573  0.011768  0.032667  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len'] + emotions].groupby('label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentAnalyzer, SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid.polarity_scores(\"We are here today happiness is all around\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['neg_vader'] = writings_df.text.apply(lambda t: sid.polarity_scores(t)['neg']\n",
    "                                                 if type(t)==str else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['pos_vader'] = writings_df.text.apply(lambda t: sid.polarity_scores(t)['pos']\n",
    "                                                 if type(t)==str else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len', 'neg_vader', 'pos_vader'] + emotions].groupby('label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len', 'neg_vader', 'pos_vader'] + emotions].corr('spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeline of diagnosis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_users = set(writings_df[writings_df['label']==1].subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>subject</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>subset</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>title_len</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>text_len</th>\n",
       "      <th>...</th>\n",
       "      <th>joy</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>fear</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>negative</th>\n",
       "      <th>pronouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53633</th>\n",
       "      <td>2017-01-25 14:05:43</td>\n",
       "      <td>subject12570000</td>\n",
       "      <td>Ugh, CD1, I'll be following you very shortly ...</td>\n",
       "      <td></td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[ugh, cd1, i, ll, be, following, you, very, sh...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53634</th>\n",
       "      <td>2017-01-25 14:04:43</td>\n",
       "      <td>subject12570000</td>\n",
       "      <td>15DPO. .5 degree temp drop. Mild Cramps. Look...</td>\n",
       "      <td></td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[15dpo, 5, degree, temp, drop, mild, cramps, l...</td>\n",
       "      <td>81.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53635</th>\n",
       "      <td>2017-01-25 14:01:19</td>\n",
       "      <td>subject12570000</td>\n",
       "      <td>Temp drop of .5 degrees this morning with lig...</td>\n",
       "      <td></td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[temp, drop, of, 5, degrees, this, morning, wi...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53636</th>\n",
       "      <td>2017-01-25 02:11:30</td>\n",
       "      <td>subject12570000</td>\n",
       "      <td>No AF, and I couldn't bring myself to test th...</td>\n",
       "      <td></td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[no, af, and, i, couldn, t, bring, myself, to,...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53637</th>\n",
       "      <td>2017-01-24 17:37:53</td>\n",
       "      <td>subject12570000</td>\n",
       "      <td>I've got such a pretty chart after I ovulated...</td>\n",
       "      <td></td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[i, ve, got, such, a, pretty, chart, after, i,...</td>\n",
       "      <td>52.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.096154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.057692</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514620</th>\n",
       "      <td>2017-04-12 17:06:01</td>\n",
       "      <td>subject12570000</td>\n",
       "      <td>Yes, have that wine or beer or whatever!! On ...</td>\n",
       "      <td></td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[yes, have, that, wine, or, beer, or, whatever...</td>\n",
       "      <td>82.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036585</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.073171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514621</th>\n",
       "      <td>2017-04-12 17:03:17</td>\n",
       "      <td>subject12570000</td>\n",
       "      <td>I can't believe I'm still here, waiting for t...</td>\n",
       "      <td></td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[i, can, t, believe, i, m, still, here, waitin...</td>\n",
       "      <td>82.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.109756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514622</th>\n",
       "      <td>2017-04-12 17:00:53</td>\n",
       "      <td>subject12570000</td>\n",
       "      <td>Thanks!!</td>\n",
       "      <td></td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[thanks]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514623</th>\n",
       "      <td>2017-04-12 16:43:20</td>\n",
       "      <td>subject12570000</td>\n",
       "      <td>I just want to wish you good luck!! I think y...</td>\n",
       "      <td></td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[i, just, want, to, wish, you, good, luck, i, ...</td>\n",
       "      <td>158.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.025316</td>\n",
       "      <td>0.025316</td>\n",
       "      <td>0.018987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037975</td>\n",
       "      <td>0.006329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006329</td>\n",
       "      <td>0.101266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514624</th>\n",
       "      <td>2017-04-11 23:03:01</td>\n",
       "      <td>subject12570000</td>\n",
       "      <td>Yep, seems like we're in almost the same boat...</td>\n",
       "      <td></td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[yep, seems, like, we, re, in, almost, the, sa...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>839 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         date          subject  \\\n",
       "53633    2017-01-25 14:05:43   subject12570000   \n",
       "53634    2017-01-25 14:04:43   subject12570000   \n",
       "53635    2017-01-25 14:01:19   subject12570000   \n",
       "53636    2017-01-25 02:11:30   subject12570000   \n",
       "53637    2017-01-24 17:37:53   subject12570000   \n",
       "...                       ...              ...   \n",
       "514620   2017-04-12 17:06:01   subject12570000   \n",
       "514621   2017-04-12 17:03:17   subject12570000   \n",
       "514622   2017-04-12 17:00:53   subject12570000   \n",
       "514623   2017-04-12 16:43:20   subject12570000   \n",
       "514624   2017-04-11 23:03:01   subject12570000   \n",
       "\n",
       "                                                     text title subset  label  \\\n",
       "53633    Ugh, CD1, I'll be following you very shortly ...         test      1   \n",
       "53634    15DPO. .5 degree temp drop. Mild Cramps. Look...         test      1   \n",
       "53635    Temp drop of .5 degrees this morning with lig...         test      1   \n",
       "53636    No AF, and I couldn't bring myself to test th...         test      1   \n",
       "53637    I've got such a pretty chart after I ovulated...         test      1   \n",
       "...                                                   ...   ...    ...    ...   \n",
       "514620   Yes, have that wine or beer or whatever!! On ...         test      1   \n",
       "514621   I can't believe I'm still here, waiting for t...         test      1   \n",
       "514622                                          Thanks!!          test      1   \n",
       "514623   I just want to wish you good luck!! I think y...         test      1   \n",
       "514624   Yep, seems like we're in almost the same boat...         test      1   \n",
       "\n",
       "       tokenized_title  title_len  \\\n",
       "53633               []        NaN   \n",
       "53634               []        NaN   \n",
       "53635               []        NaN   \n",
       "53636               []        NaN   \n",
       "53637               []        NaN   \n",
       "...                ...        ...   \n",
       "514620              []        NaN   \n",
       "514621              []        NaN   \n",
       "514622              []        NaN   \n",
       "514623              []        NaN   \n",
       "514624              []        NaN   \n",
       "\n",
       "                                           tokenized_text  text_len  ...  \\\n",
       "53633   [ugh, cd1, i, ll, be, following, you, very, sh...      30.0  ...   \n",
       "53634   [15dpo, 5, degree, temp, drop, mild, cramps, l...      81.0  ...   \n",
       "53635   [temp, drop, of, 5, degrees, this, morning, wi...      15.0  ...   \n",
       "53636   [no, af, and, i, couldn, t, bring, myself, to,...      34.0  ...   \n",
       "53637   [i, ve, got, such, a, pretty, chart, after, i,...      52.0  ...   \n",
       "...                                                   ...       ...  ...   \n",
       "514620  [yes, have, that, wine, or, beer, or, whatever...      82.0  ...   \n",
       "514621  [i, can, t, believe, i, m, still, here, waitin...      82.0  ...   \n",
       "514622                                           [thanks]       1.0  ...   \n",
       "514623  [i, just, want, to, wish, you, good, luck, i, ...     158.0  ...   \n",
       "514624  [yep, seems, like, we, re, in, almost, the, sa...      12.0  ...   \n",
       "\n",
       "             joy     trust  surprise  anticipation      fear  positive  \\\n",
       "53633   0.033333  0.033333  0.000000      0.066667  0.000000  0.033333   \n",
       "53634   0.012346  0.012346  0.012346      0.037037  0.000000  0.024691   \n",
       "53635   0.000000  0.000000  0.000000      0.066667  0.000000  0.000000   \n",
       "53636   0.000000  0.029412  0.000000      0.029412  0.000000  0.000000   \n",
       "53637   0.038462  0.096154  0.000000      0.038462  0.019231  0.057692   \n",
       "...          ...       ...       ...           ...       ...       ...   \n",
       "514620  0.036585  0.024390  0.024390      0.024390  0.024390  0.048780   \n",
       "514621  0.012195  0.024390  0.012195      0.024390  0.000000  0.024390   \n",
       "514622  0.000000  0.000000  0.000000      0.000000  0.000000  0.000000   \n",
       "514623  0.012658  0.025316  0.025316      0.018987  0.000000  0.037975   \n",
       "514624  0.000000  0.000000  0.000000      0.000000  0.000000  0.000000   \n",
       "\n",
       "         sadness   disgust  negative  pronouns  \n",
       "53633   0.000000  0.000000  0.000000  0.033333  \n",
       "53634   0.000000  0.012346  0.000000  0.086420  \n",
       "53635   0.000000  0.000000  0.000000  0.000000  \n",
       "53636   0.000000  0.000000  0.000000  0.147059  \n",
       "53637   0.019231  0.019231  0.019231  0.076923  \n",
       "...          ...       ...       ...       ...  \n",
       "514620  0.024390  0.012195  0.024390  0.073171  \n",
       "514621  0.000000  0.012195  0.012195  0.109756  \n",
       "514622  0.000000  0.000000  0.000000  0.000000  \n",
       "514623  0.006329  0.000000  0.006329  0.101266  \n",
       "514624  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[839 rows x 85 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[writings_df['subject']=='subject12570000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 0\n",
      "0 1 1\n",
      "7 1 0\n",
      "2 0 0\n",
      "2 0 0\n",
      "3 2 0\n",
      "8 1 0\n",
      "0 0 0\n",
      "5 5 0\n",
      "7 5 0\n",
      "0 0 0\n",
      "2 5 1\n",
      "1 0 0\n",
      "1 0 1\n",
      "0 0 0\n",
      "0 0 0\n",
      "3 1 0\n",
      "1 3 0\n",
      "2 0 0\n",
      "0 0 0\n",
      "0 0 1\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "11 14 0\n",
      "16 22 1\n",
      "0 0 0\n",
      "0 1 0\n",
      "2 2 0\n",
      "0 0 0\n",
      "11 2 1\n",
      "2 2 0\n",
      "2 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "6 0 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "1 1 0\n",
      "1 1 0\n",
      "2 0 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "2 0 0\n",
      "5 5 6\n",
      "0 0 0\n",
      "0 0 0\n",
      "8 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "14 11 2\n",
      "1 0 0\n",
      "0 1 0\n",
      "2 0 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "1 0 0\n",
      "1 0 0\n",
      "5 2 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "18 9 0\n",
      "2 0 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "2 8 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 0 1\n",
      "0 0 0\n",
      "1 0 0\n",
      "3 0 0\n",
      "0 0 0\n",
      "3 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "2 0 1\n",
      "0 0 0\n",
      "0 1 0\n",
      "0 0 0\n",
      "3 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "5 0 1\n",
      "0 0 0\n",
      "2 1 0\n",
      "2 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "3 1 0\n",
      "9 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "0 2 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "3 1 0\n",
      "5 0 0\n",
      "6 1 0\n",
      "5 1 1\n",
      "0 0 0\n",
      "3 1 0\n",
      "0 0 0\n",
      "19 15 0\n",
      "1 1 0\n",
      "0 0 0\n",
      "3 2 1\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "1 0 0\n",
      "3 0 0\n",
      "2 0 0\n",
      "2 1 0\n",
      "0 0 0\n",
      "9 1 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "4 3 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "3 0 0\n",
      "0 1 0\n",
      "0 0 0\n",
      "3 0 0\n",
      "0 1 1\n",
      "3 5 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "11 0 0\n",
      "0 0 0\n",
      "7 1 0\n",
      "0 0 0\n",
      "3 0 0\n",
      "7 0 1\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "3 2 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "3 1 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 2 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "2 0 0\n",
      "0 1 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "2 0 0\n",
      "0 0 0\n",
      "4 0 0\n",
      "2 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "0 1 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "2 0 0\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "for user in depressed_users:\n",
    "    user_vocab = Counter(sum(writings_df[writings_df['subject']==user].tokenized_text.values, []))\n",
    "    print(user_vocab['diagnosed'], user_vocab['diagnosis'], user_vocab['diagnose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mentions(tokenized_text, terms=['diagnosis', 'diagnosed', 'diagnose']):\n",
    "    for term in terms:\n",
    "        if term in tokenized_text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "writings_df['diagnosis'] = writings_df['tokenized_text'].apply(mentions)\n",
    "writings_df['depression_mention'] = writings_df['tokenized_text'].apply(lambda t: \n",
    "                                                                        mentions(t, ['depressed', 'depression']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 214)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>depression_mention</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subject12570000</th>\n",
       "      <td>839</td>\n",
       "      <td>839</td>\n",
       "      <td>839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject12600000</th>\n",
       "      <td>1960</td>\n",
       "      <td>1960</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject13340000</th>\n",
       "      <td>434</td>\n",
       "      <td>434</td>\n",
       "      <td>434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1360000</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject14100000</th>\n",
       "      <td>1553</td>\n",
       "      <td>1553</td>\n",
       "      <td>1553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject14860000</th>\n",
       "      <td>126</td>\n",
       "      <td>126</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1880000</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject19630000</th>\n",
       "      <td>1126</td>\n",
       "      <td>1126</td>\n",
       "      <td>1126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject21630000</th>\n",
       "      <td>91</td>\n",
       "      <td>91</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject24330000</th>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject27140000</th>\n",
       "      <td>1259</td>\n",
       "      <td>1259</td>\n",
       "      <td>1259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject27430000</th>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject27790000</th>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject28660000</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject30410000</th>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject31260000</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject31360000</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject32570000</th>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject32680000</th>\n",
       "      <td>184</td>\n",
       "      <td>184</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject32870000</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject33070000</th>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject3330000</th>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject33550000</th>\n",
       "      <td>1095</td>\n",
       "      <td>1095</td>\n",
       "      <td>1095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject34140000</th>\n",
       "      <td>772</td>\n",
       "      <td>772</td>\n",
       "      <td>772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject40540000</th>\n",
       "      <td>216</td>\n",
       "      <td>216</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject4190000</th>\n",
       "      <td>1174</td>\n",
       "      <td>1174</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject43060000</th>\n",
       "      <td>144</td>\n",
       "      <td>144</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject44540000</th>\n",
       "      <td>1313</td>\n",
       "      <td>1313</td>\n",
       "      <td>1313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject45160000</th>\n",
       "      <td>921</td>\n",
       "      <td>921</td>\n",
       "      <td>921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject45380000</th>\n",
       "      <td>937</td>\n",
       "      <td>937</td>\n",
       "      <td>937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject45550000</th>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject46020000</th>\n",
       "      <td>1468</td>\n",
       "      <td>1468</td>\n",
       "      <td>1468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject46800000</th>\n",
       "      <td>674</td>\n",
       "      <td>674</td>\n",
       "      <td>674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject47190000</th>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject49080000</th>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject5130000</th>\n",
       "      <td>1119</td>\n",
       "      <td>1119</td>\n",
       "      <td>1119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject52350000</th>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject5250000</th>\n",
       "      <td>142</td>\n",
       "      <td>142</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject5270000</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject54920000</th>\n",
       "      <td>673</td>\n",
       "      <td>673</td>\n",
       "      <td>673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject55540000</th>\n",
       "      <td>1303</td>\n",
       "      <td>1303</td>\n",
       "      <td>1303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject55800000</th>\n",
       "      <td>143</td>\n",
       "      <td>143</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject55910000</th>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject57560000</th>\n",
       "      <td>147</td>\n",
       "      <td>147</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject5770000</th>\n",
       "      <td>605</td>\n",
       "      <td>605</td>\n",
       "      <td>605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject58020000</th>\n",
       "      <td>946</td>\n",
       "      <td>946</td>\n",
       "      <td>946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject5820000</th>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject61630000</th>\n",
       "      <td>1295</td>\n",
       "      <td>1295</td>\n",
       "      <td>1295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject64690000</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject67820000</th>\n",
       "      <td>478</td>\n",
       "      <td>478</td>\n",
       "      <td>478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject69260000</th>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject69280000</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject69450000</th>\n",
       "      <td>1093</td>\n",
       "      <td>1093</td>\n",
       "      <td>1093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject70510000</th>\n",
       "      <td>1089</td>\n",
       "      <td>1089</td>\n",
       "      <td>1089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject7210000</th>\n",
       "      <td>1710</td>\n",
       "      <td>1710</td>\n",
       "      <td>1710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject72540000</th>\n",
       "      <td>1396</td>\n",
       "      <td>1396</td>\n",
       "      <td>1396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject73970000</th>\n",
       "      <td>1232</td>\n",
       "      <td>1232</td>\n",
       "      <td>1232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject74260000</th>\n",
       "      <td>258</td>\n",
       "      <td>258</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject74710000</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject76680000</th>\n",
       "      <td>602</td>\n",
       "      <td>602</td>\n",
       "      <td>602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject77700000</th>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject78910000</th>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject8320000</th>\n",
       "      <td>420</td>\n",
       "      <td>420</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject83980000</th>\n",
       "      <td>1150</td>\n",
       "      <td>1150</td>\n",
       "      <td>1150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject84770000</th>\n",
       "      <td>106</td>\n",
       "      <td>106</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject85420000</th>\n",
       "      <td>104</td>\n",
       "      <td>104</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject85930000</th>\n",
       "      <td>173</td>\n",
       "      <td>173</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject86530000</th>\n",
       "      <td>963</td>\n",
       "      <td>963</td>\n",
       "      <td>963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject88640000</th>\n",
       "      <td>194</td>\n",
       "      <td>194</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject88840000</th>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject89250000</th>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject90630000</th>\n",
       "      <td>1191</td>\n",
       "      <td>1191</td>\n",
       "      <td>1191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject91390000</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject97430000</th>\n",
       "      <td>434</td>\n",
       "      <td>434</td>\n",
       "      <td>434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject97670000</th>\n",
       "      <td>681</td>\n",
       "      <td>681</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9830000</th>\n",
       "      <td>1019</td>\n",
       "      <td>1019</td>\n",
       "      <td>1019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject99320000</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject99510000</th>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9980000</th>\n",
       "      <td>1239</td>\n",
       "      <td>1239</td>\n",
       "      <td>1239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject1345</th>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject1445</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject1487</th>\n",
       "      <td>84</td>\n",
       "      <td>84</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject1488</th>\n",
       "      <td>657</td>\n",
       "      <td>657</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject1914</th>\n",
       "      <td>758</td>\n",
       "      <td>758</td>\n",
       "      <td>758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject2585</th>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject2607</th>\n",
       "      <td>710</td>\n",
       "      <td>710</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject2673</th>\n",
       "      <td>285</td>\n",
       "      <td>285</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject2790</th>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject2960</th>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject3112</th>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject3408</th>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject3802</th>\n",
       "      <td>1104</td>\n",
       "      <td>1104</td>\n",
       "      <td>1104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject3805</th>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject3986</th>\n",
       "      <td>244</td>\n",
       "      <td>244</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject3988</th>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject4471</th>\n",
       "      <td>1043</td>\n",
       "      <td>1043</td>\n",
       "      <td>1043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject4534</th>\n",
       "      <td>215</td>\n",
       "      <td>215</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject5099</th>\n",
       "      <td>619</td>\n",
       "      <td>619</td>\n",
       "      <td>619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject5209</th>\n",
       "      <td>822</td>\n",
       "      <td>822</td>\n",
       "      <td>822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject5880</th>\n",
       "      <td>1046</td>\n",
       "      <td>1046</td>\n",
       "      <td>1046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject5913</th>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject5926</th>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject6246</th>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject625</th>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject6305</th>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject6728</th>\n",
       "      <td>106</td>\n",
       "      <td>106</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject6752</th>\n",
       "      <td>1369</td>\n",
       "      <td>1369</td>\n",
       "      <td>1369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject6794</th>\n",
       "      <td>1067</td>\n",
       "      <td>1067</td>\n",
       "      <td>1067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject6948</th>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject7464</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject7467</th>\n",
       "      <td>1162</td>\n",
       "      <td>1162</td>\n",
       "      <td>1162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject7776</th>\n",
       "      <td>1094</td>\n",
       "      <td>1094</td>\n",
       "      <td>1094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject7921</th>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject8039</th>\n",
       "      <td>108</td>\n",
       "      <td>108</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject8159</th>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject8590</th>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject8662</th>\n",
       "      <td>432</td>\n",
       "      <td>432</td>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject8720</th>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject8773</th>\n",
       "      <td>296</td>\n",
       "      <td>296</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject8878</th>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject8969</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject9096</th>\n",
       "      <td>556</td>\n",
       "      <td>556</td>\n",
       "      <td>556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject9246</th>\n",
       "      <td>288</td>\n",
       "      <td>288</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject9298</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject9306</th>\n",
       "      <td>678</td>\n",
       "      <td>678</td>\n",
       "      <td>678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject9359</th>\n",
       "      <td>104</td>\n",
       "      <td>104</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject9565</th>\n",
       "      <td>397</td>\n",
       "      <td>397</td>\n",
       "      <td>397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject9579</th>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject9585</th>\n",
       "      <td>241</td>\n",
       "      <td>241</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject9840</th>\n",
       "      <td>950</td>\n",
       "      <td>950</td>\n",
       "      <td>950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_subject9942</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject1095</th>\n",
       "      <td>1034</td>\n",
       "      <td>1034</td>\n",
       "      <td>1034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject1190</th>\n",
       "      <td>297</td>\n",
       "      <td>297</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject1191</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject1199</th>\n",
       "      <td>154</td>\n",
       "      <td>154</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject1201</th>\n",
       "      <td>597</td>\n",
       "      <td>597</td>\n",
       "      <td>597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject127</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject136</th>\n",
       "      <td>501</td>\n",
       "      <td>501</td>\n",
       "      <td>501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject1361</th>\n",
       "      <td>747</td>\n",
       "      <td>747</td>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject1457</th>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject1542</th>\n",
       "      <td>165</td>\n",
       "      <td>165</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject1555</th>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject1605</th>\n",
       "      <td>1075</td>\n",
       "      <td>1075</td>\n",
       "      <td>1075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject1637</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject1839</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject1879</th>\n",
       "      <td>307</td>\n",
       "      <td>307</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject1979</th>\n",
       "      <td>170</td>\n",
       "      <td>170</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject2037</th>\n",
       "      <td>1173</td>\n",
       "      <td>1173</td>\n",
       "      <td>1173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject2219</th>\n",
       "      <td>1035</td>\n",
       "      <td>1035</td>\n",
       "      <td>1035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject2252</th>\n",
       "      <td>1061</td>\n",
       "      <td>1061</td>\n",
       "      <td>1061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject2382</th>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject2415</th>\n",
       "      <td>901</td>\n",
       "      <td>901</td>\n",
       "      <td>901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject2440</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject2704</th>\n",
       "      <td>164</td>\n",
       "      <td>164</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject2712</th>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject2871</th>\n",
       "      <td>1127</td>\n",
       "      <td>1127</td>\n",
       "      <td>1127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject3125</th>\n",
       "      <td>689</td>\n",
       "      <td>689</td>\n",
       "      <td>689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject3336</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject3364</th>\n",
       "      <td>122</td>\n",
       "      <td>122</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject337</th>\n",
       "      <td>771</td>\n",
       "      <td>771</td>\n",
       "      <td>771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject3670</th>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject374</th>\n",
       "      <td>488</td>\n",
       "      <td>488</td>\n",
       "      <td>488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject3902</th>\n",
       "      <td>662</td>\n",
       "      <td>662</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject405</th>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject416</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject4359</th>\n",
       "      <td>1031</td>\n",
       "      <td>1031</td>\n",
       "      <td>1031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject4493</th>\n",
       "      <td>837</td>\n",
       "      <td>837</td>\n",
       "      <td>837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject4630</th>\n",
       "      <td>411</td>\n",
       "      <td>411</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject4675</th>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject4879</th>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject5080</th>\n",
       "      <td>1177</td>\n",
       "      <td>1177</td>\n",
       "      <td>1177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject5537</th>\n",
       "      <td>1832</td>\n",
       "      <td>1832</td>\n",
       "      <td>1832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject5649</th>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject5723</th>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject5936</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject6101</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject6146</th>\n",
       "      <td>341</td>\n",
       "      <td>341</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject6150</th>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject6494</th>\n",
       "      <td>495</td>\n",
       "      <td>495</td>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject669</th>\n",
       "      <td>1043</td>\n",
       "      <td>1043</td>\n",
       "      <td>1043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject6760</th>\n",
       "      <td>125</td>\n",
       "      <td>125</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject7042</th>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject7133</th>\n",
       "      <td>143</td>\n",
       "      <td>143</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject7135</th>\n",
       "      <td>503</td>\n",
       "      <td>503</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject7142</th>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject7157</th>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject7181</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject7326</th>\n",
       "      <td>118</td>\n",
       "      <td>118</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject7364</th>\n",
       "      <td>518</td>\n",
       "      <td>518</td>\n",
       "      <td>518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject7367</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject7515</th>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject7533</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject7680</th>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject7703</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject7925</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject7930</th>\n",
       "      <td>101</td>\n",
       "      <td>101</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject7970</th>\n",
       "      <td>1039</td>\n",
       "      <td>1039</td>\n",
       "      <td>1039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject8577</th>\n",
       "      <td>244</td>\n",
       "      <td>244</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject8683</th>\n",
       "      <td>1701</td>\n",
       "      <td>1701</td>\n",
       "      <td>1701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject8741</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject8771</th>\n",
       "      <td>222</td>\n",
       "      <td>222</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject8843</th>\n",
       "      <td>91</td>\n",
       "      <td>91</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject888</th>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject8901</th>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject896</th>\n",
       "      <td>577</td>\n",
       "      <td>577</td>\n",
       "      <td>577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject9001</th>\n",
       "      <td>84</td>\n",
       "      <td>84</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject9115</th>\n",
       "      <td>1269</td>\n",
       "      <td>1269</td>\n",
       "      <td>1269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject933</th>\n",
       "      <td>541</td>\n",
       "      <td>541</td>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject9358</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject9552</th>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject96</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject9683</th>\n",
       "      <td>341</td>\n",
       "      <td>341</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject9763</th>\n",
       "      <td>142</td>\n",
       "      <td>142</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_subject9951</th>\n",
       "      <td>1062</td>\n",
       "      <td>1062</td>\n",
       "      <td>1062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   label  diagnosis  depression_mention\n",
       "subject                                                \n",
       "subject12570000      839        839                 839\n",
       "subject12600000     1960       1960                1960\n",
       "subject13340000      434        434                 434\n",
       "subject1360000        17         17                  17\n",
       "subject14100000     1553       1553                1553\n",
       "subject14860000      126        126                 126\n",
       "subject1880000        25         25                  25\n",
       "subject19630000     1126       1126                1126\n",
       "subject21630000       91         91                  91\n",
       "subject24330000       67         67                  67\n",
       "subject27140000     1259       1259                1259\n",
       "subject27430000       45         45                  45\n",
       "subject27790000      172        172                 172\n",
       "subject28660000       20         20                  20\n",
       "subject30410000       69         69                  69\n",
       "subject31260000       13         13                  13\n",
       "subject31360000       13         13                  13\n",
       "subject32570000       40         40                  40\n",
       "subject32680000      184        184                 184\n",
       "subject32870000       64         64                  64\n",
       "subject33070000      150        150                 150\n",
       "subject3330000        54         54                  54\n",
       "subject33550000     1095       1095                1095\n",
       "subject34140000      772        772                 772\n",
       "subject40540000      216        216                 216\n",
       "subject4190000      1174       1174                1174\n",
       "subject43060000      144        144                 144\n",
       "subject44540000     1313       1313                1313\n",
       "subject45160000      921        921                 921\n",
       "subject45380000      937        937                 937\n",
       "subject45550000       61         61                  61\n",
       "subject46020000     1468       1468                1468\n",
       "subject46800000      674        674                 674\n",
       "subject47190000      303        303                 303\n",
       "subject49080000       44         44                  44\n",
       "subject5130000      1119       1119                1119\n",
       "subject52350000       28         28                  28\n",
       "subject5250000       142        142                 142\n",
       "subject5270000        20         20                  20\n",
       "subject54920000      673        673                 673\n",
       "subject55540000     1303       1303                1303\n",
       "subject55800000      143        143                 143\n",
       "subject55910000       29         29                  29\n",
       "subject57560000      147        147                 147\n",
       "subject5770000       605        605                 605\n",
       "subject58020000      946        946                 946\n",
       "subject5820000       368        368                 368\n",
       "subject61630000     1295       1295                1295\n",
       "subject64690000       11         11                  11\n",
       "subject67820000      478        478                 478\n",
       "subject69260000      119        119                 119\n",
       "subject69280000       10         10                  10\n",
       "subject69450000     1093       1093                1093\n",
       "subject70510000     1089       1089                1089\n",
       "subject7210000      1710       1710                1710\n",
       "subject72540000     1396       1396                1396\n",
       "subject73970000     1232       1232                1232\n",
       "subject74260000      258        258                 258\n",
       "subject74710000       26         26                  26\n",
       "subject76680000      602        602                 602\n",
       "subject77700000       67         67                  67\n",
       "subject78910000      313        313                 313\n",
       "subject8320000       420        420                 420\n",
       "subject83980000     1150       1150                1150\n",
       "subject84770000      106        106                 106\n",
       "subject85420000      104        104                 104\n",
       "subject85930000      173        173                 173\n",
       "subject86530000      963        963                 963\n",
       "subject88640000      194        194                 194\n",
       "subject88840000       94         94                  94\n",
       "subject89250000       56         56                  56\n",
       "subject90630000     1191       1191                1191\n",
       "subject91390000       64         64                  64\n",
       "subject97430000      434        434                 434\n",
       "subject97670000      681        681                 681\n",
       "subject9830000      1019       1019                1019\n",
       "subject99320000       32         32                  32\n",
       "subject99510000       76         76                  76\n",
       "subject9980000      1239       1239                1239\n",
       "test_subject1345     105        105                 105\n",
       "test_subject1445      23         23                  23\n",
       "test_subject1487      84         84                  84\n",
       "test_subject1488     657        657                 657\n",
       "test_subject1914     758        758                 758\n",
       "test_subject2585      78         78                  78\n",
       "test_subject2607     710        710                 710\n",
       "test_subject2673     285        285                 285\n",
       "test_subject2790      60         60                  60\n",
       "test_subject2960     300        300                 300\n",
       "test_subject3112      46         46                  46\n",
       "test_subject3408     238        238                 238\n",
       "test_subject3802    1104       1104                1104\n",
       "test_subject3805      63         63                  63\n",
       "test_subject3986     244        244                 244\n",
       "test_subject3988      30         30                  30\n",
       "test_subject4471    1043       1043                1043\n",
       "test_subject4534     215        215                 215\n",
       "test_subject5099     619        619                 619\n",
       "test_subject5209     822        822                 822\n",
       "test_subject5880    1046       1046                1046\n",
       "test_subject5913      65         65                  65\n",
       "test_subject5926     103        103                 103\n",
       "test_subject6246      55         55                  55\n",
       "test_subject625      183        183                 183\n",
       "test_subject6305      34         34                  34\n",
       "test_subject6728     106        106                 106\n",
       "test_subject6752    1369       1369                1369\n",
       "test_subject6794    1067       1067                1067\n",
       "test_subject6948     156        156                 156\n",
       "test_subject7464      12         12                  12\n",
       "test_subject7467    1162       1162                1162\n",
       "test_subject7776    1094       1094                1094\n",
       "test_subject7921     237        237                 237\n",
       "test_subject8039     108        108                 108\n",
       "test_subject8159      79         79                  79\n",
       "test_subject8590      36         36                  36\n",
       "test_subject8662     432        432                 432\n",
       "test_subject8720      45         45                  45\n",
       "test_subject8773     296        296                 296\n",
       "test_subject8878      43         43                  43\n",
       "test_subject8969      15         15                  15\n",
       "test_subject9096     556        556                 556\n",
       "test_subject9246     288        288                 288\n",
       "test_subject9298      14         14                  14\n",
       "test_subject9306     678        678                 678\n",
       "test_subject9359     104        104                 104\n",
       "test_subject9565     397        397                 397\n",
       "test_subject9579     151        151                 151\n",
       "test_subject9585     241        241                 241\n",
       "test_subject9840     950        950                 950\n",
       "test_subject9942      99         99                  99\n",
       "train_subject1095   1034       1034                1034\n",
       "train_subject1190    297        297                 297\n",
       "train_subject1191     16         16                  16\n",
       "train_subject1199    154        154                 154\n",
       "train_subject1201    597        597                 597\n",
       "train_subject127      12         12                  12\n",
       "train_subject136     501        501                 501\n",
       "train_subject1361    747        747                 747\n",
       "train_subject1457     94         94                  94\n",
       "train_subject1542    165        165                 165\n",
       "train_subject1555     46         46                  46\n",
       "train_subject1605   1075       1075                1075\n",
       "train_subject1637     12         12                  12\n",
       "train_subject1839     25         25                  25\n",
       "train_subject1879    307        307                 307\n",
       "train_subject1979    170        170                 170\n",
       "train_subject2037   1173       1173                1173\n",
       "train_subject2219   1035       1035                1035\n",
       "train_subject2252   1061       1061                1061\n",
       "train_subject2382     44         44                  44\n",
       "train_subject2415    901        901                 901\n",
       "train_subject2440     25         25                  25\n",
       "train_subject2704    164        164                 164\n",
       "train_subject2712    183        183                 183\n",
       "train_subject2871   1127       1127                1127\n",
       "train_subject3125    689        689                 689\n",
       "train_subject3336     23         23                  23\n",
       "train_subject3364    122        122                 122\n",
       "train_subject337     771        771                 771\n",
       "train_subject3670     40         40                  40\n",
       "train_subject374     488        488                 488\n",
       "train_subject3902    662        662                 662\n",
       "train_subject405      48         48                  48\n",
       "train_subject416      16         16                  16\n",
       "train_subject4359   1031       1031                1031\n",
       "train_subject4493    837        837                 837\n",
       "train_subject4630    411        411                 411\n",
       "train_subject4675     63         63                  63\n",
       "train_subject4879     88         88                  88\n",
       "train_subject5080   1177       1177                1177\n",
       "train_subject5537   1832       1832                1832\n",
       "train_subject5649     43         43                  43\n",
       "train_subject5723     96         96                  96\n",
       "train_subject5936     15         15                  15\n",
       "train_subject6101     22         22                  22\n",
       "train_subject6146    341        341                 341\n",
       "train_subject6150     44         44                  44\n",
       "train_subject6494    495        495                 495\n",
       "train_subject669    1043       1043                1043\n",
       "train_subject6760    125        125                 125\n",
       "train_subject7042     47         47                  47\n",
       "train_subject7133    143        143                 143\n",
       "train_subject7135    503        503                 503\n",
       "train_subject7142     43         43                  43\n",
       "train_subject7157    107        107                 107\n",
       "train_subject7181     10         10                  10\n",
       "train_subject7326    118        118                 118\n",
       "train_subject7364    518        518                 518\n",
       "train_subject7367     23         23                  23\n",
       "train_subject7515     59         59                  59\n",
       "train_subject7533     11         11                  11\n",
       "train_subject7680     35         35                  35\n",
       "train_subject7703     17         17                  17\n",
       "train_subject7925     16         16                  16\n",
       "train_subject7930    101        101                 101\n",
       "train_subject7970   1039       1039                1039\n",
       "train_subject8577    244        244                 244\n",
       "train_subject8683   1701       1701                1701\n",
       "train_subject8741     10         10                  10\n",
       "train_subject8771    222        222                 222\n",
       "train_subject8843     91         91                  91\n",
       "train_subject888     119        119                 119\n",
       "train_subject8901    111        111                 111\n",
       "train_subject896     577        577                 577\n",
       "train_subject9001     84         84                  84\n",
       "train_subject9115   1269       1269                1269\n",
       "train_subject933     541        541                 541\n",
       "train_subject9358     21         21                  21\n",
       "train_subject9552     27         27                  27\n",
       "train_subject96       10         10                  10\n",
       "train_subject9683    341        341                 341\n",
       "train_subject9763    142        142                 142\n",
       "train_subject9951   1062       1062                1062"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[writings_df['label']==1].groupby('subject').sum()[['label', 'diagnosis', 'depression_mention']\n",
    "                                                             ].groupby()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb7827e5ed0>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature = 'positive'\n",
    "writings_df[writings_df['label']==1].groupby('subject').mean()[feature].hist(bins=20, alpha=0.4, label='positive')\n",
    "writings_df[writings_df['label']==0].groupby('subject').mean()[feature].hist(bins=40, alpha=0.4, label='negative')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ana/resources/FakeOrFact/features/LIWC/LIWC/liwc.dic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-d651060248c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mliwc_readDict\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreadDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mliwc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/ana/resources/FakeOrFact/features/LIWC/LIWC/liwc.dic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/eRisk/liwc_readDict.py\u001b[0m in \u001b[0;36mreadDict\u001b[0;34m(dictionaryPath)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Check to make sure the dictionary is properly formatted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionaryPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdictionaryFile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionaryFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"%\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/ana/resources/FakeOrFact/features/LIWC/LIWC/liwc.dic'"
     ]
    }
   ],
   "source": [
    "from liwc_readDict import readDict\n",
    "\n",
    "liwc = readDict('/home/ana/resources/FakeOrFact/features/LIWC/LIWC/liwc.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'liwc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-fca405838e9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mliwc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'liwc' is not defined"
     ]
    }
   ],
   "source": [
    "categories = [c for (w,c) in liwc]\n",
    "set(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_dict = {}\n",
    "for (w, c) in liwc:\n",
    "    if c not in liwc_dict:\n",
    "        liwc_dict[c] = []\n",
    "    liwc_dict[c].append(w)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_dict['pronoun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_liwc_categories(tokens, category_words, relative=True):\n",
    "    category_cnt = 0\n",
    "    if not tokens:\n",
    "        return None\n",
    "    text_len = len(tokens)\n",
    "    for t in tokens:\n",
    "        for word in category_words:\n",
    "            if t==word or (word[-1]=='*' and t.startswith(word[:-1])) \\\n",
    "            or (t==word.split(\"'\")[0]):\n",
    "                category_cnt += 1\n",
    "                break # one token cannot belong to more than one word in the category\n",
    "    if relative:\n",
    "        return category_cnt/text_len\n",
    "    else:\n",
    "        return category_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __encode_liwc_categories(self, tokens, relative=True):\n",
    "        categories_cnt = [0 for c in self.liwc_categories]\n",
    "        if not tokens:\n",
    "            return None\n",
    "        text_len = len(tokens)\n",
    "        for i, category in enumerate(self.liwc_categories):\n",
    "            for t in tokens:\n",
    "                print(\"token\", t)\n",
    "                if t in self.liwc_words_for_categories[category]:\n",
    "                    categories_cnt[i] += 1\n",
    "            if relative:\n",
    "                categories_cnt[i] = categories_cnt[i]/text_len\n",
    "        return categories_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from functools import partial\n",
    "# for categ in ['negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']:#liwc_dict.keys():\n",
    "for categ in liwc_dict.keys():\n",
    "    if categ in writings_df.columns:\n",
    "        continue\n",
    "    print(\"Encoding for category %s...\" % categ)\n",
    "    writings_df[categ] = writings_df['all_tokens'].apply(partial(encode_liwc_categories, \n",
    "                                                                   category_words=liwc_dict[categ], \n",
    "                                                                   relative=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# categories for all words in vocabulary\n",
    "# vocabulary = pickle.load(open(\"all_vocab_clpsych_erisk_50000.pkl\", \"rb\"))\n",
    "liwc_words_for_categories = {}\n",
    "for categ in liwc_dict.keys():\n",
    "    liwc_words_for_categories[categ] = set()\n",
    "    for word in liwc_dict[categ]:\n",
    "        for t in vocabulary:\n",
    "            if t==word or (word[-1]=='*' and t.startswith(word[:-1])) \\\n",
    "                or (t==word.split(\"'\")[0]):\n",
    "                    liwc_words_for_categories[categ].add(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(liwc_words_for_categories, open(\"data/liwc_categories_for_vocabulary_erisk_clpsych_stop_40K.pkl\", \"wb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df.groupby('subject').mean()[['label', 'negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df[['label', 'negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']].groupby('label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df.groupby('subject').mean()[['label'] + categories].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare your hyperparameters search:\n",
    "tune_epochs=150\n",
    "config = {\n",
    "      \"algorithm\": \"bayes\",\n",
    "      \"parameters\": {\n",
    "          \"lstm_units\": {\"type\": \"integer\", \"min\": 32, \"max\": 300},\n",
    "          \"lstm_units_user\": {\"type\": \"integer\", \"min\": 5, \"max\": 40},\n",
    "          \"dense_bow_units\": {\"type\": \"integer\", \"min\": 5, \"max\": 35},\n",
    "          \"filters\": {\"type\": \"integer\", \"min\": 30, \"max\": 250},\n",
    "          \"kernel_size\": {\"type\": \"integer\", \"min\": 3, \"max\": 7},\n",
    "          \"dense_sentence_units\": {\"type\": \"integer\", \"min\": 0, \"max\": 10},\n",
    "          \"dense_user_units\": {\"type\": \"integer\", \"min\": 0, \"max\": 0},\n",
    "          \"bert_dense_units\": {\"type\": \"integer\", \"min\": 10, \"max\": 200},\n",
    "          \"bert_finetune_layers\": {\"type\": \"integer\", \"min\": 0, \"max\": 2},\n",
    "          \"lr\": {\"type\": \"float\", \"min\": 0.0005, \"max\": 0.2, \"scalingType\": \"loguniform\"},\n",
    "          \"l2_dense\": {\"type\": \"float\", \"min\": 0.0000001, \"max\": 0.1, \"scalingType\": \"loguniform\"},\n",
    "          \"l2_embeddings\": {\"type\": \"float\", \"min\": 0.00000001, \"max\": 0.2, \"scalingType\": \"loguniform\"},\n",
    "          \"l2_bert\": {\"type\": \"float\", \"min\": 0.00000001, \"max\": 0.2, \"scalingType\": \"loguniform\"},\n",
    "          \"dropout\": {\"type\": \"float\", \"min\": 0, \"max\": 0.5, \"scalingType\": \"uniform\"},\n",
    "          \"norm_momentum\": {\"type\": \"float\", \"min\": 0, \"max\": 0.99, \"scalingType\": \"uniform\"},\n",
    "          \"optimizer\": {\"type\": \"categorical\", \"values\": [\"\"]},#\"adam\", \"adagrad\", \"\"]},\n",
    "          \"batch_size\": {\"type\": \"integer\", \"min\": 5, \"max\": 128, \"scalingType\": \"loguniform\"},\n",
    "          \"positive_class_weight\": {\"type\": \"integer\", \"min\": 3, \"max\": 10},\n",
    "          \"trainable_embeddings\": {\"type\": \"discrete\", \"values\": [True, False]},\n",
    "          \"sample_seqs\": {\"type\": \"discrete\", \"values\": [True, False]},\n",
    "          \"bert_trainable\": {\"type\": \"discrete\", \"values\": [True, False]},\n",
    "          \"bert_pooling\": {\"type\": \"categorical\", \"values\": ['first', 'mean']},\n",
    "#           \"hierarchical\": {\"type\": \"discrete\", \"values\": [True, False]},\n",
    "          \"freeze_patience\": {\"type\": \"integer\", \"min\": 2, \"max\": tune_epochs+1},\n",
    "          \"lr_reduce_factor\": {\"type\": \"float\", \"min\": 0.0001, \"max\": 0.8},\n",
    "          \"scheduled_lr_reduce_factor\": {\"type\": \"float\", \"min\": 0.0001, \"max\": 0.8},\n",
    "          \"lr_reduce_patience\": {\"type\": \"integer\", \"min\": 2, \"max\": tune_epochs+1},\n",
    "          \"scheduled_lr_reduce_patience\": {\"type\": \"integer\", \"min\": 2, \"max\": tune_epochs+1},\n",
    "          \"early_stopping_patience\": {\"type\": \"integer\", \"min\": 2, \"max\": tune_epochs+1},\n",
    "          \"decay\": {\"type\": \"float\", \"min\": 0.00000001, \"max\": 0.5, \"scalingType\": \"loguniform\"},\n",
    "#           \"ignore_layers_values\": {\"type\": \"categorical\", \"values\": [\"attention\", \"batchnorm\", \"bert_layer\"]},\n",
    "          \"sampling_distr\": {\"type\": \"categorical\", \"values\": [\"exp\", \"uniform\"]},\n",
    "          \"posts_per_group\": {\"type\": \"integer\", \"min\": 10, \"max\": 100},\n",
    "          \"post_groups_per_user\": {\"type\": \"integer\", \"min\": 1, \"max\": 50},\n",
    "#           \"posts_per_user\": {\"type\": \"integer\", \"min\": 0, \"max\": 1000},\n",
    "          \"maxlen\": {\"type\": \"integer\", \"min\":64, \"max\": 512},\n",
    "      },\n",
    "      \"spec\": {\n",
    "          \"metric\": \"loss\",\n",
    "          \"objective\": \"minimize\",\n",
    "      },\n",
    "  }\n",
    "optimizer = Optimizer(config, api_key=\"eoBdVyznAhfg3bK9pZ58ZSXfv\")\n",
    "\n",
    "hyperparams_config = hyperparams\n",
    "\n",
    "for experiment in optimizer.get_experiments(project_name=\"mental\"):\n",
    "    experiment.add_tag(\"tune\")\n",
    "    experiment.add_tag(dataset_type)\n",
    "    \n",
    "    print(hyperparams_config)\n",
    "    \n",
    "\n",
    "    data_generator_train = DataGenerator(user_level_data, subjects_split, set_type='train',\n",
    "                                         seq_len=hyperparams_config[\"maxlen\"],\n",
    "                                                     sample_seqs=hyperparams_config['sample_seqs'],\n",
    "                                                     sampling_distr=hyperparams_config['sampling_distr'],\n",
    "                                                    posts_per_group=hyperparams_config['posts_per_group'],\n",
    "                                                    post_groups_per_user=1,#hyperparams_config['post_groups_per_user'],\n",
    "                                                    max_posts_per_user=hyperparams_config['posts_per_user'],\n",
    "                                    hierarchical=hyperparams['hierarchical'])\n",
    "    data_generator_valid = DataGenerator(user_level_data, subjects_split, set_type='valid',\n",
    "                                         seq_len=hyperparams_config[\"maxlen\"],\n",
    "                                         \n",
    "                                                    posts_per_group=hyperparams_config['posts_per_group'],\n",
    "                                                    post_groups_per_user=hyperparams_config['post_groups_per_user'],\n",
    "                                                    max_posts_per_user=None,\n",
    "                                                    sample_seqs=False, \n",
    "                                                     shuffle=False,\n",
    "                                                hierarchical=hyperparams['hierarchical'])\n",
    "    try:\n",
    "        if hyperparams['hierarchical']:\n",
    "            model = build_hierarchical_model(hyperparams_config, hyperparams_features, embedding_matrix, emotions, stopword_list,\n",
    "                            liwc_categories=[c for c in categories if c in writings_df.columns]\n",
    "        ,\n",
    "                           ignore_layer=hyperparams_config['ignore_layer'])\n",
    "        else:\n",
    "            model = build_model(hyperparams_config, hyperparams_features, embedding_matrix, emotions, stopword_list,\n",
    "                            liwc_categories=[c for c in categories if c in writings_df.columns]\n",
    "        ,\n",
    "                           ignore_layer=hyperparams_config['ignore_layer'])\n",
    "        model.summary()\n",
    "\n",
    "        freeze_layer = FreezeLayer(model, patience=hyperparams['freeze_patience'], set_to=not hyperparams['trainable_embeddings'])\n",
    "        weights_history = WeightsHistory()\n",
    "        lr_history = LRHistory()\n",
    "        reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=hyperparams['reduce_lr_factor'],\n",
    "                                  patience=hyperparams['reduce_lr_patience'], min_lr=0.000001, verbose=1)\n",
    "        lr_schedule = callbacks.LearningRateScheduler(lambda epoch, lr: \n",
    "                                                      lr if (epoch+1)%hyperparams['scheduled_reduce_lr_freq']!=0 else\n",
    "                                                      lr*hyperparams['scheduled_reduce_lr_factor'], verbose=1)\n",
    "\n",
    "        model, history = train_model(model, hyperparams, data_generator_train, data_generator_valid,\n",
    "                           epochs=tune_epochs,\n",
    "                          class_weight={0:1, 1:experiment.get_parameter('positive_class_weight')}, \n",
    "                                     start_epoch=0,\n",
    "                          callback_list = [\n",
    "    #                                  weights_history, \n",
    "                                           reduce_lr, \n",
    "    #                                        lr_history, \n",
    "                                           lr_schedule\n",
    "                                          ],\n",
    "                          model_path='models/experiment', workers=4)\n",
    "    except tf.errors.ResourceExhaustedError as e:\n",
    "        print(e)\n",
    "        sess.close()\n",
    "        sess = tf.Session(config=sess_config)\n",
    "        initialize_vars(sess)\n",
    "\n",
    "    \n",
    "    loss = history.history['loss'][-1]\n",
    "    \n",
    "    \n",
    "    # Test the model\n",
    "    for param in config['parameters'].keys():    \n",
    "        hyperparams_config[param] = experiment.get_parameter(param)\n",
    "    if not hyperparams_config['optimizer']:\n",
    "        hyperparams_config['optimizer'] = optimizers.Adam(lr=hyperparams_config['lr'], \n",
    "                                   decay=hyperparams_config['decay'])\n",
    "#     hyperparams_config[\"ignore_layer\"] = []\n",
    "#     if hyperparams_config[\"ignore_layers_values\"]:\n",
    "#         hyperparams_config[\"ignore_layer\"] = [hyperparams_config[\"ignore_layers_values\"]]\n",
    "    hyperparams_config[\"ignore_layer\"] = [\"bert_layer\", \"batchnorm\"]\n",
    "        \n",
    "#     freeze_layer = FreezeLayer(model, patience=experiment.get_parameter('freeze_patience'),\n",
    "#                               set_to=not experiment.get_parameter('trainable_embeddings'))\n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            factor=experiment.get_parameter('lr_reduce_factor'),\n",
    "                                            patience=experiment.get_parameter('lr_reduce_patience'), \n",
    "                                            min_lr=0.00000001, verbose=1)\n",
    "    \n",
    "    # Report the loss, if not auto-logged:\n",
    "    experiment.log_metric(\"loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2conda",
   "language": "python",
   "name": "tf2conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
