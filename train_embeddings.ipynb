{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from mittens import Mittens\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading GloVe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dim = 100\n",
    "glove_filepath = '/home/anasab/resources/glove.twitter.27B/glove.twitter.27B.%dd.txt' % embeddings_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove2dict(glove_filename):\n",
    "    with open(glove_filename, encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter=' ',quoting=csv.QUOTE_NONE)\n",
    "        embed = {line[0]: np.array(list(map(float, line[1:])))\n",
    "                for line in reader}\n",
    "    return embed\n",
    "\n",
    "pre_glove = glove2dict(glove_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading & preprocessing text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_clpsych = pd.DataFrame.from_dict(json.load(open('writings_df_clpsych_all.json')))\n",
    "writings_erisk_selfharm = pickle.load(open('writings_df_selfharm_liwc_subsets', 'rb'))\n",
    "writings_erisk_anorexia = pickle.load(open('writings_df_anorexia_liwc', 'rb'))\n",
    "writings_erisk_depression = pickle.load(open('writings_df_depression_liwc', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select just depressed/non depressed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_clpsych = writings_clpsych[~writings_clpsych['condition'].isin(['depression', 'ptsd'])]\n",
    "writings_erisk_selfharm = writings_erisk_selfharm[writings_erisk_selfharm['label']==0]\n",
    "writings_erisk_anorexia = writings_erisk_anorexia[writings_erisk_anorexia['label']==0]\n",
    "writings_erisk_depression = writings_erisk_depression[writings_erisk_depression['label']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_clpsych = \"\\n\".join(writings_clpsych.text.dropna().values)\n",
    "texts_erisk_selfharm = \"\\n\".join(writings_erisk_selfharm.dropna().text.values + writings_erisk_selfharm.dropna().title.values)\n",
    "texts_erisk_anorexia = \"\\n\".join(writings_erisk_anorexia.text.dropna().values + writings_erisk_anorexia.title.dropna().values)\n",
    "texts_erisk_depression = \"\\n\".join(writings_erisk_depression.text.dropna().values + writings_erisk_depression.title.dropna().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TweetTokenizer()\n",
    "sw = stopwords.words(\"english\")\n",
    "def tokenize_tweets(t, tokenizer=tt, stop=False):\n",
    "    tokens = tokenizer.tokenize(t.lower())\n",
    "    tokens_clean = [token for token in tokens if \n",
    "                            re.match(\"^[a-z]*$\", token)]\n",
    "    if not stop:\n",
    "        tokens_clean = [token for token in tokens_clean \n",
    "                        if token not in sw]\n",
    "    return tokens_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clpsych_texts_tokenized = tokenize_tweets(texts_clpsych, stop=True)\n",
    "erisk_depression_texts_tokenized = tokenize_tweets(texts_erisk_depression, stop=True)\n",
    "erisk_anorexia_texts_tokenized = tokenize_tweets(texts_erisk_anorexia, stop=True)\n",
    "erisk_selfharm_texts_tokenized = tokenize_tweets(texts_erisk_selfharm, stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_texts_tokenized_clean = [token.lower() for token in all_texts_tokenized if (token.lower() not in sw)\n",
    "#                             and re.match(\"^[a-z]*$\", token.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oov = [token for token in all_texts_tokenized_clean if token not in pre_glove.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rareoov(xdict, val):\n",
    "    return [k for (k,v) in Counter(xdict).items() if v<=val]\n",
    "oov_rare = get_rareoov(oov, 1)\n",
    "# corp_vocab = list(pre_glove.keys()) + \n",
    "corp_vocab = list(set(oov) - set(oov_rare))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clpsych_vocab_negative = Counter(clpsych_texts_tokenized).most_common()\n",
    "clpsych_vocab_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erisk_depression_vocab_negative = Counter(erisk_depression_texts_tokenized).most_common()\n",
    "erisk_depression_vocab_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erisk_anorexia_vocab_negative = Counter(erisk_anorexia_texts_tokenized).most_common()\n",
    "erisk_anorexia_vocab_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erisk_selfharm_vocab_negative = Counter(erisk_selfharm_texts_tokenized).most_common()\n",
    "erisk_selfharm_vocab_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Texts size: \\nclpsych %d, \\nerisk depression %d, \\nerisk anorexia %d, \\nerisk selfharm %d\\n\" % (\n",
    "    len(clpsych_texts_tokenized), len(erisk_depression_texts_tokenized), len(erisk_anorexia_texts_tokenized), len(erisk_selfharm_texts_tokenized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vocab size: \\nclpsych %d, \\nerisk depression %d, \\nerisk anorexia %d, \\nerisk selfharm %d\\n\" % (\n",
    "    len(clpsych_vocab_negative), len(erisk_depression_vocab_negative), len(erisk_anorexia_vocab_negative), \n",
    "    len(erisk_selfharm_vocab_negative)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = texts_clpsych + texts_erisk_depression + texts_erisk_anorexia + texts_erisk_selfharm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocab = Counter(clpsych_texts_tokenized + erisk_depression_texts_tokenized + \n",
    "                    erisk_anorexia_texts_tokenized + erisk_selfharm_texts_tokenized)\n",
    "len(all_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(all_texts, open(\"all_texts_clpsych_erisk_negative.pkl\", \"wb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(texts_erisk_selfharm, open(\"texts_erisk_selfharm.pkl\", \"wb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(all_vocab, open(\"all_vocab_clpsyck_erisk_negative_stop.pkl\", \"wb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocab_20000 = Counter(clpsych_texts_tokenized + erisk_depression_texts_tokenized + \n",
    "                    erisk_anorexia_texts_tokenized + erisk_selfharm_texts_tokenized).most_common(20000)\n",
    "pickle.dump(list(set([k for k,v in all_vocab_20000])), open(\"all_vocab_clpsych_erisk_negative_stop_20000.pkl\", \"wb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[k for k,v in all_vocab_10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clpsych_vocab_negative[30:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning with Mittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mittens import Mittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings = pickle.load(open(\"original_glove_clpsych_erisk_stop_20000.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mittens_embeddings = pickle.load(open(\"finetuned_glove_clpsych_erisk_20000.pkl\", \"rb\"))\n",
    "mittens_embeddings2 = pickle.load(open(\"finetuned_glove_clpsych_erisk_stop_20000_2.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mittens_embeddings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_to_csv(embeddings_dict, outfile_path=\"mittens_embeddings.tsv\",\n",
    "                      metadata_path=\"mittens_embeddings_meta.tsv\", separator=\"\\t\"):\n",
    "    words = []\n",
    "    with open(outfile_path, \"w+\") as f:\n",
    "        for key, vector in embeddings_dict.items():\n",
    "            if not metadata_path:\n",
    "                f.write(separator.join([key] + [str(n) for n in vector]) + \"\\n\")\n",
    "            else:\n",
    "                f.write(separator.join([str(n) for n in vector]) + \"\\n\")\n",
    "                words.append(key)\n",
    "    if metadata_path:\n",
    "        with open(metadata_path, \"w+\") as f:\n",
    "            for word in words:\n",
    "                f.write(word + \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_to_csv(glove_embeddings, \"glove_clpsych_erisk_stop_embeddings_20000.tsv\", \n",
    "                  \"glove_clpsych_erisk_stop_meta_20000.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_to_csv(mittens_embeddings2, \"mittens_clpsych_erisk_stop_embeddings_20000.tsv\", \n",
    "                  \"mittens_clpsych_erisk_stop_meta_20000.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pickle.load(open(\"vocab_clpsych_10000.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_matrix(embeddings_dict):\n",
    "    return np.array(list(embeddings_dict.values()))\n",
    "def get_embeddings_dict(embeddings_matrix, embeddings_keys):\n",
    "    embeddings_dict = {}\n",
    "    for i, k in enumerate(embeddings_keys):\n",
    "        embeddings_dict[k] = embeddings_matrix[i]\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_embeddings_dict(get_embeddings_matrix(glove_embeddings), glove_embeddings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embeddings(embedding_matrix):\n",
    "    emb_mean = np.mean(embedding_matrix,axis = 0)\n",
    "    emb_std = np.std(embedding_matrix, axis = 0)\n",
    "    return (embedding_matrix-emb_mean)/emb_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.array(list(glove_embeddings.values())).mean(axis=1)).hist(alpha=0.5, bins=50)\n",
    "pd.Series(normalize_embeddings(np.array(list(glove_embeddings.values()))).mean(axis=1)).hist(alpha=0.5, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.array(list(mittens_embeddings.values())).mean(axis=1)).hist(alpha=0.5, bins=50)\n",
    "pd.Series(normalize_embeddings(np.array(list(mittens_embeddings.values()))).mean(axis=1)).hist(alpha=0.5, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.array(list(mittens_embeddings2.values())).mean(axis=1)).hist(alpha=0.5, bins=50)\n",
    "pd.Series(normalize_embeddings(np.array(list(mittens_embeddings2.values()))).mean(axis=1)).hist(alpha=0.5, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings_normalized = get_embeddings_dict(normalize_embeddings(\n",
    "    get_embeddings_matrix(glove_embeddings)), glove_embeddings.keys())\n",
    "glove_embeddings_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mittens_embeddings_normalized = get_embeddings_dict(normalize_embeddings(\n",
    "    get_embeddings_matrix(mittens_embeddings)), mittens_embeddings.keys())\n",
    "mittens_embeddings_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mittens_embeddings2_normalized = get_embeddings_dict(normalize_embeddings(\n",
    "    get_embeddings_matrix(mittens_embeddings)), mittens_embeddings2.keys())\n",
    "mittens_embeddings2_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.array(list(mittens_embeddings.values())).mean(axis=1)).hist(alpha=0.5, bins=50)\n",
    "pd.Series(np.array(list(mittens_embeddings_normalized.values())).mean(axis=1)).hist(alpha=0.5, bins=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.array(list(glove_embeddings.values())).mean(axis=1)).hist(alpha=0.5, bins=50)\n",
    "pd.Series(np.array(list(glove_embeddings_normalized.values())).mean(axis=1)).hist(alpha=0.5, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(glove_embeddings_normalized, open(\"original_glove_clpsych_erisk_normalized_20000.pkl\", \"wb+\"))\n",
    "pickle.dump(mittens_embeddings_normalized, open(\"finetuned_glove_clpsych_erisk_normalized_20000.pkl\", \"wb+\"))\n",
    "pickle.dump(mittens_embeddings2_normalized, open(\"finetuned_glove_clpsych_erisk_normalized_2_20000.pkl\", \"wb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_to_csv(glove_embeddings_normalized, \"glove_clpsych_erisk_normalized_embeddings_20000.tsv\", \n",
    "                  \"glove_clpsych_erisk_normalized_meta_20000.tsv\")\n",
    "embeddings_to_csv(mittens_embeddings_normalized, \"mittens_clpsych_erisk_normalized_embeddings_20000.tsv\", \n",
    "                  \"mittens_clpsych_erisk_normalized_meta_20000.tsv\")\n",
    "embeddings_to_csv(mittens_embeddings2_normalized, \"mittens_clpsych_erisk2_normalized_embeddings_20000.tsv\", \n",
    "                  \"mittens2]_clpsych_erisk2_normalized_meta_20000.tsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['i' in mittens_embeddings2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
