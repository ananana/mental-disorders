{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from mittens import Mittens\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading GloVe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dim = 100\n",
    "glove_filepath = '/home/anasab/resources/glove.twitter.27B/glove.twitter.27B.%dd.txt' % embeddings_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove2dict(glove_filename):\n",
    "    with open(glove_filename, encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter=' ',quoting=csv.QUOTE_NONE)\n",
    "        embed = {line[0]: np.array(list(map(float, line[1:])))\n",
    "                for line in reader}\n",
    "    return embed\n",
    "\n",
    "pre_glove = glove2dict(glove_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading & preprocessing text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_clpsych = pd.DataFrame.from_dict(json.load(open('data/writings_df_clpsych_all.json')))\n",
    "writings_erisk_selfharm = pickle.load(open('data/writings_df_selfharm_liwc_subsets', 'rb'))\n",
    "writings_erisk_anorexia = pickle.load(open('data/writings_df_anorexia_liwc', 'rb'))\n",
    "writings_erisk_depression = pickle.load(open('data/writings_df_depression_liwc', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select just depressed/non depressed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_clpsych = writings_clpsych[~writings_clpsych['condition'].isin(['depression', 'ptsd'])]\n",
    "writings_erisk_selfharm = writings_erisk_selfharm[writings_erisk_selfharm['label']==0]\n",
    "writings_erisk_anorexia = writings_erisk_anorexia[writings_erisk_anorexia['label']==0]\n",
    "writings_erisk_depression = writings_erisk_depression[writings_erisk_depression['label']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_clpsych_grouped = writings_clpsych.groupby('subject').aggregate(lambda l: \" \".join(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_clpsych_grouped.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_clpsych = \"\\n\".join(writings_clpsych.text.dropna().values)\n",
    "texts_erisk_selfharm = \"\\n\".join(writings_erisk_selfharm.dropna().text.values + writings_erisk_selfharm.dropna().title.values)\n",
    "texts_erisk_anorexia = \"\\n\".join(writings_erisk_anorexia.text.dropna().values + writings_erisk_anorexia.title.dropna().values)\n",
    "texts_erisk_depression = \"\\n\".join(writings_erisk_depression.text.dropna().values + writings_erisk_depression.title.dropna().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TweetTokenizer()\n",
    "sw = stopwords.words(\"english\")\n",
    "def tokenize_tweets(t, tokenizer=tt, stop=False):\n",
    "    tokens = tokenizer.tokenize(t.lower())\n",
    "    tokens_clean = [token for token in tokens if \n",
    "                            re.match(\"^[a-z]*$\", token)]\n",
    "    if not stop:\n",
    "        tokens_clean = [token for token in tokens_clean \n",
    "                        if token not in sw]\n",
    "    return tokens_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clpsych_texts_tokenized = tokenize_tweets(texts_clpsych, stop=True)\n",
    "erisk_depression_texts_tokenized = tokenize_tweets(texts_erisk_depression, stop=True)\n",
    "erisk_anorexia_texts_tokenized = tokenize_tweets(texts_erisk_anorexia, stop=True)\n",
    "erisk_selfharm_texts_tokenized = tokenize_tweets(texts_erisk_selfharm, stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_texts_tokenized_clean = [token.lower() for token in all_texts_tokenized if (token.lower() not in sw)\n",
    "#                             and re.match(\"^[a-z]*$\", token.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oov = [token for token in all_texts_tokenized_clean if token not in pre_glove.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rareoov(xdict, val):\n",
    "    return [k for (k,v) in Counter(xdict).items() if v<=val]\n",
    "oov_rare = get_rareoov(oov, 1)\n",
    "# corp_vocab = list(pre_glove.keys()) + \n",
    "corp_vocab = list(set(oov) - set(oov_rare))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clpsych_vocab = Counter(clpsych_texts_tokenized).most_common()\n",
    "clpsych_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erisk_depression_vocab = Counter(erisk_depression_texts_tokenized).most_common()\n",
    "erisk_depression_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erisk_anorexia_vocab = Counter(erisk_anorexia_texts_tokenized).most_common()\n",
    "erisk_anorexia_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erisk_selfharm_vocab = Counter(erisk_selfharm_texts_tokenized).most_common()\n",
    "erisk_selfharm_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Texts size: \\nclpsych %d, \\nerisk depression %d, \\nerisk anorexia %d, \\nerisk selfharm %d\\n\" % (\n",
    "    len(clpsych_texts_tokenized), len(erisk_depression_texts_tokenized), len(erisk_anorexia_texts_tokenized), len(erisk_selfharm_texts_tokenized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vocab size: \\nclpsych %d, \\nerisk depression %d, \\nerisk anorexia %d, \\nerisk selfharm %d\\n\" % (\n",
    "    len(clpsych_vocab), len(erisk_depression_vocab), len(erisk_anorexia_vocab), \n",
    "    len(erisk_selfharm_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = texts_clpsych + texts_erisk_depression + texts_erisk_anorexia + texts_erisk_selfharm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocab = Counter(clpsych_texts_tokenized + erisk_depression_texts_tokenized + \n",
    "                    erisk_anorexia_texts_tokenized + erisk_selfharm_texts_tokenized)\n",
    "len(all_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(all_texts, open(\"all_texts_clpsych_erisk.pkl\", \"wb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(texts_erisk_selfharm, open(\"texts_erisk_selfharm.pkl\", \"wb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(all_vocab, open(\"all_vocab_clpsyck_erisk_stop.pkl\", \"wb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocab_40000 = Counter(clpsych_texts_tokenized + erisk_depression_texts_tokenized + \n",
    "                    erisk_anorexia_texts_tokenized + erisk_selfharm_texts_tokenized).most_common(40000)\n",
    "pickle.dump(list(set([k for k,v in all_vocab_40000])), open(\"all_vocab_clpsych_erisk_stop_40000.pkl\", \"wb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[k for k,v in all_vocab_10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clpsych_vocab_negative[30:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning with Mittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mittens import Mittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings = pickle.load(open(\"original_glove_clpsych_erisk_stop_20000.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mittens_embeddings = pickle.load(open(\"finetuned_glove_clpsych_erisk_20000.pkl\", \"rb\"))\n",
    "mittens_embeddings = pickle.load(open(\"embeddings/finetuned_glove_clpsych_erisk_stop_20000_2.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mittens_embeddings['me']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_to_csv(embeddings_dict, outfile_path=\"mittens_embeddings.tsv\",\n",
    "                      metadata_path=\"mittens_embeddings_meta.tsv\", separator=\"\\t\"):\n",
    "    words = []\n",
    "    with open(outfile_path, \"w+\") as f:\n",
    "        for key, vector in embeddings_dict.items():\n",
    "            if not metadata_path:\n",
    "                f.write(separator.join([key] + [str(n) for n in vector]) + \"\\n\")\n",
    "            else:\n",
    "                f.write(separator.join([str(n) for n in vector]) + \"\\n\")\n",
    "                words.append(key)\n",
    "    if metadata_path:\n",
    "        with open(metadata_path, \"w+\") as f:\n",
    "            for word in words:\n",
    "                f.write(word + \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_to_csv(glove_embeddings, \"glove_clpsych_erisk_stop_embeddings_20000.tsv\", \n",
    "                  \"glove_clpsych_erisk_stop_meta_20000.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_to_csv(mittens_embeddings, \"mittens_clpsych_erisk_stop_positive_embeddings_20000.tsv\", \n",
    "                  \"mittens_clpsych_erisk_stop_positive_meta_20000.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pickle.load(open(\"vocab_clpsych_10000.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_matrix(embeddings_dict):\n",
    "    return np.array(list(embeddings_dict.values()))\n",
    "def get_embeddings_dict(embeddings_matrix, embeddings_keys):\n",
    "    embeddings_dict = {}\n",
    "    for i, k in enumerate(embeddings_keys):\n",
    "        embeddings_dict[k] = embeddings_matrix[i]\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_embeddings_dict(get_embeddings_matrix(glove_embeddings), glove_embeddings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embeddings(embedding_matrix):\n",
    "    emb_mean = np.mean(embedding_matrix,axis = 0)\n",
    "    emb_std = np.std(embedding_matrix, axis = 0)\n",
    "    return (embedding_matrix-emb_mean)/emb_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.array(list(glove_embeddings.values())).mean(axis=1)).hist(alpha=0.5, bins=50)\n",
    "pd.Series(normalize_embeddings(np.array(list(glove_embeddings.values()))).mean(axis=1)).hist(alpha=0.5, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.array(list(mittens_embeddings.values())).mean(axis=1)).hist(alpha=0.5, bins=50)\n",
    "pd.Series(normalize_embeddings(np.array(list(mittens_embeddings.values()))).mean(axis=1)).hist(alpha=0.5, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.array(list(mittens_embeddings2.values())).mean(axis=1)).hist(alpha=0.5, bins=50)\n",
    "pd.Series(normalize_embeddings(np.array(list(mittens_embeddings2.values()))).mean(axis=1)).hist(alpha=0.5, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings_normalized = get_embeddings_dict(normalize_embeddings(\n",
    "    get_embeddings_matrix(glove_embeddings)), glove_embeddings.keys())\n",
    "glove_embeddings_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mittens_embeddings_normalized = get_embeddings_dict(normalize_embeddings(\n",
    "    get_embeddings_matrix(mittens_embeddings)), mittens_embeddings.keys())\n",
    "mittens_embeddings_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mittens_embeddings2_normalized = get_embeddings_dict(normalize_embeddings(\n",
    "    get_embeddings_matrix(mittens_embeddings)), mittens_embeddings2.keys())\n",
    "mittens_embeddings2_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.array(list(mittens_embeddings.values())).mean(axis=1)).hist(alpha=0.5, bins=50)\n",
    "pd.Series(np.array(list(mittens_embeddings_normalized.values())).mean(axis=1)).hist(alpha=0.5, bins=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.array(list(glove_embeddings.values())).mean(axis=1)).hist(alpha=0.5, bins=50)\n",
    "pd.Series(np.array(list(glove_embeddings_normalized.values())).mean(axis=1)).hist(alpha=0.5, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(glove_embeddings_normalized, open(\"original_glove_clpsych_erisk_normalized_20000.pkl\", \"wb+\"))\n",
    "pickle.dump(mittens_embeddings_normalized, open(\"finetuned_glove_clpsych_erisk_stop_normalized_20000.pkl\", \"wb+\"))\n",
    "# pickle.dump(mittens_embeddings2_normalized, open(\"finetuned_glove_clpsych_erisk_normalized_2_20000.pkl\", \"wb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_to_csv(glove_embeddings_normalized, \"glove_clpsych_erisk_normalized_embeddings_20000.tsv\", \n",
    "                  \"glove_clpsych_erisk_normalized_meta_20000.tsv\")\n",
    "embeddings_to_csv(mittens_embeddings_normalized, \"mittens_clpsych_erisk_normalized_embeddings_20000.tsv\", \n",
    "                  \"mittens_clpsych_erisk_normalized_meta_20000.tsv\")\n",
    "embeddings_to_csv(mittens_embeddings2_normalized, \"mittens_clpsych_erisk2_normalized_embeddings_20000.tsv\", \n",
    "                  \"mittens2]_clpsych_erisk2_normalized_meta_20000.tsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in mittens_embeddings2.keys() if w in stopwords.words(\"english\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sorted(np.array(list(mittens_embeddings_normalized.values())).mean(axis=1).tolist(), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings comparison - feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_positive = pickle.load(open(\"finetuned_glove_clpsych_erisk_positive_stop_20000.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_negative = pickle.load(open(\"finetuned_glove_clpsych_erisk_negative_stop_20000.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_word2vec_format(embeddings, filepath, voc=20000, size=100, sep=\" \"):\n",
    "    with open(filepath, \"w+\") as f:\n",
    "        f.write(sep.join([str(voc), str(size)]))\n",
    "        for key, vector in embeddings.items():\n",
    "            f.write(\"\\n\")\n",
    "            f.write(key + sep + sep.join([str(n) for n in vector]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_word2vec_format(embeddings_positive, \"finetuned_clpsych_erisk_positive_stop_20000.wv\")\n",
    "write_word2vec_format(embeddings_negative, \"finetuned_clpsych_erisk_negative_stop_20000.wv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_positive = KeyedVectors.load_word2vec_format(\"finetuned_clpsych_erisk_positive_stop_20000.wv\", binary=False)\n",
    "model_negative = KeyedVectors.load_word2vec_format(\"finetuned_clpsych_erisk_negative_stop_20000.wv\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_positive.most_similar(\"rituals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_negative.most_similar(\"rituals\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_words_uneven = set()\n",
    "head_words_even = set()\n",
    "neighbors_uneven = []\n",
    "overlap_thresh = 0.1\n",
    "overlap_thresh_up = 0.75\n",
    "nr_neighbors = 100\n",
    "overlaps = []\n",
    "\n",
    "for word in set(embeddings_positive.keys()).intersection(embeddings_negative.keys()):\n",
    "    neighbors_positive = model_positive.most_similar(word, topn=nr_neighbors)    \n",
    "    neighbors_negative = model_negative.most_similar(word, topn=nr_neighbors)\n",
    "    overlap = len(\n",
    "        set([k for k,v in neighbors_positive]).intersection(\n",
    "        set([k for k,v in neighbors_negative]))\n",
    "    )\n",
    "    overlaps.append(overlap)\n",
    "    if overlap < overlap_thresh*nr_neighbors:\n",
    "        head_words_uneven.add(word)\n",
    "    if overlap > overlap_thresh_up*nr_neighbors:\n",
    "        head_words_even.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(overlaps).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(head_words_even)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(head_words_even).intersection(common_words)\n",
    "pickle.dump(set(head_words_even).intersection(common_words), \n",
    "            open(\"common_words_even_neighbors_overlap.pkl\", \"wb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(head_words_uneven).intersection(common_words)\n",
    "pickle.dump(set(head_words_uneven).intersection(common_words), \n",
    "            open(\"common_words_uneven_neighbors_overlap.pkl\", \"wb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(head_words_uneven).intersection(common_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "def plot_scatter(word, topn):\n",
    "    most_similar_negative = [(k, v) for k, v in model_negative.most_similar(word, topn=topn)\n",
    "                             if k in model_positive]\n",
    "    plt.scatter(        \n",
    "        [model_negative.distance(word, k) for k,v in most_similar_negative],\n",
    "        [model_positive.distance(word, k) for k,v in most_similar_negative]\n",
    "    )\n",
    "def get_correlation(word, topn, func=pearsonr):\n",
    "    if word not in model_positive or word not in model_negative:\n",
    "        return\n",
    "    most_similar_negative = [(k, v) for k, v in model_negative.most_similar(word, topn=topn)\n",
    "                             if k in model_positive]\n",
    "    return func(\n",
    "        [model_negative.distance(word, k) for k,v in most_similar_negative] ,       \n",
    "        [model_positive.distance(word, k) for k,v in most_similar_negative]\n",
    "    )[0]\n",
    "plot_scatter(\"rituals\", 1000)\n",
    "get_correlation(\"rituals\", 1000, pearsonr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_vocab = {\"sad\", \"happy\", \"depressed\", \"me\", \"you\", \"i\", \"health\"}\n",
    "for topn in range(10, 1500, 50):\n",
    "    corrs = []\n",
    "    for w in seed_vocab:\n",
    "        corr = get_correlation(w, topn)\n",
    "        corrs.append(corr)\n",
    "    print(topn, sum(corrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = {}\n",
    "for word in model_negative.vocab:\n",
    "    correlations[word] = get_correlation(word, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_anticorrelated = sorted([(w,c) for w,c in\n",
    "    correlations.items()\n",
    "       if w in common_words\n",
    "       and w not in head_words_uneven],\n",
    "        key=lambda t: t[1] if t[1] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_correlated = sorted([(w,c) for w,c in\n",
    "    correlations.items()\n",
    "       if w in common_words],\n",
    "        key=lambda t: t[1] if t[1] else 0, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_anticorrelated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([k for k,v in sorted_correlated], open(\"common_words_uneven_neighbors_correlated2.pkl\", \"wb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from liwc_readDict import readDict\n",
    "\n",
    "liwc = readDict('/home/anasab/resources/liwc.dic')\n",
    "categories = [c for (w,c) in liwc]\n",
    "set(categories)\n",
    "liwc_dict = {}\n",
    "for (w, c) in liwc:\n",
    "    if c not in liwc_dict:\n",
    "        liwc_dict[c] = []\n",
    "    liwc_dict[c].append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocab = pickle.load(open(\"all_vocab_clpsyck_erisk.pkl\", \"rb\"))\n",
    "common_words = set([k for k,v in all_vocab.most_common(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
