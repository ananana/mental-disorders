{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from mittens import Mittens\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading GloVe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dim = 100\n",
    "glove_filepath = '/home/anasab/resources/glove.twitter.27B/glove.twitter.27B.%dd.txt' % embeddings_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove2dict(glove_filename):\n",
    "    with open(glove_filename, encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter=' ',quoting=csv.QUOTE_NONE)\n",
    "        embed = {line[0]: np.array(list(map(float, line[1:])))\n",
    "                for line in reader}\n",
    "    return embed\n",
    "\n",
    "pre_glove = glove2dict(glove_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading & preprocessing text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_clpsych = pd.DataFrame.from_dict(json.load(open('writings_df_clpsych_all.json')))\n",
    "writings_erisk_selfharm = pickle.load(open('writings_df_selfharm_liwc_subsets', 'rb'))\n",
    "writings_erisk_anorexia = pickle.load(open('writings_df_anorexia_liwc', 'rb'))\n",
    "writings_erisk_depression = pickle.load(open('writings_df_depression_liwc', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_clpsych.text = writings_clpsych.text.dropna()\n",
    "# writings_erisk_depression.text = writings_erisk_depression.text.dropna()\n",
    "# writings_erisk_anorexia.text = writings_erisk_anorexia.text.dropna()\n",
    "# writings_erisk_selfharm.text = writings_erisk_selfharm.text.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_clpsych = \"\\n\".join(writings_clpsych.text.dropna().values)\n",
    "texts_erisk_selfharm = \"\\n\".join(writings_erisk_selfharm.dropna().text.values + writings_erisk_selfharm.dropna().title.values)\n",
    "texts_erisk_anorexia = \"\\n\".join(writings_erisk_anorexia.text.dropna().values + writings_erisk_anorexia.title.dropna().values)\n",
    "texts_erisk_depression = \"\\n\".join(writings_erisk_depression.text.dropna().values + writings_erisk_depression.title.dropna().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TweetTokenizer()\n",
    "sw = stopwords.words(\"english\")\n",
    "def tokenize_tweets(t, tokenizer=tt):\n",
    "    tokens = tokenizer.tokenize(t.lower())\n",
    "    tokens_clean = [token for token in tokens if (token not in sw)\n",
    "                            and re.match(\"^[a-z]*$\", token)]\n",
    "    return tokens_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clpsych_texts_tokenized = tokenize_tweets(texts_clpsych)\n",
    "erisk_depression_texts_tokenized = tokenize_tweets(texts_erisk_depression)\n",
    "erisk_anorexia_texts_tokenized = tokenize_tweets(texts_erisk_anorexia)\n",
    "erisk_selfharm_texts_tokenized = tokenize_tweets(texts_erisk_selfharm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_texts_tokenized_clean = [token.lower() for token in all_texts_tokenized if (token.lower() not in sw)\n",
    "#                             and re.match(\"^[a-z]*$\", token.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oov = [token for token in all_texts_tokenized_clean if token not in pre_glove.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rareoov(xdict, val):\n",
    "    return [k for (k,v) in Counter(xdict).items() if v<=val]\n",
    "oov_rare = get_rareoov(oov, 1)\n",
    "# corp_vocab = list(pre_glove.keys()) + \n",
    "corp_vocab = list(set(oov) - set(oov_rare))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clpsych_vocab = Counter(clpsych_texts_tokenized).most_common()\n",
    "clpsych_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erisk_depression_vocab = Counter(erisk_depression_texts_tokenized).most_common()\n",
    "erisk_depression_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erisk_anorexia_vocab = Counter(erisk_anorexia_texts_tokenized).most_common()\n",
    "erisk_anorexia_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erisk_selfharm_vocab = Counter(erisk_selfharm_texts_tokenized).most_common()\n",
    "erisk_selfharm_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Texts size: \\nclpsych %d, \\nerisk depression %d, \\nerisk anorexia %d, \\nerisk selfharm %d\\n\" % (\n",
    "    len(clpsych_texts_tokenized), len(erisk_depression_texts_tokenized), len(erisk_anorexia_texts_tokenized), len(erisk_selfharm_texts_tokenized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vocab size: \\nclpsych %d, \\nerisk depression %d, \\nerisk anorexia %d, \\nerisk selfharm %d\\n\" % (\n",
    "    len(clpsych_vocab), len(erisk_depression_vocab), len(erisk_anorexia_vocab), len(erisk_selfharm_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = texts_clpsych + texts_erisk_depression + texts_erisk_anorexia + texts_erisk_selfharm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocab = Counter(clpsych_texts_tokenized + erisk_depression_texts_tokenized + \n",
    "                    erisk_anorexia_texts_tokenized + erisk_selfharm_texts_tokenized)\n",
    "len(all_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(all_texts, open(\"all_texts_clpsych_erisk.pkl\", \"wb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(texts_erisk_selfharm, open(\"texts_erisk_selfharm.pkl\", \"wb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(all_vocab, open(\"all_vocab_clpsyck_erisk.pkl\", \"wb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocab_10000 = Counter(clpsych_texts_tokenized + erisk_depression_texts_tokenized + \n",
    "                    erisk_anorexia_texts_tokenized + erisk_selfharm_texts_tokenized).most_common(10000)\n",
    "pickle.dump(list(set([k for k,v in all_vocab_10000])), open(\"all_vocab_clpsych_erisk_10000.pkl\", \"wb+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[k for k,v in all_vocab_10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning with Mittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,1), vocabulary=corp_vocab)\n",
    "X = cv.fit_transform([all_texts])\n",
    "Xc = (X.T * X)\n",
    "Xc.setdiag(0)\n",
    "coocc_ar = Xc.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mittens_model = Mittens(n=50, max_iter=1000)\n",
    "new_embeddings = mittens_model.fit(\n",
    "    coocc_ar,\n",
    "    vocab=corp_vocab,\n",
    "    initial_embedding_dict= pre_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mittens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mittens import Mittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mittens_embeddings = pickle.load(open(\"finetuned_glove_clpsych.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mittens_embeddings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_to_csv(embeddings_dict, outfile_path=\"mittens_embeddings.tsv\",\n",
    "                      metadata_path=\"mittens_embeddings_meta.tsv\", separator=\"\\t\"):\n",
    "    words = []\n",
    "    with open(outfile_path, \"w+\") as f:\n",
    "        for key, vector in embeddings_dict.items():\n",
    "            if not metadata_path:\n",
    "                f.write(separator.join([key] + [str(n) for n in vector]) + \"\\n\")\n",
    "            else:\n",
    "                f.write(separator.join([str(n) for n in vector]) + \"\\n\")\n",
    "                words.append(key)\n",
    "    if metadata_path:\n",
    "        with open(metadata_path, \"w+\") as f:\n",
    "            for word in words:\n",
    "                f.write(word + \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_to_csv(mittens_embeddings, \"mittens_clpsych_embeddings.tsv\", \"mittens_clpsych_meta.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pickle.load(open(\"vocab_clpsych_10000.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mittens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
