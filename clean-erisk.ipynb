{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import xml.etree.ElementTree as ET\n",
    "import glob, os\n",
    "# import numpy as np\n",
    "from comet_ml import Experiment, Optimizer\n",
    "import pickle\n",
    "# import logging\n",
    "# import sys\n",
    "# from sklearn.utils import class_weight\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer, TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "# import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # When cudnn implementation not found, run this\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Note: when starting kernel, for gpu_available to be true, this needs to be run\n",
    "# only reserve 1 GPU\n",
    "os.environ['TFHUB_CACHE_DIR'] = '/home/anasab/tf_cache'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH']='true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = \"selfharm\"\n",
    "transfer_type = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eRisk 2020 T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/anasab/' \n",
    "\n",
    "datadirs_T1_2020 = {\n",
    "    'train': ['./data/'],\n",
    "    'test': ['./DATA/']\n",
    "}\n",
    "datadir_root_T1_2020 = {\n",
    "    'train': root_dir + '/eRisk/data/eRisk2020_T1_train/eRISK2020_T1_training_data/eRISK2020_training_data/',\n",
    "    'test': root_dir + '/eRisk/data/2020/T1/'\n",
    "}\n",
    "    \n",
    "labels_files_T1_2020 = {\n",
    "    'train': ['golden_truth.txt'],\n",
    "    'test': ['T1_erisk_golden_truth.txt']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_erisk_data import read_texts_2020\n",
    "if dataset_type == 'selfharm':\n",
    "#     writings_df = read_texts_2020(datadir_root_T1_2020,\n",
    "#                    datadirs_T1_2020,\n",
    "#                    labels_files_T1_2020,\n",
    "#                    test_suffix='0000',\n",
    "#                     chunked_subsets=None)\n",
    "    writings_df = pickle.load(open('data/writings_df_%s_all' % dataset_type, 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regtokenizer = RegexpTokenizer(r'\\w+')\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "def tokenize(t, tokenizer=regtokenizer):\n",
    "    return regtokenizer.tokenize(t.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_encoders import encode_pronouns, encode_emotions, tokenize_fields, encode_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df = tokenize_fields(writings_df, tokenize_fct=tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features and encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_features = json.load(open('config.json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if transfer_type:\n",
    "    pretrained_model_path = hyperparams_features['pretrained_model_path']\n",
    "    hyperparams, hyperparams_features = load_params(hyperparams_features['pretrained_model_path'])\n",
    "    hyperparams_features['pretrained_model_path'] = pretrained_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resource_loading import load_NRC, load_LIWC\n",
    "\n",
    "nrc_lexicon_path = root_dir + '/resources/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'\n",
    "nrc_lexicon = load_NRC(nrc_lexicon_path)\n",
    "emotions = list(nrc_lexicon.keys())\n",
    "liwc_dict = load_LIWC(root_dir + '/resources/liwc.dic')\n",
    "\n",
    "liwc_categories = set(liwc_dict.keys())\n",
    "liwc_words_for_categories = pickle.load(open(\"data/liwc_categories_for_vocabulary_erisk_clpsych_stop_20K.pkl\", \"rb\"))\n",
    "\n",
    "stopword_list = stopwords.words(\"english\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loading import load_erisk_data\n",
    "from resource_loading import load_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(writings_df['subset'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_level_data, subjects_split, vocabulary = load_erisk_data(writings_df, \n",
    "                                                           hyperparams_features=hyperparams_features,\n",
    "                                                                                logger=None,\n",
    "                                                              by_subset=True,\n",
    "                                                                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataGenerator import DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import build_hierarchical_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = DataGenerator(user_level_data=user_level_data, subjects_split=subjects_split, set_type='train',\n",
    "                 batch_size=32, seq_len=512, hyperparams_features=hyperparams_features,\n",
    "                 post_groups_per_user=None, posts_per_group=10, post_offset = 0,\n",
    "                 pronouns=[\"i\", \"me\", \"my\", \"mine\", \"myself\"], \n",
    "                 compute_liwc=False, \n",
    "                 max_posts_per_user=None,\n",
    "                 shuffle=True, keep_last_batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subjects_split['valid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import build_hierarchical_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = json.load(open('hyperparams.json'))\n",
    "hyperparams['optimizer'] = optimizers.Adam(lr=hyperparams['lr'], #beta_1=0.9, beta_2=0.999, epsilon=0.0001,\n",
    "                                   decay=hyperparams['decay'])\n",
    "    \n",
    "if transfer_type:\n",
    "#     hyperparams, _ = load_params(hyperparams_features['pretrained_model_path'])\n",
    "    if 'optimizer' not in hyperparams:\n",
    "        hyperparams['optimizer'] = optimizers.Adam(lr=hyperparams['lr'], #beta_1=0.9, beta_2=0.999, epsilon=0.0001,\n",
    "                                       decay=hyperparams['decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "from train import initialize_experiment, train\n",
    "\n",
    "# experiment = initialize_experiment(hyperparams, nrc_lexicon_path, emotions, hyperparams_features['embeddings_path'], \n",
    "#                       dataset_type, transfer_type, hyperparams_features)\n",
    "# models, history = train(user_level_data, subjects_split, \n",
    "#           hyperparams=hyperparams, hyperparams_features=hyperparams_features, \n",
    "#               experiment=experiment, dataset_type=dataset_type, transfer_type=transfer_type,stopwords_dim=len(stopword_list),\n",
    "#               validation_set='valid',\n",
    "#           version=102, epochs=25, start_epoch=0\n",
    "#\n",
    "#                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_save_model import load_saved_model_weights, load_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_features['pretrained_model_path']='models/lstm_selfharm_hierarchical113'\n",
    "# hyperparams_features['pretrained_model_path']='models/lstm_selfharm_hierarchical107'\n",
    "hyperparams_features['pretrained_model_path']='models/lstm_selfharm_hierarchical117'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams, _ = load_params(hyperparams_features['pretrained_model_path'])\n",
    "hyperparams['optimizer'] = optimizers.Adam(lr=hyperparams['lr'], #beta_1=0.9, beta_2=0.999, epsilon=0.0001,\n",
    "                                   decay=hyperparams['decay'])\n",
    "model = load_saved_model_weights(hyperparams_features['pretrained_model_path'], \n",
    "                                                      hyperparams, hyperparams_features, \n",
    "                                                      h5=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = DataGenerator(user_level_data=user_level_data, subjects_split=subjects_split, set_type='train',\n",
    "                 batch_size=32, seq_len=256, hyperparams_features=hyperparams_features,\n",
    "                 post_groups_per_user=None, posts_per_group=hyperparams['posts_per_group'], post_offset = 0,\n",
    "                 pronouns=[\"i\", \"me\", \"my\", \"mine\", \"myself\"], \n",
    "                 compute_liwc=False, \n",
    "                 max_posts_per_user=None,\n",
    "                 shuffle=True, keep_last_batch=True)\n",
    "# predictions = model.predict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from evaluate_subjects import evaluate_for_subjects\n",
    "# evaluate_for_subjects(model, subjects_split['test'], \n",
    "#                       user_level_data, hyperparams, hyperparams_features, rolling_window=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from feature_encoders import encode_liwc_categories\n",
    "def load_erisk_server_data(datarounds_json, tokenizer,\n",
    "                    pronouns = [\"i\", \"me\", \"my\", \"mine\", \"myself\"],\n",
    "                   logger=None):\n",
    "\n",
    "    subjects_split = {'test': []}\n",
    "    user_level_texts = {}\n",
    "    for datapoint in datarounds_json:\n",
    "#         for datapoint in datapoints_json:\n",
    "            words = []\n",
    "            raw_text = \"\"\n",
    "            if \"title\" in datapoint:\n",
    "                tokenized_title = tokenizer.tokenize(datapoint[\"title\"])\n",
    "                words.extend(tokenized_title)\n",
    "                raw_text += datapoint[\"title\"]\n",
    "            if \"content\" in datapoint:\n",
    "                tokenized_text = tokenizer.tokenize(datapoint[\"content\"])\n",
    "                words.extend(tokenized_text)\n",
    "                raw_text += datapoint[\"content\"]\n",
    "            \n",
    "            liwc_categs = encode_liwc_categories(words, liwc_categories, liwc_words_for_categories)\n",
    "            if datapoint[\"nick\"] not in user_level_texts.keys():\n",
    "                user_level_texts[datapoint[\"nick\"]] = {}\n",
    "                user_level_texts[datapoint[\"nick\"]]['texts'] = [words]\n",
    "                user_level_texts[datapoint[\"nick\"]]['raw'] = [raw_text]\n",
    "                subjects_split['test'].append(datapoint['nick'])\n",
    "            else:\n",
    "                user_level_texts[datapoint[\"nick\"]]['texts'].append(words)\n",
    "                user_level_texts[datapoint[\"nick\"]]['raw'].append(raw_text)\n",
    "            \n",
    "    return user_level_texts, subjects_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_datapoint(jlpath):\n",
    "    datapoints = []\n",
    "    with open(jlpath) as f:\n",
    "        for line in f:\n",
    "            datapoints.append(json.loads(line))\n",
    "    return datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, sys\n",
    "from resource_loading import load_vocabulary\n",
    "data_erisk = []\n",
    "with open('data11.jl') as f:\n",
    "    for line in f:\n",
    "        data_erisk.append(json.loads(line))\n",
    "with open('data8.jl') as f:\n",
    "    for line in f:\n",
    "        data_erisk.append(json.loads(line))\n",
    "with open('data11.jl') as f:\n",
    "    for line in f:\n",
    "        data_erisk.append(json.loads(line))\n",
    "\n",
    "erisk_server_data, erisk_server_subjects_split = load_erisk_server_data(data_erisk, \n",
    "                                                                                    \n",
    "                                        tokenizer=RegexpTokenizer('\\w+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EriskDataGenerator import EriskDataGenerator\n",
    "\n",
    "generator = EriskDataGenerator(hyperparams_features=hyperparams_features,\n",
    "                                seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
    "                                     max_posts_per_user=None,\n",
    "                                    posts_per_group=hyperparams['posts_per_group'],\n",
    "                                    post_groups_per_user=None, \n",
    "                                    shuffle=False, return_subjects=True,\n",
    "                                            compute_liwc=True)\n",
    "generator.add_data_round(data_erisk)\n",
    "generator.add_data_round(data_erisk)\n",
    "model.predict(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(stopwords.words(\"english\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# erisk_server_data, erisk_server_subjects_split, vocabulary = load_erisk_server_data(data,\n",
    "#                        tokenizer=regtokenizer,\n",
    "#                        liwc_words_for_categories=liwc_words_for_categories,\n",
    "#                     voc_size=hyperparams_features['max_features'],\n",
    "#                     emotion_lexicon=nrc_lexicon,\n",
    "#                     emotions=emotions,\n",
    "#                     user_level=hyperparams_features['user_level'],\n",
    "#                        vocabulary=vocabulary_dict,\n",
    "#     #                                                            vocabulary=pickle.load(open('vocabulary20K_selfharm.pkl', 'rb'))\n",
    "#                     logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = DataGenerator(erisk_server_data, erisk_server_subjects_split, \n",
    "                                     set_type='test', \n",
    "                                                       hyperparams_features=hyperparams_features,\n",
    "                                seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
    "                                     max_posts_per_user=None,\n",
    "                                    posts_per_group=hyperparams['posts_per_group'],\n",
    "                                    post_groups_per_user=None, \n",
    "                                    shuffle=False, return_subjects=True,\n",
    "                                            compute_liwc=True)\n",
    "dg.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "server_erisk_predictions = model.predict(DataGenerator(erisk_server_data, erisk_server_subjects_split, \n",
    "                                     set_type='test', \n",
    "                                                       hyperparams_features=hyperparams_features,\n",
    "                                seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
    "                                     max_posts_per_user=None,\n",
    "                                    posts_per_group=hyperparams['posts_per_group'],\n",
    "                                    post_groups_per_user=None, \n",
    "                                    shuffle=False,\n",
    "                                            compute_liwc=True)\n",
    "                                                               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.Series(server_erisk_predictions.flatten()).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_generator = EriskDataGenerator(hyperparams_features=hyperparams_features,\n",
    "#                             seq_len=hyperparams['maxlen'], batch_size=1,\n",
    "#                                  max_posts_per_user=None,\n",
    "#                                 posts_per_group=hyperparams['posts_per_group'],\n",
    "#                                 post_groups_per_user=None, \n",
    "#                                 shuffle=False, return_subjects=True,\n",
    "#                                         compute_liwc=True)\n",
    "data_round1 = [{\n",
    "\"redditor\": 338, \"content\": \"\", \n",
    "\"date\": \"2014-12-12T04:21:13.000+0000\", \n",
    "\"id\": 168996, \n",
    "\"title\": \"    Copy the Reindeer\", \n",
    "\"number\": 1, \n",
    "\"nick\": \"subject8081\"},\n",
    "{\"redditor\": 339, \n",
    "\"content\": \"    When I don't have the aisle seat and have to climb over people to use the bathroom. I have a tiny girl bladder.\", \n",
    "\"date\": \"2013-10-10T13:17:01.000+0000\", \n",
    "\"id\": 169297, \n",
    "\"title\": \"\", \n",
    "\"number\": 1, \n",
    "\"nick\": \"subject2621\"},\n",
    "{\"redditor\": 340, \n",
    "\"content\": \"    I have a question about being a visitor in Nioh(Random encounters)\", \n",
    "\"date\": \"2017-05-09T17:01:50.000+0000\", \n",
    "\"id\": 169531, \"title\": \"    Nioh - Become a visitor\", \n",
    "\"number\": 1, \n",
    "\"nick\": \"subject992\"}]\n",
    "\n",
    "data_round2 = [{\n",
    "\"redditor\": 340, \n",
    "\"content\": \"    New text\", \n",
    "\"date\": \"2017-05-09T17:02:50.000+0000\", \n",
    "\"id\": 169532, \n",
    "\"title\": \"    Nioh - Become a visitor\", \"number\": 2, \"nick\": \"subject992\"}]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# def scores_to_alerts(predictions_dict, conservative_alerts=False, \n",
    "#                      alert_threshold=0.5, rolling_window=0):\n",
    "#     '''Generates alerts decisions (1/0) from a dictionary of prediction scores per user\n",
    "#     Parameters:\n",
    "#         predictions_dict: dictionary with ordered predictions per user (indexed by user id) \n",
    "#         rolling_window: window of rolling average to be computed across prediction scores\n",
    "#             history for a given user in order to get a \"smoothed\" prediction for each datapoint\n",
    "#             If 0, then no rolling average is computed.\n",
    "#         conservative_alerts: if True, will only emit positive alerts if enough input posts are\n",
    "#             used for prediction (will only trust predictions based on at least as many posts\n",
    "#             as were used in one datapoint in the training stage)\n",
    "#         posts_per_datapoint: integer denoting number of posts per datapoint used in the training stage\n",
    "#             used in case of conservative_alerts=True\n",
    "#         alert_threshold: threshold on the score value above which to emit a positive alert\n",
    "#         Returns: nested dictionary indexed by users, including the original prediction score\n",
    "#             ('scores' key) and the alert value (1/0) (the 'alerts' key)'''\n",
    "#     users = predictions_dict.keys()\n",
    "#     scores_per_user = dict(predictions_dict)\n",
    "#     def _rolling_average(scores, window):\n",
    "#         if window < len(scores):\n",
    "#             return scores\n",
    "#         rolling_predictions = []\n",
    "#         rolling_predictions[:rolling_window-1] = scores[:rolling_window-1]\n",
    "#         rolling_predictions.extend(np.convolve(scores, np.ones(rolling_window), 'valid') / rolling_window)\n",
    "#         return rolling_predictions\n",
    "#     if rolling_window:\n",
    "#         scores_per_user = {u: _rolling_average(scores_per_user[u], rolling_window) for u in users}\n",
    "#     alerts_per_user = {}\n",
    "#     for u in users:\n",
    "#         if conservative_alerts:\n",
    "#             alerts_per_user[u] = [0 for p in scores_per_user[u]]\n",
    "#         else:\n",
    "#             alerts_per_user[u] = [int(p>=alert_threshold) for p in scores_per_user[u]]\n",
    "#     return {u: {'scores': scores_per_user[u], 'alerts': alerts_per_user[u]} for u in users}\n",
    "\n",
    "\n",
    "\n",
    "# # model_path = RUNS_MODEL_PATHS[run_nr]\n",
    "# # hyperparams, hyperparams_features = load_params(model_path)\n",
    "\n",
    "# # model = load_saved_model_weights(model_path, hyperparams, hyperparams_features, \n",
    "# #                                                   h5=True)\n",
    "# rolling_window=50\n",
    "# conservative_alerts=False\n",
    "# alert_threshold=0.5\n",
    "# data_generator = EriskDataGenerator(hyperparams_features=hyperparams_features,\n",
    "#                             seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
    "#                                  max_posts_per_user=None,\n",
    "#                                 posts_per_group=hyperparams['posts_per_group'],\n",
    "#                                 post_groups_per_user=None, \n",
    "#                                 shuffle=False, return_subjects=True,\n",
    "#                                         compute_liwc=True)\n",
    "\n",
    "# for data_round in [data_round1, data_round2]:\n",
    "#     data_generator.add_data_round(data_round)\n",
    "\n",
    "\n",
    "# predictions_per_user = {}\n",
    "# for dp in data_generator:\n",
    "#     prediction = model.predict_step(dp)\n",
    "#     u = dp[1][0]\n",
    "#     print(u)\n",
    "#     if u not in predictions_per_user:\n",
    "#         predictions_per_user[u] = []\n",
    "#     predictions_per_user[u].append(prediction.numpy()[0].item())\n",
    "# alerts_per_user = scores_to_alerts(predictions_per_user, rolling_window=rolling_window,\n",
    "#     alert_threshold=alert_threshold,\n",
    "#     conservative_alerts=(conservative_alerts and len(data_rounds) < hyperparams['posts_per_group']))\n",
    "# print(alerts_per_user)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predict_erisk import predict, scores_to_alerts\n",
    "# predict(run_nr=1, data_rounds=[data_round1, data_round2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = EriskDataGenerator(hyperparams_features=hyperparams_features,\n",
    "                            seq_len=hyperparams['maxlen'], batch_size=1,\n",
    "                                 max_posts_per_user=None,\n",
    "                                posts_per_group=hyperparams['posts_per_group'],\n",
    "                                post_groups_per_user=None, \n",
    "                                shuffle=False, return_subjects=True,\n",
    "                                        compute_liwc=True)\n",
    "for data_round in [data_round1, data_round2]:\n",
    "    data_generator.add_data_round(data_round)\n",
    "rolling_window=0\n",
    "alert_threshold=0.5\n",
    "conservative_alerts=False\n",
    "\n",
    "predictions_per_user = {}\n",
    "for dp in data_generator:\n",
    "    prediction = model.predict_step(dp)\n",
    "    u = dp[1][0]\n",
    "    print(dp[1])\n",
    "    if u not in predictions_per_user:\n",
    "        predictions_per_user[u] = []\n",
    "    predictions_per_user[u].append(prediction.numpy()[0].item())\n",
    "alerts_per_user = scores_to_alerts(predictions_per_user, rolling_window=rolling_window,\n",
    "    alert_threshold=alert_threshold,\n",
    "    conservative_alerts=(conservative_alerts and len(data_rounds) < hyperparams['posts_per_group']))\n",
    "print(alerts_per_user)\n",
    "print(predictions_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(run_nr=1, data_rounds=[data_round1, data_round2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seding results to server!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects = [d['nick'] for d in read_json_datapoint(\"client/data0.jl\")]\n",
    "# results = {s: 0 for s in subjects}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_next_data(rnd, results):\n",
    "#     # TODO: send results to get data\n",
    "#     response = build_response(results)\n",
    "#     # Send response\n",
    "#     data = {\"...\"}\n",
    "#     # Make sure it's the correct round\n",
    "#     assert data['number'] == rnd\n",
    "#     serialize_data(data)\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_next_data_dummy(rnd, results):\n",
    "#     return read_json_datapoint(\"client/data0.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for round and model\n",
    "results = {key: {} for key in models_runs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_chunk(rnds):\n",
    "    # Send same results for a chunk of rounds to get new posts\n",
    "    data = [read_json_datapoint(\"data_server/data%d.jl\" % i) for i in rnds]\n",
    "#     data_chunks = []\n",
    "#     for rnd in rnds:\n",
    "#         # TODO: REPLACE THIS WITH THE CORRECT ONE\n",
    "#         data = get_next_data_dummy(rnd, results)\n",
    "#         data_chunks.append(data)\n",
    "#         all_data[rnd] = data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def predict_for_round_chunk(model, hyperparams, hyperparams_features, vocabulary, data_chunk, subjects=[],\n",
    "                           model_key='', cache_round=None): \n",
    "    # preload for subjects not occurring in the round with results from previous round\n",
    "#     results = {s: 0 for s in subjects}\n",
    "    if cache_round:\n",
    "        results = load_results(model_key, cache_round)\n",
    "    else:\n",
    "        results = {s: 0 for s in subjects}\n",
    "        \n",
    "    \n",
    "    erisk_server_data, erisk_server_subjects_split, vocabulary = load_erisk_server_data(data_chunk,\n",
    "                       tokenizer=regtokenizer,\n",
    "                       liwc_words_for_categories=liwc_words_for_categories,\n",
    "                    voc_size=hyperparams_features['max_features'],\n",
    "                    emotion_lexicon=nrc_lexicon,\n",
    "                    emotions=emotions,\n",
    "                    user_level=1,\n",
    "                       vocabulary=vocabulary,\n",
    "                    logger=logger)\n",
    "\n",
    "    for features, subjects, _ in DataGenerator(erisk_server_data, erisk_server_subjects_split, \n",
    "                                         set_type='test', vocabulary=vocabulary, \n",
    "                                       hierarchical=hyperparams['hierarchical'],\n",
    "                                    seq_len=hyperparams['maxlen'], batch_size=hyperparams['batch_size'],\n",
    "                                         max_posts_per_user=None,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams['posts_per_group'],\n",
    "                                        post_groups_per_user=None, \n",
    "                                         sample_seqs=False, shuffle=False,\n",
    "                                      return_subjects=True):\n",
    "        predictions = model.predict_on_batch(features)\n",
    "        print(len(features[0]), len(subjects), len(predictions), len(results))\n",
    "        for i,s in enumerate(subjects):\n",
    "            results[\"subject\" + str(s)] = predictions[i].item()\n",
    "    return(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_response(results, decision_thresh=0.5, model_name='', rnd=0):\n",
    "    response = []\n",
    "    for subject, score in results.items():\n",
    "        prediction = 1 if score >= decision_thresh else 0\n",
    "        response.append({'nick': subject, 'score': score, 'decision': prediction})\n",
    "    json.dump(response, open(\"data_server/response_run%s_rnd%d.json\" % (model_name, rnd), 'w+'))\n",
    "    return response\n",
    "# build_response(results, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_ensemble_results(rnd, all_results, model_keys_to_average=['lstm_seq', 'cnn_hierarch']):\n",
    "#     subjects = [s for s in all_results[model_keys_to_average[0]][rnd]]\n",
    "#     results_ensemble = {}\n",
    "#     for sub in subjects:\n",
    "#         s = 0\n",
    "#         for k in model_keys_to_average:\n",
    "#             s += all_results[k][rnd][sub]\n",
    "#         results_ensemble[sub] = s/len(model_keys_to_average)\n",
    "#     return results_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble_results(results_to_average):\n",
    "    subjects = [s for s in results_to_average[0]]\n",
    "    results_ensemble = {}\n",
    "    for sub in subjects:\n",
    "        s = 0\n",
    "        for res in results_to_average:\n",
    "            s += res[sub]\n",
    "        results_ensemble[sub] = s/len(results_to_average)\n",
    "    return results_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transfer_results(rnd, all_results, model_to_average='lstm_seq', rounds_back=100):\n",
    "    subjects = [s for s in all_results[model_to_average][rnd]]\n",
    "    results_ensemble = {}\n",
    "    for sub in subjects:\n",
    "        s = 0\n",
    "        existing_rounds = 0\n",
    "        for prev_rnd in range(rnd-rounds_back, rnd+1):\n",
    "#             print(\"rolling rnds\", prev_rnd)\n",
    "            if prev_rnd in all_results[model_to_average]:\n",
    "                s += all_results[model_to_average][prev_rnd][sub]\n",
    "                existing_rounds += 1\n",
    "        results_ensemble[sub] = s/existing_rounds\n",
    "#         print(\"Have found a rolling window of %d for the transfer model\" % existing_rounds)\n",
    "    return results_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(model_key, rnd):\n",
    "    results = {}\n",
    "    with open(\"data_server/response_run%s_rnd%d.json\" % (models_runs[model_key], rnd)) as f:\n",
    "        response = json.loads(f.read())\n",
    "        for line in response:\n",
    "            results[line['nick']] = line['score']\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results['lstm_seq'][20] = load_results('lstm_seq', 20)\n",
    "# results['lstm_seq'][40] = \n",
    "results# results['bert'][40] = load_results('bert', 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnds = range(500,600)\n",
    "decision_thresh = 0.5\n",
    "data_chunks = get_data_chunk(rnds)\n",
    "subjects = [d['nick'] for d in read_json_datapoint(\"data_server/data0.jl\")]\n",
    "# for key in ['transfer', 'ensemble']:\n",
    "for model_key in [\n",
    "                  'lstm_seq',\n",
    "                    'bert',\n",
    "                  'cnn_hierarch', \n",
    "                  'transfer', \n",
    "                  'ensemble',\n",
    "                ]:\n",
    "    print(model_key)\n",
    "    end_rnd = rnds[-1]\n",
    "#     if model_key=='lstm_seq':\n",
    "#         results[model_key][end_rnd]=load_results('lstm_seq', 20)\n",
    "    if model_key=='cnn_hierarch':\n",
    "        results[model_key][end_rnd]=load_results('cnn_hierarch', 40)\n",
    "    elif model_key=='bert':\n",
    "        results[model_key][end_rnd]=load_results('bert', end_rnd)\n",
    "    elif model_key=='ensemble':\n",
    "        model_keys_to_average=['bert', 'cnn_hierarch', 'lstm_seq']\n",
    "        missing_models = [m for m in model_keys_to_average if not results[m]]\n",
    "        if len(missing_models)!=0:\n",
    "            print(\"Missing models! cannot compute ensemble results\", missing_models)\n",
    "            continue\n",
    "        results_to_average = [results[m][end_rnd] for m in model_keys_to_average]\n",
    "#         results[model_key][end_rnd] = get_ensemble_results(rnd, results, \n",
    "#                                                 model_keys_to_average)\n",
    "        results[model_key][end_rnd] = get_ensemble_results(results_to_average)\n",
    "    ## For now\n",
    "    elif model_key=='transfer':\n",
    "        results[model_key][end_rnd]=get_transfer_results(\n",
    "            end_rnd, results, model_to_average='lstm_seq', rounds_back=60)\n",
    "#         results[model_key][end_rnd]=results['lstm_seq'][end_rnd]\n",
    "    ##\n",
    "    else:\n",
    "        with session_collection[model_key].as_default():\n",
    "            with session_collection[model_key].graph.as_default():\n",
    "                results[model_key][end_rnd] = predict_for_round_chunk(models_collection[model_key], \n",
    "                                              hyperparams_collection[model_key], hyperparams_features, \n",
    "                                              vocabulary_dict, \n",
    "                                          data_chunks, subjects=subjects, model_key=model_key, cache_round=499)\n",
    "\n",
    "    \n",
    "    print(len(results[model_key][end_rnd].values()), \"positive:\", \n",
    "      len([r for r in results[model_key][end_rnd].values() if r >=0.5]))\n",
    "    response1 = build_response(results[model_key][end_rnd], rnd=end_rnd, \n",
    "                               model_name=models_runs[model_key], decision_thresh=decision_thresh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['lstm_seq'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(list(results['transfer'][180].values())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "pearsonr(list(results['bert'][220].values()), list(results['bert'][180].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(list(results['lstm_seq'][160].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on eRisk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = {}\n",
    "# labels = {}\n",
    "# featuresall = {}\n",
    "# with session_collection['lstm_seq2'].as_default():\n",
    "#     with session_collection['lstm_seq2'].graph.as_default():\n",
    "        # for features, subjects, lbls in DataGenerator(user_level_data, subjects_split, \n",
    "        #                                          set_type='train', vocabulary=vocabulary_dict,\n",
    "        #                                        hierarchical=hyperparams1['hierarchical'],\n",
    "        #                                     seq_len=hyperparams1['maxlen'], batch_size=hyperparams1['batch_size'],\n",
    "        #                                          max_posts_per_user=None,\n",
    "        #                                        pad_with_duplication=False,\n",
    "        #                                         posts_per_group=hyperparams1['posts_per_group'],\n",
    "        #                                         post_groups_per_user=None, \n",
    "        #                                          sample_seqs=False, shuffle=False,\n",
    "        #                                                return_subjects=True):\n",
    "\n",
    "        #     predictions = loaded_model.predict_on_batch(features)\n",
    "        #     print(len(features[0]), len(subjects), len(predictions), len(labels), len(results))\n",
    "        #     for i,s in enumerate(subjects):\n",
    "        #         if s not in results:\n",
    "        #             results[s] = []\n",
    "        #             featuresall[s] = []\n",
    "        #         results[s].append(predictions[i].item())\n",
    "        #         featuresall[s].append([features[j][i] for j in range(len(features))])\n",
    "        #         labels[s] = lbls[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in results:\n",
    "    if not labels[subject]:\n",
    "        if np.std(results[subject])>0.0:\n",
    "            print(subject), print(results[subject][0], results[subject][-1]-results[subject][0])\n",
    "            pd.Series(results[subject]).rolling(window=5).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[featuresall[4278][i][0].sum() for i in range(len(featuresall[4278]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(user_level_data['subject4278']['raw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, s, y in DataGenerator(user_level_data, subjects_split, \n",
    "                                         set_type='test', vocabulary=vocabulary_dict,\n",
    "                                       hierarchical=hyperparams1['hierarchical'],\n",
    "                                    seq_len=hyperparams1['maxlen'], batch_size=hyperparams1['batch_size'],\n",
    "                                         max_posts_per_user=None,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams1['posts_per_group'],\n",
    "                                        post_groups_per_user=None, \n",
    "                                         sample_seqs=False, shuffle=False,\n",
    "                                               return_subjects=True):\n",
    "    print(\"subject\", s, \"features\", x[0].sum(axis=1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_key='lstm_seq'\n",
    "with session_collection[model_key].as_default():\n",
    "    with session_collection[model_key].graph.as_default():\n",
    "        res = models_collection[model_key].evaluate_generator(DataGenerator(user_level_data, subjects_split, \n",
    "                                              liwc_words_for_categories=liwc_words_for_categories,\n",
    "                                         set_type='test', vocabulary=vocabulary_dict,\n",
    "                                       hierarchical=hyperparams_collection[model_key]['hierarchical'],\n",
    "                                    seq_len=hyperparams_collection[model_key]['maxlen'], \n",
    "                                    batch_size=hyperparams['batch_size'],\n",
    "                                         max_posts_per_user=None,\n",
    "                                       pad_with_duplication=False,\n",
    "                                        posts_per_group=hyperparams_collection[model_key]['posts_per_group'],\n",
    "                                        post_groups_per_user=1,#None, \n",
    "                                         sample_seqs=False, shuffle=False,\n",
    "                                             compute_liwc=False))\n",
    "        print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfconda",
   "language": "python",
   "name": "tfconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "362px",
    "width": "218px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
