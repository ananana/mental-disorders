{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob, os\n",
    "import numpy as np\n",
    "from comet_ml import Experiment, Optimizer\n",
    "import pickle\n",
    "import logging\n",
    "import sys\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_KERAS'] = '1'\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Lambda, BatchNormalization, TimeDistributed, \\\n",
    "     Bidirectional, Input, concatenate, Flatten, RepeatVector, Activation, Multiply, Permute#, CuDNNLSTM\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import callbacks, optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model, Sequence\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_seed = 1234\n",
    "# tf.set_random_seed(my_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('training')\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_subject_writings(subject_file):\n",
    "    writings = []\n",
    "    with open(subject_file) as sf:\n",
    "        contents = sf.read()\n",
    "        root = ET.fromstring(contents)\n",
    "        try:\n",
    "            subject = root.findall('ID')[0].text.strip()\n",
    "        except Exception:\n",
    "            print('Cannot extract ID', contents[:500], '\\n-------\\n')        \n",
    "        for w in root.iter('WRITING'):\n",
    "            subject_writings = {'subject': subject}\n",
    "            for title in w.findall('TITLE'):\n",
    "                subject_writings['title'] = title.text\n",
    "            for text in w.findall('TEXT'):\n",
    "                subject_writings['text'] = text.text\n",
    "            for date in w.findall('DATE'):\n",
    "                subject_writings['date'] = date.text\n",
    "            writings.append(subject_writings)\n",
    "    return writings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dir = '/home/anasab/' \n",
    "root_dir = '/home/anasab/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir_T2 = root_dir + '/eRisk/data/eRisk2020_T2/eRisk2020_T2_TRAINING_DATA/'\n",
    "labels_file_T2 = root_dir + '/eRisk/data/eRisk2020_T2/eRisk2020_T2_TRAINING_DATA/Depression Questionnaires_anon.txt'\n",
    "nr_questions = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts(datadir_T2,\n",
    "                labels_file_T2):\n",
    "    writings = []\n",
    "    writings_df = pd.DataFrame()\n",
    "    labels_df = pd.DataFrame()\n",
    "\n",
    "        \n",
    "    for subject_file in os.listdir(datadir_T2):\n",
    "        if not subject_file.startswith('subject'):\n",
    "            continue\n",
    "        writings.extend(read_subject_writings(os.path.join(datadir_T2, subject_file)))\n",
    "    writings_df = pd.DataFrame(writings)\n",
    "    \n",
    "    labels_df = pd.read_csv(os.path.join(labels_file_T2), \n",
    "                                 delimiter='\\s+', names=['subject'] + ['label%i' % i for i in range(nr_questions)])\n",
    "\n",
    "    labels_df = labels_df.set_index('subject')\n",
    "    \n",
    "    writings_df = writings_df.join(labels_df, on='subject')\n",
    "    \n",
    "    return writings_df, labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df, labels_df = read_texts(datadir_T2, labels_file_T2)\n",
    "writings_df = pickle.load(open('writings_df_T2_liwc.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>label0</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "      <th>label3</th>\n",
       "      <th>label4</th>\n",
       "      <th>label5</th>\n",
       "      <th>label6</th>\n",
       "      <th>...</th>\n",
       "      <th>feel</th>\n",
       "      <th>excl</th>\n",
       "      <th>future</th>\n",
       "      <th>nonfl</th>\n",
       "      <th>ppron</th>\n",
       "      <th>shehe</th>\n",
       "      <th>i</th>\n",
       "      <th>we</th>\n",
       "      <th>you</th>\n",
       "      <th>they</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subject1272</th>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>...</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject2341</th>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>...</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject2432</th>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "      <td>...</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject2827</th>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>...</td>\n",
       "      <td>659</td>\n",
       "      <td>659</td>\n",
       "      <td>659</td>\n",
       "      <td>659</td>\n",
       "      <td>659</td>\n",
       "      <td>659</td>\n",
       "      <td>659</td>\n",
       "      <td>659</td>\n",
       "      <td>659</td>\n",
       "      <td>659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject2903</th>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>...</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject2961</th>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>...</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject3707</th>\n",
       "      <td>1022</td>\n",
       "      <td>1022</td>\n",
       "      <td>1022</td>\n",
       "      <td>1022</td>\n",
       "      <td>1022</td>\n",
       "      <td>1022</td>\n",
       "      <td>1022</td>\n",
       "      <td>1022</td>\n",
       "      <td>1022</td>\n",
       "      <td>1022</td>\n",
       "      <td>...</td>\n",
       "      <td>1022</td>\n",
       "      <td>1022</td>\n",
       "      <td>1022</td>\n",
       "      <td>1022</td>\n",
       "      <td>1022</td>\n",
       "      <td>1022</td>\n",
       "      <td>1022</td>\n",
       "      <td>1022</td>\n",
       "      <td>1022</td>\n",
       "      <td>1022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject3993</th>\n",
       "      <td>1510</td>\n",
       "      <td>1510</td>\n",
       "      <td>1510</td>\n",
       "      <td>1510</td>\n",
       "      <td>1510</td>\n",
       "      <td>1510</td>\n",
       "      <td>1510</td>\n",
       "      <td>1510</td>\n",
       "      <td>1510</td>\n",
       "      <td>1510</td>\n",
       "      <td>...</td>\n",
       "      <td>1509</td>\n",
       "      <td>1509</td>\n",
       "      <td>1509</td>\n",
       "      <td>1509</td>\n",
       "      <td>1509</td>\n",
       "      <td>1509</td>\n",
       "      <td>1509</td>\n",
       "      <td>1509</td>\n",
       "      <td>1509</td>\n",
       "      <td>1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject4058</th>\n",
       "      <td>1028</td>\n",
       "      <td>1028</td>\n",
       "      <td>1028</td>\n",
       "      <td>1028</td>\n",
       "      <td>1028</td>\n",
       "      <td>1028</td>\n",
       "      <td>1028</td>\n",
       "      <td>1028</td>\n",
       "      <td>1028</td>\n",
       "      <td>1028</td>\n",
       "      <td>...</td>\n",
       "      <td>1028</td>\n",
       "      <td>1028</td>\n",
       "      <td>1028</td>\n",
       "      <td>1028</td>\n",
       "      <td>1028</td>\n",
       "      <td>1028</td>\n",
       "      <td>1028</td>\n",
       "      <td>1028</td>\n",
       "      <td>1028</td>\n",
       "      <td>1028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject436</th>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject5791</th>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>...</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject5897</th>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>...</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject6619</th>\n",
       "      <td>514</td>\n",
       "      <td>514</td>\n",
       "      <td>514</td>\n",
       "      <td>514</td>\n",
       "      <td>514</td>\n",
       "      <td>514</td>\n",
       "      <td>514</td>\n",
       "      <td>514</td>\n",
       "      <td>514</td>\n",
       "      <td>514</td>\n",
       "      <td>...</td>\n",
       "      <td>514</td>\n",
       "      <td>514</td>\n",
       "      <td>514</td>\n",
       "      <td>514</td>\n",
       "      <td>514</td>\n",
       "      <td>514</td>\n",
       "      <td>514</td>\n",
       "      <td>514</td>\n",
       "      <td>514</td>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject6635</th>\n",
       "      <td>1054</td>\n",
       "      <td>1054</td>\n",
       "      <td>1054</td>\n",
       "      <td>1054</td>\n",
       "      <td>1054</td>\n",
       "      <td>1054</td>\n",
       "      <td>1054</td>\n",
       "      <td>1054</td>\n",
       "      <td>1054</td>\n",
       "      <td>1054</td>\n",
       "      <td>...</td>\n",
       "      <td>1054</td>\n",
       "      <td>1054</td>\n",
       "      <td>1054</td>\n",
       "      <td>1054</td>\n",
       "      <td>1054</td>\n",
       "      <td>1054</td>\n",
       "      <td>1054</td>\n",
       "      <td>1054</td>\n",
       "      <td>1054</td>\n",
       "      <td>1054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject6900</th>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>...</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject7039</th>\n",
       "      <td>561</td>\n",
       "      <td>561</td>\n",
       "      <td>561</td>\n",
       "      <td>561</td>\n",
       "      <td>561</td>\n",
       "      <td>561</td>\n",
       "      <td>561</td>\n",
       "      <td>561</td>\n",
       "      <td>561</td>\n",
       "      <td>561</td>\n",
       "      <td>...</td>\n",
       "      <td>560</td>\n",
       "      <td>560</td>\n",
       "      <td>560</td>\n",
       "      <td>560</td>\n",
       "      <td>560</td>\n",
       "      <td>560</td>\n",
       "      <td>560</td>\n",
       "      <td>560</td>\n",
       "      <td>560</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9218</th>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>...</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9454</th>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>...</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9694</th>\n",
       "      <td>1001</td>\n",
       "      <td>1001</td>\n",
       "      <td>1001</td>\n",
       "      <td>1001</td>\n",
       "      <td>1001</td>\n",
       "      <td>1001</td>\n",
       "      <td>1001</td>\n",
       "      <td>1001</td>\n",
       "      <td>1001</td>\n",
       "      <td>1001</td>\n",
       "      <td>...</td>\n",
       "      <td>995</td>\n",
       "      <td>995</td>\n",
       "      <td>995</td>\n",
       "      <td>995</td>\n",
       "      <td>995</td>\n",
       "      <td>995</td>\n",
       "      <td>995</td>\n",
       "      <td>995</td>\n",
       "      <td>995</td>\n",
       "      <td>995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9798</th>\n",
       "      <td>1284</td>\n",
       "      <td>1284</td>\n",
       "      <td>1284</td>\n",
       "      <td>1284</td>\n",
       "      <td>1284</td>\n",
       "      <td>1284</td>\n",
       "      <td>1284</td>\n",
       "      <td>1284</td>\n",
       "      <td>1284</td>\n",
       "      <td>1284</td>\n",
       "      <td>...</td>\n",
       "      <td>1281</td>\n",
       "      <td>1281</td>\n",
       "      <td>1281</td>\n",
       "      <td>1281</td>\n",
       "      <td>1281</td>\n",
       "      <td>1281</td>\n",
       "      <td>1281</td>\n",
       "      <td>1281</td>\n",
       "      <td>1281</td>\n",
       "      <td>1281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             title  text  date  label0  label1  label2  label3  label4  \\\n",
       "subject                                                                  \n",
       "subject1272    120   120   120     120     120     120     120     120   \n",
       "subject2341    129   129   129     129     129     129     129     129   \n",
       "subject2432    332   332   332     332     332     332     332     332   \n",
       "subject2827    663   663   663     663     663     663     663     663   \n",
       "subject2903    313   313   313     313     313     313     313     313   \n",
       "subject2961    180   180   180     180     180     180     180     180   \n",
       "subject3707   1022  1022  1022    1022    1022    1022    1022    1022   \n",
       "subject3993   1510  1510  1510    1510    1510    1510    1510    1510   \n",
       "subject4058   1028  1028  1028    1028    1028    1028    1028    1028   \n",
       "subject436      29    29    29      29      29      29      29      29   \n",
       "subject5791    303   303   303     303     303     303     303     303   \n",
       "subject5897     82    82    82      82      82      82      82      82   \n",
       "subject6619    514   514   514     514     514     514     514     514   \n",
       "subject6635   1054  1054  1054    1054    1054    1054    1054    1054   \n",
       "subject6900    313   313   313     313     313     313     313     313   \n",
       "subject7039    561   561   561     561     561     561     561     561   \n",
       "subject9218    323   323   323     323     323     323     323     323   \n",
       "subject9454    180   180   180     180     180     180     180     180   \n",
       "subject9694   1001  1001  1001    1001    1001    1001    1001    1001   \n",
       "subject9798   1284  1284  1284    1284    1284    1284    1284    1284   \n",
       "\n",
       "             label5  label6  ...  feel  excl  future  nonfl  ppron  shehe  \\\n",
       "subject                      ...                                            \n",
       "subject1272     120     120  ...   120   120     120    120    120    120   \n",
       "subject2341     129     129  ...   129   129     129    129    129    129   \n",
       "subject2432     332     332  ...   332   332     332    332    332    332   \n",
       "subject2827     663     663  ...   659   659     659    659    659    659   \n",
       "subject2903     313     313  ...   313   313     313    313    313    313   \n",
       "subject2961     180     180  ...   180   180     180    180    180    180   \n",
       "subject3707    1022    1022  ...  1022  1022    1022   1022   1022   1022   \n",
       "subject3993    1510    1510  ...  1509  1509    1509   1509   1509   1509   \n",
       "subject4058    1028    1028  ...  1028  1028    1028   1028   1028   1028   \n",
       "subject436       29      29  ...    29    29      29     29     29     29   \n",
       "subject5791     303     303  ...   303   303     303    303    303    303   \n",
       "subject5897      82      82  ...    82    82      82     82     82     82   \n",
       "subject6619     514     514  ...   514   514     514    514    514    514   \n",
       "subject6635    1054    1054  ...  1054  1054    1054   1054   1054   1054   \n",
       "subject6900     313     313  ...   313   313     313    313    313    313   \n",
       "subject7039     561     561  ...   560   560     560    560    560    560   \n",
       "subject9218     323     323  ...   323   323     323    323    323    323   \n",
       "subject9454     180     180  ...   180   180     180    180    180    180   \n",
       "subject9694    1001    1001  ...   995   995     995    995    995    995   \n",
       "subject9798    1284    1284  ...  1281  1281    1281   1281   1281   1281   \n",
       "\n",
       "                i    we   you  they  \n",
       "subject                              \n",
       "subject1272   120   120   120   120  \n",
       "subject2341   129   129   129   129  \n",
       "subject2432   332   332   332   332  \n",
       "subject2827   659   659   659   659  \n",
       "subject2903   313   313   313   313  \n",
       "subject2961   180   180   180   180  \n",
       "subject3707  1022  1022  1022  1022  \n",
       "subject3993  1509  1509  1509  1509  \n",
       "subject4058  1028  1028  1028  1028  \n",
       "subject436     29    29    29    29  \n",
       "subject5791   303   303   303   303  \n",
       "subject5897    82    82    82    82  \n",
       "subject6619   514   514   514   514  \n",
       "subject6635  1054  1054  1054  1054  \n",
       "subject6900   313   313   313   313  \n",
       "subject7039   560   560   560   560  \n",
       "subject9218   323   323   323   323  \n",
       "subject9454   180   180   180   180  \n",
       "subject9694   995   995   995   995  \n",
       "subject9798  1281  1281  1281  1281  \n",
       "\n",
       "[20 rows x 103 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.groupby('subject').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>label0</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "      <th>label3</th>\n",
       "      <th>label4</th>\n",
       "      <th>label5</th>\n",
       "      <th>...</th>\n",
       "      <th>feel</th>\n",
       "      <th>excl</th>\n",
       "      <th>future</th>\n",
       "      <th>nonfl</th>\n",
       "      <th>ppron</th>\n",
       "      <th>shehe</th>\n",
       "      <th>i</th>\n",
       "      <th>we</th>\n",
       "      <th>you</th>\n",
       "      <th>they</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject5791</td>\n",
       "      <td></td>\n",
       "      <td>Great, thanks a ton!</td>\n",
       "      <td>2018-10-30 17:35:30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject5791</td>\n",
       "      <td>The search button gives me a 404 error</td>\n",
       "      <td>Just downloaded GBA4ios 2.1 and when I go to ...</td>\n",
       "      <td>2018-10-30 17:19:41</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.074468</td>\n",
       "      <td>0.074468</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject5791</td>\n",
       "      <td></td>\n",
       "      <td>Remindme! 1 week</td>\n",
       "      <td>2018-10-30 14:33:49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject5791</td>\n",
       "      <td></td>\n",
       "      <td>Me too please</td>\n",
       "      <td>2018-10-19 18:06:38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject5791</td>\n",
       "      <td></td>\n",
       "      <td>Any chance you can pm me what this spoiler is...</td>\n",
       "      <td>2018-10-19 18:04:14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10936</th>\n",
       "      <td>subject3993</td>\n",
       "      <td>Alternative Currency Being Considered in Penn...</td>\n",
       "      <td></td>\n",
       "      <td>2009-01-07 18:41:30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10937</th>\n",
       "      <td>subject3993</td>\n",
       "      <td>Asus' new keyboard. Oh wait... thats not a ke...</td>\n",
       "      <td></td>\n",
       "      <td>2009-01-07 17:13:53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10938</th>\n",
       "      <td>subject3993</td>\n",
       "      <td>Homeland Security USA - tripe to entertain mo...</td>\n",
       "      <td></td>\n",
       "      <td>2009-01-07 07:09:19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10939</th>\n",
       "      <td>subject3993</td>\n",
       "      <td>10 dead as Israeli missile hits near U.N. sch...</td>\n",
       "      <td></td>\n",
       "      <td>2009-01-06 17:15:24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10940</th>\n",
       "      <td>subject3993</td>\n",
       "      <td>iPhone Apps: Helping People Get Rich Quick</td>\n",
       "      <td></td>\n",
       "      <td>2009-01-06 07:25:07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10941 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           subject                                              title  \\\n",
       "0      subject5791                                                      \n",
       "1      subject5791            The search button gives me a 404 error    \n",
       "2      subject5791                                                      \n",
       "3      subject5791                                                      \n",
       "4      subject5791                                                      \n",
       "...            ...                                                ...   \n",
       "10936  subject3993   Alternative Currency Being Considered in Penn...   \n",
       "10937  subject3993   Asus' new keyboard. Oh wait... thats not a ke...   \n",
       "10938  subject3993   Homeland Security USA - tripe to entertain mo...   \n",
       "10939  subject3993   10 dead as Israeli missile hits near U.N. sch...   \n",
       "10940  subject3993        iPhone Apps: Helping People Get Rich Quick    \n",
       "\n",
       "                                                    text  \\\n",
       "0                                  Great, thanks a ton!    \n",
       "1       Just downloaded GBA4ios 2.1 and when I go to ...   \n",
       "2                                      Remindme! 1 week    \n",
       "3                                         Me too please    \n",
       "4       Any chance you can pm me what this spoiler is...   \n",
       "...                                                  ...   \n",
       "10936                                                      \n",
       "10937                                                      \n",
       "10938                                                      \n",
       "10939                                                      \n",
       "10940                                                      \n",
       "\n",
       "                        date  label0  label1  label2  label3  label4  label5  \\\n",
       "0       2018-10-30 17:35:30        1       0       1       1       0       0   \n",
       "1       2018-10-30 17:19:41        1       0       1       1       0       0   \n",
       "2       2018-10-30 14:33:49        1       0       1       1       0       0   \n",
       "3       2018-10-19 18:06:38        1       0       1       1       0       0   \n",
       "4       2018-10-19 18:04:14        1       0       1       1       0       0   \n",
       "...                      ...     ...     ...     ...     ...     ...     ...   \n",
       "10936   2009-01-07 18:41:30        0       0       0       0       0       0   \n",
       "10937   2009-01-07 17:13:53        0       0       0       0       0       0   \n",
       "10938   2009-01-07 07:09:19        0       0       0       0       0       0   \n",
       "10939   2009-01-06 17:15:24        0       0       0       0       0       0   \n",
       "10940   2009-01-06 07:25:07        0       0       0       0       0       0   \n",
       "\n",
       "       ...  feel      excl    future  nonfl     ppron  shehe         i   we  \\\n",
       "0      ...   0.0  0.000000  0.000000    0.0  0.000000    0.0  0.000000  0.0   \n",
       "1      ...   0.0  0.074468  0.074468    0.0  0.063830    0.0  0.063830  0.0   \n",
       "2      ...   0.0  0.000000  0.000000    0.0  0.000000    0.0  0.000000  0.0   \n",
       "3      ...   0.0  0.000000  0.000000    0.0  0.333333    0.0  0.333333  0.0   \n",
       "4      ...   0.0  0.027778  0.111111    0.0  0.138889    0.0  0.111111  0.0   \n",
       "...    ...   ...       ...       ...    ...       ...    ...       ...  ...   \n",
       "10936  ...   0.0  0.000000  0.000000    0.0  0.000000    0.0  0.000000  0.0   \n",
       "10937  ...   0.0  0.076923  0.076923    0.0  0.000000    0.0  0.000000  0.0   \n",
       "10938  ...   0.0  0.000000  0.000000    0.0  0.000000    0.0  0.000000  0.0   \n",
       "10939  ...   0.0  0.000000  0.000000    0.0  0.000000    0.0  0.000000  0.0   \n",
       "10940  ...   0.0  0.000000  0.000000    0.0  0.000000    0.0  0.000000  0.0   \n",
       "\n",
       "            you  they  \n",
       "0      0.000000   0.0  \n",
       "1      0.000000   0.0  \n",
       "2      0.000000   0.0  \n",
       "3      0.000000   0.0  \n",
       "4      0.027778   0.0  \n",
       "...         ...   ...  \n",
       "10936  0.000000   0.0  \n",
       "10937  0.000000   0.0  \n",
       "10938  0.000000   0.0  \n",
       "10939  0.000000   0.0  \n",
       "10940  0.000000   0.0  \n",
       "\n",
       "[10941 rows x 104 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def tokenize(t):\n",
    "    return tokenizer.tokenize(t.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fields(writings_df):\n",
    "    writings_df['tokenized_title'] = writings_df['title'].apply(lambda t: tokenize(t) \n",
    "                                                                if type(t)==str and t else None)\n",
    "    writings_df['title_len'] = writings_df['tokenized_title'].apply(lambda t: len(t) \n",
    "                                                                    if type(t)==list and t else None)\n",
    "    writings_df['tokenized_text'] = writings_df['text'].apply(lambda t: tokenize(t) \n",
    "                                                              if type(t)==str and t else None)\n",
    "    writings_df['text_len'] = writings_df['tokenized_text'].apply(lambda t: len(t) \n",
    "                                                                  if type(t)==list and t else None)\n",
    "    return writings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df = tokenize_fields(writings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10409.000000\n",
       "mean        50.365069\n",
       "std         84.811676\n",
       "min          1.000000\n",
       "25%          9.000000\n",
       "50%         24.000000\n",
       "75%         54.000000\n",
       "max       1567.000000\n",
       "Name: text_len, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.text_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1119.000000\n",
       "mean       11.246649\n",
       "std         6.979392\n",
       "min         1.000000\n",
       "25%         6.000000\n",
       "50%        10.000000\n",
       "75%        15.000000\n",
       "max        51.000000\n",
       "Name: title_len, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.title_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      20.000000\n",
       "mean      547.050000\n",
       "std       446.144828\n",
       "min        29.000000\n",
       "25%       180.000000\n",
       "50%       327.500000\n",
       "75%      1006.250000\n",
       "max      1510.000000\n",
       "Name: title, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.groupby('subject').count().title.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      20.000000\n",
       "mean      547.050000\n",
       "std       446.144828\n",
       "min        29.000000\n",
       "25%       180.000000\n",
       "50%       327.500000\n",
       "75%      1006.250000\n",
       "max      1510.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.groupby('subject').count().text.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_features = {\n",
    "    \"max_features\": 20000,\n",
    "    # cut texts after this number of words\n",
    "    # (among top max_features most common words)\n",
    "    \"maxlen\": 100,\n",
    "    \"embedding_dim\": 50,\n",
    "    \"user_level\": True,\n",
    "    \"posts_per_user\": 10,\n",
    "    \"batch_size\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_NRC(nrc_path):\n",
    "    word_emotions = {}\n",
    "    emotion_words = {}\n",
    "    with open(nrc_path) as in_f:\n",
    "        for line in in_f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            word, emotion, label = line.split()\n",
    "            if word not in word_emotions:\n",
    "                word_emotions[word] = set()\n",
    "            if emotion not in emotion_words:\n",
    "                emotion_words[emotion] = set()\n",
    "            label = int(label)\n",
    "            if label:\n",
    "                word_emotions[word].add(emotion)\n",
    "                emotion_words[emotion].add(word)\n",
    "    return emotion_words\n",
    "\n",
    "nrc_lexicon_path = root_dir + '/resources/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'\n",
    "nrc_lexicon = load_NRC(nrc_lexicon_path)\n",
    "emotions = list(nrc_lexicon.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_emotions(tokens, emotion_lexicon, emotions, relative=True):\n",
    "    text_len = len(tokens)\n",
    "    encoded_emotions = [0 for e in emotions]\n",
    "    for i, emotion in enumerate(emotions):\n",
    "        try:\n",
    "            emotion_words = [t for t in tokens if t in emotion_lexicon[emotion]]\n",
    "            if relative:\n",
    "                encoded_emotions[i] = len(emotion_words) / len(tokens)\n",
    "            else:\n",
    "                encoded_emotions[i] = len(emotion_words)\n",
    "        except ValueError:\n",
    "            print(\"Emotion not found.\")\n",
    "    return encoded_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from liwc_readDict import readDict\n",
    "\n",
    "liwc = readDict(root_dir + '/resources/liwc.dic')\n",
    "\n",
    "categories = set([c for (w,c) in liwc])\n",
    "len(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Personal pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_person_pronouns = {\"i\", \"me\", \"my\", \"mine\", \"myself\"}\n",
    "def encode_pronouns(tokens, pronouns={\"i\", \"me\", \"my\", \"mine\", \"myself\"}, relative=True):\n",
    "    if not tokens:\n",
    "        return np.nan\n",
    "    text_len = len(tokens)\n",
    "    nr_pronouns = len([t for t in tokens if t in pronouns])\n",
    "    if relative:\n",
    "        return nr_pronouns/text_len\n",
    "    else:\n",
    "        return nr_pronouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words(\"english\")\n",
    "def encode_stopwords(tokens, stopwords=stopword_list):\n",
    "    encoded_stopwords = [0 for s in stopword_list]\n",
    "    if not tokens:\n",
    "        return encoded_stopwords\n",
    "    for i, stopword in enumerate(stopwords):\n",
    "        if stopword in tokens:\n",
    "            encoded_stopwords[i] += 1\n",
    "    return encoded_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from easybert import Bert\n",
    "# bert = Bert(\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = bert.embed(\"A sequence of words is a sequebce.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode_bert(sequence):\n",
    "#     return bert.embed(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix='/home/anasab/eRisk/data/'\n",
    "# train_df = pd.read_csv(prefix + 'train.csv', header=None)\n",
    "# train_df.head()\n",
    "\n",
    "# eval_df = pd.read_csv(prefix + 'test.csv', header=None)\n",
    "# eval_df.head()\n",
    "\n",
    "# train_df[0] = (train_df[0] == 2).astype(int)\n",
    "# eval_df[0] = (eval_df[0] == 2).astype(int)\n",
    "\n",
    "# train_df = pd.DataFrame({\n",
    "#     'text': train_df[1].replace(r'\\n', ' ', regex=True),\n",
    "#     'label':train_df[0]\n",
    "# })\n",
    "\n",
    "# print(train_df.head())\n",
    "\n",
    "# eval_df = pd.DataFrame({\n",
    "#     'text': eval_df[1].replace(r'\\n', ' ', regex=True),\n",
    "#     'label':eval_df[0]\n",
    "# })\n",
    "\n",
    "# print(eval_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "\n",
    "# # Create a TransformerModel\n",
    "# model = ClassificationModel('roberta', 'roberta-base')\n",
    "\n",
    "# # Train the model\n",
    "# model.train_model(train_df)\n",
    "\n",
    "# # Evaluate the model\n",
    "# result, model_outputs, wrong_predictions = model.eval_model(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bert import albert_tokenization\n",
    "# from bert import bert_tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "# bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "bert_path = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\"\n",
    "import bert\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_layer = hub.KerasLayer(bert_path,# signature='tokens' , signature_outputs_as_dict=True,\n",
    "                            trainable=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "    Args:\n",
    "      guid: Unique id for the example.\n",
    "      text_a: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "        Only must be specified for sequence pair tasks.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text_for_bert(tokenizer, example, max_seq_length=256):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "#     if isinstance(example, PaddingInputExample):\n",
    "#         input_ids = [0] * max_seq_length\n",
    "#         input_mask = [0] * max_seq_length\n",
    "#         segment_ids = [0] * max_seq_length\n",
    "#         label = 0\n",
    "#         return input_ids, input_mask, segment_ids, label\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    if len(tokens_a) > max_seq_length - 2:\n",
    "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
    "\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    return input_ids, input_mask, segment_ids, example.label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "#     bert_module =  hub.Module(bert_path)\n",
    "#     tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "\n",
    "#     vocab_file, do_lower_case = sess.run(\n",
    "#         [\n",
    "#             tokenization_info[\"vocab_file\"],\n",
    "#             tokenization_info[\"do_lower_case\"],\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([101,\n",
       "  9617,\n",
       "  2024,\n",
       "  8210,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate tokenizer\n",
    "# bert_tokenizer = FullTokenizer()\n",
    "bert_tokenizer = create_tokenizer_from_hub_module()\n",
    "\n",
    "encode_text_for_bert(bert_tokenizer, InputExample(None, \n",
    "                                               \"Ana are mere\"), \n",
    "                       hyperparams_features['maxlen'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfhub albert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = tf.keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "# input_mask = tf.keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "# sequence_mask = tf.keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "\n",
    "# albert = hub.KerasLayer(\n",
    "#     \"https://tfhub.dev/google/albert_xlarge/3\",\n",
    "#     trainable=True,\n",
    "#     signature=\"tokens\",\n",
    "#     output_key=\"pooled_output\",\n",
    "# )\n",
    "\n",
    "# features = {\n",
    "#     \"input_ids\": input_ids,\n",
    "#     \"input_mask\": input_mask,\n",
    "#     \"segment_ids\": sequence_mask,\n",
    "# }\n",
    "# out = albert(features)\n",
    "# model = tf.keras.Model(inputs=[input_ids, input_mask, sequence_mask], outputs=out)\n",
    "# model.compile(\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def encode_labels(labels):\n",
    "    '''Convert ia to i and ib to -i'''\n",
    "    encoded_labels = []\n",
    "    for i, l in enumerate(labels):\n",
    "        try:\n",
    "            encoded_labels.append(int(l))\n",
    "        except Exception as e:\n",
    "            logger.debug(\"Encoding label %s\\n\" % l)\n",
    "        \n",
    "            if str(l)[-1] == 'a':\n",
    "                encoded_labels.append(int(l[0]))\n",
    "            elif str(l)[-1] == 'b':\n",
    "                encoded_labels.append(-int(l[0]))\n",
    "            else:\n",
    "                logger.warning(\"Coult not encode label %s\\n\" % l)\n",
    "    return encoded_labels\n",
    "\n",
    "def load_erisk_data(writings_df, voc_size, emotion_lexicon, seq_len, emotions =  \n",
    "                    ['anger', 'anticipation', 'disgust', 'fear', 'joy', \n",
    "                     'negative', 'positive', 'sadness', 'surprise', 'trust'],\n",
    "                    liwc_categories = categories, ignore_features=[],\n",
    "                    pronouns = [\"i\", \"me\", \"my\", \"mine\", \"myself\"],\n",
    "                    train_prop=0.7, valid_prop=0.3, test_slice=2,\n",
    "                    nr_slices=5,\n",
    "                    min_post_len=3, min_word_len=1, \n",
    "                    user_level=True, vocabulary=None,\n",
    "                   logger=logger):\n",
    "    logger.debug(\"Loading data...\\n\")\n",
    "    if not vocabulary:\n",
    "        vocabulary = {}\n",
    "        word_freqs = Counter()\n",
    "        for words in writings_df.tokenized_text:\n",
    "            word_freqs.update(words)\n",
    "        for words in writings_df.tokenized_title:\n",
    "            word_freqs.update(words)\n",
    "        i = 1\n",
    "        for w, f in word_freqs.most_common(voc_size-2): # keeping voc_size-1 for unk\n",
    "            if len(w) < min_word_len:\n",
    "                continue\n",
    "            vocabulary[w] = i\n",
    "            i += 1\n",
    "\n",
    "    if 'subset' in writings_df.columns:\n",
    "        training_subjects = list(set(writings_df[writings_df['subset']=='train'].subject))\n",
    "        test_subjects = list(set(writings_df[writings_df['subset']=='test'].subject))\n",
    "    else:\n",
    "        all_subjects = sorted(list(set(writings_df.subject)))\n",
    "        training_subjects_size = int(len(all_subjects) * train_prop)\n",
    "        test_subjects_size = len(all_subjects) - training_subjects_size\n",
    "        # Cross-validation, with fixed slice as input\n",
    "        test_prop = 1-train_prop\n",
    "        test_slice = min(test_slice, nr_slices)\n",
    "        logger.debug(\"start index: %f, from %f\\n\" % (\n",
    "            len(all_subjects)*(1/nr_slices)*test_slice, test_prop*test_slice))\n",
    "        start_slice = int(len(all_subjects)*(1/nr_slices)*test_slice)\n",
    "        test_subjects = all_subjects[start_slice: start_slice+test_subjects_size]\n",
    "        training_subjects = [s for s in all_subjects if s not in test_subjects]\n",
    "    training_subjects = sorted(training_subjects) # ensuring reproducibility\n",
    "    valid_subjects_size = int(len(training_subjects) * valid_prop)\n",
    "    valid_subjects = training_subjects[:valid_subjects_size]\n",
    "    training_subjects = training_subjects[valid_subjects_size:]\n",
    "    categories = [c for c in liwc_categories if c in writings_df.columns]\n",
    "    logger.debug(\"%d training users, %d validation users, %d test users.\" % (\n",
    "        len(training_subjects), \n",
    "          len(valid_subjects),\n",
    "          len(test_subjects)))\n",
    "    subjects_split = {'train': training_subjects, \n",
    "                      'valid': valid_subjects, \n",
    "                      'test': test_subjects}\n",
    "\n",
    "    user_level_texts = {}\n",
    "    for row in writings_df.sort_values(by='date').itertuples():\n",
    "        words = []\n",
    "        raw_text = \"\"\n",
    "        if row.tokenized_title:\n",
    "            words.extend(row.tokenized_title)\n",
    "            raw_text += row.title\n",
    "        if row.tokenized_text:\n",
    "            words.extend(row.tokenized_text)\n",
    "            raw_text += row.text\n",
    "        if not words or len(words)<min_post_len:\n",
    "            print(row.subject)\n",
    "            continue\n",
    "        labels = [getattr(row, 'label%d'%i) for i in range(nr_questions)]\n",
    "        liwc_categs = [getattr(row, categ) for categ in categories]\n",
    "        if row.subject not in user_level_texts.keys():\n",
    "            user_level_texts[row.subject] = {}\n",
    "            user_level_texts[row.subject]['texts'] = [words]\n",
    "            user_level_texts[row.subject]['labels'] = encode_labels(labels)\n",
    "            user_level_texts[row.subject]['liwc'] = [liwc_categs]\n",
    "            user_level_texts[row.subject]['raw'] = [raw_text]\n",
    "        else:\n",
    "            user_level_texts[row.subject]['texts'].append(words)\n",
    "            user_level_texts[row.subject]['liwc'].append(liwc_categs)\n",
    "            user_level_texts[row.subject]['raw'].append(raw_text)\n",
    "\n",
    "    return user_level_texts, subjects_split, vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Loading data...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:training:Loading data...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start index: 8.000000, from 0.600000\n",
      "\n",
      "start index: 8.000000, from 0.600000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:training:start index: 8.000000, from 0.600000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 training users, 4 validation users, 6 test users.\n",
      "10 training users, 4 validation users, 6 test users.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:training:10 training users, 4 validation users, 6 test users.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject3993\n",
      "subject3993\n",
      "subject3993\n",
      "subject9798\n",
      "subject9798\n",
      "subject9798\n",
      "subject9798\n",
      "subject9798\n",
      "subject9798\n",
      "subject9798\n",
      "subject9798\n",
      "subject2903\n",
      "subject9798\n",
      "subject6619\n",
      "subject2903\n",
      "subject2903\n",
      "subject9798\n",
      "subject6619\n",
      "subject6619\n",
      "subject6619\n",
      "subject6619\n",
      "subject6619\n",
      "subject6619\n",
      "subject6619\n",
      "subject6619\n",
      "subject6619\n",
      "subject6619\n",
      "subject3993\n",
      "subject5791\n",
      "subject6619\n",
      "subject5791\n",
      "subject9798\n",
      "subject6619\n",
      "subject9798\n",
      "subject6619\n",
      "subject6619\n",
      "subject6619\n",
      "subject6619\n",
      "subject6619\n",
      "subject6619\n",
      "subject5791\n",
      "subject7039\n",
      "subject5791\n",
      "subject5791\n",
      "subject6619\n",
      "subject6619\n",
      "subject6619\n",
      "subject6619\n",
      "subject6619\n",
      "subject6635\n",
      "subject6619\n",
      "subject6635\n",
      "subject6619\n",
      "subject6635\n",
      "subject6635\n",
      "subject7039\n",
      "subject3993\n",
      "subject6635\n",
      "subject6619\n",
      "subject7039\n",
      "subject9694\n",
      "subject6619\n",
      "subject7039\n",
      "subject7039\n",
      "subject7039\n",
      "subject7039\n",
      "subject7039\n",
      "subject7039\n",
      "subject7039\n",
      "subject7039\n",
      "subject7039\n",
      "subject6619\n",
      "subject7039\n",
      "subject9694\n",
      "subject6635\n",
      "subject6635\n",
      "subject7039\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject7039\n",
      "subject6619\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject6635\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject6635\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9798\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject6635\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject5791\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject6635\n",
      "subject5791\n",
      "subject5791\n",
      "subject9694\n",
      "subject9694\n",
      "subject6619\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject6619\n",
      "subject6619\n",
      "subject7039\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9798\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject6619\n",
      "subject9694\n",
      "subject4058\n",
      "subject9694\n",
      "subject7039\n",
      "subject9694\n",
      "subject4058\n",
      "subject2903\n",
      "subject4058\n",
      "subject2903\n",
      "subject2903\n",
      "subject7039\n",
      "subject4058\n",
      "subject2903\n",
      "subject2903\n",
      "subject3993\n",
      "subject9694\n",
      "subject9694\n",
      "subject3993\n",
      "subject6635\n",
      "subject6635\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject3993\n",
      "subject7039\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject3993\n",
      "subject3993\n",
      "subject9694\n",
      "subject3993\n",
      "subject3993\n",
      "subject2903\n",
      "subject7039\n",
      "subject9694\n",
      "subject5791\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject7039\n",
      "subject3993\n",
      "subject2903\n",
      "subject2903\n",
      "subject9694\n",
      "subject436\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject4058\n",
      "subject9798\n",
      "subject4058\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject3993\n",
      "subject4058\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject4058\n",
      "subject4058\n",
      "subject4058\n",
      "subject4058\n",
      "subject3993\n",
      "subject4058\n",
      "subject4058\n",
      "subject4058\n",
      "subject4058\n",
      "subject4058\n",
      "subject9694\n",
      "subject3993\n",
      "subject9694\n",
      "subject3993\n",
      "subject4058\n",
      "subject3993\n",
      "subject9694\n",
      "subject4058\n",
      "subject3993\n",
      "subject3993\n",
      "subject5897\n",
      "subject9694\n",
      "subject9694\n",
      "subject4058\n",
      "subject2341\n",
      "subject9694\n",
      "subject9694\n",
      "subject4058\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9798\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject3993\n",
      "subject9798\n",
      "subject4058\n",
      "subject3993\n",
      "subject9798\n",
      "subject9798\n",
      "subject4058\n",
      "subject4058\n",
      "subject4058\n",
      "subject9798\n",
      "subject9798\n",
      "subject9694\n",
      "subject9694\n",
      "subject2432\n",
      "subject9798\n",
      "subject4058\n",
      "subject3993\n",
      "subject3993\n",
      "subject3993\n",
      "subject3993\n",
      "subject9798\n",
      "subject3993\n",
      "subject4058\n",
      "subject3993\n",
      "subject4058\n",
      "subject9694\n",
      "subject9798\n",
      "subject9798\n",
      "subject9798\n",
      "subject3993\n",
      "subject3993\n",
      "subject9798\n",
      "subject9694\n",
      "subject9798\n",
      "subject9798\n",
      "subject9798\n",
      "subject9798\n",
      "subject9798\n",
      "subject9798\n",
      "subject9798\n",
      "subject9798\n",
      "subject9798\n",
      "subject9798\n",
      "subject3993\n",
      "subject9798\n",
      "subject9798\n",
      "subject7039\n",
      "subject9798\n",
      "subject9798\n",
      "subject9798\n",
      "subject9798\n",
      "subject3993\n",
      "subject9694\n",
      "subject3993\n",
      "subject3993\n",
      "subject3993\n",
      "subject9798\n",
      "subject9798\n",
      "subject3993\n",
      "subject3993\n",
      "subject3993\n",
      "subject3993\n",
      "subject9798\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject2432\n",
      "subject3993\n",
      "subject9694\n",
      "subject3993\n",
      "subject3993\n",
      "subject3993\n",
      "subject4058\n",
      "subject3993\n",
      "subject3707\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject3707\n",
      "subject4058\n",
      "subject4058\n",
      "subject9798\n",
      "subject9798\n",
      "subject9694\n",
      "subject7039\n",
      "subject9694\n",
      "subject9798\n",
      "subject9798\n",
      "subject3707\n",
      "subject9798\n",
      "subject9694\n",
      "subject9798\n",
      "subject4058\n",
      "subject9798\n",
      "subject3993\n",
      "subject9694\n",
      "subject9798\n",
      "subject9694\n",
      "subject9798\n",
      "subject9798\n",
      "subject3707\n",
      "subject9798\n",
      "subject9798\n",
      "subject7039\n",
      "subject9798\n",
      "subject2432\n",
      "subject2432\n",
      "subject9798\n",
      "subject9798\n",
      "subject9798\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject3707\n",
      "subject2432\n",
      "subject2432\n",
      "subject9798\n",
      "subject9694\n",
      "subject9694\n",
      "subject2432\n",
      "subject2432\n",
      "subject2432\n",
      "subject9694\n",
      "subject9798\n",
      "subject9798\n",
      "subject9798\n",
      "subject3707\n",
      "subject9798\n",
      "subject9694\n",
      "subject2432\n",
      "subject2432\n",
      "subject3707\n",
      "subject9798\n",
      "subject2432\n",
      "subject2432\n",
      "subject9694\n",
      "subject2432\n",
      "subject9694\n",
      "subject9798\n",
      "subject7039\n",
      "subject9798\n",
      "subject9798\n",
      "subject9798\n",
      "subject9694\n",
      "subject2432\n",
      "subject9694\n",
      "subject3707\n",
      "subject2432\n",
      "subject2432\n",
      "subject2432\n",
      "subject9694\n",
      "subject9694\n",
      "subject2432\n",
      "subject1272\n",
      "subject9798\n",
      "subject9798\n",
      "subject1272\n",
      "subject1272\n",
      "subject1272\n",
      "subject1272\n",
      "subject1272\n",
      "subject1272\n",
      "subject1272\n",
      "subject1272\n",
      "subject1272\n",
      "subject1272\n",
      "subject1272\n",
      "subject1272\n",
      "subject1272\n",
      "subject1272\n",
      "subject1272\n",
      "subject1272\n",
      "subject9798\n",
      "subject3707\n",
      "subject3707\n",
      "subject1272\n",
      "subject1272\n",
      "subject1272\n",
      "subject1272\n",
      "subject2432\n",
      "subject1272\n",
      "subject2432\n",
      "subject1272\n",
      "subject1272\n",
      "subject3707\n",
      "subject2432\n",
      "subject2432\n",
      "subject2432\n",
      "subject9694\n",
      "subject9694\n",
      "subject9798\n",
      "subject9798\n",
      "subject9694\n",
      "subject9694\n",
      "subject9798\n",
      "subject9694\n",
      "subject2432\n",
      "subject2432\n",
      "subject9694\n",
      "subject9694\n",
      "subject2432\n",
      "subject9798\n",
      "subject2432\n",
      "subject3707\n",
      "subject9694\n",
      "subject9798\n",
      "subject2432\n",
      "subject9798\n",
      "subject9798\n",
      "subject2432\n",
      "subject9694\n",
      "subject2432\n",
      "subject9694\n",
      "subject9798\n",
      "subject3707\n",
      "subject9694\n",
      "subject2432\n",
      "subject2432\n",
      "subject9694\n",
      "subject1272\n",
      "subject3707\n",
      "subject9798\n",
      "subject9694\n",
      "subject2432\n",
      "subject9694\n",
      "subject2432\n",
      "subject2432\n",
      "subject2341\n",
      "subject2432\n",
      "subject9798\n",
      "subject2432\n",
      "subject9798\n",
      "subject9798\n",
      "subject9798\n",
      "subject3707\n",
      "subject2432\n",
      "subject2432\n",
      "subject2432\n",
      "subject2432\n",
      "subject9798\n",
      "subject4058\n",
      "subject9454\n",
      "subject9798\n",
      "subject2432\n",
      "subject9454\n",
      "subject9694\n",
      "subject9694\n",
      "subject3993\n",
      "subject9694\n",
      "subject9454\n",
      "subject9454\n",
      "subject9454\n",
      "subject9798\n",
      "subject2432\n",
      "subject3993\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject3707\n",
      "subject9694\n",
      "subject9694\n",
      "subject9798\n",
      "subject9454\n",
      "subject9454\n",
      "subject3707\n",
      "subject9798\n",
      "subject2827\n",
      "subject9454\n",
      "subject9694\n",
      "subject9454\n",
      "subject9798\n",
      "subject9694\n",
      "subject9218\n",
      "subject9218\n",
      "subject9218\n",
      "subject2432\n",
      "subject9218\n",
      "subject9218\n",
      "subject9218\n",
      "subject9454\n",
      "subject3707\n",
      "subject2827\n",
      "subject9798\n",
      "subject9218\n",
      "subject2827\n",
      "subject2827\n",
      "subject9218\n",
      "subject2827\n",
      "subject2827\n",
      "subject2827\n",
      "subject2827\n",
      "subject9454\n",
      "subject9454\n",
      "subject2827\n",
      "subject2827\n",
      "subject9694\n",
      "subject9694\n",
      "subject3707\n",
      "subject2827\n",
      "subject2827\n",
      "subject9454\n",
      "subject9218\n",
      "subject2827\n",
      "subject9218\n",
      "subject9218\n",
      "subject3707\n",
      "subject9218\n",
      "subject3707\n",
      "subject9218\n",
      "subject9218\n",
      "subject2827\n",
      "subject9454\n",
      "subject2827\n",
      "subject9454\n",
      "subject3993\n",
      "subject2827\n",
      "subject9798\n",
      "subject9454\n",
      "subject9454\n",
      "subject9454\n",
      "subject2961\n",
      "subject2827\n",
      "subject9454\n",
      "subject9218\n",
      "subject9218\n",
      "subject9218\n",
      "subject9218\n",
      "subject2432\n",
      "subject9454\n",
      "subject9798\n",
      "subject6900\n",
      "subject2961\n",
      "subject2961\n",
      "subject2961\n",
      "subject9454\n",
      "subject2827\n",
      "subject2827\n",
      "subject9218\n",
      "subject2827\n",
      "subject2827\n",
      "subject9218\n",
      "subject2827\n",
      "subject2961\n",
      "subject9218\n",
      "subject9798\n",
      "subject5897\n",
      "subject9694\n",
      "subject9218\n",
      "subject2827\n",
      "subject9218\n",
      "subject2827\n",
      "subject9218\n",
      "subject9694\n",
      "subject2432\n",
      "subject2827\n",
      "subject9454\n",
      "subject2432\n",
      "subject2961\n",
      "subject9218\n",
      "subject9694\n",
      "subject2961\n",
      "subject2432\n",
      "subject9454\n",
      "subject2432\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9798\n",
      "subject2827\n",
      "subject2432\n",
      "subject9798\n",
      "subject9694\n",
      "subject6900\n",
      "subject9798\n",
      "subject3993\n",
      "subject9694\n",
      "subject6900\n",
      "subject6900\n",
      "subject6900\n",
      "subject6900\n",
      "subject9798\n",
      "subject3993\n",
      "subject9694\n",
      "subject6900\n",
      "subject9694\n",
      "subject9694\n",
      "subject2903\n",
      "subject4058\n",
      "subject3707\n",
      "subject3707\n",
      "subject9694\n",
      "subject9694\n",
      "subject9694\n",
      "subject9798\n",
      "subject3993\n",
      "subject9694\n",
      "subject2903\n",
      "subject6900\n",
      "subject6900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject6900\n",
      "subject9798\n",
      "subject2903\n",
      "subject2903\n"
     ]
    }
   ],
   "source": [
    "user_level_data, subjects_split, vocabulary = load_erisk_data(writings_df, \n",
    "                                                            seq_len=hyperparams_features['maxlen'],\n",
    "                                                            voc_size=hyperparams_features['max_features'],\n",
    "                                                           emotion_lexicon=nrc_lexicon,\n",
    "                                                           emotions=emotions,\n",
    "                                                           user_level=hyperparams_features['user_level'],\n",
    "                                                                                logger=logger\n",
    "#                                                            vocabulary=pickle.load(open('vocabulary20K_selfharm.pkl', 'rb'))\n",
    "                                                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 20000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "def load_embeddings(path, embedding_dim, voc):\n",
    "    # random matrix with mean value = 0\n",
    "    embedding_matrix = np.random.random((len(voc)+2, embedding_dim)) - 0.5 # voc + unk + pad value(0)\n",
    "\n",
    "    f = open(path)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_i = voc.get(word)\n",
    "        if word_i is not None:\n",
    "            embedding_matrix[word_i] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print('Total %s word vectors.' % len(embedding_matrix))\n",
    "\n",
    " \n",
    "    return embedding_matrix\n",
    "\n",
    "pretrained_embeddings_path = root_dir + '/resources/glove.twitter.27B/glove.twitter.27B.%dd.txt' % hyperparams_features['embedding_dim']\n",
    "embedding_matrix = load_embeddings(pretrained_embeddings_path, hyperparams_features['embedding_dim'], vocabulary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, user_level_data, subjects_split, set_type='train', bert_tokenizer=bert_tokenizer,\n",
    "                 batch_size=hyperparams_features['batch_size'], seq_len=hyperparams_features['maxlen'], \n",
    "                 voc_size=hyperparams_features['max_features'], emotion_lexicon=nrc_lexicon,\n",
    "                 emotions=emotions, pronouns=[\"i\", \"me\", \"my\", \"mine\", \"myself\"], \n",
    "                 max_posts_per_user=hyperparams_features['posts_per_user'],\n",
    "                 shuffle=True):\n",
    "        'Initialization'\n",
    "        self.seq_len = seq_len\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        self.subjects_split = subjects_split\n",
    "        self.set = set_type\n",
    "        self.emotion_lexicon = emotion_lexicon\n",
    "        self.batch_size = batch_size\n",
    "        self.data = user_level_data\n",
    "        self.emotions = emotions\n",
    "        self.pronouns = pronouns\n",
    "        self.shuffle = shuffle\n",
    "        self.voc_size = voc_size\n",
    "        self.max_posts_per_user = max_posts_per_user\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __encode_text(self, tokens, raw_text):\n",
    "        # Using voc_size-1 value for OOV token\n",
    "        encoded_tokens = [vocabulary.get(w, self.voc_size-1) for w in tokens]\n",
    "        encoded_emotions = encode_emotions(tokens, self.emotion_lexicon, self.emotions)\n",
    "        encoded_pronouns = encode_pronouns(tokens, self.pronouns)\n",
    "        encoded_stopwords = encode_stopwords(tokens)\n",
    "        bert_ids, bert_masks, bert_segments, label = encode_text_for_bert(self.bert_tokenizer, InputExample(None, \n",
    "                                               raw_text), self.seq_len)\n",
    "        return (encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords,\n",
    "               bert_ids, bert_masks, bert_segments)\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.subjects_split[self.set]) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        user_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Find users\n",
    "        users = [self.subjects_split[self.set][i] for i in user_indexes\n",
    "                    if self.subjects_split[self.set][i] in self.data.keys()] # TODO: maybe needs a warning that user is missing\n",
    "\n",
    "        post_indexes = {}\n",
    "        # Sample post ids\n",
    "        for subject in users:\n",
    "            posts_len = len(self.data[subject]['texts'])\n",
    "            posts_index_sample = sorted(np.random.choice(posts_len, \n",
    "                                                         min(self.max_posts_per_user, posts_len),\n",
    "                                                         replace=False))\n",
    "            post_indexes[subject] = posts_index_sample\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(users, post_indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.subjects_split[self.set]))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, users, post_indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        tokens_data = []\n",
    "        categ_data = []\n",
    "        sparse_data = []\n",
    "        subjects = []\n",
    "        bert_ids_data = []\n",
    "        bert_masks_data = []\n",
    "        bert_segments_data = []\n",
    "        labels = []\n",
    "\n",
    "        for subject in users:\n",
    "            texts = self.data[subject]['texts']\n",
    "            raw_texts = self.data[subject]['raw']\n",
    "            label = self.data[subject]['label']\n",
    "            liwc_scores = self.data[subject]['liwc']\n",
    "            \n",
    "            # Sample\n",
    "            texts = [texts[i] for i in post_indexes[subject]]\n",
    "            liwc_selection = [liwc_scores[i] for i in post_indexes[subject]]\n",
    "            raw_texts = [raw_texts[i] for i in post_indexes[subject]]\n",
    "            \n",
    "            all_words = [sum(texts, [])] # merge all texts in one list -- Ok, why sum?? this is wrong!!\n",
    "            liwc_aggreg = [np.array(liwc_selection).mean(axis=0).tolist()]\n",
    "            all_raw_texts = [\" \".join(raw_texts)]\n",
    "            \n",
    "            for i, words in enumerate(all_words):\n",
    "                encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords, \\\n",
    "                    bert_ids, bert_masks, bert_segments = self.__encode_text(words, all_raw_texts[i])\n",
    "                subject_id = int(subject.split('t')[1])\n",
    "                tokens_data.append(encoded_tokens)\n",
    "                categ_data.append(encoded_emotions + [encoded_pronouns] + liwc_aggreg[i])\n",
    "                sparse_data.append(encoded_stopwords)\n",
    "                bert_ids_data.append(bert_ids)\n",
    "                bert_masks_data.append(bert_masks)\n",
    "                bert_segments_data.append(bert_segments)\n",
    "                \n",
    "                labels.append(label)\n",
    "                subjects.append(subject_id)\n",
    "\n",
    "        \n",
    "        # using zeros for padding\n",
    "        tokens_data_padded = sequence.pad_sequences(tokens_data, maxlen=self.seq_len)\n",
    "\n",
    "        return ([np.array(tokens_data_padded), np.array(categ_data), np.array(sparse_data),\n",
    "                 np.array(bert_ids_data), np.array(bert_masks_data), np.array(bert_segments_data),\n",
    "                np.array(subjects)],\n",
    "                np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, user_level_data, subjects_split, batch_size=hyperparams_features['batch_size'], \n",
    "                 seq_len=hyperparams_features['maxlen'], voc_size=hyperparams_features['max_features'], \n",
    "                 emotion_lexicon=nrc_lexicon, set_type='train', test_user_indexes=[0],\n",
    "                 emotions=emotions, pronouns=[\"i\", \"me\", \"my\", \"mine\", \"myself\"], \n",
    "                 max_posts_per_user=hyperparams_features['posts_per_user'],\n",
    "                 bert_tokenizer=bert_tokenizer,\n",
    "                 shuffle=True):\n",
    "        'Initialization'\n",
    "        self.seq_len = seq_len\n",
    "        self.emotion_lexicon = emotion_lexicon\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.data = user_level_data\n",
    "        self.all_users = list(self.data.keys())\n",
    "        self.emotions = emotions\n",
    "        self.pronouns = pronouns\n",
    "        self.set = set_type\n",
    "        self.subjects_split = subjects_split\n",
    "        self.shuffle = shuffle\n",
    "        self.voc_size = voc_size\n",
    "        self.max_posts_per_user = max_posts_per_user\n",
    "        self.test_user_indexes = test_user_indexes\n",
    "        self.on_epoch_end()\n",
    "\n",
    "\n",
    "    def __encode_text(self, tokens, raw_text):\n",
    "        # Using voc_size-1 value for OOV token\n",
    "        encoded_tokens = [vocabulary.get(w, self.voc_size-1) for w in tokens]\n",
    "        encoded_emotions = encode_emotions(tokens, self.emotion_lexicon, self.emotions)\n",
    "        encoded_pronouns = encode_pronouns(tokens, self.pronouns)\n",
    "        encoded_stopwords = encode_stopwords(tokens)\n",
    "        bert_ids, bert_masks, bert_segments, label = encode_text_for_bert(self.bert_tokenizer, InputExample(None, \n",
    "                                               raw_text), self.seq_len)\n",
    "        return (encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords,\n",
    "               bert_ids, bert_masks, bert_segments)\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.subjects_split[self.set]) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        user_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Find users\n",
    "        users = [self.subjects_split[self.set][i] for i in user_indexes\n",
    "                    if self.subjects_split[self.set][i] in self.data.keys()] # TODO: maybe needs a warning that user is missing\n",
    "\n",
    "        post_indexes = {}\n",
    "        # Sample post ids\n",
    "        for subject in users:\n",
    "            posts_len = len(self.data[subject]['texts'])\n",
    "            posts_index_sample = sorted(np.random.choice(posts_len, \n",
    "                                                         min(self.max_posts_per_user, posts_len),\n",
    "                                                         replace=False))\n",
    "            post_indexes[subject] = posts_index_sample\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(users, post_indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.subjects_split[self.set]))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, users, post_indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        tokens_data = []\n",
    "        categ_data = []\n",
    "        sparse_data = []\n",
    "        subjects = []\n",
    "        bert_ids_data = []\n",
    "        bert_masks_data = []\n",
    "        bert_segments_data = []\n",
    "        labels = []\n",
    "        for subject in users:\n",
    "            texts = self.data[subject]['texts']\n",
    "            label = self.data[subject]['labels']\n",
    "            raw_texts = self.data[subject]['raw']\n",
    "\n",
    "            # Sample\n",
    "            texts = [texts[i] for i in post_indexes[subject]]\n",
    "            liwc_selection = [self.data[subject]['liwc'][i] for i in post_indexes[subject]]\n",
    "            raw_texts = [raw_texts[i] for i in post_indexes[subject]]\n",
    "\n",
    "            all_words = [sum(texts, [])] # merge all texts in one list\n",
    "            liwc_aggreg = [np.array(liwc_selection).mean(axis=0).tolist()]\n",
    "            all_raw_texts = [\" \".join(raw_texts)]\n",
    "\n",
    "            \n",
    "            for i, words in enumerate(all_words):\n",
    "                encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords, \\\n",
    "                    bert_ids, bert_masks, bert_segments = self.__encode_text(words, all_raw_texts[i])\n",
    "                subject_id = int(subject.split('t')[1])\n",
    "                tokens_data.append(encoded_tokens)\n",
    "                categ_data.append(encoded_emotions + [encoded_pronouns] + liwc_aggreg[i])\n",
    "                sparse_data.append(encoded_stopwords)\n",
    "                labels.append(label)\n",
    "                bert_ids_data.append(bert_ids)\n",
    "                bert_masks_data.append(bert_masks)\n",
    "                bert_segments_data.append(bert_segments)\n",
    "                \n",
    "                subjects.append(subject_id)\n",
    "\n",
    "        \n",
    "        # using zeros for padding\n",
    "        tokens_data_padded = sequence.pad_sequences(tokens_data, maxlen=self.seq_len)\n",
    "\n",
    "        return ([np.array(tokens_data_padded), np.array(categ_data), np.array(sparse_data),\n",
    "#                 np.array(subjects),\n",
    "                np.array(bert_ids_data), np.array(bert_masks_data), np.array(bert_segments_data),],\n",
    "                np.array(labels).reshape(-1, len(labels)).tolist()) # to have one array per output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Don't split into the 3 sets, do leave-one-out cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subjects_split(test_size=hyperparams_features['batch_size']):\n",
    "    test_user_indexes = [np.random.randint(len(user_level_data)) for i in range(test_size)]\n",
    "\n",
    "    subjects_split = {'test': [u for i,u in \n",
    "                               enumerate(user_level_data.keys()) if i in test_user_indexes],\n",
    "                     'train': [u for i,u in \n",
    "                               enumerate(user_level_data.keys()) if i not in test_user_indexes],}\n",
    "    return subjects_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[   97,  4565,    52,    79,   214,    51,    10,     4,   285,\n",
      "          717,     2,    36,     1,   294,     5,   202,    62,   970,\n",
      "           12,   108,    15,    52,    79,     3,  5811,    35,   559,\n",
      "            8,   272,    50,    23,    11,     5,    17,   942,     3,\n",
      "         3877,     1,   508,     1,   184,     1,   616,    34,   789,\n",
      "           18,    71,    44,     1,    68,     7,   290,    18,     1,\n",
      "         1593,     5,     1,  3408,     5,    97,   102,    31,   646,\n",
      "           35,    55,    23,     6,   291,     3,    10,    51,    57,\n",
      "        13934,     8,   274,    55,    77,    23,  1218,     8,     1,\n",
      "          272,   170,    71,     5,   112,  2051,   473,     1,   682,\n",
      "           86,    40,    16,  2104,    77,   472,    53,     3,   731,\n",
      "          545]], dtype=int32), array([[0.01456311, 0.01941748, 0.        , 0.01941748, 0.01456311,\n",
      "        0.02427184, 0.03883495, 0.00485437, 0.01456311, 0.02427184,\n",
      "        0.02427184, 0.03542292, 0.01157407, 0.64589108, 0.03935185,\n",
      "        0.15212135, 0.        , 0.01435185, 0.022998  , 0.20787961,\n",
      "        0.04842593, 0.11750212, 0.13569627, 0.0037037 , 0.12976438,\n",
      "        0.0482829 , 0.        , 0.02160911, 0.01455455, 0.00740741,\n",
      "        0.27638273, 0.0037037 , 0.01666667, 0.17585682, 0.01185897,\n",
      "        0.        , 0.        , 0.12614499, 0.04176638, 0.01435185,\n",
      "        0.09229614, 0.01296296, 0.05511011, 0.        , 0.        ,\n",
      "        0.0225    , 0.        , 0.03642392, 0.03154116, 0.0267017 ,\n",
      "        0.01139601, 0.2046739 , 0.07811523, 0.        , 0.0242839 ,\n",
      "        0.01666667, 0.        , 0.00185185, 0.09729941, 0.0125    ,\n",
      "        0.22236948, 0.02037037, 0.00185185, 0.        , 0.01      ,\n",
      "        0.        , 0.        , 0.02926638, 0.0027027 , 0.01324786,\n",
      "        0.00925926, 0.        , 0.01371083, 0.04055556, 0.06985697]]), array([[1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n",
      "        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0]]), array([[  101,  2339,  2003,  2976,  6648,  2025,  1999,  2256,  2190,\n",
      "         5426,  1029,  2024,  2017,  2033,  1029,  2009,  1005,  1055,\n",
      "         2035,  1999,  1996,  2208,  1012,  2009,  2790,  2111,  2020,\n",
      "         2006,  2367,  8311,  1012,  2009,  2106,  2025,  3043,  1996,\n",
      "         6120,  1997,  2111,  1005,  1055,  3096,  1010,  2074,  2065,\n",
      "         2027,  2318,  2043,  1996,  3105,  2001,  2062, 12382,  1012,\n",
      "        27137,  1012,  1996,  6956,  2734,  2169,  2060,  2000,  2663,\n",
      "         1996,  2162,  1012,  1045,  2587,  2009,  2044,  1013,  1054,\n",
      "         1013,  2739,  2318,  6318,  1996,  2679,  1997, 15554,  1010,\n",
      "         2021,  2009,  2855,  2150,  1037,  4033,  2005, 13157,  1012,\n",
      "         1045,  2572,  2025,  3331,  2055,  2157,  3358,  2111,  1010,\n",
      "          102]]), array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])]\n"
     ]
    }
   ],
   "source": [
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# TODO: it is skipping the last batch\n",
    "x_data = {'train': [], 'test': []}\n",
    "y_data = {'train': [], 'test': []}\n",
    "subjects_split = get_subjects_split()\n",
    "for set_type in ['test']:\n",
    "    for x, y in DataGenerator(user_level_data, batch_size=hyperparams_features['batch_size'],\n",
    "                            set_type=set_type,\n",
    "                             subjects_split=subjects_split):\n",
    "        print(x)\n",
    "        x_data[set_type].append(x)\n",
    "        y_data[set_type].append(y)\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': ['subject4058'],\n",
       " 'train': ['subject3993',\n",
       "  'subject9798',\n",
       "  'subject7039',\n",
       "  'subject436',\n",
       "  'subject6619',\n",
       "  'subject6635',\n",
       "  'subject2903',\n",
       "  'subject5791',\n",
       "  'subject9694',\n",
       "  'subject5897',\n",
       "  'subject2341',\n",
       "  'subject2432',\n",
       "  'subject3707',\n",
       "  'subject1272',\n",
       "  'subject9218',\n",
       "  'subject9454',\n",
       "  'subject2961',\n",
       "  'subject2827',\n",
       "  'subject6900']}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjects_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1,2],[3,4]]).reshape(2,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(subjects_split[s]) for s in ['train', 'test']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': [],\n",
       " 'test': [[[0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [1],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [1],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [0],\n",
       "   [3],\n",
       "   [2],\n",
       "   [1],\n",
       "   [0],\n",
       "   [0],\n",
       "   [3],\n",
       "   [0]]]}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'lstm_units': 10,\n",
    "    'dense_bow_units': 20,\n",
    "    'dropout': 0.0,\n",
    "    'l2_dense': 0.00000011,\n",
    "    'l2_embeddings': 0.000001,\n",
    "    'dense_sentence_units': 100,\n",
    "    'optimizer': 'adam',\n",
    "    'bert_dense_units': 100,\n",
    "    'decay': 0.00001,\n",
    "    'lr': 0.001,\n",
    "    \"trainable_embeddings\": True,\n",
    "    \"reduce_lr_factor\": 0.0002,\n",
    "    \"reduce_lr_patience\": 1000,\n",
    "    \"freeze_patience\": 500,\n",
    "    'threshold': 0.5,\n",
    "    'bert_len': 768,\n",
    "    'ignore_layer': ['batchnorm'],\n",
    "    'norm_momentum': 0.1,\n",
    "\n",
    "}\n",
    "if not hyperparams['optimizer']:\n",
    "    hyperparams['optimizer'] = optimizers.Adam(lr=hyperparams['lr'], #beta_1=0.9, beta_2=0.999, epsilon=0.0001,\n",
    "                                   decay=hyperparams['decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Metrics():\n",
    "#     def __init__(self, threshold=0.5):\n",
    "#         self.threshold=threshold\n",
    "        \n",
    "#     def recall_m(self, y_true, y_pred):\n",
    "#             y_labels = y_true\n",
    "#             y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), self.threshold), K.floatx())        \n",
    "#             possible_positives = K.sum(K.round(K.clip(y_labels, 0, 1)))\n",
    "#             true_positives = K.sum(K.round(K.clip(y_labels * y_pred, 0, 1)))\n",
    "#             recall = true_positives / (possible_positives + K.epsilon())\n",
    "#             return recall\n",
    "\n",
    "#     def precision_m(self, y_true, y_pred):\n",
    "#             y_labels = y_true\n",
    "#             y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), self.threshold), K.floatx())        \n",
    "#             true_positives = K.sum(K.round(K.clip(y_labels * y_pred, 0, 1)))\n",
    "#             predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "#             precision = true_positives / (predicted_positives + K.epsilon())\n",
    "#             return precision\n",
    "\n",
    "#     def f1_m(self, y_true, y_pred):\n",
    "#         precision = self.precision_m(y_true, y_pred)\n",
    "#         recall = self.recall_m(y_true, y_pred)\n",
    "#         return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "# def binary_crossentropy_custom(y_true, y_pred):\n",
    "#     y_labels = y_true\n",
    "#     return K.binary_crossentropy(y_labels, \n",
    "#                                  y_pred)\n",
    "\n",
    "# metrics_class = Metrics(threshold=hyperparams['threshold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopwords_list,\n",
    "                liwc_categories, nr_classes,\n",
    "               ignore_layer=[]):\n",
    "\n",
    "    tokens_features = Input(shape=(hyperparams_features['maxlen'],), name='word_seq')\n",
    "    embedding_layer = Embedding(hyperparams_features['max_features'], \n",
    "                                hyperparams_features['embedding_dim'], \n",
    "                                input_length=hyperparams_features['maxlen'],\n",
    "                                embeddings_regularizer=regularizers.l2(hyperparams['l2_embeddings']),\n",
    "                                weights=[embedding_matrix], \n",
    "                                trainable=hyperparams['trainable_embeddings'],\n",
    "                               name='embeddings_layer')(\n",
    "        tokens_features)\n",
    "#     if 'batchnorm' not in ignore_layer:\n",
    "#         embedding_layer_norm = BatchNormalization(axis=-1, momentum=hyperparams['norm_momentum'],\n",
    "#                                                      name='embeddings_layer_norm')(embedding_layer)\n",
    "#     lstm_layers = Bidirectional(LSTM(hyperparams['lstm_units']))(embedding_layer)\n",
    "\n",
    "\n",
    "    lstm_layers = LSTM(hyperparams['lstm_units'], \n",
    "                           return_sequences='attention' not in ignore_layer,\n",
    "                      name='LSTM_layer')(embedding_layer)\n",
    "    \n",
    "    # Attention\n",
    "    if 'attention' not in ignore_layer:\n",
    "        attention = Dense(1, activation='tanh', name='attention')(lstm_layers)\n",
    "        attention = Flatten()(attention)\n",
    "        attention = Activation('softmax')(attention)\n",
    "        attention = RepeatVector(hyperparams['lstm_units'])(attention)\n",
    "        attention = Permute([2, 1])(attention)\n",
    "\n",
    "        sent_representation = Multiply()([lstm_layers, attention])\n",
    "        sent_representation = Lambda(lambda xin: K.sum(xin, axis=1), \n",
    "                                     output_shape=(hyperparams['lstm_units'],)\n",
    "                                    )(sent_representation)\n",
    "\n",
    "        \n",
    "    else:\n",
    "        sent_representation = lstm_layers\n",
    "        \n",
    "    \n",
    "    sent_representation = Dropout(hyperparams['dropout'], name='lstm_att_dropout')(sent_representation)\n",
    "    if hyperparams['dense_sentence_units']:\n",
    "        sent_representation = Dense(units=hyperparams['dense_sentence_units'],\n",
    "                                   name='dense_sent_representation')(sent_representation)\n",
    "    numerical_features = Input(shape=(len(emotions) + 1 + len(liwc_categories),), name='numeric_input') # emotions and pronouns\n",
    "    \n",
    "    in_id_bert = Input(shape=(hyperparams_features['maxlen'],), name=\"input_ids_bert\")\n",
    "    in_mask_bert = Input(shape=(hyperparams_features['maxlen'],), name=\"input_masks_bert\")\n",
    "    in_segment_bert = Input(shape=(hyperparams_features['maxlen'],), name=\"segment_ids_bert\")\n",
    "    bert_layer = hub.KerasLayer(\n",
    "        \"https://tfhub.dev/google/albert_xlarge/3\",\n",
    "        trainable=False,\n",
    "        signature=\"tokens\",\n",
    "        output_key=\"pooled_output\",\n",
    "    )\n",
    "\n",
    "    bert_features = {\n",
    "        \"input_ids\": in_id_bert,\n",
    "        \"input_mask\": in_mask_bert,\n",
    "        \"segment_ids\": in_segment_bert,\n",
    "    }\n",
    "    bert_output = bert_layer(bert_features)\n",
    "    dense_layer_bert = Dense(units=hyperparams['bert_dense_units'],\n",
    "                        kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                        name='bert_dense_layer',\n",
    "                       )(bert_output)\n",
    "    sparse_features = Input(shape=(len(stopwords_list),), name='sparse_input') # stopwords\n",
    "\n",
    "    if hyperparams['dense_bow_units']:\n",
    "        dense_layer_sparse = Dense(units=hyperparams['dense_bow_units'],\n",
    "                              name='sparse_feat_dense_layer',\n",
    "                                kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                              )(sparse_features)\n",
    "    else:\n",
    "        dense_layer_sparse = sparse_features\n",
    "    \n",
    "    if 'batchnorm' not in ignore_layer:\n",
    "        numerical_features_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                     name='numerical_features_norm')(numerical_features)\n",
    "        sent_representation_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                      name='sent_repr_norm')(sent_representation)\n",
    "\n",
    "        dense_layer_sparse_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                         name='sparse_features_norm')(dense_layer_sparse)\n",
    "        dense_layer_bert_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "                                                         name='bert_features_norm')(dense_layer_bert)\n",
    "        \n",
    "    subjects = Input(shape=(1,), name='subjects')\n",
    "    \n",
    "\n",
    "    all_layers = {\n",
    "        'lstm_layers': sent_representation,\n",
    "        'numerical_dense_layer': numerical_features,\n",
    "        'sparse_feat_dense_layer': dense_layer_sparse,\n",
    "        'bert_layer': dense_layer_bert\n",
    "    }\n",
    "    if 'batchnorm' not in ignore_layer:\n",
    "        all_layers = {\n",
    "            'lstm_layers': sent_representation_norm,\n",
    "            'numerical_dense_layer': numerical_features_norm,\n",
    "            'sparse_feat_dense_layer': dense_layer_sparse_norm,\n",
    "            'bert_layer': dense_layer_bert_norm\n",
    "        }\n",
    "    layers_to_merge = []\n",
    "    for n, l in all_layers.items():\n",
    "        if n in ignore_layer:\n",
    "            continue\n",
    "        layers_to_merge.append(l)\n",
    "        \n",
    "    if len(layers_to_merge) == 1:\n",
    "        merged_layers = layers_to_merge[0]\n",
    "    else:\n",
    "        merged_layers = concatenate(layers_to_merge)\n",
    "    output_layers = []\n",
    "    for label in range(nr_classes):\n",
    "        output_layer = Dense(1, activation='softmax',\n",
    "                         name='output_layer%d' % label,\n",
    "                        kernel_regularizer=regularizers.l2(hyperparams['l2_dense']))(merged_layers)\n",
    "        output_layers.append(output_layer)\n",
    "\n",
    "    # Compile model\n",
    "    model = Model(inputs=[tokens_features, numerical_features, sparse_features,\n",
    "                         in_id_bert, in_mask_bert, in_segment_bert], \n",
    "                  outputs=output_layers)\n",
    "\n",
    "    model.compile(hyperparams['optimizer'], {'output_layer%d'%i: \n",
    "                                             'mean_squared_error' for i in range(nr_classes)},\n",
    "                  metrics={'output_layer%d' % label: \n",
    "                           ['accuracy', 'mean_squared_error'] for label in range(nr_classes)})\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "word_seq (InputLayer)           [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embeddings_layer (Embedding)    (None, 100, 50)      1000000     word_seq[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "LSTM_layer (LSTM)               (None, 100, 10)      2440        embeddings_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention (Dense)               (None, 100, 1)       11          LSTM_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 100)          0           attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 100)          0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector (RepeatVector)    (None, 10, 100)      0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "permute (Permute)               (None, 100, 10)      0           repeat_vector[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 100, 10)      0           LSTM_layer[0][0]                 \n",
      "                                                                 permute[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 10)           0           multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_ids_bert (InputLayer)     [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks_bert (InputLayer)   [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids_bert (InputLayer)   [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_att_dropout (Dropout)      (None, 10)           0           lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sparse_input (InputLayer)       [(None, 179)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      (None, 2048)         59017392    input_ids_bert[0][0]             \n",
      "                                                                 input_masks_bert[0][0]           \n",
      "                                                                 segment_ids_bert[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_sent_representation (Dens (None, 100)          1100        lstm_att_dropout[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "numeric_input (InputLayer)      [(None, 75)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sparse_feat_dense_layer (Dense) (None, 20)           3600        sparse_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bert_dense_layer (Dense)        (None, 100)          204900      keras_layer_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 295)          0           dense_sent_representation[0][0]  \n",
      "                                                                 numeric_input[0][0]              \n",
      "                                                                 sparse_feat_dense_layer[0][0]    \n",
      "                                                                 bert_dense_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "output_layer0 (Dense)           (None, 1)            296         concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 60,229,739\n",
      "Trainable params: 1,212,347\n",
      "Non-trainable params: 59,017,392\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopword_list,\n",
    "                    liwc_categories=[c for c in categories if c in writings_df.columns],\n",
    "                    nr_classes=1,\n",
    "                   ignore_layer=hyperparams['ignore_layer'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, \n",
    "                data_generator_train, data_generator_valid,\n",
    "                epochs, start_epoch=0, workers=4,\n",
    "                callback_list = [],\n",
    "                model_path='/tmp/model',\n",
    "               verbose=1):\n",
    "    logging.info('Train...')\n",
    "    experiment.log_parameter('callbacks', callbacks)\n",
    "\n",
    "    history = model.fit_generator(data_generator_train,\n",
    "#               batch_size=batch_size,\n",
    "#                 steps_per_epoch=steps_per_epoch,\n",
    "              epochs=epochs, initial_epoch=start_epoch, \n",
    "              validation_data=data_generator_valid,\n",
    "                        verbose=verbose,\n",
    "#               validation_split=0.3,\n",
    "                       workers=workers,\n",
    "            callbacks = [\n",
    "                callbacks.ModelCheckpoint(filepath='%s_best' % model_path, verbose=1, \n",
    "                                          save_best_only=True),\n",
    "                callbacks.EarlyStopping(patience=500), *callback_list\n",
    "            ])\n",
    "    model.save(model_path)\n",
    "    experiment.log_parameter('model_path', model_path)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary:\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     url: https://www.comet.ml/ananana/mental/81432da8b90b4dcba49474b12d695748\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     sys.gpu.0.free_memory [2]    : (350224384.0, 350224384.0)\n",
      "COMET INFO:     sys.gpu.0.gpu_utilization [2]: (0.0, 0.0)\n",
      "COMET INFO:     sys.gpu.0.total_memory       : (8370061312.0, 8370061312.0)\n",
      "COMET INFO:     sys.gpu.0.used_memory [2]    : (8019836928.0, 8019836928.0)\n",
      "COMET INFO:     sys.gpu.1.free_memory [2]    : (8239185920.0, 8239185920.0)\n",
      "COMET INFO:     sys.gpu.1.gpu_utilization [2]: (0.0, 0.0)\n",
      "COMET INFO:     sys.gpu.1.total_memory       : (8367439872.0, 8367439872.0)\n",
      "COMET INFO:     sys.gpu.1.used_memory [2]    : (128253952.0, 128253952.0)\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     git-patch: 1\n",
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/ananana/mental/5a77836e030f43b7bb9b036feec407f7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(api_key=\"eoBdVyznAhfg3bK9pZ58ZSXfv\",\n",
    "                        project_name=\"mental\", workspace=\"ananana\", disabled=False)\n",
    "\n",
    "experiment.log_parameters(hyperparams_features)\n",
    "\n",
    "experiment.log_parameter('emotion_lexicon', nrc_lexicon_path)\n",
    "experiment.log_parameter('emotions', emotions)\n",
    "experiment.log_parameter('embeddings_path', pretrained_embeddings_path)\n",
    "\n",
    "experiment.add_tag('T2')\n",
    "experiment.log_parameters(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-4775b4cb9518>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m            \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                       \u001b[0mcallback_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                       model_path='models/mlp_t21', workers=1)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-83-eb6fc0a59582>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, data_generator_train, data_generator_valid, epochs, start_epoch, workers, callback_list, model_path, verbose)\u001b[0m\n\u001b[1;32m     19\u001b[0m                 callbacks.ModelCheckpoint(filepath='%s_best' % model_path, verbose=1, \n\u001b[1;32m     20\u001b[0m                                           save_best_only=True),\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcallback_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             ])\n\u001b[1;32m     23\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tftf3.6/local/lib/python3.6/site-packages/comet_ml/monkey_patching.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m                     )\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Call after callbacks once we have the return value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tftf3.6/local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tftf3.6/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1412\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m   @deprecation.deprecated(\n",
      "\u001b[0;32m~/tftf3.6/local/lib/python3.6/site-packages/comet_ml/monkey_patching.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m                     )\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Call after callbacks once we have the return value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tftf3.6/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tftf3.6/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    748\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m           model=self)\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tftf3.6/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tftf3.6/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, shuffle, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tftf3.6/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[1;32m    790\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m     \u001b[0moutput_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_dynamic_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m     \u001b[0moutput_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tftf3.6/local/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tftf3.6/local/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tftf3.6/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_get_dynamic_shape\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_dynamic_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m       \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m       \u001b[0;31m# Unknown number of dimensions, `as_list` cannot be called.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "subjects_split = get_subjects_split()\n",
    "data_generator_train = DataGenerator(user_level_data, set_type='train', \n",
    "                                     subjects_split=subjects_split)\n",
    "data_generator_valid = DataGenerator(user_level_data, set_type='test',  \n",
    "                                     subjects_split=subjects_split)\n",
    "model, history = train_model(model, data_generator_train, data_generator_valid,\n",
    "           epochs=1000, start_epoch=0,\n",
    "                      callback_list = [],\n",
    "                      model_path='models/mlp_t21', workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([array([[    2,   100,   216,    74,     1,  1035,     8,     1,   889,\n",
      "          291,     3,     1,  1991,    11,     1,  1927,  1164,   247,\n",
      "            8,   171,   127,  5001,     2,    73,     2,   236,     3,\n",
      "          109,    44,     4,   317,    24,  5315,  4116,    18,    16,\n",
      "         1255,    12,   173,   174,  1238,   377,   145,  1000,    15,\n",
      "           64,    31,   153,    31,   340, 17929, 17930,     2,   287,\n",
      "            1,  1252,   908,     5,    84,   434,     2,  3645,     3,\n",
      "            9,  1249,    21,    16,   871,    11,    16,  2786,    15,\n",
      "            2,   105,  1029,     1,  2426,     3,   321,   142,     2,\n",
      "          688,    53,     1,   438,   405,  4908,    64,     1,  3256,\n",
      "         1177,     1,   540,    20,    67,  1971,    65,  1532,     1,\n",
      "          525]], dtype=int32), array([[8.15660685e-03, 2.28384992e-02, 9.78792822e-03, 1.63132137e-02,\n",
      "        2.93637847e-02, 2.12071778e-02, 5.05709625e-02, 8.15660685e-03,\n",
      "        3.26264274e-03, 2.61011419e-02, 6.03588907e-02, 4.16666667e-03,\n",
      "        2.24106071e-02, 1.36298804e-02, 6.99855067e-02, 3.87009482e-02,\n",
      "        2.08333333e-03, 4.14331074e-02, 5.37235311e-01, 0.00000000e+00,\n",
      "        4.82235191e-02, 6.71494884e-02, 1.23774510e-02, 9.55414013e-04,\n",
      "        1.88755113e-01, 1.81524565e-02, 3.44827586e-03, 1.87557694e-02,\n",
      "        5.02569274e-02, 1.28508478e-01, 1.76450426e-01, 9.55414013e-04,\n",
      "        1.79303493e-01, 1.16119886e-02, 3.92156863e-03, 5.01596428e-02,\n",
      "        7.26675346e-02, 4.83310409e-02, 3.88989991e-03, 1.29143851e-01,\n",
      "        1.69583015e-01, 3.08766660e-02, 5.48442227e-02, 2.87096398e-02,\n",
      "        6.94456355e-02, 8.63818571e-02, 2.02746097e-02, 1.13287106e-02,\n",
      "        3.18471338e-04, 9.26074390e-03, 1.04240961e-01, 2.53184713e-02,\n",
      "        6.36942675e-04, 1.59235669e-03, 6.12745098e-03, 2.16792958e-02,\n",
      "        3.30799836e-02, 2.59575760e-01, 1.53453897e-02, 0.00000000e+00,\n",
      "        6.36942675e-04, 8.05945128e-03, 3.35721868e-03, 1.22151584e-02,\n",
      "        3.30911200e-02, 9.93737330e-02, 7.70504477e-03, 7.28167202e-03,\n",
      "        9.06421789e-03, 2.78408033e-02, 6.08347886e-02, 1.85245469e-02,\n",
      "        3.53260870e-03, 6.36942675e-04, 3.52777911e-02]]), array([[1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
      "        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
      "        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
      "        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0]]), array([[  101,  1996, 10722, 14905, 20974,  2466,  2001, 10392,  1012,\n",
      "         1045,  2293,  2129,  3809,  1998,  9677,  2002,  2081,  2014,\n",
      "         1012,  1031,  1000,  2021,  2339,  2079,  2017,  2215,  1996,\n",
      "         8351,  2000,  4521,  1996, 25945,  3676,  5910,  1029,  1000,\n",
      "         1033,  1006,  8299,  1024,  1013,  1013,  1045,  1012, 10047,\n",
      "        27390,  1012,  4012,  1013, 12170, 19190,  2319,  2860,  1012,\n",
      "        16545,  2290,  1007,  1031,  1000,  2821,  1010,  2272,  1010,\n",
      "         2272,   999,  1000,  1033,  1006,  8299,  1024,  1013,  1013,\n",
      "         1045,  1012, 10047, 27390,  1012,  4012,  1013, 24185, 19862,\n",
      "         2509,  2546,  1012, 16545,  2290,  1007,  1008,  1044, 10695,\n",
      "        10695, 10695, 10695, 10695, 10695,  2290,  1008,  1000,  1045,\n",
      "          102]]), array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])], [[1], [1], [0], [1], [0], [0], [2], [0], [0], [0], [0], [1], [0], [0], [0], [0], [2], [0], [0], [0], [2]])\n"
     ]
    }
   ],
   "source": [
    "for d in DataGenerator(user_level_data, set_type='train', \n",
    "                                     subjects_split=subjects_split):\n",
    "    print((d))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tokens(row):\n",
    "    tokens = []\n",
    "    if row.tokenized_text:\n",
    "        tokens += row.tokenized_text\n",
    "    if row.tokenized_title:\n",
    "        tokens += row.tokenized_title\n",
    "    return tokens\n",
    "writings_df['all_tokens'] = writings_df.apply (lambda row: merge_tokens(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: include the title\n",
    "def extract_emotions(tokens, emotion, relative=True):\n",
    "    if not tokens:\n",
    "        return None\n",
    "    emotion_words = [t for t in tokens \n",
    "                     if t in nrc_lexicon[emotion]]\n",
    "    if relative:\n",
    "        return len(emotion_words) / len(tokens)\n",
    "    else:\n",
    "        return len(emotion_words)\n",
    "    \n",
    "    return encoded_emotions\n",
    "\n",
    "from functools import partial\n",
    "for emotion in emotions:\n",
    "    writings_df[emotion] = writings_df['all_tokens'].apply(partial(extract_emotions, emotion=emotion, \n",
    "                                                                   relative=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['pronouns'] = writings_df['all_tokens'].apply(partial(encode_pronouns, relative=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df[['label%i'%i for i in range(21)] + ['text', 'pronouns', 'text_len'] + emotions].corr('spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df['label15'] = writings_df['label15'].apply(lambda l: encode_labels([l])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df['label17'] = writings_df['label17'].apply(lambda l: encode_labels([l])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df.groupby('subject').mean()[\n",
    "    ['label%i'%i for i in range(21)] + ['pronouns', 'text_len'] + emotions].corr(\n",
    "    'spearman')[['pronouns', 'text_len'] + emotions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df.corrwith?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from liwc_readDict import readDict\n",
    "\n",
    "liwc = readDict('/home/ana/resources/FakeOrFact/features/LIWC/LIWC/liwc.dic')\n",
    "categories = [c for (w,c) in liwc]\n",
    "set(categories)\n",
    "liwc_dict = {}\n",
    "for (w, c) in liwc:\n",
    "    if c not in liwc_dict:\n",
    "        liwc_dict[c] = []\n",
    "    liwc_dict[c].append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_liwc_categories(tokens, category_words, relative=True):\n",
    "    category_cnt = 0\n",
    "    if not tokens:\n",
    "        return None\n",
    "    text_len = len(tokens)\n",
    "    for t in tokens:\n",
    "        for word in category_words:\n",
    "            if t==word or (word[-1]=='*' and t.startswith(word[:-1])) \\\n",
    "            or (t==word.split(\"'\")[0]):\n",
    "                category_cnt += 1\n",
    "                break # one token cannot belong to more than one word in the category\n",
    "    if relative:\n",
    "        return category_cnt/text_len\n",
    "    else:\n",
    "        return category_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from functools import partial\n",
    "# for categ in ['negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']:#liwc_dict.keys():\n",
    "for categ in liwc_dict.keys():\n",
    "    if categ in writings_df.columns:\n",
    "        continue\n",
    "    print(\"Encoding for category %s...\" % categ)\n",
    "    writings_df[categ] = writings_df['all_tokens'].apply(partial(encode_liwc_categories, \n",
    "                                                                   category_words=liwc_dict[categ], \n",
    "                                                                   relative=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_categs = ['posemo', 'negemo', 'anx', 'sad', 'affect', 'feel', 'social', 'health', \n",
    "                   'sexual', 'present', 'cogmech', 'inhib']\n",
    "writings_df.groupby('subject').mean()[\n",
    "    ['label%i'%i for i in range(21)] + relevant_categs].corr(\n",
    "    'spearman')[relevant_categs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(writings_df.groupby('subject').min()[\n",
    "    ['label%i'%i for i in range(21)] + list(liwc_dict.keys())].corr()[list(liwc_dict.keys())].mean().sort_values().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(writings_df, open('writings_df_T2_liwc.pkl', 'wb+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tftf3.6",
   "language": "python",
   "name": "tftf3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
