{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob, os\n",
    "import numpy as np\n",
    "from comet_ml import Experiment, Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_KERAS'] = '1'\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, \\\n",
    "            Input, concatenate, Add, Lambda\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import callbacks, optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir_T1 = '/home/ana/eRisk/data/eRisk2020_T1_train/eRISK2020_T1_training_data/eRISK2020_training_data/data/'\n",
    "labels_file_T1 = '/home/ana/eRisk/data/eRisk2020_T1_train/eRISK2020_T1_training_data/eRISK2020_training_data/golden_truth.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_T1 = pd.read_csv(labels_file_T1, delimiter=' ', names=['subject', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fce3097c150>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAARXUlEQVR4nO3df4xldXnH8fdHVtGyFLDoZLtsuxjXRISIOkEak3ZWjCJNXE3EQFCokq5abGzKH0X9Q1tLgm1XEgm1bgNhtehK/dHdUGyDyJRqBGUVWX6UuuoW1yW7tSB1UKng0z/m0I7L7M7d+2Mu8533K7mZc77ne+55npnhM2fPPfeSqkKS1JanjbsASdLwGe6S1CDDXZIaZLhLUoMMd0lq0IpxFwBw/PHH19q1a/va95FHHuGoo44abkFPcfa8PNjz8jBIzzt27PhhVT1nvm1PiXBfu3Ytt99+e1/7Tk9PMzU1NdyCnuLseXmw5+VhkJ6T/MfBtnlZRpIaZLhLUoMMd0lqkOEuSQ0y3CWpQQuGe5JnJvlakm8luTvJn3bjJya5Lcm3k3w6yTO68SO79V3d9rWjbUGSdKBeztwfBV5ZVS8GTgXOTHI68CHg8qpaBzwEXNjNvxB4qKqeD1zezZMkLaIFw71mzXSrT+8eBbwS+Ew3vgV4fbe8oVun235GkgytYknSgtLL57knOQLYATwfuBL4S+DW7uycJGuAL1TVyUnuAs6sqj3dtu8AL6+qHx7wnBuBjQATExMv27p1a18NzMzMsHLlyr72XarseXmw5+VhkJ7Xr1+/o6om59vW0ztUq+px4NQkxwKfB14437Tu63xn6U/6C1JVm4HNAJOTk9XvO7SuuHYbm778SF/7Dmr3Zb87luP6Lr7lwZ6Xh1H1fFh3y1TVj4Bp4HTg2CRP/HE4AdjbLe8B1gB0248BHhxGsZKk3vRyt8xzujN2kjwLeBVwL3Az8MZu2gXAtm55e7dOt/1L5f/LT5IWVS+XZVYBW7rr7k8Drquq65PcA2xN8ufAN4GruvlXAZ9IsovZM/ZzRlC3JOkQFgz3qroTeMk8498FTptn/GfA2UOpTpLUF9+hKkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNWjDck6xJcnOSe5PcneTd3fgHkvwgyR3d46w5+7wnya4k9yV5zSgbkCQ92Yoe5jwGXFxV30hyNLAjyY3dtsur6q/mTk5yEnAO8CLg14EvJnlBVT0+zMIlSQe34Jl7VT1QVd/oln8M3AusPsQuG4CtVfVoVX0P2AWcNoxiJUm9Oaxr7knWAi8BbuuG3pXkziRXJzmuG1sNfH/Obns49B8DSdKQpap6m5isBP4FuLSqPpdkAvghUMAHgVVV9bYkVwJfraq/6/a7Crihqj57wPNtBDYCTExMvGzr1q19NbD/wYfZ99O+dh3YKauPGctxZ2ZmWLly5ViOPS72vDzY8+FZv379jqqanG9bL9fcSfJ04LPAtVX1OYCq2jdn+98C13ere4A1c3Y/Adh74HNW1WZgM8Dk5GRNTU31UsqTXHHtNjbt7KmNodt93tRYjjs9PU2/36+lyp6XB3senl7ulglwFXBvVX14zviqOdPeANzVLW8HzklyZJITgXXA14ZXsiRpIb2c8r4CeAuwM8kd3dh7gXOTnMrsZZndwNsBquruJNcB9zB7p81F3ikjSYtrwXCvqi8DmWfTDYfY51Lg0gHqkiQNwHeoSlKDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJatCC4Z5kTZKbk9yb5O4k7+7Gn53kxiTf7r4e140nyUeS7EpyZ5KXjroJSdIv6+XM/THg4qp6IXA6cFGSk4BLgJuqah1wU7cO8FpgXffYCHx06FVLkg5pwXCvqgeq6hvd8o+Be4HVwAZgSzdtC/D6bnkD8PGadStwbJJVQ69cknRQqareJydrgVuAk4H7q+rYOdseqqrjklwPXFZVX+7GbwL+pKpuP+C5NjJ7Zs/ExMTLtm7d2lcD+x98mH0/7WvXgZ2y+pixHHdmZoaVK1eO5djjYs/Lgz0fnvXr1++oqsn5tq3o9UmSrAQ+C/xRVf13koNOnWfsSX9BqmozsBlgcnKypqamei3ll1xx7TY27ey5jaHafd7UWI47PT1Nv9+vpcqelwd7Hp6e7pZJ8nRmg/3aqvpcN7zvicst3df93fgeYM2c3U8A9g6nXElSL3q5WybAVcC9VfXhOZu2Axd0yxcA2+aMn9/dNXM68HBVPTDEmiVJC+jlesYrgLcAO5Pc0Y29F7gMuC7JhcD9wNndthuAs4BdwE+Atw61YknSghYM9+6F0YNdYD9jnvkFXDRgXZKkAfgOVUlqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ1aMNyTXJ1kf5K75ox9IMkPktzRPc6as+09SXYluS/Ja0ZVuCTp4Ho5c78GOHOe8cur6tTucQNAkpOAc4AXdfv8dZIjhlWsJKk3C4Z7Vd0CPNjj820AtlbVo1X1PWAXcNoA9UmS+rBigH3fleR84Hbg4qp6CFgN3Dpnzp5u7EmSbAQ2AkxMTDA9Pd1XERPPgotPeayvfQfVb82DmpmZGduxx8Welwd7Hp5+w/2jwAeB6r5uAt4GZJ65Nd8TVNVmYDPA5ORkTU1N9VXIFdduY9POQf5G9W/3eVNjOe709DT9fr+WKnteHux5ePq6W6aq9lXV41X1C+Bv+f9LL3uANXOmngDsHaxESdLh6ivck6yas/oG4Ik7abYD5yQ5MsmJwDrga4OVKEk6XAtez0jyKWAKOD7JHuD9wFSSU5m95LIbeDtAVd2d5DrgHuAx4KKqenw0pUuSDmbBcK+qc+cZvuoQ8y8FLh2kKEnSYHyHqiQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1aMFwT3J1kv1J7poz9uwkNyb5dvf1uG48ST6SZFeSO5O8dJTFS5Lm18uZ+zXAmQeMXQLcVFXrgJu6dYDXAuu6x0bgo8MpU5J0OBYM96q6BXjwgOENwJZueQvw+jnjH69ZtwLHJlk1rGIlSb3p95r7RFU9ANB9fW43vhr4/px5e7oxSdIiWjHk58s8YzXvxGQjs5dumJiYYHp6uq8DTjwLLj7lsb72HVS/NQ9qZmZmbMceF3teHux5ePoN931JVlXVA91ll/3d+B5gzZx5JwB753uCqtoMbAaYnJysqampvgq54tptbNo57L9Rvdl93tRYjjs9PU2/36+lyp6XB3senn4vy2wHLuiWLwC2zRk/v7tr5nTg4Scu30iSFs+Cp7xJPgVMAccn2QO8H7gMuC7JhcD9wNnd9BuAs4BdwE+At46gZknSAhYM96o69yCbzphnbgEXDVqUJGkwvkNVkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSg1YMsnOS3cCPgceBx6pqMsmzgU8Da4HdwJuq6qHBypQkHY5hnLmvr6pTq2qyW78EuKmq1gE3deuSpEU0issyG4At3fIW4PUjOIYk6RBSVf3vnHwPeAgo4GNVtTnJj6rq2DlzHqqq4+bZdyOwEWBiYuJlW7du7auG/Q8+zL6f9rXrwE5ZfcxYjjszM8PKlSvHcuxxseflwZ4Pz/r163fMuWrySwa65g68oqr2JnkucGOSf+t1x6raDGwGmJycrKmpqb4KuOLabWzaOWgb/dl93tRYjjs9PU2/36+lyp6XB3senoEuy1TV3u7rfuDzwGnAviSrALqv+wctUpJ0ePoO9yRHJTn6iWXg1cBdwHbggm7aBcC2QYuUJB2eQa5nTACfT/LE83yyqv4pydeB65JcCNwPnD14mZKkw9F3uFfVd4EXzzP+X8AZgxQlSRrMeF6JlKSnkLWX/OPYjn3NmUeN5Hn9+AFJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJatDIwj3JmUnuS7IrySWjOo4k6clGEu5JjgCuBF4LnAScm+SkURxLkvRkozpzPw3YVVXfrar/AbYCG0Z0LEnSAVaM6HlXA9+fs74HePncCUk2Ahu71Zkk9/V5rOOBH/a570DyoXEcFRhjz2Nkz8vDsut5/YcG6vk3D7ZhVOGeecbql1aqNgObBz5QcntVTQ76PEuJPS8P9rw8jKrnUV2W2QOsmbN+ArB3RMeSJB1gVOH+dWBdkhOTPAM4B9g+omNJkg4wkssyVfVYkncB/wwcAVxdVXeP4lgM4dLOEmTPy4M9Lw8j6TlVtfAsSdKS4jtUJalBhrskNWjJhPtCH2eQ5Mgkn+6235Zk7eJXOVw99PzHSe5JcmeSm5Ic9J7XpaLXj61I8sYklWTJ3zbXS89J3tT9rO9O8snFrnHYevjd/o0kNyf5Zvf7fdY46hyWJFcn2Z/kroNsT5KPdN+PO5O8dOCDVtVT/sHsi7LfAZ4HPAP4FnDSAXP+APibbvkc4NPjrnsRel4P/Eq3/M7l0HM372jgFuBWYHLcdS/Cz3kd8E3guG79ueOuexF63gy8s1s+Cdg97roH7Pm3gZcCdx1k+1nAF5h9j9DpwG2DHnOpnLn38nEGG4At3fJngDOSzPdmqqViwZ6r6uaq+km3eiuz7ydYynr92IoPAn8B/GwxixuRXnr+feDKqnoIoKr2L3KNw9ZLzwX8ard8DEv8fTJVdQvw4CGmbAA+XrNuBY5NsmqQYy6VcJ/v4wxWH2xOVT0GPAz82qJUNxq99DzXhcz+5V/KFuw5yUuANVV1/WIWNkK9/JxfALwgyVeS3JrkzEWrbjR66fkDwJuT7AFuAP5wcUobm8P9731Bo/r4gWFb8OMMepyzlPTcT5I3A5PA74y0otE7ZM9JngZcDvzeYhW0CHr5Oa9g9tLMFLP/OvvXJCdX1Y9GXNuo9NLzucA1VbUpyW8Bn+h6/sXoyxuLoefXUjlz7+XjDP5vTpIVzP5T7lD/DHqq6+kjHJK8Cngf8LqqenSRahuVhXo+GjgZmE6ym9lrk9uX+Iuqvf5ub6uqn1fV94D7mA37paqXni8ErgOoqq8Cz2T2Q8VaNfSPbFkq4d7LxxlsBy7olt8IfKm6VyqWqAV77i5RfIzZYF/q12FhgZ6r6uGqOr6q1lbVWmZfZ3hdVd0+nnKHopff7X9g9sVzkhzP7GWa7y5qlcPVS8/3A2cAJHkhs+H+n4ta5eLaDpzf3TVzOvBwVT0w0DOO+1Xkw3i1+Szg35l9lf193difMfsfN8z+8P8e2AV8DXjeuGtehJ6/COwD7uge28dd86h7PmDuNEv8bpkef84BPgzcA+wEzhl3zYvQ80nAV5i9k+YO4NXjrnnAfj8FPAD8nNmz9AuBdwDvmPMzvrL7fuwcxu+1Hz8gSQ1aKpdlJEmHwXCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDfpfH1+wbsTsp6IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_T1.label.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subject671</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9917</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject6238</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject8581</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject7238</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject2182</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9829</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject3270</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject6464</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject8721</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>340 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             label\n",
       "subject           \n",
       "subject671       1\n",
       "subject9917      1\n",
       "subject6238      1\n",
       "subject8581      1\n",
       "subject7238      1\n",
       "...            ...\n",
       "subject2182      0\n",
       "subject9829      0\n",
       "subject3270      0\n",
       "subject6464      0\n",
       "subject8721      0\n",
       "\n",
       "[340 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_T1 = labels_T1.set_index('subject')\n",
    "labels_T1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject8292.xml\n",
      "subject6644.xml\n",
      "subject7982.xml\n",
      "subject9260.xml\n",
      "subject9918.xml\n",
      "subject4284.xml\n",
      "subject9829.xml\n",
      "subject7661.xml\n",
      "subject8361.xml\n",
      "subject4831.xml\n",
      "subject2181.xml\n",
      "subject9077.xml\n",
      "subject2922.xml\n",
      "subject2238.xml\n",
      "subject4513.xml\n",
      "subject269.xml\n",
      "subject2678.xml\n",
      "subject9197.xml\n",
      "subject4143.xml\n",
      "subject2605.xml\n",
      "subject4226.xml\n",
      "subject7627.xml\n",
      "subject5150.xml\n",
      "subject4510.xml\n",
      "subject2182.xml\n",
      "subject280.xml\n",
      "subject1105.xml\n",
      "subject187.xml\n",
      "subject8001.xml\n",
      "subject9285.xml\n",
      "subject2621.xml\n",
      "subject4414.xml\n",
      "subject2685.xml\n",
      "subject9961.xml\n",
      "subject8065.xml\n",
      "subject8225.xml\n",
      "subject6866.xml\n",
      "subject9949.xml\n",
      "subject1507.xml\n",
      "subject8329.xml\n",
      "subject9411.xml\n",
      "subject7857.xml\n",
      "subject1545.xml\n",
      "subject9811.xml\n",
      "subject5000.xml\n",
      "subject4843.xml\n",
      "subject569.xml\n",
      "subject51.xml\n",
      "subject9156.xml\n",
      "subject6453.xml\n",
      "subject1210.xml\n",
      "subject5528.xml\n",
      "subject1485.xml\n",
      "subject5935.xml\n",
      "subject4527.xml\n",
      "subject3301.xml\n",
      "subject4074.xml\n",
      "subject6093.xml\n",
      "subject2088.xml\n",
      "subject8990.xml\n",
      "subject6459.xml\n",
      "subject7830.xml\n",
      "subject8395.xml\n",
      "subject4247.xml\n",
      "subject3667.xml\n",
      "subject5003.xml\n",
      "subject992.xml\n",
      "subject5644.xml\n",
      "subject242.xml\n",
      "subject7764.xml\n",
      "subject3283.xml\n",
      "subject6322.xml\n",
      "subject7678.xml\n",
      "subject6668.xml\n",
      "subject4333.xml\n",
      "subject1288.xml\n",
      "subject8200.xml\n",
      "subject5383.xml\n",
      "subject9039.xml\n",
      "subject7698.xml\n",
      "subject9652.xml\n",
      "subject5223.xml\n",
      "subject9725.xml\n",
      "subject1512.xml\n",
      "subject3994.xml\n",
      "subject7018.xml\n",
      "subject3644.xml\n",
      "subject1786.xml\n",
      "subject1027.xml\n",
      "subject8094.xml\n",
      "subject974.xml\n",
      "subject2947.xml\n",
      "subject9575.xml\n",
      "subject4570.xml\n",
      "subject5062.xml\n",
      "subject4729.xml\n",
      "subject5100.xml\n",
      "subject5177.xml\n",
      "subject505.xml\n",
      "subject5974.xml\n",
      "subject7499.xml\n",
      "subject1264.xml\n",
      "subject4071.xml\n",
      "subject7740.xml\n",
      "subject8721.xml\n",
      "subject9222.xml\n",
      "subject8432.xml\n",
      "subject2547.xml\n",
      "subject5995.xml\n",
      "subject6930.xml\n",
      "subject8472.xml\n",
      "subject6918.xml\n",
      "subject4198.xml\n",
      "subject501.xml\n",
      "subject7777.xml\n",
      "subject5375.xml\n",
      "subject7229.xml\n",
      "subject4762.xml\n",
      "subject5622.xml\n",
      "subject7637.xml\n",
      "subject47.xml\n",
      "subject1962.xml\n",
      "subject8795.xml\n",
      "subject4785.xml\n",
      "subject5840.xml\n",
      "subject3014.xml\n",
      "subject6464.xml\n",
      "subject522.xml\n",
      "subject5984.xml\n",
      "subject641.xml\n",
      "subject7326.xml\n",
      "subject4227.xml\n",
      "subject7428.xml\n",
      "subject203.xml\n",
      "subject6946.xml\n",
      "subject4563.xml\n",
      "subject682.xml\n",
      "subject9014.xml\n",
      "subject7435.xml\n",
      "subject8626.xml\n",
      "subject4459.xml\n",
      "subject733.xml\n",
      "subject7238.xml\n",
      "subject6428.xml\n",
      "subject7262.xml\n",
      "subject0.xml\n",
      "subject2269.xml\n",
      "subject8233.xml\n",
      "subject2522.xml\n",
      "subject5456.xml\n",
      "subject1064.xml\n",
      "subject8822.xml\n",
      "subject5033.xml\n",
      "subject1089.xml\n",
      "subject3277.xml\n",
      "subject5549.xml\n",
      "subject6352.xml\n",
      "subject6652.xml\n",
      "subject7669.xml\n",
      "subject5833.xml\n",
      "subject4795.xml\n",
      "subject4002.xml\n",
      "subject5878.xml\n",
      "subject1524.xml\n",
      "subject3928.xml\n",
      "subject9318.xml\n",
      "subject2935.xml\n",
      "subject1093.xml\n",
      "subject6786.xml\n",
      "subject3612.xml\n",
      "subject9114.xml\n",
      "subject4719.xml\n",
      "subject7439.xml\n",
      "subject1623.xml\n",
      "subject6290.xml\n",
      "subject8973.xml\n",
      "subject3844.xml\n",
      "subject7898.xml\n",
      "subject3605.xml\n",
      "subject2097.xml\n",
      "subject9381.xml\n",
      "subject3178.xml\n",
      "subject5908.xml\n",
      "subject3191.xml\n",
      "subject4196.xml\n",
      "subject8882.xml\n",
      "subject8845.xml\n",
      "subject5256.xml\n",
      "subject7318.xml\n",
      "subject4777.xml\n",
      "subject6309.xml\n",
      "subject4479.xml\n",
      "subject9393.xml\n",
      "subject4961.xml\n",
      "subject6247.xml\n",
      "subject1055.xml\n",
      "subject4644.xml\n",
      "subject7338.xml\n",
      "subject6284.xml\n",
      "subject5699.xml\n",
      "subject2580.xml\n",
      "subject2446.xml\n",
      "subject5409.xml\n",
      "subject1914.xml\n",
      "subject7263.xml\n",
      "subject5148.xml\n",
      "subject1793.xml\n",
      "subject9729.xml\n",
      "subject7952.xml\n",
      "subject9917.xml\n",
      "subject3868.xml\n",
      "subject5793.xml\n",
      "subject4934.xml\n",
      "subject3674.xml\n",
      "subject6019.xml\n",
      "subject2974.xml\n",
      "subject2857.xml\n",
      "subject855.xml\n",
      "subject5937.xml\n",
      "subject671.xml\n",
      "subject4318.xml\n",
      "subject5112.xml\n",
      "subject9249.xml\n",
      "subject7107.xml\n",
      "subject2996.xml\n",
      "subject5603.xml\n",
      "subject511.xml\n",
      "subject6518.xml\n",
      "subject5140.xml\n",
      "subject3737.xml\n",
      "subject9095.xml\n",
      "subject3227.xml\n",
      "subject7355.xml\n",
      "subject1617.xml\n",
      "subject6670.xml\n",
      "subject5387.xml\n",
      "subject3883.xml\n",
      "subject6146.xml\n",
      "subject2949.xml\n",
      "subject1763.xml\n",
      "subject2980.xml\n",
      "subject8933.xml\n",
      "subject6833.xml\n",
      "subject8802.xml\n",
      "subject8657.xml\n",
      "subject6259.xml\n",
      "subject1947.xml\n",
      "subject3635.xml\n",
      "subject8978.xml\n",
      "subject6423.xml\n",
      "subject1748.xml\n",
      "subject4702.xml\n",
      "subject8062.xml\n",
      "subject3555.xml\n",
      "subject2577.xml\n",
      "subject2475.xml\n",
      "subject8357.xml\n",
      "subject9492.xml\n",
      "subject3914.xml\n",
      "subject2495.xml\n",
      "subject7581.xml\n",
      "subject3725.xml\n",
      "subject6173.xml\n",
      "subject2247.xml\n",
      "subject8481.xml\n",
      "subject7946.xml\n",
      "subject7131.xml\n",
      "subject4848.xml\n",
      "subject747.xml\n",
      "subject5270.xml\n",
      "subject5979.xml\n",
      "subject6041.xml\n",
      "subject1950.xml\n",
      "subject7333.xml\n",
      "subject7247.xml\n",
      "subject814.xml\n",
      "subject5938.xml\n",
      "subject9160.xml\n",
      "subject6238.xml\n",
      "subject6957.xml\n",
      "subject8770.xml\n",
      "subject9497.xml\n",
      "subject807.xml\n",
      "subject6899.xml\n",
      "subject4014.xml\n",
      "subject2696.xml\n",
      "subject1885.xml\n",
      "subject8064.xml\n",
      "subject8081.xml\n",
      "subject2690.xml\n",
      "subject7462.xml\n",
      "subject8193.xml\n",
      "subject4526.xml\n",
      "subject7316.xml\n",
      "subject7290.xml\n",
      "subject463.xml\n",
      "subject4379.xml\n",
      "subject3181.xml\n",
      "subject5920.xml\n",
      "subject1728.xml\n",
      "subject2567.xml\n",
      "subject3904.xml\n",
      "subject4392.xml\n",
      "subject8581.xml\n",
      "subject9242.xml\n",
      "subject379.xml\n",
      "subject3881.xml\n",
      "subject8565.xml\n",
      "subject4505.xml\n",
      "subject3977.xml\n",
      "subject7489.xml\n",
      "subject2948.xml\n",
      "subject5342.xml\n",
      "subject8544.xml\n",
      "subject6903.xml\n",
      "subject7377.xml\n",
      "subject8769.xml\n",
      "subject3270.xml\n",
      "subject3224.xml\n",
      "subject2239.xml\n",
      "subject7801.xml\n",
      "subject3596.xml\n",
      "subject1469.xml\n",
      "subject4278.xml\n",
      "subject5282.xml\n",
      "subject3357.xml\n",
      "subject6013.xml\n",
      "subject5036.xml\n",
      "subject796.xml\n",
      "subject7692.xml\n",
      "subject7560.xml\n",
      "subject6035.xml\n",
      "subject1824.xml\n",
      "subject8726.xml\n",
      "subject6665.xml\n",
      "subject835.xml\n",
      "subject3117.xml\n",
      "subject519.xml\n",
      "subject1655.xml\n",
      "subject217.xml\n"
     ]
    }
   ],
   "source": [
    "writings = []\n",
    "for subject_file in os.listdir(datadir_T1):\n",
    "    print(subject_file)\n",
    "    with open(os.path.join(datadir_T1, subject_file)) as sf:\n",
    "        contents = sf.read()\n",
    "        root = ET.fromstring(contents)\n",
    "        try:\n",
    "            subject = root.findall('ID')[0].text\n",
    "        except Exception:\n",
    "            print('Cannot extract ID', contents[:500], '\\n-------\\n')        \n",
    "        for w in root.iter('WRITING'):\n",
    "            subject_writings = {'subject': subject}\n",
    "            for title in w.findall('TITLE'):\n",
    "                subject_writings['title'] = title.text\n",
    "            for text in w.findall('TEXT'):\n",
    "                subject_writings['text'] = text.text\n",
    "            for date in w.findall('DATE'):\n",
    "                subject_writings['date'] = date.text\n",
    "            writings.append(subject_writings)\n",
    "            # TODO: Date\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df = pd.DataFrame(writings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ana/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>If anyone could help with which sub to put thi...</td>\n",
       "      <td>2016-08-02 09:22:12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>I'm literally never gonna stop waiting...</td>\n",
       "      <td>2016-08-05 09:35:55</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>This is a really interesting study! Makes sens...</td>\n",
       "      <td>2016-08-05 21:36:24</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>Mostly always me during this whole charade</td>\n",
       "      <td>2016-08-09 08:39:41</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>What is your best advice to a healthy, success...</td>\n",
       "      <td>2016-11-02 05:33:33</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170664</th>\n",
       "      <td>subject217</td>\n",
       "      <td>scary</td>\n",
       "      <td>2018-06-24 14:26:01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170668</th>\n",
       "      <td>subject217</td>\n",
       "      <td>rescuing man after his car got stuck on Rub' a...</td>\n",
       "      <td>2018-07-05 15:31:29</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170680</th>\n",
       "      <td>subject217</td>\n",
       "      <td>Gorilla in streets of Riyadh,KSA</td>\n",
       "      <td>2018-07-24 22:22:48</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170681</th>\n",
       "      <td>subject217</td>\n",
       "      <td>Gorilla in streets of Riyadh,KSA</td>\n",
       "      <td>2018-07-24 22:46:11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170696</th>\n",
       "      <td>subject217</td>\n",
       "      <td>WHAT DID YOU SAY TO ME?</td>\n",
       "      <td>2018-08-20 10:54:11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42757 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            subject                                              title  \\\n",
       "0       subject8292  If anyone could help with which sub to put thi...   \n",
       "1       subject8292          I'm literally never gonna stop waiting...   \n",
       "2       subject8292  This is a really interesting study! Makes sens...   \n",
       "4       subject8292         Mostly always me during this whole charade   \n",
       "7       subject8292  What is your best advice to a healthy, success...   \n",
       "...             ...                                                ...   \n",
       "170664   subject217                                              scary   \n",
       "170668   subject217  rescuing man after his car got stuck on Rub' a...   \n",
       "170680   subject217                   Gorilla in streets of Riyadh,KSA   \n",
       "170681   subject217                   Gorilla in streets of Riyadh,KSA   \n",
       "170696   subject217                            WHAT DID YOU SAY TO ME?   \n",
       "\n",
       "                       date text  \n",
       "0       2016-08-02 09:22:12  NaN  \n",
       "1       2016-08-05 09:35:55  NaN  \n",
       "2       2016-08-05 21:36:24  NaN  \n",
       "4       2016-08-09 08:39:41  NaN  \n",
       "7       2016-11-02 05:33:33  NaN  \n",
       "...                     ...  ...  \n",
       "170664  2018-06-24 14:26:01  NaN  \n",
       "170668  2018-07-05 15:31:29  NaN  \n",
       "170680  2018-07-24 22:22:48  NaN  \n",
       "170681  2018-07-24 22:46:11  NaN  \n",
       "170696  2018-08-20 10:54:11  NaN  \n",
       "\n",
       "[42757 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[writings_df['text'].isna()][~writings_df['title'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['label'] = writings_df['subject'].apply(\n",
    "    lambda s: labels_T1.loc[s, 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fce2f36dcd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "writings_df.label.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def tokenize(t):\n",
    "    return tokenizer.tokenize(t.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'wasn', 't', 'ready', 'to', 'leave', 'buh', 'buw', 'dd', 'sasa']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"I wasn't ready to leave! buh-buw(dd). Sasa .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['tokenized_title'] = writings_df['title'].apply(lambda t: tokenize(t) if type(t)==str else None)\n",
    "writings_df['title_len'] = writings_df['tokenized_title'].apply(lambda t: len(t) if type(t)==list else None)\n",
    "writings_df['tokenized_text'] = writings_df['text'].apply(lambda t: tokenize(t) if type(t)==str else None)\n",
    "writings_df['text_len'] = writings_df['tokenized_text'].apply(lambda t: len(t) if type(t)==list else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    127941.000000\n",
       "mean         32.268929\n",
       "std          82.590713\n",
       "min           0.000000\n",
       "25%           6.000000\n",
       "50%          13.000000\n",
       "75%          31.000000\n",
       "max        7201.000000\n",
       "Name: text_len, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.text_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    49762.000000\n",
       "mean        10.699771\n",
       "std          9.282454\n",
       "min          0.000000\n",
       "25%          4.000000\n",
       "50%          8.000000\n",
       "75%         14.000000\n",
       "max        149.000000\n",
       "Name: title_len, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.title_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title_len</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subject0</th>\n",
       "      <td>0</td>\n",
       "      <td>20.285714</td>\n",
       "      <td>31.711712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1027</th>\n",
       "      <td>0</td>\n",
       "      <td>7.769231</td>\n",
       "      <td>1.190476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1055</th>\n",
       "      <td>0</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>79.983193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1064</th>\n",
       "      <td>1</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>68.410256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1089</th>\n",
       "      <td>0</td>\n",
       "      <td>9.823529</td>\n",
       "      <td>13.254902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9917</th>\n",
       "      <td>1</td>\n",
       "      <td>8.983607</td>\n",
       "      <td>95.806897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9918</th>\n",
       "      <td>0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.865269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject992</th>\n",
       "      <td>0</td>\n",
       "      <td>5.872928</td>\n",
       "      <td>19.876190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9949</th>\n",
       "      <td>0</td>\n",
       "      <td>10.609756</td>\n",
       "      <td>42.346979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject9961</th>\n",
       "      <td>0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>26.389313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>340 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             label  title_len   text_len\n",
       "subject                                 \n",
       "subject0         0  20.285714  31.711712\n",
       "subject1027      0   7.769231   1.190476\n",
       "subject1055      0  16.666667  79.983193\n",
       "subject1064      1  13.000000  68.410256\n",
       "subject1089      0   9.823529  13.254902\n",
       "...            ...        ...        ...\n",
       "subject9917      1   8.983607  95.806897\n",
       "subject9918      0   5.000000  11.865269\n",
       "subject992       0   5.872928  19.876190\n",
       "subject9949      0  10.609756  42.346979\n",
       "subject9961      0   5.000000  26.389313\n",
       "\n",
       "[340 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.groupby('subject').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>title_len</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>Okay friends so I messed up and posted to do a...</td>\n",
       "      <td>2017-04-25 22:37:57</td>\n",
       "      <td>Sorry for that, I truly didn't think it was go...</td>\n",
       "      <td>0</td>\n",
       "      <td>[okay, friends, so, i, messed, up, and, posted...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>[sorry, for, that, i, truly, didn, t, think, i...</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-09-16 06:29:13</td>\n",
       "      <td>You've got plenty of time to fix that. You can...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[you, ve, got, plenty, of, time, to, fix, that...</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-11-24 01:33:22</td>\n",
       "      <td>LCD, Glass animals, Kendrick, The Weeknd, Jack...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[lcd, glass, animals, kendrick, the, weeknd, j...</td>\n",
       "      <td>127.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>Getting that coachella bod</td>\n",
       "      <td>2018-01-09 00:54:06</td>\n",
       "      <td>First I want to say whatever skin is your skin...</td>\n",
       "      <td>0</td>\n",
       "      <td>[getting, that, coachella, bod]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[first, i, want, to, say, whatever, skin, is, ...</td>\n",
       "      <td>149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-03-12 17:14:03</td>\n",
       "      <td>Not the same but me and my wife saw a man and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[not, the, same, but, me, and, my, wife, saw, ...</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170652</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-05-28 12:23:00</td>\n",
       "      <td>/r/keto /r/ketorecipes /r/ketodessert all are ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[r, keto, r, ketorecipes, r, ketodessert, all,...</td>\n",
       "      <td>197.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170653</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-05-28 12:32:36</td>\n",
       "      <td>its okay dont worry . as long as you don't exc...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[its, okay, dont, worry, as, long, as, you, do...</td>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170662</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-06-20 00:33:57</td>\n",
       "      <td>the national number is :1919 here are more com...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[the, national, number, is, 1919, here, are, m...</td>\n",
       "      <td>115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170693</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 11:29:21</td>\n",
       "      <td>this is my personal experience ,it may not ref...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[this, is, my, personal, experience, it, may, ...</td>\n",
       "      <td>153.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170695</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 20:00:31</td>\n",
       "      <td>i am aware of stats now and then. i was just s...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[i, am, aware, of, stats, now, and, then, i, w...</td>\n",
       "      <td>198.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7655 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            subject                                              title  \\\n",
       "122     subject8292  Okay friends so I messed up and posted to do a...   \n",
       "390     subject8292                                                NaN   \n",
       "498     subject8292                                                NaN   \n",
       "752     subject8292                         Getting that coachella bod   \n",
       "904     subject8292                                                NaN   \n",
       "...             ...                                                ...   \n",
       "170652   subject217                                                NaN   \n",
       "170653   subject217                                                NaN   \n",
       "170662   subject217                                                NaN   \n",
       "170693   subject217                                                NaN   \n",
       "170695   subject217                                                NaN   \n",
       "\n",
       "                       date  \\\n",
       "122     2017-04-25 22:37:57   \n",
       "390     2017-09-16 06:29:13   \n",
       "498     2017-11-24 01:33:22   \n",
       "752     2018-01-09 00:54:06   \n",
       "904     2018-03-12 17:14:03   \n",
       "...                     ...   \n",
       "170652  2018-05-28 12:23:00   \n",
       "170653  2018-05-28 12:32:36   \n",
       "170662  2018-06-20 00:33:57   \n",
       "170693  2018-08-19 11:29:21   \n",
       "170695  2018-08-19 20:00:31   \n",
       "\n",
       "                                                     text  label  \\\n",
       "122     Sorry for that, I truly didn't think it was go...      0   \n",
       "390     You've got plenty of time to fix that. You can...      0   \n",
       "498     LCD, Glass animals, Kendrick, The Weeknd, Jack...      0   \n",
       "752     First I want to say whatever skin is your skin...      0   \n",
       "904     Not the same but me and my wife saw a man and ...      0   \n",
       "...                                                   ...    ...   \n",
       "170652  /r/keto /r/ketorecipes /r/ketodessert all are ...      0   \n",
       "170653  its okay dont worry . as long as you don't exc...      0   \n",
       "170662  the national number is :1919 here are more com...      0   \n",
       "170693  this is my personal experience ,it may not ref...      0   \n",
       "170695  i am aware of stats now and then. i was just s...      0   \n",
       "\n",
       "                                          tokenized_title  title_len  \\\n",
       "122     [okay, friends, so, i, messed, up, and, posted...       34.0   \n",
       "390                                                  None        NaN   \n",
       "498                                                  None        NaN   \n",
       "752                       [getting, that, coachella, bod]        4.0   \n",
       "904                                                  None        NaN   \n",
       "...                                                   ...        ...   \n",
       "170652                                               None        NaN   \n",
       "170653                                               None        NaN   \n",
       "170662                                               None        NaN   \n",
       "170693                                               None        NaN   \n",
       "170695                                               None        NaN   \n",
       "\n",
       "                                           tokenized_text  text_len  \n",
       "122     [sorry, for, that, i, truly, didn, t, think, i...     120.0  \n",
       "390     [you, ve, got, plenty, of, time, to, fix, that...     104.0  \n",
       "498     [lcd, glass, animals, kendrick, the, weeknd, j...     127.0  \n",
       "752     [first, i, want, to, say, whatever, skin, is, ...     149.0  \n",
       "904     [not, the, same, but, me, and, my, wife, saw, ...     151.0  \n",
       "...                                                   ...       ...  \n",
       "170652  [r, keto, r, ketorecipes, r, ketodessert, all,...     197.0  \n",
       "170653  [its, okay, dont, worry, as, long, as, you, do...     109.0  \n",
       "170662  [the, national, number, is, 1919, here, are, m...     115.0  \n",
       "170693  [this, is, my, personal, experience, it, may, ...     153.0  \n",
       "170695  [i, am, aware, of, stats, now, and, then, i, w...     198.0  \n",
       "\n",
       "[7655 rows x 9 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[(~writings_df['text_len'].isnull()) & (writings_df['text_len'] > 100)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features and encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_features = {\n",
    "    \"max_features\": 20000,\n",
    "    # cut texts after this number of words\n",
    "    # (among top max_features most common words)\n",
    "    \"maxlen\": 100,\n",
    "    \"embedding_dim\": 100\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_NRC(nrc_path):\n",
    "    word_emotions = {}\n",
    "    emotion_words = {}\n",
    "    with open(nrc_path) as in_f:\n",
    "        for line in in_f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            word, emotion, label = line.split()\n",
    "            if word not in word_emotions:\n",
    "                word_emotions[word] = set()\n",
    "            if emotion not in emotion_words:\n",
    "                emotion_words[emotion] = set()\n",
    "            label = int(label)\n",
    "            if label:\n",
    "                word_emotions[word].add(emotion)\n",
    "                emotion_words[emotion].add(word)\n",
    "    return emotion_words\n",
    "\n",
    "nrc_lexicon_path = '/home/ana/resources/NRC-Sentiment-Emotion-Lexicons/NRC-Sentiment-Emotion-Lexicons/NRC-Emotion-Lexicon-v0.92/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'\n",
    "nrc_lexicon = load_NRC(nrc_lexicon_path)\n",
    "emotions = list(nrc_lexicon.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_emotions(tokens, emotion_lexicon, emotions, relative=True):\n",
    "    text_len = len(tokens)\n",
    "    encoded_emotions = [0 for e in emotions]\n",
    "    for i, emotion in enumerate(emotions):\n",
    "        try:\n",
    "            emotion_words = [t for t in tokens if t in emotion_lexicon[emotion]]\n",
    "            if relative:\n",
    "                encoded_emotions[i] = len(emotion_words) / len(tokens)\n",
    "            else:\n",
    "                encoded_emotions[i] = len(emotion_words)\n",
    "        except ValueError:\n",
    "            print(\"Emotion not found.\")\n",
    "    return encoded_emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Char n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(tokens):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Personal pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_person_pronouns = {\"i\", \"me\", \"my\", \"mine\"}\n",
    "def encode_pronouns(tokens, pronouns={\"i\", \"me\", \"my\", \"mine\"}, relative=True):\n",
    "    if not tokens:\n",
    "        return np.nan\n",
    "    text_len = len(tokens)\n",
    "    nr_pronouns = len([t for t in tokens if t in pronouns])\n",
    "    if relative:\n",
    "        return nr_pronouns/text_len\n",
    "    else:\n",
    "        return nr_pronouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words(\"english\")\n",
    "def encode_stopwords(tokens, stopwords=stopword_list):\n",
    "    encoded_stopwords = [0 for s in stopword_list]\n",
    "    if not tokens:\n",
    "        return encoded_stopwords\n",
    "    for i, stopword in enumerate(stopwords):\n",
    "        if stopword in tokens:\n",
    "            encoded_stopwords[i] += 1\n",
    "    return encoded_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def load_erisk_data(writings_df, voc_size, emotion_lexicon, seq_len, emotions =  \n",
    "                    ['anger', 'anticipation', 'disgust', 'fear', 'joy', \n",
    "                     'negative', 'positive', 'sadness', 'surprise', 'trust'],\n",
    "                    pronouns = [\"i\", \"me\", \"my\", \"mine\"],\n",
    "                    train_prop=0.7, min_post_len=3, min_word_len=1):\n",
    "    print(\"Loading data...\")\n",
    "    vocabulary = {}\n",
    "    word_freqs = Counter()\n",
    "    for words in writings_df.tokenized_text:\n",
    "        word_freqs.update(words)\n",
    "    for words in writings_df.tokenized_title:\n",
    "        word_freqs.update(words)\n",
    "    i = 1\n",
    "    for w, f in word_freqs.most_common(voc_size-2): # keeping voc_size-1 for unk\n",
    "        if len(w) < min_word_len:\n",
    "            continue\n",
    "        vocabulary[w] = i\n",
    "        i += 1\n",
    "    tokens_data_train = []\n",
    "    categ_data_train = []\n",
    "    sparse_data_train = []\n",
    "    tokens_data_test = []\n",
    "    categ_data_test = []\n",
    "    sparse_data_test = []\n",
    "    labels_train = []\n",
    "    labels_test = []\n",
    "    # TODO: shuffle?\n",
    "    all_subjects = sorted(list(set(writings_df.subject)))\n",
    "    training_subjects_size = int(len(all_subjects) * train_prop)\n",
    "    training_subjects = all_subjects[:training_subjects_size]\n",
    "    training_rows = writings_df[writings_df['subject'].isin(training_subjects)].sample(frac=1) # shuffling\n",
    "    test_rows = writings_df[~writings_df['subject'].isin(training_subjects)].sample(frac=1)\n",
    "    def encode_text(tokens):\n",
    "        # Using voc_size-1 value for OOV token\n",
    "        encoded_tokens = [vocabulary.get(w, voc_size-1) for w in tokens]\n",
    "        encoded_emotions = encode_emotions(tokens, emotion_lexicon, emotions)\n",
    "        encoded_pronouns = encode_pronouns(tokens, pronouns)\n",
    "        encoded_stopwords = encode_stopwords(tokens)\n",
    "        return (encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords)\n",
    "    for row in training_rows.itertuples():\n",
    "        words = []\n",
    "        if row.tokenized_title:\n",
    "            words.extend(row.tokenized_title)\n",
    "        if row.tokenized_text:\n",
    "            words.extend(row.tokenized_text)\n",
    "        if not words or len(words)<min_post_len:\n",
    "            continue\n",
    "        label = row.label\n",
    "        encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords = encode_text(words)\n",
    "        tokens_data_train.append(encoded_tokens)\n",
    "        categ_data_train.append(encoded_emotions + [encoded_pronouns])\n",
    "        sparse_data_train.append(encoded_stopwords)\n",
    "        labels_train.append(label)\n",
    "    for row in test_rows[~test_rows['tokenized_text'].isna()].itertuples():\n",
    "        words = []\n",
    "        if row.tokenized_title:\n",
    "            words.extend(row.tokenized_title)\n",
    "        if row.tokenized_text:\n",
    "            words.extend(row.tokenized_text)   \n",
    "        if not words or len(words)<min_post_len:\n",
    "            continue\n",
    "        label = row.label\n",
    "        encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords = encode_text(words)\n",
    "        tokens_data_test.append(encoded_tokens)\n",
    "        categ_data_test.append(encoded_emotions + [encoded_pronouns])\n",
    "        sparse_data_test.append(encoded_stopwords)\n",
    "        labels_test.append(label)\n",
    "        \n",
    "    # using zeros for padding\n",
    "    tokens_data_train_padded = sequence.pad_sequences(tokens_data_train, maxlen=seq_len)\n",
    "    tokens_data_test_padded = sequence.pad_sequences(tokens_data_test, maxlen=seq_len)\n",
    "        \n",
    "    return ([np.array(tokens_data_train_padded), np.array(categ_data_train), np.array(sparse_data_train)], \n",
    "            np.array(labels_train)), \\\n",
    "            ([np.array(tokens_data_test_padded), np.array(categ_data_test), np.array(sparse_data_test)], \n",
    "             np.array(labels_test)), vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test), voc = load_erisk_data(writings_df, \n",
    "                                                            seq_len=hyperparams_features['maxlen'],\n",
    "                                                            voc_size=hyperparams_features['max_features'],\n",
    "                                                           emotion_lexicon=nrc_lexicon,\n",
    "                                                           emotions=emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111375 train sequences\n",
      "31863 test sequences\n"
     ]
    }
   ],
   "source": [
    "x_train_seq, x_train_categ, x_train_sparse = x_train\n",
    "x_test_seq, x_test_categ, x_test_sparse = x_test\n",
    "print(len(x_train_seq), 'train sequences')\n",
    "print(len(x_test_seq), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4430 positive training examples\n",
      "2092 positive test examples\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(y_train).sum(), \"positive training examples\")\n",
    "print(pd.Series(y_test).sum(), \"positive test examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.52071158, 12.57054176])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 20000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "def load_embeddings(path, embedding_dim, voc):\n",
    "    # random matrix with mean value = 0\n",
    "    embedding_matrix = np.random.random((len(voc)+2, embedding_dim)) - 0.5 # voc + unk + pad value(0)\n",
    "#     embedding_matrix = np.zeros((len(voc)+1, embedding_dim))\n",
    "\n",
    "    f = open(path)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_i = voc.get(word)\n",
    "        if word_i is not None:\n",
    "            embedding_matrix[word_i] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print('Total %s word vectors.' % len(embedding_matrix))\n",
    "\n",
    " \n",
    "    return embedding_matrix\n",
    "\n",
    "# pretrained_embeddings_path = '/home/ana/resources/glove.6B/glove.6B.%dd.txt' % hyperparams_features['embedding_dim']\n",
    "pretrained_embeddings_path = '/home/ana/resources/glove.twitter.27B/glove.twitter.27B.%dd.txt' % hyperparams_features['embedding_dim']\n",
    "embedding_matrix = load_embeddings(pretrained_embeddings_path, hyperparams_features['embedding_dim'], voc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_sparse[90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'content_dense_units': 32,\n",
    "    'stopwords_dense_units': 5,\n",
    "    'dropout': 0.14,\n",
    "    'l2_dense': 0.0001,\n",
    "    'optimizer': 'adagrad', #None,\n",
    "    'decay': 0.0001,\n",
    "    'lr': 0.00001,\n",
    "    \"batch_size\": 128,\n",
    "    \"trainable_embeddings\": False,\n",
    "\n",
    "}\n",
    "if not hyperparams['optimizer']:\n",
    "    hyperparams['optimizer'] = optimizers.Adam(lr=hyperparams['lr'], beta_1=0.9, beta_2=0.999, epsilon=1e-08,\n",
    "                                   decay=hyperparams['decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopwords_list,\n",
    "               ignore_layer=[]):\n",
    "\n",
    "    tokens_features = Input(shape=(hyperparams_features['maxlen'],), name='word_seq')\n",
    "    embedding_layer = Embedding(hyperparams_features['max_features'], \n",
    "                                hyperparams_features['embedding_dim'], \n",
    "                                input_length=hyperparams_features['maxlen'],\n",
    "                                mask_zero=True,\n",
    "                                weights=[embedding_matrix], \n",
    "                                trainable=hyperparams['trainable_embeddings'],\n",
    "                               name='embeddings_layer')(\n",
    "        tokens_features)\n",
    "    summed_embeddings = Lambda(lambda x: K.mean(x, axis=1), name='averaged_embeddings')(embedding_layer)\n",
    "    content_dense_layer = Dense(units=hyperparams['content_dense_units'],\n",
    "                        kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                      name='content_dense_layer')(summed_embeddings)\n",
    "    \n",
    "    numerical_features = Input(shape=(len(emotions) + 1,), name='numeric_input') # emotions and pronouns\n",
    "    dense_layer = Dense(units=1,\n",
    "                        kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                        name='numerical_dense_layer',\n",
    "                       )(numerical_features)\n",
    "    sparse_features = Input(shape=(len(stopwords_list),), name='sparse_input') # stopwords\n",
    "    dense_layer_sparse = Dense(units=hyperparams['stopwords_dense_units'],\n",
    "                              name='sparse_feat_dense_layer',\n",
    "                            kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                              )(sparse_features)\n",
    "#     # TODO: this is getting out of hand. Refactor this ablation part.\n",
    "    if 'lstm_layers' in ignore_layer:\n",
    "        output_layer = Dense(1, activation='sigmoid')(numerical_features)\n",
    "    elif 'numerical_dense_layer' in ignore_layer and 'sparse_feat_dense_layer' in ignore_layer:\n",
    "        output_layer = Dense(1, activation='sigmoid')(content_dense_layer)\n",
    "    elif 'numerical_dense_layer' in ignore_layer:\n",
    "        merged_layers = concatenate([content_dense_layer, dense_layer_sparse])\n",
    "        output_layer = Dense(1, activation='sigmoid')(merged_layers)\n",
    "\n",
    "    elif 'sparse_feat_dense_layer' in ignore_layer:\n",
    "        merged_layers = concatenate([content_dense_layer, dense_layer])\n",
    "        output_layer = Dense(1, activation='sigmoid')(merged_layers)\n",
    "\n",
    "    else:\n",
    "        merged_layers = concatenate([content_dense_layer, dense_layer, dense_layer_sparse])\n",
    "        output_layer = Dense(1, activation='sigmoid')(merged_layers)\n",
    "\n",
    "    # Compile model\n",
    "    model = Model(inputs=[tokens_features, numerical_features, sparse_features], outputs=output_layer)\n",
    "    model.compile(hyperparams['optimizer'], 'binary_crossentropy',\n",
    "                  metrics=['binary_accuracy', f1_m, precision_m, recall_m])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "word_seq (InputLayer)           (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embeddings_layer (Embedding)    (None, 100, 100)     2000000     word_seq[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "averaged_embeddings (Lambda)    (None, 100)          0           embeddings_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "numeric_input (InputLayer)      (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sparse_input (InputLayer)       (None, 179)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "content_dense_layer (Dense)     (None, 32)           3232        averaged_embeddings[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "numerical_dense_layer (Dense)   (None, 1)            12          numeric_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sparse_feat_dense_layer (Dense) (None, 5)            900         sparse_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 38)           0           content_dense_layer[0][0]        \n",
      "                                                                 numerical_dense_layer[0][0]      \n",
      "                                                                 sparse_feat_dense_layer[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            39          concatenate_6[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,004,183\n",
      "Trainable params: 4,183\n",
      "Non-trainable params: 2,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopword_list,\n",
    "                   ignore_layer=[])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, 'models/mlp_plus2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary:\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     url: https://www.comet.ml/ananana/mental/68d6d66e3ec24be5a1ef76392e534c37\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     batch_binary_accuracy [1496]: (0.5397727489471436, 0.9375)\n",
      "COMET INFO:     batch_f1_m [1496]           : (0.0, 0.2926828861236572)\n",
      "COMET INFO:     batch_loss [1496]           : (0.33802884817123413, 1.2814513444900513)\n",
      "COMET INFO:     batch_precision_m [1496]    : (0.0, 0.20000000298023224)\n",
      "COMET INFO:     batch_recall_m [1496]       : (0.0, 0.8333333134651184)\n",
      "COMET INFO:     binary_accuracy [16]        : (0.7316902279853821, 0.7489921450614929)\n",
      "COMET INFO:     epoch_duration [16]         : (37.91879049700037, 71.70895796500008)\n",
      "COMET INFO:     f1_m [16]                   : (0.11330197751522064, 0.13926729559898376)\n",
      "COMET INFO:     loss [16]                   : (0.6247770232281968, 0.6660432423234119)\n",
      "COMET INFO:     lr [16]                     : (7.99999907030724e-05, 0.009999999776482582)\n",
      "COMET INFO:     precision_m [16]            : (0.07104870676994324, 0.08275255560874939)\n",
      "COMET INFO:     recall_m [16]               : (0.43890315294265747, 0.5427228212356567)\n",
      "COMET INFO:     step                        : 13936\n",
      "COMET INFO:     sys.cpu.percent.01 [17]     : (94.5, 100.0)\n",
      "COMET INFO:     sys.cpu.percent.02 [17]     : (40.0, 67.5)\n",
      "COMET INFO:     sys.cpu.percent.03 [17]     : (33.8, 62.3)\n",
      "COMET INFO:     sys.cpu.percent.04 [17]     : (33.9, 60.9)\n",
      "COMET INFO:     sys.cpu.percent.avg [17]    : (56.2, 70.25)\n",
      "COMET INFO:     sys.gpu.0.total_memory      : (1073414144.0, 1073414144.0)\n",
      "COMET INFO:     sys.load.avg [17]           : (4.1, 7.09)\n",
      "COMET INFO:     sys.ram.total [17]          : (8277307392.0, 8277307392.0)\n",
      "COMET INFO:     sys.ram.used [17]           : (7479226368.0, 7731335168.0)\n",
      "COMET INFO:     val_binary_accuracy [16]    : (0.6580045819282532, 0.7964096069335938)\n",
      "COMET INFO:     val_f1_m [16]               : (0.18526694178581238, 0.23002947866916656)\n",
      "COMET INFO:     val_loss [16]               : (0.5890166335226665, 0.6453649353177385)\n",
      "COMET INFO:     val_precision_m [16]        : (0.11142803728580475, 0.15671609342098236)\n",
      "COMET INFO:     val_recall_m [16]           : (0.47863203287124634, 0.6034835577011108)\n",
      "COMET INFO:   Other [count]:\n",
      "COMET INFO:     trainable_params: 2004183\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     git-patch  : 1\n",
      "COMET INFO:     histogram3d: 65\n",
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/ananana/mental/056eb8f621a4428f8d8d18451d82cdc5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(api_key=\"eoBdVyznAhfg3bK9pZ58ZSXfv\",\n",
    "                        project_name=\"mental\", workspace=\"ananana\")\n",
    "\n",
    "experiment.log_parameters(hyperparams_features)\n",
    "\n",
    "experiment.log_parameter('emotion_lexicon', nrc_lexicon_path)\n",
    "experiment.log_parameter('emotions', emotions)\n",
    "experiment.log_parameter('embeddings_path', pretrained_embeddings_path)\n",
    "\n",
    "experiment.log_parameters(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'i': 2,\n",
       " 'to': 3,\n",
       " 'a': 4,\n",
       " 'and': 5,\n",
       " 'of': 6,\n",
       " 'it': 7,\n",
       " 'in': 8,\n",
       " 'you': 9,\n",
       " 'that': 10,\n",
       " 'is': 11,\n",
       " 's': 12,\n",
       " 'for': 13,\n",
       " 'this': 14,\n",
       " 't': 15,\n",
       " 'on': 16,\n",
       " 'with': 17,\n",
       " 'but': 18,\n",
       " 'my': 19,\n",
       " '8217': 20,\n",
       " 'be': 21,\n",
       " 'was': 22,\n",
       " 'have': 23,\n",
       " 'are': 24,\n",
       " 'not': 25,\n",
       " 'they': 26,\n",
       " 'as': 27,\n",
       " 'if': 28,\n",
       " 'so': 29,\n",
       " 'just': 30,\n",
       " 'what': 31,\n",
       " 'can': 32,\n",
       " 'like': 33,\n",
       " 'he': 34,\n",
       " 'or': 35,\n",
       " 'at': 36,\n",
       " 'we': 37,\n",
       " 'me': 38,\n",
       " 'from': 39,\n",
       " 'your': 40,\n",
       " 'm': 41,\n",
       " 'do': 42,\n",
       " 'com': 43,\n",
       " 'all': 44,\n",
       " 'about': 45,\n",
       " 'an': 46,\n",
       " 'one': 47,\n",
       " 'there': 48,\n",
       " 'would': 49,\n",
       " 'out': 50,\n",
       " 'up': 51,\n",
       " 'when': 52,\n",
       " 'more': 53,\n",
       " 'get': 54,\n",
       " 'don': 55,\n",
       " 'people': 56,\n",
       " 'by': 57,\n",
       " 'will': 58,\n",
       " 'no': 59,\n",
       " 'how': 60,\n",
       " 'https': 61,\n",
       " 'gt': 62,\n",
       " 'has': 63,\n",
       " 'them': 64,\n",
       " 'his': 65,\n",
       " 'time': 66,\n",
       " 'some': 67,\n",
       " 're': 68,\n",
       " 'know': 69,\n",
       " 'think': 70,\n",
       " 'who': 71,\n",
       " 'their': 72,\n",
       " 'because': 73,\n",
       " 'had': 74,\n",
       " 'she': 75,\n",
       " 'here': 76,\n",
       " 'good': 77,\n",
       " 'really': 78,\n",
       " 'www': 79,\n",
       " 'r': 80,\n",
       " 'now': 81,\n",
       " 've': 82,\n",
       " 'been': 83,\n",
       " 'only': 84,\n",
       " 'her': 85,\n",
       " 'also': 86,\n",
       " 'were': 87,\n",
       " 'than': 88,\n",
       " 'see': 89,\n",
       " 'any': 90,\n",
       " 'http': 91,\n",
       " 'even': 92,\n",
       " 'make': 93,\n",
       " 'other': 94,\n",
       " 'then': 95,\n",
       " '128056': 96,\n",
       " 'much': 97,\n",
       " '1': 98,\n",
       " 'which': 99,\n",
       " 'him': 100,\n",
       " 'could': 101,\n",
       " 'go': 102,\n",
       " '2': 103,\n",
       " 'first': 104,\n",
       " 'd': 105,\n",
       " 'want': 106,\n",
       " 'new': 107,\n",
       " 'why': 108,\n",
       " 'well': 109,\n",
       " 'did': 110,\n",
       " 'too': 111,\n",
       " 'right': 112,\n",
       " 'way': 113,\n",
       " 'into': 114,\n",
       " 'very': 115,\n",
       " 'after': 116,\n",
       " 'being': 117,\n",
       " 'over': 118,\n",
       " 'back': 119,\n",
       " 'still': 120,\n",
       " 'got': 121,\n",
       " 'most': 122,\n",
       " 'reddit': 123,\n",
       " 'should': 124,\n",
       " 'something': 125,\n",
       " 'post': 126,\n",
       " 'going': 127,\n",
       " 'll': 128,\n",
       " '3': 129,\n",
       " 'our': 130,\n",
       " 'these': 131,\n",
       " 'thanks': 132,\n",
       " 'never': 133,\n",
       " '8220': 134,\n",
       " 'need': 135,\n",
       " '8221': 136,\n",
       " 'say': 137,\n",
       " 'where': 138,\n",
       " 'us': 139,\n",
       " 'use': 140,\n",
       " 'am': 141,\n",
       " 'love': 142,\n",
       " 'work': 143,\n",
       " 'day': 144,\n",
       " 'same': 145,\n",
       " 'off': 146,\n",
       " 'sure': 147,\n",
       " 'game': 148,\n",
       " 'before': 149,\n",
       " 'said': 150,\n",
       " 'didn': 151,\n",
       " '10': 152,\n",
       " 'lot': 153,\n",
       " 'those': 154,\n",
       " 'thing': 155,\n",
       " 'years': 156,\n",
       " 'someone': 157,\n",
       " 'does': 158,\n",
       " 'take': 159,\n",
       " 'best': 160,\n",
       " 'made': 161,\n",
       " 'feel': 162,\n",
       " 'great': 163,\n",
       " 'two': 164,\n",
       " 'actually': 165,\n",
       " 'better': 166,\n",
       " 'look': 167,\n",
       " '_': 168,\n",
       " 'its': 169,\n",
       " '5': 170,\n",
       " 'every': 171,\n",
       " 'down': 172,\n",
       " 'year': 173,\n",
       " 'pretty': 174,\n",
       " 'life': 175,\n",
       " 'doesn': 176,\n",
       " 'amp': 177,\n",
       " 'things': 178,\n",
       " 'many': 179,\n",
       " 'though': 180,\n",
       " '4': 181,\n",
       " 'while': 182,\n",
       " 'last': 183,\n",
       " 'always': 184,\n",
       " 'around': 185,\n",
       " 'man': 186,\n",
       " 'imgur': 187,\n",
       " 'thank': 188,\n",
       " 'world': 189,\n",
       " 'give': 190,\n",
       " 'find': 191,\n",
       " 'little': 192,\n",
       " 'help': 193,\n",
       " 'anything': 194,\n",
       " 'u': 195,\n",
       " 'through': 196,\n",
       " 'trump': 197,\n",
       " 'long': 198,\n",
       " 'used': 199,\n",
       " 'o': 200,\n",
       " 'yeah': 201,\n",
       " 'may': 202,\n",
       " 'few': 203,\n",
       " 'ever': 204,\n",
       " 'thought': 205,\n",
       " 'since': 206,\n",
       " 'yes': 207,\n",
       " 'doing': 208,\n",
       " 'getting': 209,\n",
       " 'shit': 210,\n",
       " 'anyone': 211,\n",
       " 'comments': 212,\n",
       " 'probably': 213,\n",
       " 'let': 214,\n",
       " 'point': 215,\n",
       " 'person': 216,\n",
       " 'bad': 217,\n",
       " 'both': 218,\n",
       " 'watch': 219,\n",
       " 'another': 220,\n",
       " 'again': 221,\n",
       " 'own': 222,\n",
       " 'might': 223,\n",
       " 'isn': 224,\n",
       " 'looking': 225,\n",
       " 'maybe': 226,\n",
       " 'everyone': 227,\n",
       " 'old': 228,\n",
       " 'come': 229,\n",
       " 'put': 230,\n",
       " 'mean': 231,\n",
       " 'without': 232,\n",
       " 'try': 233,\n",
       " 'please': 234,\n",
       " 'looks': 235,\n",
       " 'keep': 236,\n",
       " 'trying': 237,\n",
       " 'big': 238,\n",
       " 'video': 239,\n",
       " 'different': 240,\n",
       " 'show': 241,\n",
       " 'guy': 242,\n",
       " 'next': 243,\n",
       " 'e': 244,\n",
       " 'part': 245,\n",
       " 'play': 246,\n",
       " 'using': 247,\n",
       " 'makes': 248,\n",
       " 'already': 249,\n",
       " 'enough': 250,\n",
       " '2018': 251,\n",
       " 'found': 252,\n",
       " 'money': 253,\n",
       " 'oh': 254,\n",
       " 'real': 255,\n",
       " 'such': 256,\n",
       " '9685': 257,\n",
       " 'high': 258,\n",
       " 'end': 259,\n",
       " 'between': 260,\n",
       " '12388': 261,\n",
       " '6': 262,\n",
       " 'done': 263,\n",
       " 'read': 264,\n",
       " 'link': 265,\n",
       " 'lol': 266,\n",
       " 'nice': 267,\n",
       " 'having': 268,\n",
       " 'away': 269,\n",
       " 'tell': 270,\n",
       " 'nothing': 271,\n",
       " 'kind': 272,\n",
       " 'making': 273,\n",
       " 'question': 274,\n",
       " 'live': 275,\n",
       " 'fuck': 276,\n",
       " 'v': 277,\n",
       " 'start': 278,\n",
       " 'else': 279,\n",
       " 'team': 280,\n",
       " 'today': 281,\n",
       " 'hard': 282,\n",
       " '0': 283,\n",
       " 'against': 284,\n",
       " 'once': 285,\n",
       " 'days': 286,\n",
       " 'seen': 287,\n",
       " 'far': 288,\n",
       " 'bit': 289,\n",
       " 'able': 290,\n",
       " 'org': 291,\n",
       " 'place': 292,\n",
       " 'week': 293,\n",
       " 'everything': 294,\n",
       " 'each': 295,\n",
       " 'free': 296,\n",
       " 'wrong': 297,\n",
       " 'school': 298,\n",
       " 'least': 299,\n",
       " 'times': 300,\n",
       " 'less': 301,\n",
       " 'edit': 302,\n",
       " 'source': 303,\n",
       " 'idea': 304,\n",
       " 'top': 305,\n",
       " 'went': 306,\n",
       " 'during': 307,\n",
       " 'name': 308,\n",
       " 'believe': 309,\n",
       " 'until': 310,\n",
       " 'won': 311,\n",
       " 'state': 312,\n",
       " 'definitely': 313,\n",
       " 'seems': 314,\n",
       " 'guys': 315,\n",
       " 'saying': 316,\n",
       " 'home': 317,\n",
       " 'hope': 318,\n",
       " 'title': 319,\n",
       " 'story': 320,\n",
       " '7': 321,\n",
       " 'youtube': 322,\n",
       " 'fucking': 323,\n",
       " 'small': 324,\n",
       " 'says': 325,\n",
       " 'full': 326,\n",
       " 'rep': 327,\n",
       " 'n': 328,\n",
       " 'yet': 329,\n",
       " 'case': 330,\n",
       " 'sorry': 331,\n",
       " 'stop': 332,\n",
       " 'twitter': 333,\n",
       " 'friend': 334,\n",
       " 'under': 335,\n",
       " 'stuff': 336,\n",
       " 'reason': 337,\n",
       " 'ago': 338,\n",
       " 'support': 339,\n",
       " 'change': 340,\n",
       " '8': 341,\n",
       " 'job': 342,\n",
       " 'however': 343,\n",
       " 'house': 344,\n",
       " 'wasn': 345,\n",
       " 'family': 346,\n",
       " 'left': 347,\n",
       " 'second': 348,\n",
       " 'friends': 349,\n",
       " 'etc': 350,\n",
       " 'guess': 351,\n",
       " 'comment': 352,\n",
       " 'true': 353,\n",
       " 'problem': 354,\n",
       " 'power': 355,\n",
       " 'gets': 356,\n",
       " 'myself': 357,\n",
       " 'called': 358,\n",
       " 'news': 359,\n",
       " 'buy': 360,\n",
       " 'either': 361,\n",
       " 'rules': 362,\n",
       " 'sub': 363,\n",
       " 'set': 364,\n",
       " 'wanted': 365,\n",
       " 'happy': 366,\n",
       " 'w': 367,\n",
       " 'water': 368,\n",
       " 'fun': 369,\n",
       " 'white': 370,\n",
       " 'almost': 371,\n",
       " 'fact': 372,\n",
       " 'wiki': 373,\n",
       " 'experience': 374,\n",
       " 'open': 375,\n",
       " 'call': 376,\n",
       " 'whole': 377,\n",
       " 'started': 378,\n",
       " 'understand': 379,\n",
       " 'wouldn': 380,\n",
       " '20': 381,\n",
       " 'night': 382,\n",
       " '9': 383,\n",
       " 'article': 384,\n",
       " 'h': 385,\n",
       " 'side': 386,\n",
       " '100': 387,\n",
       " 'check': 388,\n",
       " 'came': 389,\n",
       " 'dog': 390,\n",
       " 'took': 391,\n",
       " 'net': 392,\n",
       " 'possible': 393,\n",
       " 'months': 394,\n",
       " 'jpg': 395,\n",
       " 'hours': 396,\n",
       " 'face': 397,\n",
       " 'food': 398,\n",
       " 'saw': 399,\n",
       " 'playing': 400,\n",
       " 'doi': 401,\n",
       " 'care': 402,\n",
       " '12': 403,\n",
       " 'internet': 404,\n",
       " 'study': 405,\n",
       " 'talking': 406,\n",
       " 'remember': 407,\n",
       " 'content': 408,\n",
       " 'mind': 409,\n",
       " 'de': 410,\n",
       " 'ask': 411,\n",
       " 'level': 412,\n",
       " 'told': 413,\n",
       " 'instead': 414,\n",
       " 'season': 415,\n",
       " 'god': 416,\n",
       " 'cool': 417,\n",
       " 'thinking': 418,\n",
       " 'working': 419,\n",
       " 'must': 420,\n",
       " 'human': 421,\n",
       " 'based': 422,\n",
       " 'country': 423,\n",
       " 'talk': 424,\n",
       " '225': 425,\n",
       " 'p': 426,\n",
       " 'movie': 427,\n",
       " 'aren': 428,\n",
       " 'run': 429,\n",
       " 'body': 430,\n",
       " '233': 431,\n",
       " 'agree': 432,\n",
       " 'posted': 433,\n",
       " '227': 434,\n",
       " 'black': 435,\n",
       " '65039': 436,\n",
       " 'awesome': 437,\n",
       " 'party': 438,\n",
       " 'wait': 439,\n",
       " 'games': 440,\n",
       " 'journal': 441,\n",
       " 'others': 442,\n",
       " 'group': 443,\n",
       " 'likely': 444,\n",
       " 'exactly': 445,\n",
       " 'worth': 446,\n",
       " 'government': 447,\n",
       " 'gonna': 448,\n",
       " 'que': 449,\n",
       " 'coming': 450,\n",
       " 'women': 451,\n",
       " 'means': 452,\n",
       " 'tried': 453,\n",
       " 'information': 454,\n",
       " '30': 455,\n",
       " 'pay': 456,\n",
       " 'haven': 457,\n",
       " 'head': 458,\n",
       " 'future': 459,\n",
       " '2017': 460,\n",
       " 'hey': 461,\n",
       " 'c': 462,\n",
       " 'taking': 463,\n",
       " 'single': 464,\n",
       " 'literally': 465,\n",
       " 'hear': 466,\n",
       " 'hate': 467,\n",
       " 'super': 468,\n",
       " 'self': 469,\n",
       " 'goes': 470,\n",
       " 'health': 471,\n",
       " 'amazing': 472,\n",
       " 'hand': 473,\n",
       " 'message': 474,\n",
       " 'within': 475,\n",
       " 'issue': 476,\n",
       " 'comes': 477,\n",
       " 'happened': 478,\n",
       " 'sounds': 479,\n",
       " 'system': 480,\n",
       " 'sense': 481,\n",
       " 'car': 482,\n",
       " 'couple': 483,\n",
       " 'type': 484,\n",
       " 'half': 485,\n",
       " 'social': 486,\n",
       " 'usually': 487,\n",
       " 'facebook': 488,\n",
       " 'order': 489,\n",
       " '3901': 490,\n",
       " '8211': 491,\n",
       " 'close': 492,\n",
       " '3900': 493,\n",
       " 'comic': 494,\n",
       " 'book': 495,\n",
       " 'three': 496,\n",
       " 'children': 497,\n",
       " 'needs': 498,\n",
       " 'interesting': 499,\n",
       " 'futurology': 500,\n",
       " 'number': 501,\n",
       " 'past': 502,\n",
       " 'data': 503,\n",
       " 'girl': 504,\n",
       " 'quite': 505,\n",
       " 'low': 506,\n",
       " 'kids': 507,\n",
       " '128514': 508,\n",
       " 'reference': 509,\n",
       " 'course': 510,\n",
       " 'subreddit': 511,\n",
       " 'dont': 512,\n",
       " 'ok': 513,\n",
       " 'heard': 514,\n",
       " 'weeks': 515,\n",
       " 'yourself': 516,\n",
       " '000': 517,\n",
       " 'together': 518,\n",
       " 'front': 519,\n",
       " 'especially': 520,\n",
       " 'war': 521,\n",
       " 'important': 522,\n",
       " 'control': 523,\n",
       " 'picture': 524,\n",
       " 'happen': 525,\n",
       " 'sometimes': 526,\n",
       " 'fine': 527,\n",
       " 'eat': 528,\n",
       " 'parents': 529,\n",
       " 'list': 530,\n",
       " 'rather': 531,\n",
       " 'line': 532,\n",
       " 'law': 533,\n",
       " 'thread': 534,\n",
       " 'opinion': 535,\n",
       " 'release': 536,\n",
       " 'often': 537,\n",
       " 'public': 538,\n",
       " 'works': 539,\n",
       " 'seem': 540,\n",
       " '8216': 541,\n",
       " '50': 542,\n",
       " 'later': 543,\n",
       " 'vote': 544,\n",
       " 'non': 545,\n",
       " 'american': 546,\n",
       " 'matter': 547,\n",
       " 'favorite': 548,\n",
       " 'posts': 549,\n",
       " 'phone': 550,\n",
       " '15': 551,\n",
       " 'wish': 552,\n",
       " 'original': 553,\n",
       " 'deal': 554,\n",
       " 'b': 555,\n",
       " 'hit': 556,\n",
       " 'google': 557,\n",
       " '8212': 558,\n",
       " 'leave': 559,\n",
       " 'win': 560,\n",
       " 'completely': 561,\n",
       " 'im': 562,\n",
       " 'add': 563,\n",
       " 'damn': 564,\n",
       " 'ones': 565,\n",
       " 'example': 566,\n",
       " 'space': 567,\n",
       " 'due': 568,\n",
       " 'cat': 569,\n",
       " 'lost': 570,\n",
       " 'entire': 571,\n",
       " 'fight': 572,\n",
       " 'absolutely': 573,\n",
       " 'kill': 574,\n",
       " 'basically': 575,\n",
       " 'whether': 576,\n",
       " '11': 577,\n",
       " 'image': 578,\n",
       " 'haha': 579,\n",
       " 'history': 580,\n",
       " 'removed': 581,\n",
       " 'early': 582,\n",
       " 'cause': 583,\n",
       " 'easy': 584,\n",
       " 'die': 585,\n",
       " 'room': 586,\n",
       " 'op': 587,\n",
       " 'couldn': 588,\n",
       " 'media': 589,\n",
       " 'become': 590,\n",
       " 'neutrality': 591,\n",
       " 'players': 592,\n",
       " 'death': 593,\n",
       " 'baby': 594,\n",
       " 'men': 595,\n",
       " 'okay': 596,\n",
       " 'minutes': 597,\n",
       " 'similar': 598,\n",
       " 'answer': 599,\n",
       " 'age': 600,\n",
       " 'company': 601,\n",
       " 'child': 602,\n",
       " 'sound': 603,\n",
       " 'gif': 604,\n",
       " 'music': 605,\n",
       " 'huge': 606,\n",
       " 'move': 607,\n",
       " 'whatever': 608,\n",
       " 'results': 609,\n",
       " 'weight': 610,\n",
       " 'dude': 611,\n",
       " 'page': 612,\n",
       " 'city': 613,\n",
       " 'questions': 614,\n",
       " 'pain': 615,\n",
       " 'hell': 616,\n",
       " 'online': 617,\n",
       " 'ban': 618,\n",
       " 'woman': 619,\n",
       " 'community': 620,\n",
       " 'status': 621,\n",
       " 'soon': 622,\n",
       " 'fire': 623,\n",
       " 'month': 624,\n",
       " 'turn': 625,\n",
       " 'class': 626,\n",
       " 'dead': 627,\n",
       " 'red': 628,\n",
       " 'stay': 629,\n",
       " 'quality': 630,\n",
       " 'watching': 631,\n",
       " 'police': 632,\n",
       " 'simply': 633,\n",
       " 'per': 634,\n",
       " 'business': 635,\n",
       " 'president': 636,\n",
       " 'asked': 637,\n",
       " 'general': 638,\n",
       " 'en': 639,\n",
       " 'weird': 640,\n",
       " 'x': 641,\n",
       " 'chance': 642,\n",
       " 'wants': 643,\n",
       " 'research': 644,\n",
       " 'act': 645,\n",
       " 'seeing': 646,\n",
       " 'area': 647,\n",
       " 'higher': 648,\n",
       " 'court': 649,\n",
       " 'song': 650,\n",
       " 'linked': 651,\n",
       " 'reading': 652,\n",
       " 'press': 653,\n",
       " 'dad': 654,\n",
       " 'light': 655,\n",
       " 'rest': 656,\n",
       " 'advice': 657,\n",
       " 'price': 658,\n",
       " 'wife': 659,\n",
       " 'photo': 660,\n",
       " 'shot': 661,\n",
       " 'mine': 662,\n",
       " 'episode': 663,\n",
       " 'project': 664,\n",
       " 'amount': 665,\n",
       " 'sex': 666,\n",
       " 'finally': 667,\n",
       " 'large': 668,\n",
       " 'mods': 669,\n",
       " 'outside': 670,\n",
       " 'shows': 671,\n",
       " 'honestly': 672,\n",
       " 'series': 673,\n",
       " 'funny': 674,\n",
       " 'account': 675,\n",
       " 'taken': 676,\n",
       " 'risk': 677,\n",
       " 'save': 678,\n",
       " 'states': 679,\n",
       " 'given': 680,\n",
       " 'knew': 681,\n",
       " 'mom': 682,\n",
       " 'felt': 683,\n",
       " 'sleep': 684,\n",
       " 'perfect': 685,\n",
       " 'word': 686,\n",
       " 'player': 687,\n",
       " 'issues': 688,\n",
       " 'behind': 689,\n",
       " 'j': 690,\n",
       " 'hot': 691,\n",
       " 'played': 692,\n",
       " 'fan': 693,\n",
       " 'across': 694,\n",
       " 'wow': 695,\n",
       " 'running': 696,\n",
       " 'copy': 697,\n",
       " 'blue': 698,\n",
       " 'cut': 699,\n",
       " 'anyway': 700,\n",
       " 'normal': 701,\n",
       " '2016': 702,\n",
       " 'share': 703,\n",
       " 'uk': 704,\n",
       " 'store': 705,\n",
       " 'feeling': 706,\n",
       " 'current': 707,\n",
       " 'young': 708,\n",
       " 'points': 709,\n",
       " 'abstract': 710,\n",
       " 'kid': 711,\n",
       " 'takes': 712,\n",
       " 'vs': 713,\n",
       " 'short': 714,\n",
       " 'actual': 715,\n",
       " 'g': 716,\n",
       " 'rule': 717,\n",
       " 'although': 718,\n",
       " 'looked': 719,\n",
       " 'recently': 720,\n",
       " 'site': 721,\n",
       " 'xbox': 722,\n",
       " 'bring': 723,\n",
       " 'moment': 724,\n",
       " 'enjoy': 725,\n",
       " 'late': 726,\n",
       " 'f': 727,\n",
       " 'specific': 728,\n",
       " '18': 729,\n",
       " 'happens': 730,\n",
       " 'related': 731,\n",
       " 'stupid': 732,\n",
       " 'version': 733,\n",
       " 'words': 734,\n",
       " 'middle': 735,\n",
       " 'imagine': 736,\n",
       " 'sign': 737,\n",
       " 'bill': 738,\n",
       " 'test': 739,\n",
       " 'along': 740,\n",
       " 'longer': 741,\n",
       " 'poor': 742,\n",
       " 'break': 743,\n",
       " '25': 744,\n",
       " 'tv': 745,\n",
       " 'garfield': 746,\n",
       " 'giving': 747,\n",
       " 'crazy': 748,\n",
       " 'college': 749,\n",
       " 'near': 750,\n",
       " 'unless': 751,\n",
       " 'situation': 752,\n",
       " 'eating': 753,\n",
       " 'million': 754,\n",
       " 'needed': 755,\n",
       " 'learn': 756,\n",
       " 'joke': 757,\n",
       " 'follow': 758,\n",
       " 'living': 759,\n",
       " 'themselves': 760,\n",
       " 'known': 761,\n",
       " 'instagram': 762,\n",
       " 'blood': 763,\n",
       " 'l': 764,\n",
       " 'pick': 765,\n",
       " 'inside': 766,\n",
       " 'totally': 767,\n",
       " 'star': 768,\n",
       " 'evidence': 769,\n",
       " 'common': 770,\n",
       " 'discussion': 771,\n",
       " 'beautiful': 772,\n",
       " 'png': 773,\n",
       " 'album': 774,\n",
       " 'personal': 775,\n",
       " 'serious': 776,\n",
       " 'bought': 777,\n",
       " 'interested': 778,\n",
       " '16': 779,\n",
       " 'currently': 780,\n",
       " 'build': 781,\n",
       " 'plan': 782,\n",
       " 'gave': 783,\n",
       " 'effect': 784,\n",
       " 'killed': 785,\n",
       " 'clear': 786,\n",
       " 'gay': 787,\n",
       " 'pkk': 788,\n",
       " 'character': 789,\n",
       " 'asking': 790,\n",
       " 'worked': 791,\n",
       " 'fast': 792,\n",
       " 'pass': 793,\n",
       " 'relationship': 794,\n",
       " 'local': 795,\n",
       " 'office': 796,\n",
       " 'air': 797,\n",
       " 'league': 798,\n",
       " 'national': 799,\n",
       " 'political': 800,\n",
       " 'several': 801,\n",
       " 'size': 802,\n",
       " 'main': 803,\n",
       " 'gun': 804,\n",
       " 'process': 805,\n",
       " 'brain': 806,\n",
       " 'report': 807,\n",
       " 'difference': 808,\n",
       " 'above': 809,\n",
       " 'sort': 810,\n",
       " 'explain': 811,\n",
       " 'fair': 812,\n",
       " 'major': 813,\n",
       " 'science': 814,\n",
       " 'term': 815,\n",
       " 'certain': 816,\n",
       " 'lose': 817,\n",
       " 'film': 818,\n",
       " 'html': 819,\n",
       " 'boy': 820,\n",
       " '11088': 821,\n",
       " 'eyes': 822,\n",
       " 'strong': 823,\n",
       " 'response': 824,\n",
       " 'anymore': 825,\n",
       " 'himself': 826,\n",
       " 'glad': 827,\n",
       " 'problems': 828,\n",
       " 'obviously': 829,\n",
       " 'associated': 830,\n",
       " 'worst': 831,\n",
       " 'morning': 832,\n",
       " 'knows': 833,\n",
       " 'allowed': 834,\n",
       " 'sell': 835,\n",
       " 'trade': 836,\n",
       " 'congress': 837,\n",
       " 'positive': 838,\n",
       " 'alone': 839,\n",
       " 'fat': 840,\n",
       " 'hold': 841,\n",
       " 'attack': 842,\n",
       " 'card': 843,\n",
       " 'action': 844,\n",
       " 'form': 845,\n",
       " 'credit': 846,\n",
       " 'kinda': 847,\n",
       " 'hour': 848,\n",
       " 'recommend': 849,\n",
       " 'mod': 850,\n",
       " 'starting': 851,\n",
       " 'service': 852,\n",
       " 'decided': 853,\n",
       " '99': 854,\n",
       " 'ass': 855,\n",
       " 'piece': 856,\n",
       " 'special': 857,\n",
       " 'personally': 858,\n",
       " 'effects': 859,\n",
       " 'worse': 860,\n",
       " 'paste': 861,\n",
       " 'average': 862,\n",
       " 'market': 863,\n",
       " 'seriously': 864,\n",
       " 'appreciate': 865,\n",
       " 'heart': 866,\n",
       " 'y': 867,\n",
       " 'dark': 868,\n",
       " 'earth': 869,\n",
       " 'key': 870,\n",
       " 'drop': 871,\n",
       " '160': 872,\n",
       " 'til': 873,\n",
       " 'hi': 874,\n",
       " 'skin': 875,\n",
       " 'event': 876,\n",
       " 'including': 877,\n",
       " 'product': 878,\n",
       " 'wikipedia': 879,\n",
       " 'thoughts': 880,\n",
       " 'simple': 881,\n",
       " 'review': 882,\n",
       " 'nature': 883,\n",
       " 'united': 884,\n",
       " 'luck': 885,\n",
       " 'hands': 886,\n",
       " 'bed': 887,\n",
       " '40': 888,\n",
       " 'ideas': 889,\n",
       " 'send': 890,\n",
       " 'gone': 891,\n",
       " 'value': 892,\n",
       " 'view': 893,\n",
       " 'posting': 894,\n",
       " 'safe': 895,\n",
       " 'extra': 896,\n",
       " 'gives': 897,\n",
       " 'hair': 898,\n",
       " 'attention': 899,\n",
       " 'according': 900,\n",
       " 'drive': 901,\n",
       " 'meme': 902,\n",
       " 'gold': 903,\n",
       " '13': 904,\n",
       " 'info': 905,\n",
       " 'drug': 906,\n",
       " 'mother': 907,\n",
       " 'green': 908,\n",
       " 'except': 909,\n",
       " 'itself': 910,\n",
       " 'k': 911,\n",
       " 'rights': 912,\n",
       " 'race': 913,\n",
       " 'spend': 914,\n",
       " 'ball': 915,\n",
       " 'provide': 916,\n",
       " 'option': 917,\n",
       " 'america': 918,\n",
       " 'iamhoneydill': 919,\n",
       " 'cost': 920,\n",
       " 'app': 921,\n",
       " 'son': 922,\n",
       " 'academic': 923,\n",
       " 'mostly': 924,\n",
       " 'stand': 925,\n",
       " 'force': 926,\n",
       " 'decision': 927,\n",
       " 'avoid': 928,\n",
       " 'available': 929,\n",
       " 'potential': 930,\n",
       " 'create': 931,\n",
       " 'final': 932,\n",
       " 'members': 933,\n",
       " 'energy': 934,\n",
       " 'meant': 935,\n",
       " 'website': 936,\n",
       " '14': 937,\n",
       " 'videos': 938,\n",
       " 'shouldn': 939,\n",
       " 'sad': 940,\n",
       " 'walk': 941,\n",
       " 'fake': 942,\n",
       " 'quick': 943,\n",
       " 'total': 944,\n",
       " 'fit': 945,\n",
       " 'proof': 946,\n",
       " 'lower': 947,\n",
       " 'straight': 948,\n",
       " '24': 949,\n",
       " 'wonder': 950,\n",
       " 'holy': 951,\n",
       " 'ready': 952,\n",
       " 'offers': 953,\n",
       " 'feels': 954,\n",
       " 'king': 955,\n",
       " 'art': 956,\n",
       " 'campaign': 957,\n",
       " 'user': 958,\n",
       " '17': 959,\n",
       " 'countries': 960,\n",
       " 'users': 961,\n",
       " 'compared': 962,\n",
       " 'majority': 963,\n",
       " 'recent': 964,\n",
       " 'park': 965,\n",
       " 'access': 966,\n",
       " 'allow': 967,\n",
       " 'note': 968,\n",
       " 'language': 969,\n",
       " 'clinton': 970,\n",
       " 'correct': 971,\n",
       " 'search': 972,\n",
       " 'south': 973,\n",
       " 'spent': 974,\n",
       " 'changed': 975,\n",
       " 'choice': 976,\n",
       " 'pro': 977,\n",
       " 'damage': 978,\n",
       " 'lead': 979,\n",
       " 'figure': 980,\n",
       " 'paper': 981,\n",
       " 'following': 982,\n",
       " 'plus': 983,\n",
       " 'cannot': 984,\n",
       " 'co': 985,\n",
       " 'turned': 986,\n",
       " '8201': 987,\n",
       " 'multiple': 988,\n",
       " 'four': 989,\n",
       " 'welcome': 990,\n",
       " 'further': 991,\n",
       " 'ability': 992,\n",
       " 'legal': 993,\n",
       " 'official': 994,\n",
       " 'popular': 995,\n",
       " 'expect': 996,\n",
       " 'offer': 997,\n",
       " 'lives': 998,\n",
       " 'upvote': 999,\n",
       " 'added': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightsHistory(callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        pass\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        experiment.log_histogram_3d(self.model.get_layer('content_dense_layer').get_weights()[0], \n",
    "                                    name='content_dense_weights',\n",
    "                                   step=epoch)  \n",
    "        experiment.log_histogram_3d(model.get_layer('embeddings_layer').get_weights()[0], \n",
    "                            name='embedding_weights',\n",
    "                           step=epoch)\n",
    "        experiment.log_histogram_3d(model.get_layer('numerical_dense_layer').get_weights()[0], \n",
    "                                    name='numerical_dense_weights',\n",
    "                                   step=epoch)\n",
    "        experiment.log_histogram_3d(model.get_layer('sparse_feat_dense_layer').get_weights()[0], \n",
    "                            name='sparse_dense_weights',\n",
    "                           step=epoch)\n",
    "        \n",
    "class FreezeLayer(callbacks.Callback):\n",
    "    def __init__(self, logs={}, patience=5, layer='embeddings_layer', verbose=1, set_to=False):\n",
    "        super(FreezeLayer, self).__init__()\n",
    "        self.freeze_epoch = patience\n",
    "        self.freeze_layer = layer\n",
    "        self.verbose = verbose\n",
    "        self.set_to = set_to\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        if epoch == self.freeze_epoch:\n",
    "            layer = model.get_layer(self.freeze_layer)\n",
    "            old_value = layer.trainable\n",
    "            layer.trainable = self.set_to\n",
    "            if self.verbose:\n",
    "                print(\"Setting %s layer from %s to trainable=%s...\" % (layer.name, old_value,\n",
    "                                                               model.get_layer(self.freeze_layer).trainable))\n",
    "        \n",
    "weights_history = WeightsHistory()\n",
    "\n",
    "freeze_layer = FreezeLayer(patience=6, set_to=True)\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.05,\n",
    "                              patience=4, min_lr=0.0001, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, \n",
    "                x_train, y_train, x_test, y_test, \n",
    "                batch_size, epochs, class_weight, start_epoch=0, workers=4,\n",
    "                model_path='/tmp/model'):\n",
    "    print('Train...')\n",
    "    experiment.log_parameter('class_weight', class_weight.values())\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs, initial_epoch=start_epoch, \n",
    "              class_weight=class_weight,\n",
    "              validation_data=[x_test, y_test],\n",
    "#               validation_split=0.3,\n",
    "                       workers=workers,\n",
    "            callbacks = [\n",
    "                callbacks.ModelCheckpoint(filepath='%s_best' % model_path, verbose=1, save_best_only=True),\n",
    "                callbacks.EarlyStopping(patience=15), weights_history, reduce_lr, freeze_layer,\n",
    "            ])\n",
    "    model.save(model_path)\n",
    "    experiment.log_parameter('model_path', model_path)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 111375 samples, validate on 31863 samples\n",
      "Epoch 1/30\n",
      "111360/111375 [============================>.] - ETA: 0s - loss: 0.6652 - binary_accuracy: 0.7434 - f1_m: 0.1211 - precision_m: 0.0783 - recall_m: 0.4277\n",
      "Epoch 00001: val_loss improved from inf to 0.63361, saving model to models/mlp_plus3_best\n",
      "111375/111375 [==============================] - 73s 653us/sample - loss: 0.6653 - binary_accuracy: 0.7434 - f1_m: 0.1209 - precision_m: 0.0783 - recall_m: 0.4272 - val_loss: 0.6336 - val_binary_accuracy: 0.7247 - val_f1_m: 0.2082 - val_precision_m: 0.1306 - val_recall_m: 0.5715\n",
      "Epoch 2/30\n",
      "111360/111375 [============================>.] - ETA: 0s - loss: 0.6431 - binary_accuracy: 0.7479 - f1_m: 0.1329 - precision_m: 0.0801 - recall_m: 0.4901\n",
      "Epoch 00002: val_loss improved from 0.63361 to 0.58390, saving model to models/mlp_plus3_best\n",
      "111375/111375 [==============================] - 67s 606us/sample - loss: 0.6431 - binary_accuracy: 0.7479 - f1_m: 0.1328 - precision_m: 0.0801 - recall_m: 0.4895 - val_loss: 0.5839 - val_binary_accuracy: 0.7764 - val_f1_m: 0.2205 - val_precision_m: 0.1459 - val_recall_m: 0.4905\n",
      "Epoch 3/30\n",
      "111232/111375 [============================>.] - ETA: 0s - loss: 0.6341 - binary_accuracy: 0.7403 - f1_m: 0.1356 - precision_m: 0.0807 - recall_m: 0.5188\n",
      "Epoch 00003: val_loss did not improve from 0.58390\n",
      "111375/111375 [==============================] - 67s 600us/sample - loss: 0.6342 - binary_accuracy: 0.7402 - f1_m: 0.1354 - precision_m: 0.0806 - recall_m: 0.5179 - val_loss: 0.6780 - val_binary_accuracy: 0.6064 - val_f1_m: 0.1768 - val_precision_m: 0.1038 - val_recall_m: 0.6573\n",
      "Epoch 4/30\n",
      "111360/111375 [============================>.] - ETA: 0s - loss: 0.6300 - binary_accuracy: 0.7361 - f1_m: 0.1350 - precision_m: 0.0796 - recall_m: 0.5261\n",
      "Epoch 00004: val_loss did not improve from 0.58390\n",
      "111375/111375 [==============================] - 66s 595us/sample - loss: 0.6301 - binary_accuracy: 0.7361 - f1_m: 0.1349 - precision_m: 0.0795 - recall_m: 0.5255 - val_loss: 0.6769 - val_binary_accuracy: 0.6070 - val_f1_m: 0.1769 - val_precision_m: 0.1040 - val_recall_m: 0.6556\n",
      "Epoch 5/30\n",
      "111360/111375 [============================>.] - ETA: 0s - loss: 0.6290 - binary_accuracy: 0.7333 - f1_m: 0.1366 - precision_m: 0.0808 - recall_m: 0.5271\n",
      "Epoch 00005: val_loss did not improve from 0.58390\n",
      "111375/111375 [==============================] - 66s 590us/sample - loss: 0.6290 - binary_accuracy: 0.7333 - f1_m: 0.1370 - precision_m: 0.0810 - recall_m: 0.5277 - val_loss: 0.6424 - val_binary_accuracy: 0.6641 - val_f1_m: 0.1864 - val_precision_m: 0.1126 - val_recall_m: 0.5980\n",
      "Epoch 6/30\n",
      "111104/111375 [============================>.] - ETA: 0s - loss: 0.6261 - binary_accuracy: 0.7329 - f1_m: 0.1374 - precision_m: 0.0806 - recall_m: 0.5500\n",
      "Epoch 00006: val_loss did not improve from 0.58390\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0004999999888241291.\n",
      "111375/111375 [==============================] - 69s 616us/sample - loss: 0.6260 - binary_accuracy: 0.7330 - f1_m: 0.1380 - precision_m: 0.0810 - recall_m: 0.5514 - val_loss: 0.6552 - val_binary_accuracy: 0.6493 - val_f1_m: 0.1846 - val_precision_m: 0.1104 - val_recall_m: 0.6180\n",
      "Setting embeddings_layer layer from False to trainable=True...\n",
      "Epoch 7/30\n",
      "111232/111375 [============================>.] - ETA: 0s - loss: 0.6239 - binary_accuracy: 0.7223 - f1_m: 0.1367 - precision_m: 0.0796 - recall_m: 0.5628\n",
      "Epoch 00007: val_loss did not improve from 0.58390\n",
      "111375/111375 [==============================] - 64s 575us/sample - loss: 0.6238 - binary_accuracy: 0.7224 - f1_m: 0.1365 - precision_m: 0.0794 - recall_m: 0.5627 - val_loss: 0.6211 - val_binary_accuracy: 0.6984 - val_f1_m: 0.1981 - val_precision_m: 0.1217 - val_recall_m: 0.5767\n",
      "Epoch 8/30\n",
      "111104/111375 [============================>.] - ETA: 0s - loss: 0.6245 - binary_accuracy: 0.7377 - f1_m: 0.1386 - precision_m: 0.0814 - recall_m: 0.5373\n",
      "Epoch 00008: val_loss did not improve from 0.58390\n",
      "111375/111375 [==============================] - 70s 633us/sample - loss: 0.6246 - binary_accuracy: 0.7377 - f1_m: 0.1384 - precision_m: 0.0814 - recall_m: 0.5364 - val_loss: 0.6223 - val_binary_accuracy: 0.6965 - val_f1_m: 0.1971 - val_precision_m: 0.1215 - val_recall_m: 0.5777\n",
      "Epoch 9/30\n",
      "111104/111375 [============================>.] - ETA: 0s - loss: 0.6236 - binary_accuracy: 0.7393 - f1_m: 0.1398 - precision_m: 0.0820 - recall_m: 0.5429\n",
      "Epoch 00009: val_loss did not improve from 0.58390\n",
      "111375/111375 [==============================] - 66s 596us/sample - loss: 0.6235 - binary_accuracy: 0.7393 - f1_m: 0.1396 - precision_m: 0.0818 - recall_m: 0.5422 - val_loss: 0.6226 - val_binary_accuracy: 0.6960 - val_f1_m: 0.1961 - val_precision_m: 0.1205 - val_recall_m: 0.5727\n",
      "Epoch 10/30\n",
      "111232/111375 [============================>.] - ETA: 0s - loss: 0.6237 - binary_accuracy: 0.7374 - f1_m: 0.1386 - precision_m: 0.0813 - recall_m: 0.5478\n",
      "Epoch 00010: val_loss did not improve from 0.58390\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "111375/111375 [==============================] - 66s 591us/sample - loss: 0.6238 - binary_accuracy: 0.7373 - f1_m: 0.1384 - precision_m: 0.0813 - recall_m: 0.5473 - val_loss: 0.6165 - val_binary_accuracy: 0.7042 - val_f1_m: 0.1990 - val_precision_m: 0.1229 - val_recall_m: 0.5734\n",
      "Epoch 11/30\n",
      "111232/111375 [============================>.] - ETA: 0s - loss: 0.6238 - binary_accuracy: 0.7446 - f1_m: 0.1404 - precision_m: 0.0831 - recall_m: 0.5281\n",
      "Epoch 00011: val_loss did not improve from 0.58390\n",
      "111375/111375 [==============================] - 66s 590us/sample - loss: 0.6239 - binary_accuracy: 0.7447 - f1_m: 0.1402 - precision_m: 0.0830 - recall_m: 0.5274 - val_loss: 0.6175 - val_binary_accuracy: 0.7031 - val_f1_m: 0.1990 - val_precision_m: 0.1226 - val_recall_m: 0.5721\n",
      "Epoch 12/30\n",
      "111360/111375 [============================>.] - ETA: 0s - loss: 0.6241 - binary_accuracy: 0.7442 - f1_m: 0.1406 - precision_m: 0.0829 - recall_m: 0.5358\n",
      "Epoch 00012: val_loss did not improve from 0.58390\n",
      "111375/111375 [==============================] - 66s 596us/sample - loss: 0.6241 - binary_accuracy: 0.7443 - f1_m: 0.1409 - precision_m: 0.0831 - recall_m: 0.5363 - val_loss: 0.6195 - val_binary_accuracy: 0.7002 - val_f1_m: 0.1967 - val_precision_m: 0.1215 - val_recall_m: 0.5677\n",
      "CPU times: user 6min 30s, sys: 6.32 s, total: 6min 36s\n",
      "Wall time: 13min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = train_model(model, x_train, y_train, x_test, y_test,\n",
    "           epochs=30, batch_size=hyperparams['batch_size'],\n",
    "                      class_weight={0:0.5, 1:12}, \n",
    "                      model_path='models/mlp_plus3', workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.09271837, -0.03045974,  0.05543307, ..., -0.09386808,\n",
       "         -0.1191508 ,  0.13767627],\n",
       "        [-0.06285473,  0.0887652 ,  0.6177369 , ..., -0.46111107,\n",
       "          0.5900506 ,  0.31393498],\n",
       "        [ 0.11833031,  0.77615637,  0.21613106, ..., -0.49132916,\n",
       "          0.98082536, -0.32070193],\n",
       "        ...,\n",
       "        [-0.70135725, -0.61761826, -0.44837347, ..., -0.08644207,\n",
       "          0.06559046,  0.26356876],\n",
       "        [ 0.5360635 , -0.44415718, -0.80922186, ..., -0.70944965,\n",
       "         -0.04771215,  1.1725096 ],\n",
       "        [ 0.11810102, -0.46829095,  0.25338915, ...,  0.49094057,\n",
       "          0.11555818, -0.00985517]], dtype=float32),\n",
       " array([[ 0.16868843,  0.18346812,  0.32914123, ...,  0.33653575,\n",
       "          0.1287443 , -0.19949372],\n",
       "        [ 0.24449368,  0.39361542,  0.25753158, ...,  0.354353  ,\n",
       "          0.29757273, -0.27906018],\n",
       "        [ 0.02489973, -0.1721038 , -0.22556768, ..., -0.19781807,\n",
       "         -0.20217238,  0.09184619],\n",
       "        ...,\n",
       "        [-0.20533846,  0.04856887, -0.09877386, ..., -0.18334748,\n",
       "          0.03157208,  0.12623453],\n",
       "        [-0.14218995, -0.21545364, -0.25165603, ...,  0.01940574,\n",
       "         -0.10068312,  0.1401268 ],\n",
       "        [ 0.03354325,  0.06700797,  0.11569116, ...,  0.00573775,\n",
       "          0.01674456, -0.05876466]], dtype=float32),\n",
       " array([-0.00011477, -0.00357572, -0.00591882,  0.00101501, -0.00097952,\n",
       "         0.00019989, -0.00026309,  0.00081891, -0.00123269, -0.00878439,\n",
       "        -0.01927224, -0.00017995,  0.00313806,  0.00173323, -0.0004682 ,\n",
       "        -0.00112653, -0.00171283,  0.00087789, -0.00042545, -0.00099473,\n",
       "         0.00360947,  0.00136692,  0.00129877, -0.00120004,  0.00043639,\n",
       "        -0.00088784, -0.00037474, -0.00113306,  0.00231488,  0.00058363,\n",
       "        -0.00184987, -0.00179912], dtype=float32),\n",
       " array([[-0.21287188],\n",
       "        [ 0.19944821],\n",
       "        [-0.10470545],\n",
       "        [-0.07438299],\n",
       "        [ 0.04195324],\n",
       "        [-0.30135968],\n",
       "        [ 0.20245309],\n",
       "        [ 0.05287293],\n",
       "        [-0.0363421 ],\n",
       "        [ 0.1112868 ],\n",
       "        [-0.10199606]], dtype=float32),\n",
       " array([0.00146276], dtype=float32),\n",
       " array([[-8.06844011e-02, -2.56869078e-01, -1.81464329e-01,\n",
       "          2.13077545e-01,  3.15849632e-02],\n",
       "        [ 5.64741269e-02, -1.17046446e-01, -2.80779079e-02,\n",
       "          1.48314193e-01,  1.79306090e-01],\n",
       "        [ 1.02752913e-02,  1.88665278e-02,  3.79858762e-02,\n",
       "          2.02192366e-01, -9.15387878e-04],\n",
       "        [-2.56610406e-03,  7.40094576e-04, -9.80727524e-02,\n",
       "         -1.36497850e-02, -3.49026807e-02],\n",
       "        [ 8.93698186e-02, -1.25746503e-01, -6.88961372e-02,\n",
       "         -2.94461958e-02, -4.03089263e-02],\n",
       "        [-5.37730381e-02, -7.69214407e-02,  5.00551946e-02,\n",
       "          5.46619017e-03,  2.37169657e-02],\n",
       "        [-4.26567942e-02,  4.22515683e-02,  4.64831889e-02,\n",
       "         -3.79464254e-02, -4.15811455e-03],\n",
       "        [ 5.37615009e-02, -1.53455501e-02, -8.27374309e-02,\n",
       "         -2.96021793e-02,  8.43310449e-03],\n",
       "        [-1.37710914e-01, -1.24686450e-01,  5.92195354e-02,\n",
       "          5.28535657e-02,  8.36827904e-02],\n",
       "        [ 5.26365395e-33, -5.86299885e-33, -3.71414779e-33,\n",
       "          5.84985939e-33, -5.85343620e-33],\n",
       "        [ 3.04943218e-33,  5.81700873e-33, -5.50473572e-33,\n",
       "         -5.29218687e-33, -5.80163914e-33],\n",
       "        [ 5.70678410e-33, -5.74032169e-33,  5.54291320e-33,\n",
       "          5.85037147e-33,  5.84056748e-33],\n",
       "        [-5.85020653e-33, -5.81896667e-33,  5.87423547e-33,\n",
       "          5.68375837e-33, -5.72488304e-33],\n",
       "        [ 1.00620948e-01, -1.44561857e-01, -1.81987599e-01,\n",
       "         -2.90762540e-02,  1.39050156e-01],\n",
       "        [-1.18711181e-01,  3.71486358e-02,  1.86930168e-02,\n",
       "         -1.11441202e-01, -1.56070227e-02],\n",
       "        [ 6.45192042e-02,  6.81727678e-02,  1.08875975e-01,\n",
       "          1.01258785e-01,  9.29144584e-03],\n",
       "        [-2.57428270e-02,  2.54217889e-02,  2.82361079e-02,\n",
       "         -2.26228964e-02, -2.64278776e-03],\n",
       "        [-1.08244605e-01,  9.65374932e-02,  1.78654492e-01,\n",
       "          3.24488766e-02, -2.11941637e-02],\n",
       "        [-6.12514168e-02,  1.15802221e-01, -9.35886502e-02,\n",
       "         -1.11633956e-01, -7.85748735e-02],\n",
       "        [-7.80219957e-02, -9.02772620e-02,  1.05472609e-01,\n",
       "          8.00713673e-02,  4.53711003e-02],\n",
       "        [-7.37851188e-02,  8.23170245e-02,  1.98181868e-01,\n",
       "         -5.52506670e-02, -2.44970024e-02],\n",
       "        [ 1.48783043e-01, -1.10848747e-01, -2.58012265e-01,\n",
       "          5.14729954e-02,  2.79027969e-02],\n",
       "        [-5.77195424e-33, -5.82135953e-33,  5.86458797e-33,\n",
       "          5.82058003e-33,  5.82175993e-33],\n",
       "        [ 2.12252915e-01, -3.51136416e-01, -9.00253132e-02,\n",
       "          2.34732971e-01,  1.23177312e-01],\n",
       "        [ 1.16952837e-01, -1.07591830e-01, -6.00635298e-02,\n",
       "          7.46081863e-03,  1.88191701e-02],\n",
       "        [-7.93962777e-02,  1.06355958e-01,  5.17125092e-02,\n",
       "         -8.66208449e-02, -1.55881243e-02],\n",
       "        [-5.68448976e-02, -1.22458652e-01, -7.38091245e-02,\n",
       "          5.29212840e-02, -7.43302628e-02],\n",
       "        [ 5.44623504e-33,  5.79011489e-33,  4.90879168e-33,\n",
       "          5.83267587e-33, -5.83998524e-33],\n",
       "        [-2.73237214e-03, -1.98919885e-02, -8.39022473e-02,\n",
       "          1.48451924e-01,  1.45613834e-01],\n",
       "        [ 5.82989603e-02,  1.36667257e-02, -1.81280989e-02,\n",
       "         -1.01557232e-01, -2.26764721e-04],\n",
       "        [ 1.27869025e-01, -1.12577073e-01,  1.35001801e-02,\n",
       "          5.23795709e-02,  1.76165074e-01],\n",
       "        [ 1.10477298e-01,  6.46426994e-03, -1.53643806e-02,\n",
       "          5.67466915e-02,  9.55433100e-02],\n",
       "        [-7.15615004e-02, -2.52317954e-02,  3.50222550e-02,\n",
       "          1.70291811e-01,  1.08704120e-01],\n",
       "        [-1.27141559e-02,  2.05403268e-02,  3.20707150e-02,\n",
       "         -1.40520716e-02,  1.33330433e-03],\n",
       "        [-1.90287083e-01,  1.53198138e-01,  1.45383298e-01,\n",
       "         -1.06684797e-01, -3.69549282e-02],\n",
       "        [-9.64216366e-02,  9.56088863e-03,  8.07210132e-02,\n",
       "         -1.78578541e-01, -9.09438431e-02],\n",
       "        [-4.38596755e-02,  1.26678282e-02,  2.15550512e-01,\n",
       "         -2.28554755e-01, -1.23640634e-01],\n",
       "        [ 5.91082275e-02,  1.28030196e-01,  9.94001701e-02,\n",
       "         -5.99464923e-02,  4.66646962e-02],\n",
       "        [-6.08056299e-02,  2.62306705e-02,  5.64872473e-02,\n",
       "         -4.00121659e-02, -2.53026956e-03],\n",
       "        [-1.65609404e-01,  8.76798108e-02,  4.18621451e-02,\n",
       "         -2.42389943e-02, -9.86314937e-02],\n",
       "        [ 1.18023098e-01,  3.02694310e-02,  1.92885734e-02,\n",
       "          2.89207269e-02, -5.53799346e-02],\n",
       "        [ 5.81298413e-33, -5.80884970e-33, -2.18841416e-33,\n",
       "          5.64969328e-33,  2.33808839e-33],\n",
       "        [-1.40131652e-01,  8.86788145e-02, -5.63219301e-02,\n",
       "         -3.38054411e-02,  4.38473467e-03],\n",
       "        [-2.05047533e-01,  2.50462890e-01, -1.29505731e-02,\n",
       "          2.96477247e-02, -1.48165107e-01],\n",
       "        [ 1.06089339e-01, -4.95774448e-02,  9.07303672e-03,\n",
       "         -3.44233550e-02, -2.43596230e-02],\n",
       "        [ 7.38308132e-02, -1.31408051e-01,  1.44766256e-01,\n",
       "          1.08597971e-01, -9.10453200e-02],\n",
       "        [-1.13529757e-01,  9.61575359e-02, -9.20107663e-02,\n",
       "         -7.23455697e-02, -8.25294033e-02],\n",
       "        [ 1.23471759e-01, -6.39645457e-02,  1.30771026e-01,\n",
       "         -7.17164017e-03,  1.47038862e-01],\n",
       "        [ 7.26341978e-02, -1.86268970e-01,  1.77666917e-02,\n",
       "          9.36161429e-02,  5.58081083e-02],\n",
       "        [ 5.91446180e-04, -4.57379110e-02, -5.66031374e-02,\n",
       "          7.15136528e-02,  9.07672048e-02],\n",
       "        [ 1.21818572e-01, -1.74952626e-01, -9.28248838e-02,\n",
       "         -1.26919493e-01, -8.53619203e-02],\n",
       "        [-8.19540396e-02, -8.64910260e-02, -1.27395868e-01,\n",
       "          5.20672090e-03,  2.66812686e-02],\n",
       "        [-2.19524056e-02, -6.49034008e-02, -1.39314398e-01,\n",
       "          6.53437823e-02,  9.72759649e-02],\n",
       "        [ 1.29350156e-01, -7.24117458e-02,  3.42656225e-02,\n",
       "          3.50121805e-03, -9.98051018e-02],\n",
       "        [-1.38409495e-01, -1.47337735e-01, -1.06165111e-01,\n",
       "          6.02953993e-02,  6.68617636e-02],\n",
       "        [-1.06323928e-01, -1.13354743e-01, -1.09116495e-01,\n",
       "         -5.91039732e-02, -2.26230789e-02],\n",
       "        [ 9.90453064e-02,  1.76388532e-01,  1.63106546e-01,\n",
       "          4.84073609e-02,  9.18230414e-02],\n",
       "        [-1.27836186e-02,  1.49409786e-01, -1.40193760e-01,\n",
       "          1.34740368e-01, -9.63507965e-02],\n",
       "        [-5.98483048e-02,  1.51843116e-01,  5.58544286e-02,\n",
       "         -9.24708694e-02, -1.49511090e-02],\n",
       "        [-6.06986769e-02,  5.72507977e-02,  1.52944118e-01,\n",
       "          6.36953535e-03,  1.38658774e-03],\n",
       "        [ 8.12975317e-02, -1.19363360e-01, -8.00007430e-04,\n",
       "         -2.51723099e-02, -1.99523335e-03],\n",
       "        [-4.52337004e-02, -2.02423856e-02, -2.16862466e-02,\n",
       "         -1.31542552e-02,  1.94843952e-02],\n",
       "        [-6.34404272e-02,  1.32714603e-02, -3.23863029e-02,\n",
       "          3.94590050e-02, -1.48972139e-01],\n",
       "        [ 9.62979123e-02, -1.02714330e-01, -3.78312133e-02,\n",
       "         -1.12417927e-02,  1.19703621e-01],\n",
       "        [ 1.23417526e-02, -7.17032477e-02,  1.58834476e-02,\n",
       "          2.10082177e-02, -1.66915774e-01],\n",
       "        [-8.81581455e-02, -1.27628997e-01, -1.14501357e-01,\n",
       "          1.32591752e-02, -1.30079538e-01],\n",
       "        [ 1.71580136e-01, -2.16201656e-02, -1.05797850e-01,\n",
       "          4.71282229e-02,  5.70544088e-03],\n",
       "        [-4.29212407e-04, -6.89188614e-02,  1.10667214e-01,\n",
       "          1.60024121e-01,  9.22087878e-02],\n",
       "        [ 3.71506624e-02,  1.85099557e-01, -2.91085076e-02,\n",
       "         -8.86620134e-02, -5.45381894e-03],\n",
       "        [-3.99436019e-02,  2.02669963e-01,  1.62071794e-01,\n",
       "         -3.31499949e-02, -4.73307483e-02],\n",
       "        [-1.06626235e-01,  6.94452524e-02,  3.48737240e-02,\n",
       "          4.95684855e-02, -1.57500822e-02],\n",
       "        [ 8.92586783e-02,  8.38982537e-02,  1.92529857e-01,\n",
       "          8.62107351e-02, -1.64272159e-01],\n",
       "        [ 1.35227025e-01,  9.25128609e-02,  4.13319282e-02,\n",
       "         -1.69004828e-01,  1.08736776e-01],\n",
       "        [-7.29539022e-02,  3.09307575e-02, -7.92084262e-04,\n",
       "         -1.04234386e-02, -1.62666097e-01],\n",
       "        [ 1.48312554e-01, -4.34918031e-02,  2.11198945e-02,\n",
       "          2.04032082e-02, -1.33066475e-01],\n",
       "        [-2.19529737e-02, -8.36553201e-02, -1.28097951e-01,\n",
       "          3.65824513e-02, -3.02583128e-02],\n",
       "        [-6.50558099e-02, -1.47382692e-01,  1.21463552e-01,\n",
       "          1.00725792e-01, -1.63923614e-02],\n",
       "        [-5.59930243e-02,  7.98305720e-02,  9.16824639e-02,\n",
       "         -1.80685110e-02, -1.69783849e-02],\n",
       "        [-1.37758866e-01, -2.17385143e-02,  1.17007948e-01,\n",
       "         -1.86120905e-02, -3.61987725e-02],\n",
       "        [ 2.76392195e-02,  1.10321000e-01, -1.49225980e-01,\n",
       "          3.00370473e-02, -9.62263948e-05],\n",
       "        [ 3.66124995e-02,  6.55904785e-02,  1.13289893e-01,\n",
       "         -1.06871486e-01,  8.46462846e-02],\n",
       "        [-2.62909979e-02,  1.40661135e-01, -3.86097021e-02,\n",
       "         -2.90687243e-03, -1.71914659e-02],\n",
       "        [ 1.15237094e-01, -9.98313446e-03,  1.56329975e-01,\n",
       "          1.29737586e-01, -4.47092317e-02],\n",
       "        [-1.87941298e-01,  1.01418890e-01, -5.61771840e-02,\n",
       "         -1.45262286e-01,  2.08377782e-02],\n",
       "        [-1.76756829e-01, -2.59722490e-02,  1.29100140e-02,\n",
       "         -6.02099001e-02,  1.56979053e-03],\n",
       "        [-5.33564761e-03,  8.61618891e-02,  1.47433057e-01,\n",
       "         -1.46877384e-02, -6.92161266e-03],\n",
       "        [-1.83462705e-02, -9.64738354e-02,  1.96253181e-01,\n",
       "         -1.25461342e-02,  1.25875920e-01],\n",
       "        [ 1.05652772e-03,  1.14966713e-01,  4.82690297e-02,\n",
       "         -2.84551270e-02, -4.78822663e-02],\n",
       "        [-5.99162653e-02, -8.92595109e-03,  1.01629697e-01,\n",
       "          1.16575822e-01,  3.27689154e-03],\n",
       "        [ 1.84299782e-01,  1.03204317e-01,  9.04226229e-02,\n",
       "          8.44156817e-02, -3.26022203e-03],\n",
       "        [-1.11870496e-02,  1.32459015e-01,  6.40852153e-02,\n",
       "         -1.03716493e-01, -1.26312479e-01],\n",
       "        [ 2.76593715e-02, -9.31465849e-02, -1.16123110e-01,\n",
       "         -1.34261474e-01, -4.16055173e-02],\n",
       "        [ 5.03203422e-02, -1.36023968e-01,  9.47766602e-02,\n",
       "         -2.46066065e-03, -7.39782900e-02],\n",
       "        [-1.72211677e-02,  2.31877059e-01, -1.30274398e-02,\n",
       "          7.81850815e-02, -1.32669702e-01],\n",
       "        [ 9.01865959e-02,  1.30160272e-01,  1.45366654e-01,\n",
       "          4.51421887e-02,  1.26449347e-01],\n",
       "        [-2.00935557e-01,  1.42787576e-01,  4.23852690e-02,\n",
       "         -1.88870311e-01, -3.02356426e-02],\n",
       "        [-2.71770265e-03,  1.29777133e-01, -4.64325361e-02,\n",
       "          3.21780480e-02, -2.43807379e-02],\n",
       "        [-1.13297269e-01,  3.36451009e-02,  7.27406666e-02,\n",
       "         -2.94654462e-02,  4.07490227e-03],\n",
       "        [ 8.43951032e-02, -5.57146734e-03,  1.15877502e-01,\n",
       "          6.30437359e-02,  9.74170640e-02],\n",
       "        [-5.62105179e-02,  1.45877153e-01,  1.34230286e-01,\n",
       "         -1.63998187e-01, -1.12744428e-01],\n",
       "        [ 1.52472993e-02,  4.84336726e-02, -9.21200365e-02,\n",
       "         -7.10076615e-02, -1.19299635e-01],\n",
       "        [ 3.04130279e-02, -8.62670466e-02, -2.27033403e-02,\n",
       "         -1.70526713e-01,  3.35865356e-02],\n",
       "        [-1.80818886e-01, -3.62198092e-02,  7.12880567e-02,\n",
       "         -8.26164894e-03, -1.81665868e-02],\n",
       "        [-8.16246048e-02,  2.02789577e-03,  1.11384336e-02,\n",
       "         -1.10060737e-01,  3.07459608e-02],\n",
       "        [-2.23977447e-01, -9.87958238e-02,  1.46682793e-02,\n",
       "         -1.68393776e-01, -1.42145529e-01],\n",
       "        [-2.08377447e-02,  1.91978127e-01,  2.38239467e-01,\n",
       "          7.09122121e-02, -7.44239464e-02],\n",
       "        [-7.24224821e-02,  1.30662635e-01,  5.18241487e-02,\n",
       "          5.73697276e-02,  3.11042611e-02],\n",
       "        [-3.37928198e-02, -2.83941422e-02, -1.27010182e-01,\n",
       "         -1.37575984e-01, -6.35849833e-02],\n",
       "        [ 3.83441038e-02,  1.72670275e-01,  1.95561066e-01,\n",
       "         -1.12711295e-01, -6.87183514e-02],\n",
       "        [-7.90260267e-03,  8.08920413e-02,  1.37668205e-02,\n",
       "          6.33264184e-02,  2.29270067e-02],\n",
       "        [ 1.70227543e-01, -9.47861746e-02, -7.00006187e-02,\n",
       "          9.19180736e-02, -3.25353555e-02],\n",
       "        [-1.03866361e-01, -1.24975562e-01, -2.89358087e-02,\n",
       "         -5.59122823e-02,  4.16498519e-02],\n",
       "        [ 9.87927541e-02,  6.75071999e-02,  4.89804633e-02,\n",
       "          1.29170567e-01,  6.94364011e-02],\n",
       "        [ 1.35509834e-01,  8.59537646e-02, -1.64813384e-01,\n",
       "          1.57846883e-01,  7.36548305e-02],\n",
       "        [-1.01271801e-01,  7.04409042e-03, -5.95190413e-02,\n",
       "          1.35092303e-01,  1.24283083e-01],\n",
       "        [-6.20060079e-02, -8.09612870e-02,  7.82580078e-02,\n",
       "          6.61523640e-02, -1.85139384e-02],\n",
       "        [ 5.48638590e-02,  8.22541714e-02,  1.15032598e-01,\n",
       "         -5.32945758e-03, -6.19405843e-02],\n",
       "        [-5.60935624e-02,  6.63883612e-02, -2.02919729e-02,\n",
       "         -1.01939991e-01, -4.56702150e-03],\n",
       "        [-1.36778429e-01,  1.71966329e-02, -3.62140983e-02,\n",
       "          4.30286964e-05, -1.71709403e-01],\n",
       "        [ 7.00151697e-02,  1.57518283e-01, -1.26589105e-01,\n",
       "          1.41696006e-01,  2.11962927e-02],\n",
       "        [-1.08458757e-01, -1.55549675e-01,  2.74741277e-02,\n",
       "          1.45605028e-01,  3.36713381e-02],\n",
       "        [-4.72166240e-02,  9.04113874e-02, -1.28151804e-01,\n",
       "         -3.15767080e-02,  1.41367931e-02],\n",
       "        [ 1.09834000e-01, -7.12135956e-02,  3.09243742e-02,\n",
       "         -4.49868552e-02, -1.19994327e-01],\n",
       "        [ 1.02608375e-01,  2.09972262e-01, -8.02476257e-02,\n",
       "          9.31109339e-02, -9.20940638e-02],\n",
       "        [ 4.97408919e-02, -4.04308029e-02, -6.56176582e-02,\n",
       "         -4.76062251e-03, -1.58555564e-02],\n",
       "        [-4.87580784e-02,  5.68690635e-02, -1.43229499e-01,\n",
       "         -4.57069799e-02,  4.60487083e-02],\n",
       "        [ 7.13508343e-03, -6.64857328e-02, -3.84045765e-02,\n",
       "         -5.14138415e-02,  7.51611888e-02],\n",
       "        [-3.42893861e-02,  3.44672278e-02, -1.58733115e-01,\n",
       "         -6.96020722e-02, -1.73286274e-02],\n",
       "        [-1.33785665e-01,  1.48266107e-01,  7.14268312e-02,\n",
       "          1.69679344e-01,  3.79772633e-02],\n",
       "        [-1.89914584e-01, -8.46499354e-02, -3.37021463e-02,\n",
       "          4.05094884e-02,  2.45721675e-02],\n",
       "        [-1.18542485e-01,  1.27429649e-01, -3.71283432e-03,\n",
       "         -1.25926808e-01,  3.01139168e-02],\n",
       "        [ 2.74589635e-03,  3.60489339e-02, -2.89522577e-02,\n",
       "         -3.19703333e-02,  8.02203044e-02],\n",
       "        [-5.82153365e-33, -5.86585163e-33, -5.74324683e-33,\n",
       "          5.65020572e-33,  5.84906850e-33],\n",
       "        [ 6.68375716e-02,  4.92266603e-02,  4.09029536e-02,\n",
       "          3.66139598e-03,  1.95317902e-02],\n",
       "        [ 5.68298474e-33,  5.82192597e-33,  5.72357383e-33,\n",
       "         -5.42345837e-33,  5.83979826e-33],\n",
       "        [ 8.74041021e-02, -4.32533026e-02, -1.40276015e-01,\n",
       "         -5.18037379e-02,  8.41537714e-02],\n",
       "        [-1.98252082e-01,  8.74775182e-03,  1.00591764e-01,\n",
       "         -5.67438081e-03, -1.32877097e-01],\n",
       "        [-1.08316995e-01, -7.01364577e-02,  1.04592023e-02,\n",
       "         -1.33997470e-01,  2.99070217e-02],\n",
       "        [ 8.43139514e-02,  8.69548023e-02,  4.95587699e-02,\n",
       "          1.18988700e-01,  8.70372579e-02],\n",
       "        [ 6.79047704e-02, -7.43656594e-04,  1.25040144e-01,\n",
       "         -1.52793787e-02,  3.03677209e-02],\n",
       "        [-8.60487893e-02, -1.14341946e-02,  2.38756109e-02,\n",
       "         -2.57653058e-01, -1.22768648e-01],\n",
       "        [ 5.03010415e-02,  1.13637395e-01, -1.02232330e-01,\n",
       "         -1.68804660e-01, -3.04445587e-02],\n",
       "        [-6.53867498e-02,  1.60565227e-02,  3.12017072e-02,\n",
       "         -1.23251878e-01, -1.11792423e-02],\n",
       "        [-2.74051130e-02, -5.59500083e-02,  5.35814688e-02,\n",
       "         -8.59822184e-02,  5.76844951e-03],\n",
       "        [ 1.86082926e-02, -3.55457775e-02, -4.64655794e-02,\n",
       "         -1.47359064e-02,  2.12497618e-02],\n",
       "        [ 5.78966931e-33,  5.64560145e-33, -5.74072246e-33,\n",
       "          5.64636075e-33,  5.81291507e-33],\n",
       "        [ 4.04650979e-02,  7.95000605e-03,  1.50353208e-01,\n",
       "         -1.54642537e-01, -5.44520244e-02],\n",
       "        [-5.76572228e-33,  5.13979762e-33, -5.24188673e-33,\n",
       "         -5.77933965e-33,  5.75125378e-33],\n",
       "        [ 3.39010023e-02,  6.45484328e-02,  4.14206535e-02,\n",
       "         -1.58272803e-01,  5.15246317e-02],\n",
       "        [ 5.06903286e-33, -5.58275181e-33, -5.87406025e-33,\n",
       "          5.85501981e-33, -5.63862379e-33],\n",
       "        [-1.74315929e-01, -3.12979985e-03, -6.69521317e-02,\n",
       "         -1.50414407e-02, -6.33843169e-02],\n",
       "        [ 5.79041721e-33,  5.71619209e-33,  5.79256286e-33,\n",
       "          5.62897813e-33, -5.78215826e-33],\n",
       "        [-4.63388599e-02,  2.14849301e-02, -3.91389169e-02,\n",
       "          2.05968004e-02, -3.96325160e-03],\n",
       "        [ 5.69858172e-33,  5.74674099e-33, -5.82949175e-33,\n",
       "          5.87018773e-33,  5.67619920e-33],\n",
       "        [-1.36934802e-01,  4.31159064e-02,  1.96040748e-03,\n",
       "         -9.33680534e-02,  3.17543745e-03],\n",
       "        [-5.85176773e-33, -5.82097639e-33, -5.80634406e-33,\n",
       "         -5.78239410e-33,  5.87010765e-33],\n",
       "        [ 1.01220354e-01, -2.58246716e-02,  9.77663100e-02,\n",
       "          1.41977370e-01,  3.32143791e-02],\n",
       "        [-5.83676034e-33, -3.61644878e-33, -5.83133213e-33,\n",
       "          5.80808416e-33,  2.94707123e-33],\n",
       "        [ 2.23285511e-01, -1.63250878e-01, -1.87076062e-01,\n",
       "          1.09143909e-02,  1.01231858e-01],\n",
       "        [ 5.84440657e-33,  5.79423096e-33,  5.81364131e-33,\n",
       "          5.73503270e-33, -5.44874876e-33],\n",
       "        [-9.09954980e-02,  7.99833909e-02,  9.54646468e-02,\n",
       "         -8.02876055e-02, -1.01281339e-02],\n",
       "        [ 3.39756659e-33,  5.81280303e-33,  5.87157665e-33,\n",
       "          5.83293154e-33,  5.72953983e-33],\n",
       "        [ 5.82793091e-33, -5.87565084e-33,  5.71880830e-33,\n",
       "          5.85565384e-33, -5.81317295e-33],\n",
       "        [ 3.91286805e-33, -4.76489133e-33, -5.44575603e-33,\n",
       "          5.86768503e-33,  5.78196137e-33],\n",
       "        [-5.38703090e-33,  4.33906262e-33,  5.61813125e-33,\n",
       "         -5.80982132e-33,  5.84112584e-33],\n",
       "        [-6.85679540e-03,  6.68153120e-03,  7.32495077e-03,\n",
       "         -5.73772751e-03, -9.71574977e-04],\n",
       "        [ 5.84278034e-33, -5.82538744e-33,  5.82697913e-33,\n",
       "          5.82325685e-33, -5.80976952e-33],\n",
       "        [-4.45607846e-04,  4.37931158e-04,  4.92907071e-04,\n",
       "         -3.60447506e-04, -3.48789908e-05],\n",
       "        [ 5.84748930e-33, -5.80747878e-33,  5.79316310e-33,\n",
       "         -5.80206526e-33, -5.40546449e-33],\n",
       "        [-3.92260775e-02,  1.43837214e-01,  2.80855782e-02,\n",
       "         -1.56239092e-01, -3.37050147e-02],\n",
       "        [-5.26731451e-33,  5.51184599e-33,  5.78187835e-33,\n",
       "         -5.68233786e-33,  5.72621686e-33],\n",
       "        [ 7.72412345e-02,  4.45168167e-02, -1.45913899e-01,\n",
       "         -1.76607911e-02,  1.32913352e-03],\n",
       "        [ 5.86361304e-33, -5.75600094e-33, -5.19056134e-33,\n",
       "         -5.82135365e-33, -5.82426006e-33],\n",
       "        [ 4.38279063e-02, -1.24935405e-02, -2.70069037e-02,\n",
       "         -4.33075093e-02,  1.82143804e-02],\n",
       "        [-5.87315549e-33,  5.58950466e-33,  5.84884810e-33,\n",
       "         -5.75734578e-33, -5.81061661e-33],\n",
       "        [-4.74530309e-02, -7.50960708e-02, -4.52537183e-03,\n",
       "         -6.41312897e-02, -3.10492218e-02],\n",
       "        [ 5.78612703e-33,  5.85233197e-33, -5.87309267e-33,\n",
       "          4.78716842e-33,  8.31324299e-34],\n",
       "        [-1.27427474e-01, -1.46963876e-02,  6.18471280e-02,\n",
       "         -2.55248368e-01, -7.34121278e-02],\n",
       "        [ 3.34423404e-33,  5.82405619e-33, -5.65103554e-33,\n",
       "          5.87740159e-33, -5.77841799e-33]], dtype=float32),\n",
       " array([-8.5120817e-05,  1.8219678e-03,  3.7575117e-04, -2.3721969e-03,\n",
       "         4.2083794e-03], dtype=float32),\n",
       " array([[-0.44466692],\n",
       "        [-0.5082428 ],\n",
       "        [-0.32081687],\n",
       "        [-0.4658817 ],\n",
       "        [-0.44867116],\n",
       "        [ 0.6996508 ],\n",
       "        [-0.2226679 ],\n",
       "        [ 0.5394184 ],\n",
       "        [-0.5980065 ],\n",
       "        [-1.1074255 ],\n",
       "        [-0.6317292 ],\n",
       "        [ 0.394341  ],\n",
       "        [ 0.7582211 ],\n",
       "        [ 1.0044426 ],\n",
       "        [ 0.3790878 ],\n",
       "        [ 0.6034402 ],\n",
       "        [ 0.7226199 ],\n",
       "        [ 0.53041   ],\n",
       "        [ 0.5608776 ],\n",
       "        [ 0.2604718 ],\n",
       "        [ 0.360364  ],\n",
       "        [-0.27282396],\n",
       "        [ 0.658845  ],\n",
       "        [-0.41872442],\n",
       "        [-0.41285437],\n",
       "        [-0.7739511 ],\n",
       "        [-0.6227943 ],\n",
       "        [-1.2156719 ],\n",
       "        [ 0.5153938 ],\n",
       "        [-0.4841358 ],\n",
       "        [-0.3146203 ],\n",
       "        [ 0.1926063 ],\n",
       "        [-0.08888646],\n",
       "        [ 0.351931  ],\n",
       "        [-0.3437682 ],\n",
       "        [-0.38049653],\n",
       "        [ 0.2832922 ],\n",
       "        [ 0.04606414]], dtype=float32),\n",
       " array([-0.00101785], dtype=float32)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('anger', 1247),\n",
       " ('anticipation', 839),\n",
       " ('disgust', 1058),\n",
       " ('fear', 1476),\n",
       " ('joy', 689),\n",
       " ('negative', 3324),\n",
       " ('positive', 2312),\n",
       " ('sadness', 1191),\n",
       " ('surprise', 534),\n",
       " ('trust', 1231)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(e,len(nrc_lexicon[e])) for e in nrc_lexicon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='models/mlp_plus3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies = {\n",
    "    'f1_m': f1_m,\n",
    "    'precision_m': precision_m,\n",
    "    'recall_m': recall_m\n",
    "}\n",
    "# model = load_model('models/lstm_plus1', custom_objects=dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(predictions.flatten()).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(predictions>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(predictions<0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tokens(row):\n",
    "    tokens = []\n",
    "    if row.tokenized_text:\n",
    "        tokens += row.tokenized_text\n",
    "    if row.tokenized_title:\n",
    "        tokens += row.tokenized_title\n",
    "    return tokens\n",
    "writings_df['all_tokens'] = writings_df.apply (lambda row: merge_tokens(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: include the title\n",
    "def extract_emotions(tokens, emotion, relative=True):\n",
    "    if not tokens:\n",
    "        return None\n",
    "    emotion_words = [t for t in tokens \n",
    "                     if t in nrc_lexicon[emotion]]\n",
    "    if relative:\n",
    "        return len(emotion_words) / len(tokens)\n",
    "    else:\n",
    "        return len(emotion_words)\n",
    "    \n",
    "    return encoded_emotions\n",
    "\n",
    "from functools import partial\n",
    "for emotion in emotions:\n",
    "    writings_df[emotion] = writings_df['all_tokens'].apply(partial(extract_emotions, emotion=emotion, \n",
    "                                                                   relative=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['pronouns'] = writings_df['all_tokens'].apply(partial(encode_pronouns, relative=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>text_len</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.104269</td>\n",
       "      <td>0.011986</td>\n",
       "      <td>0.020197</td>\n",
       "      <td>0.031982</td>\n",
       "      <td>0.031271</td>\n",
       "      <td>0.019335</td>\n",
       "      <td>0.040782</td>\n",
       "      <td>0.023853</td>\n",
       "      <td>0.023621</td>\n",
       "      <td>0.032969</td>\n",
       "      <td>0.020421</td>\n",
       "      <td>0.023590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pronouns</th>\n",
       "      <td>0.104269</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.636745</td>\n",
       "      <td>0.449384</td>\n",
       "      <td>0.567496</td>\n",
       "      <td>0.452098</td>\n",
       "      <td>0.464899</td>\n",
       "      <td>0.548570</td>\n",
       "      <td>0.513029</td>\n",
       "      <td>0.571303</td>\n",
       "      <td>0.524614</td>\n",
       "      <td>0.461328</td>\n",
       "      <td>0.538335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_len</th>\n",
       "      <td>0.011986</td>\n",
       "      <td>0.636745</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.708853</td>\n",
       "      <td>0.791715</td>\n",
       "      <td>0.642980</td>\n",
       "      <td>0.738146</td>\n",
       "      <td>0.728836</td>\n",
       "      <td>0.823974</td>\n",
       "      <td>0.867609</td>\n",
       "      <td>0.723653</td>\n",
       "      <td>0.650420</td>\n",
       "      <td>0.834939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.020197</td>\n",
       "      <td>0.449384</td>\n",
       "      <td>0.708853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.643459</td>\n",
       "      <td>0.762591</td>\n",
       "      <td>0.858442</td>\n",
       "      <td>0.564162</td>\n",
       "      <td>0.835345</td>\n",
       "      <td>0.681573</td>\n",
       "      <td>0.774846</td>\n",
       "      <td>0.583704</td>\n",
       "      <td>0.671042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anticipation</th>\n",
       "      <td>0.031982</td>\n",
       "      <td>0.567496</td>\n",
       "      <td>0.791715</td>\n",
       "      <td>0.643459</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.573916</td>\n",
       "      <td>0.668326</td>\n",
       "      <td>0.834784</td>\n",
       "      <td>0.684882</td>\n",
       "      <td>0.849864</td>\n",
       "      <td>0.668269</td>\n",
       "      <td>0.727331</td>\n",
       "      <td>0.818885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.031271</td>\n",
       "      <td>0.452098</td>\n",
       "      <td>0.642980</td>\n",
       "      <td>0.762591</td>\n",
       "      <td>0.573916</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.729799</td>\n",
       "      <td>0.526733</td>\n",
       "      <td>0.765865</td>\n",
       "      <td>0.603013</td>\n",
       "      <td>0.737717</td>\n",
       "      <td>0.540439</td>\n",
       "      <td>0.589641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.019335</td>\n",
       "      <td>0.464899</td>\n",
       "      <td>0.738146</td>\n",
       "      <td>0.858442</td>\n",
       "      <td>0.668326</td>\n",
       "      <td>0.729799</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.570632</td>\n",
       "      <td>0.862778</td>\n",
       "      <td>0.706676</td>\n",
       "      <td>0.824782</td>\n",
       "      <td>0.569688</td>\n",
       "      <td>0.687232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.040782</td>\n",
       "      <td>0.548570</td>\n",
       "      <td>0.728836</td>\n",
       "      <td>0.564162</td>\n",
       "      <td>0.834784</td>\n",
       "      <td>0.526733</td>\n",
       "      <td>0.570632</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.604964</td>\n",
       "      <td>0.850961</td>\n",
       "      <td>0.603296</td>\n",
       "      <td>0.722710</td>\n",
       "      <td>0.811529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0.023853</td>\n",
       "      <td>0.513029</td>\n",
       "      <td>0.823974</td>\n",
       "      <td>0.835345</td>\n",
       "      <td>0.684882</td>\n",
       "      <td>0.765865</td>\n",
       "      <td>0.862778</td>\n",
       "      <td>0.604964</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.735431</td>\n",
       "      <td>0.840379</td>\n",
       "      <td>0.597634</td>\n",
       "      <td>0.706808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>0.023621</td>\n",
       "      <td>0.571303</td>\n",
       "      <td>0.867609</td>\n",
       "      <td>0.681573</td>\n",
       "      <td>0.849864</td>\n",
       "      <td>0.603013</td>\n",
       "      <td>0.706676</td>\n",
       "      <td>0.850961</td>\n",
       "      <td>0.735431</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.702751</td>\n",
       "      <td>0.678778</td>\n",
       "      <td>0.916526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.032969</td>\n",
       "      <td>0.524614</td>\n",
       "      <td>0.723653</td>\n",
       "      <td>0.774846</td>\n",
       "      <td>0.668269</td>\n",
       "      <td>0.737717</td>\n",
       "      <td>0.824782</td>\n",
       "      <td>0.603296</td>\n",
       "      <td>0.840379</td>\n",
       "      <td>0.702751</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.584816</td>\n",
       "      <td>0.665750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.020421</td>\n",
       "      <td>0.461328</td>\n",
       "      <td>0.650420</td>\n",
       "      <td>0.583704</td>\n",
       "      <td>0.727331</td>\n",
       "      <td>0.540439</td>\n",
       "      <td>0.569688</td>\n",
       "      <td>0.722710</td>\n",
       "      <td>0.597634</td>\n",
       "      <td>0.678778</td>\n",
       "      <td>0.584816</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.660681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust</th>\n",
       "      <td>0.023590</td>\n",
       "      <td>0.538335</td>\n",
       "      <td>0.834939</td>\n",
       "      <td>0.671042</td>\n",
       "      <td>0.818885</td>\n",
       "      <td>0.589641</td>\n",
       "      <td>0.687232</td>\n",
       "      <td>0.811529</td>\n",
       "      <td>0.706808</td>\n",
       "      <td>0.916526</td>\n",
       "      <td>0.665750</td>\n",
       "      <td>0.660681</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 label  pronouns  text_len     anger  anticipation   disgust  \\\n",
       "label         1.000000  0.104269  0.011986  0.020197      0.031982  0.031271   \n",
       "pronouns      0.104269  1.000000  0.636745  0.449384      0.567496  0.452098   \n",
       "text_len      0.011986  0.636745  1.000000  0.708853      0.791715  0.642980   \n",
       "anger         0.020197  0.449384  0.708853  1.000000      0.643459  0.762591   \n",
       "anticipation  0.031982  0.567496  0.791715  0.643459      1.000000  0.573916   \n",
       "disgust       0.031271  0.452098  0.642980  0.762591      0.573916  1.000000   \n",
       "fear          0.019335  0.464899  0.738146  0.858442      0.668326  0.729799   \n",
       "joy           0.040782  0.548570  0.728836  0.564162      0.834784  0.526733   \n",
       "negative      0.023853  0.513029  0.823974  0.835345      0.684882  0.765865   \n",
       "positive      0.023621  0.571303  0.867609  0.681573      0.849864  0.603013   \n",
       "sadness       0.032969  0.524614  0.723653  0.774846      0.668269  0.737717   \n",
       "surprise      0.020421  0.461328  0.650420  0.583704      0.727331  0.540439   \n",
       "trust         0.023590  0.538335  0.834939  0.671042      0.818885  0.589641   \n",
       "\n",
       "                  fear       joy  negative  positive   sadness  surprise  \\\n",
       "label         0.019335  0.040782  0.023853  0.023621  0.032969  0.020421   \n",
       "pronouns      0.464899  0.548570  0.513029  0.571303  0.524614  0.461328   \n",
       "text_len      0.738146  0.728836  0.823974  0.867609  0.723653  0.650420   \n",
       "anger         0.858442  0.564162  0.835345  0.681573  0.774846  0.583704   \n",
       "anticipation  0.668326  0.834784  0.684882  0.849864  0.668269  0.727331   \n",
       "disgust       0.729799  0.526733  0.765865  0.603013  0.737717  0.540439   \n",
       "fear          1.000000  0.570632  0.862778  0.706676  0.824782  0.569688   \n",
       "joy           0.570632  1.000000  0.604964  0.850961  0.603296  0.722710   \n",
       "negative      0.862778  0.604964  1.000000  0.735431  0.840379  0.597634   \n",
       "positive      0.706676  0.850961  0.735431  1.000000  0.702751  0.678778   \n",
       "sadness       0.824782  0.603296  0.840379  0.702751  1.000000  0.584816   \n",
       "surprise      0.569688  0.722710  0.597634  0.678778  0.584816  1.000000   \n",
       "trust         0.687232  0.811529  0.706808  0.916526  0.665750  0.660681   \n",
       "\n",
       "                 trust  \n",
       "label         0.023590  \n",
       "pronouns      0.538335  \n",
       "text_len      0.834939  \n",
       "anger         0.671042  \n",
       "anticipation  0.818885  \n",
       "disgust       0.589641  \n",
       "fear          0.687232  \n",
       "joy           0.811529  \n",
       "negative      0.706808  \n",
       "positive      0.916526  \n",
       "sadness       0.665750  \n",
       "surprise      0.660681  \n",
       "trust         1.000000  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len'] + emotions].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pronouns</th>\n",
       "      <th>text_len</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.868213</td>\n",
       "      <td>32.031615</td>\n",
       "      <td>0.386069</td>\n",
       "      <td>0.58984</td>\n",
       "      <td>0.263683</td>\n",
       "      <td>0.478014</td>\n",
       "      <td>0.479908</td>\n",
       "      <td>0.818800</td>\n",
       "      <td>1.280788</td>\n",
       "      <td>0.385315</td>\n",
       "      <td>0.284790</td>\n",
       "      <td>0.830560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.484271</td>\n",
       "      <td>36.398389</td>\n",
       "      <td>0.529232</td>\n",
       "      <td>0.86985</td>\n",
       "      <td>0.416203</td>\n",
       "      <td>0.654371</td>\n",
       "      <td>0.769766</td>\n",
       "      <td>1.152422</td>\n",
       "      <td>1.717428</td>\n",
       "      <td>0.627088</td>\n",
       "      <td>0.375418</td>\n",
       "      <td>1.128341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pronouns   text_len     anger  anticipation   disgust      fear  \\\n",
       "label                                                                    \n",
       "0      0.868213  32.031615  0.386069       0.58984  0.263683  0.478014   \n",
       "1      2.484271  36.398389  0.529232       0.86985  0.416203  0.654371   \n",
       "\n",
       "            joy  negative  positive   sadness  surprise     trust  \n",
       "label                                                              \n",
       "0      0.479908  0.818800  1.280788  0.385315  0.284790  0.830560  \n",
       "1      0.769766  1.152422  1.717428  0.627088  0.375418  1.128341  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len'] + emotions].groupby('label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentAnalyzer, SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.66, 'pos': 0.34, 'compound': 0.5574}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid.polarity_scores(\"We are here today happiness is all around\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['neg_vader'] = writings_df.text.apply(lambda t: sid.polarity_scores(t)['neg']\n",
    "                                                 if type(t)==str else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>title_len</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>text_len</th>\n",
       "      <th>emotions</th>\n",
       "      <th>...</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>all_tokens</th>\n",
       "      <th>neg_vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>If anyone could help with which sub to put thi...</td>\n",
       "      <td>2016-08-02 09:22:12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[if, anyone, could, help, with, which, sub, to...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[if, anyone, could, help, with, which, sub, to...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>I'm literally never gonna stop waiting...</td>\n",
       "      <td>2016-08-05 09:35:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, m, literally, never, gonna, stop, waiting]</td>\n",
       "      <td>7.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[i, m, literally, never, gonna, stop, waiting]</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>This is a really interesting study! Makes sens...</td>\n",
       "      <td>2016-08-05 21:36:24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[this, is, a, really, interesting, study, make...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[this, is, a, really, interesting, study, make...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>The only thing Frank is building ...</td>\n",
       "      <td>2016-08-07 23:35:23</td>\n",
       "      <td>... Is hype. Think about it, every time he wor...</td>\n",
       "      <td>0</td>\n",
       "      <td>[the, only, thing, frank, is, building]</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[is, hype, think, about, it, every, time, he, ...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[is, hype, think, about, it, every, time, he, ...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject8292</td>\n",
       "      <td>Mostly always me during this whole charade</td>\n",
       "      <td>2016-08-09 08:39:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[mostly, always, me, during, this, whole, char...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[mostly, always, me, during, this, whole, char...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170693</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 11:29:21</td>\n",
       "      <td>this is my personal experience ,it may not ref...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[this, is, my, personal, experience, it, may, ...</td>\n",
       "      <td>153.0</td>\n",
       "      <td>0.026144</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[this, is, my, personal, experience, it, may, ...</td>\n",
       "      <td>0.089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170694</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 16:17:34</td>\n",
       "      <td>stop looking at 20 million saudis as one entit...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[stop, looking, at, 20, million, saudis, as, o...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[stop, looking, at, 20, million, saudis, as, o...</td>\n",
       "      <td>0.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170695</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-19 20:00:31</td>\n",
       "      <td>i am aware of stats now and then. i was just s...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[i, am, aware, of, stats, now, and, then, i, w...</td>\n",
       "      <td>198.0</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>[i, am, aware, of, stats, now, and, then, i, w...</td>\n",
       "      <td>0.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170696</th>\n",
       "      <td>subject217</td>\n",
       "      <td>WHAT DID YOU SAY TO ME?</td>\n",
       "      <td>2018-08-20 10:54:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[what, did, you, say, to, me]</td>\n",
       "      <td>6.0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[what, did, you, say, to, me]</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170697</th>\n",
       "      <td>subject217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-08-20 12:07:44</td>\n",
       "      <td>me smellz fish,me find no fish!...what the fuc...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[me, smellz, fish, me, find, no, fish, what, t...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[me, smellz, fish, me, find, no, fish, what, t...</td>\n",
       "      <td>0.484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170698 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            subject                                              title  \\\n",
       "0       subject8292  If anyone could help with which sub to put thi...   \n",
       "1       subject8292          I'm literally never gonna stop waiting...   \n",
       "2       subject8292  This is a really interesting study! Makes sens...   \n",
       "3       subject8292               The only thing Frank is building ...   \n",
       "4       subject8292         Mostly always me during this whole charade   \n",
       "...             ...                                                ...   \n",
       "170693   subject217                                                NaN   \n",
       "170694   subject217                                                NaN   \n",
       "170695   subject217                                                NaN   \n",
       "170696   subject217                            WHAT DID YOU SAY TO ME?   \n",
       "170697   subject217                                                NaN   \n",
       "\n",
       "                       date  \\\n",
       "0       2016-08-02 09:22:12   \n",
       "1       2016-08-05 09:35:55   \n",
       "2       2016-08-05 21:36:24   \n",
       "3       2016-08-07 23:35:23   \n",
       "4       2016-08-09 08:39:41   \n",
       "...                     ...   \n",
       "170693  2018-08-19 11:29:21   \n",
       "170694  2018-08-19 16:17:34   \n",
       "170695  2018-08-19 20:00:31   \n",
       "170696  2018-08-20 10:54:11   \n",
       "170697  2018-08-20 12:07:44   \n",
       "\n",
       "                                                     text  label  \\\n",
       "0                                                     NaN      0   \n",
       "1                                                     NaN      0   \n",
       "2                                                     NaN      0   \n",
       "3       ... Is hype. Think about it, every time he wor...      0   \n",
       "4                                                     NaN      0   \n",
       "...                                                   ...    ...   \n",
       "170693  this is my personal experience ,it may not ref...      0   \n",
       "170694  stop looking at 20 million saudis as one entit...      0   \n",
       "170695  i am aware of stats now and then. i was just s...      0   \n",
       "170696                                                NaN      0   \n",
       "170697  me smellz fish,me find no fish!...what the fuc...      0   \n",
       "\n",
       "                                          tokenized_title  title_len  \\\n",
       "0       [if, anyone, could, help, with, which, sub, to...       11.0   \n",
       "1          [i, m, literally, never, gonna, stop, waiting]        7.0   \n",
       "2       [this, is, a, really, interesting, study, make...        9.0   \n",
       "3                 [the, only, thing, frank, is, building]        6.0   \n",
       "4       [mostly, always, me, during, this, whole, char...        7.0   \n",
       "...                                                   ...        ...   \n",
       "170693                                               None        NaN   \n",
       "170694                                               None        NaN   \n",
       "170695                                               None        NaN   \n",
       "170696                      [what, did, you, say, to, me]        6.0   \n",
       "170697                                               None        NaN   \n",
       "\n",
       "                                           tokenized_text  text_len  emotions  \\\n",
       "0                                                    None       NaN       NaN   \n",
       "1                                                    None       NaN       NaN   \n",
       "2                                                    None       NaN       NaN   \n",
       "3       [is, hype, think, about, it, every, time, he, ...      26.0  0.000000   \n",
       "4                                                    None       NaN       NaN   \n",
       "...                                                   ...       ...       ...   \n",
       "170693  [this, is, my, personal, experience, it, may, ...     153.0  0.026144   \n",
       "170694  [stop, looking, at, 20, million, saudis, as, o...      15.0  0.000000   \n",
       "170695  [i, am, aware, of, stats, now, and, then, i, w...     198.0  0.030303   \n",
       "170696                                               None       NaN       NaN   \n",
       "170697  [me, smellz, fish, me, find, no, fish, what, t...      11.0  0.000000   \n",
       "\n",
       "        ...  fear  joy  negative  positive  sadness  surprise  trust  \\\n",
       "0       ...   0.0  0.0       0.0       0.0      0.0       0.0    0.0   \n",
       "1       ...   0.0  0.0       0.0       0.0      0.0       0.0    0.0   \n",
       "2       ...   0.0  0.0       0.0       3.0      0.0       0.0    0.0   \n",
       "3       ...   0.0  0.0       3.0       3.0      0.0       0.0    1.0   \n",
       "4       ...   0.0  0.0       1.0       0.0      0.0       0.0    0.0   \n",
       "...     ...   ...  ...       ...       ...      ...       ...    ...   \n",
       "170693  ...   1.0  1.0       1.0       7.0      0.0       1.0    4.0   \n",
       "170694  ...   1.0  0.0       1.0       0.0      0.0       0.0    0.0   \n",
       "170695  ...   2.0  3.0       4.0      11.0      3.0       0.0    6.0   \n",
       "170696  ...   0.0  0.0       0.0       0.0      0.0       0.0    0.0   \n",
       "170697  ...   0.0  0.0       0.0       0.0      0.0       0.0    0.0   \n",
       "\n",
       "        pronouns                                         all_tokens  neg_vader  \n",
       "0            0.0  [if, anyone, could, help, with, which, sub, to...      0.000  \n",
       "1            1.0     [i, m, literally, never, gonna, stop, waiting]      0.000  \n",
       "2            0.0  [this, is, a, really, interesting, study, make...      0.000  \n",
       "3            0.0  [is, hype, think, about, it, every, time, he, ...      0.000  \n",
       "4            1.0  [mostly, always, me, during, this, whole, char...      0.000  \n",
       "...          ...                                                ...        ...  \n",
       "170693       4.0  [this, is, my, personal, experience, it, may, ...      0.089  \n",
       "170694       0.0  [stop, looking, at, 20, million, saudis, as, o...      0.145  \n",
       "170695      16.0  [i, am, aware, of, stats, now, and, then, i, w...      0.070  \n",
       "170696       1.0                      [what, did, you, say, to, me]      0.000  \n",
       "170697       2.0  [me, smellz, fish, me, find, no, fish, what, t...      0.484  \n",
       "\n",
       "[170698 rows x 23 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['pos_vader'] = writings_df.text.apply(lambda t: sid.polarity_scores(t)['pos']\n",
    "                                                 if type(t)==str else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pronouns</th>\n",
       "      <th>text_len</th>\n",
       "      <th>neg_vader</th>\n",
       "      <th>pos_vader</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.868213</td>\n",
       "      <td>32.031615</td>\n",
       "      <td>0.054259</td>\n",
       "      <td>0.109981</td>\n",
       "      <td>0.386069</td>\n",
       "      <td>0.58984</td>\n",
       "      <td>0.263683</td>\n",
       "      <td>0.478014</td>\n",
       "      <td>0.479908</td>\n",
       "      <td>0.818800</td>\n",
       "      <td>1.280788</td>\n",
       "      <td>0.385315</td>\n",
       "      <td>0.284790</td>\n",
       "      <td>0.830560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.484271</td>\n",
       "      <td>36.398389</td>\n",
       "      <td>0.079191</td>\n",
       "      <td>0.148154</td>\n",
       "      <td>0.529232</td>\n",
       "      <td>0.86985</td>\n",
       "      <td>0.416203</td>\n",
       "      <td>0.654371</td>\n",
       "      <td>0.769766</td>\n",
       "      <td>1.152422</td>\n",
       "      <td>1.717428</td>\n",
       "      <td>0.627088</td>\n",
       "      <td>0.375418</td>\n",
       "      <td>1.128341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pronouns   text_len  neg_vader  pos_vader     anger  anticipation  \\\n",
       "label                                                                      \n",
       "0      0.868213  32.031615   0.054259   0.109981  0.386069       0.58984   \n",
       "1      2.484271  36.398389   0.079191   0.148154  0.529232       0.86985   \n",
       "\n",
       "        disgust      fear       joy  negative  positive   sadness  surprise  \\\n",
       "label                                                                         \n",
       "0      0.263683  0.478014  0.479908  0.818800  1.280788  0.385315  0.284790   \n",
       "1      0.416203  0.654371  0.769766  1.152422  1.717428  0.627088  0.375418   \n",
       "\n",
       "          trust  \n",
       "label            \n",
       "0      0.830560  \n",
       "1      1.128341  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len', 'neg_vader', 'pos_vader'] + emotions].groupby('label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>text_len</th>\n",
       "      <th>neg_vader</th>\n",
       "      <th>pos_vader</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.097800</td>\n",
       "      <td>0.033477</td>\n",
       "      <td>0.067170</td>\n",
       "      <td>0.065211</td>\n",
       "      <td>0.022057</td>\n",
       "      <td>0.025666</td>\n",
       "      <td>0.030664</td>\n",
       "      <td>0.019114</td>\n",
       "      <td>0.033977</td>\n",
       "      <td>0.022934</td>\n",
       "      <td>0.019590</td>\n",
       "      <td>0.032641</td>\n",
       "      <td>0.018109</td>\n",
       "      <td>0.024014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pronouns</th>\n",
       "      <td>0.097800</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.332071</td>\n",
       "      <td>0.193938</td>\n",
       "      <td>0.221419</td>\n",
       "      <td>0.076345</td>\n",
       "      <td>0.128030</td>\n",
       "      <td>0.094069</td>\n",
       "      <td>0.063176</td>\n",
       "      <td>0.144011</td>\n",
       "      <td>0.076670</td>\n",
       "      <td>0.106055</td>\n",
       "      <td>0.100827</td>\n",
       "      <td>0.106790</td>\n",
       "      <td>0.122914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_len</th>\n",
       "      <td>0.033477</td>\n",
       "      <td>0.332071</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.343154</td>\n",
       "      <td>0.159673</td>\n",
       "      <td>0.360460</td>\n",
       "      <td>0.386351</td>\n",
       "      <td>0.312393</td>\n",
       "      <td>0.381410</td>\n",
       "      <td>0.339398</td>\n",
       "      <td>0.370250</td>\n",
       "      <td>0.330075</td>\n",
       "      <td>0.384031</td>\n",
       "      <td>0.349498</td>\n",
       "      <td>0.389620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_vader</th>\n",
       "      <td>0.067170</td>\n",
       "      <td>0.193938</td>\n",
       "      <td>0.343154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.169624</td>\n",
       "      <td>0.384510</td>\n",
       "      <td>0.141868</td>\n",
       "      <td>0.362582</td>\n",
       "      <td>0.339245</td>\n",
       "      <td>0.126042</td>\n",
       "      <td>0.431111</td>\n",
       "      <td>0.099767</td>\n",
       "      <td>0.374256</td>\n",
       "      <td>0.159302</td>\n",
       "      <td>0.143060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_vader</th>\n",
       "      <td>0.065211</td>\n",
       "      <td>0.221419</td>\n",
       "      <td>0.159673</td>\n",
       "      <td>0.169624</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.079693</td>\n",
       "      <td>0.225925</td>\n",
       "      <td>0.087309</td>\n",
       "      <td>0.071450</td>\n",
       "      <td>0.323148</td>\n",
       "      <td>0.058266</td>\n",
       "      <td>0.270687</td>\n",
       "      <td>0.095040</td>\n",
       "      <td>0.186243</td>\n",
       "      <td>0.231954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.022057</td>\n",
       "      <td>0.076345</td>\n",
       "      <td>0.360460</td>\n",
       "      <td>0.384510</td>\n",
       "      <td>0.079693</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.196795</td>\n",
       "      <td>0.583864</td>\n",
       "      <td>0.587460</td>\n",
       "      <td>0.157202</td>\n",
       "      <td>0.631708</td>\n",
       "      <td>0.128169</td>\n",
       "      <td>0.528980</td>\n",
       "      <td>0.273195</td>\n",
       "      <td>0.169261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anticipation</th>\n",
       "      <td>0.025666</td>\n",
       "      <td>0.128030</td>\n",
       "      <td>0.386351</td>\n",
       "      <td>0.141868</td>\n",
       "      <td>0.225925</td>\n",
       "      <td>0.196795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.164649</td>\n",
       "      <td>0.241958</td>\n",
       "      <td>0.583107</td>\n",
       "      <td>0.178827</td>\n",
       "      <td>0.452457</td>\n",
       "      <td>0.198972</td>\n",
       "      <td>0.460851</td>\n",
       "      <td>0.469028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.030664</td>\n",
       "      <td>0.094069</td>\n",
       "      <td>0.312393</td>\n",
       "      <td>0.362582</td>\n",
       "      <td>0.087309</td>\n",
       "      <td>0.583864</td>\n",
       "      <td>0.164649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.440376</td>\n",
       "      <td>0.152731</td>\n",
       "      <td>0.552021</td>\n",
       "      <td>0.116588</td>\n",
       "      <td>0.490181</td>\n",
       "      <td>0.232166</td>\n",
       "      <td>0.153723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.019114</td>\n",
       "      <td>0.063176</td>\n",
       "      <td>0.381410</td>\n",
       "      <td>0.339245</td>\n",
       "      <td>0.071450</td>\n",
       "      <td>0.587460</td>\n",
       "      <td>0.241958</td>\n",
       "      <td>0.440376</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.159907</td>\n",
       "      <td>0.576962</td>\n",
       "      <td>0.141985</td>\n",
       "      <td>0.583703</td>\n",
       "      <td>0.248160</td>\n",
       "      <td>0.184240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>0.033977</td>\n",
       "      <td>0.144011</td>\n",
       "      <td>0.339398</td>\n",
       "      <td>0.126042</td>\n",
       "      <td>0.323148</td>\n",
       "      <td>0.157202</td>\n",
       "      <td>0.583107</td>\n",
       "      <td>0.152731</td>\n",
       "      <td>0.159907</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.113400</td>\n",
       "      <td>0.645827</td>\n",
       "      <td>0.176440</td>\n",
       "      <td>0.477317</td>\n",
       "      <td>0.582920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0.022934</td>\n",
       "      <td>0.076670</td>\n",
       "      <td>0.370250</td>\n",
       "      <td>0.431111</td>\n",
       "      <td>0.058266</td>\n",
       "      <td>0.631708</td>\n",
       "      <td>0.178827</td>\n",
       "      <td>0.552021</td>\n",
       "      <td>0.576962</td>\n",
       "      <td>0.113400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.105821</td>\n",
       "      <td>0.612781</td>\n",
       "      <td>0.226230</td>\n",
       "      <td>0.145220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>0.019590</td>\n",
       "      <td>0.106055</td>\n",
       "      <td>0.330075</td>\n",
       "      <td>0.099767</td>\n",
       "      <td>0.270687</td>\n",
       "      <td>0.128169</td>\n",
       "      <td>0.452457</td>\n",
       "      <td>0.116588</td>\n",
       "      <td>0.141985</td>\n",
       "      <td>0.645827</td>\n",
       "      <td>0.105821</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.139827</td>\n",
       "      <td>0.333998</td>\n",
       "      <td>0.648163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.032641</td>\n",
       "      <td>0.100827</td>\n",
       "      <td>0.384031</td>\n",
       "      <td>0.374256</td>\n",
       "      <td>0.095040</td>\n",
       "      <td>0.528980</td>\n",
       "      <td>0.198972</td>\n",
       "      <td>0.490181</td>\n",
       "      <td>0.583703</td>\n",
       "      <td>0.176440</td>\n",
       "      <td>0.612781</td>\n",
       "      <td>0.139827</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.265026</td>\n",
       "      <td>0.171245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>0.018109</td>\n",
       "      <td>0.106790</td>\n",
       "      <td>0.349498</td>\n",
       "      <td>0.159302</td>\n",
       "      <td>0.186243</td>\n",
       "      <td>0.273195</td>\n",
       "      <td>0.460851</td>\n",
       "      <td>0.232166</td>\n",
       "      <td>0.248160</td>\n",
       "      <td>0.477317</td>\n",
       "      <td>0.226230</td>\n",
       "      <td>0.333998</td>\n",
       "      <td>0.265026</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.354746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trust</th>\n",
       "      <td>0.024014</td>\n",
       "      <td>0.122914</td>\n",
       "      <td>0.389620</td>\n",
       "      <td>0.143060</td>\n",
       "      <td>0.231954</td>\n",
       "      <td>0.169261</td>\n",
       "      <td>0.469028</td>\n",
       "      <td>0.153723</td>\n",
       "      <td>0.184240</td>\n",
       "      <td>0.582920</td>\n",
       "      <td>0.145220</td>\n",
       "      <td>0.648163</td>\n",
       "      <td>0.171245</td>\n",
       "      <td>0.354746</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 label  pronouns  text_len  neg_vader  pos_vader     anger  \\\n",
       "label         1.000000  0.097800  0.033477   0.067170   0.065211  0.022057   \n",
       "pronouns      0.097800  1.000000  0.332071   0.193938   0.221419  0.076345   \n",
       "text_len      0.033477  0.332071  1.000000   0.343154   0.159673  0.360460   \n",
       "neg_vader     0.067170  0.193938  0.343154   1.000000   0.169624  0.384510   \n",
       "pos_vader     0.065211  0.221419  0.159673   0.169624   1.000000  0.079693   \n",
       "anger         0.022057  0.076345  0.360460   0.384510   0.079693  1.000000   \n",
       "anticipation  0.025666  0.128030  0.386351   0.141868   0.225925  0.196795   \n",
       "disgust       0.030664  0.094069  0.312393   0.362582   0.087309  0.583864   \n",
       "fear          0.019114  0.063176  0.381410   0.339245   0.071450  0.587460   \n",
       "joy           0.033977  0.144011  0.339398   0.126042   0.323148  0.157202   \n",
       "negative      0.022934  0.076670  0.370250   0.431111   0.058266  0.631708   \n",
       "positive      0.019590  0.106055  0.330075   0.099767   0.270687  0.128169   \n",
       "sadness       0.032641  0.100827  0.384031   0.374256   0.095040  0.528980   \n",
       "surprise      0.018109  0.106790  0.349498   0.159302   0.186243  0.273195   \n",
       "trust         0.024014  0.122914  0.389620   0.143060   0.231954  0.169261   \n",
       "\n",
       "              anticipation   disgust      fear       joy  negative  positive  \\\n",
       "label             0.025666  0.030664  0.019114  0.033977  0.022934  0.019590   \n",
       "pronouns          0.128030  0.094069  0.063176  0.144011  0.076670  0.106055   \n",
       "text_len          0.386351  0.312393  0.381410  0.339398  0.370250  0.330075   \n",
       "neg_vader         0.141868  0.362582  0.339245  0.126042  0.431111  0.099767   \n",
       "pos_vader         0.225925  0.087309  0.071450  0.323148  0.058266  0.270687   \n",
       "anger             0.196795  0.583864  0.587460  0.157202  0.631708  0.128169   \n",
       "anticipation      1.000000  0.164649  0.241958  0.583107  0.178827  0.452457   \n",
       "disgust           0.164649  1.000000  0.440376  0.152731  0.552021  0.116588   \n",
       "fear              0.241958  0.440376  1.000000  0.159907  0.576962  0.141985   \n",
       "joy               0.583107  0.152731  0.159907  1.000000  0.113400  0.645827   \n",
       "negative          0.178827  0.552021  0.576962  0.113400  1.000000  0.105821   \n",
       "positive          0.452457  0.116588  0.141985  0.645827  0.105821  1.000000   \n",
       "sadness           0.198972  0.490181  0.583703  0.176440  0.612781  0.139827   \n",
       "surprise          0.460851  0.232166  0.248160  0.477317  0.226230  0.333998   \n",
       "trust             0.469028  0.153723  0.184240  0.582920  0.145220  0.648163   \n",
       "\n",
       "               sadness  surprise     trust  \n",
       "label         0.032641  0.018109  0.024014  \n",
       "pronouns      0.100827  0.106790  0.122914  \n",
       "text_len      0.384031  0.349498  0.389620  \n",
       "neg_vader     0.374256  0.159302  0.143060  \n",
       "pos_vader     0.095040  0.186243  0.231954  \n",
       "anger         0.528980  0.273195  0.169261  \n",
       "anticipation  0.198972  0.460851  0.469028  \n",
       "disgust       0.490181  0.232166  0.153723  \n",
       "fear          0.583703  0.248160  0.184240  \n",
       "joy           0.176440  0.477317  0.582920  \n",
       "negative      0.612781  0.226230  0.145220  \n",
       "positive      0.139827  0.333998  0.648163  \n",
       "sadness       1.000000  0.265026  0.171245  \n",
       "surprise      0.265026  1.000000  0.354746  \n",
       "trust         0.171245  0.354746  1.000000  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len', 'neg_vader', 'pos_vader'] + emotions].corr('spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare your hyperparameters search:\n",
    "config = {\n",
    "      \"algorithm\": \"random\",\n",
    "      \"parameters\": {\n",
    "          \"lstm_units\": {\"type\": \"integer\", \"min\": 10, \"max\": 100},\n",
    "          \"lr\": {\"type\": \"float\", \"min\": 0.00001, \"max\": 0.05, \"scalingType\": \"loguniform\"},\n",
    "          \"l2_dense\": {\"type\": \"float\", \"min\": 0.00001, \"max\": 0.5, \"scalingType\": \"loguniform\"},\n",
    "          \"dropout\": {\"type\": \"float\", \"min\": 0, \"max\": 0.7, \"scalingType\": \"uniform\"},\n",
    "          \"optimizer\": {\"type\": \"categorical\", \"values\": [\"adam\", \"adagrad\", \"\"]},\n",
    "          \"batch_size\": {\"type\": \"integer\", \"min\": 10, \"max\": 512, \"scalingType\": \"loguniform\"},\n",
    "          \"positive_class_weight\": {\"type\": \"integer\", \"min\": 1, \"max\": 25},\n",
    "          \"trainable_embeddings\": {\"type\": \"discrete\", \"values\": [True, False]},\n",
    "          \"decay\": {\"type\": \"float\", \"min\": 0.00000001, \"max\": 0.5, \"scalingType\": \"loguniform\"},          \n",
    "      },\n",
    "      \"spec\": {\n",
    "          \"metric\": \"loss\",\n",
    "          \"objective\": \"minimize\",\n",
    "      },\n",
    "  }\n",
    "optimizer = Optimizer(config, api_key=\"eoBdVyznAhfg3bK9pZ58ZSXfv\")\n",
    "\n",
    "for experiment in optimizer.get_experiments(project_name=\"mental\"):\n",
    "    experiment.add_tag(\"tune\")\n",
    "    \n",
    "    # Test the model\n",
    "    hyperparams_config = {\n",
    "        \"lstm_units\": experiment.get_parameter('lstm_units'),\n",
    "        \"l2_dense\": experiment.get_parameter('l2_dense'),\n",
    "        \"dropout\": experiment.get_parameter('dropout'),\n",
    "        \"optimizer\": experiment.get_parameter('optimizer'),\n",
    "        \"trainable_embeddings\": experiment.get_parameter('trainable_embeddings'),\n",
    "        \"decay\": experiment.get_parameter('decay'),\n",
    "        \"lr\": experiment.get_parameter('lr'),\n",
    "        }\n",
    "    if not hyperparams_config['optimizer']:\n",
    "        hyperparams_config['optimizer'] = optimizers.Adam(lr=hyperparams_config['lr'], \n",
    "                                   decay=hyperparams_config['decay'])\n",
    "    model = build_model(hyperparams=hyperparams_config,\n",
    "                        hyperparams_features=hyperparams_features, \n",
    "                        embedding_matrix=embedding_matrix, emotions=emotions)\n",
    "    history = train_model(model, \n",
    "            x_train, y_train, x_test, y_test,\n",
    "            epochs=5, batch_size=experiment.get_parameter('batch_size'),\n",
    "                      class_weight={0:1, 1:experiment.get_parameter('positive_class_weight')}, \n",
    "                          workers=4,\n",
    "                      model_path='models/experiment')\n",
    "    loss = history.history['loss'][-1]\n",
    "    \n",
    "    # Report the loss, if not auto-logged:\n",
    "    experiment.log_metric(\"loss\", loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
