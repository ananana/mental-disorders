{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob, os\n",
    "import numpy as np\n",
    "from comet_ml import Experiment, Optimizer\n",
    "import pickle\n",
    "import logging\n",
    "import sys\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "os.environ['TF_KERAS'] = '1'\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Lambda, BatchNormalization, TimeDistributed, \\\n",
    "     Bidirectional, Input, concatenate, Flatten, RepeatVector, Activation, Multiply, Permute\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import callbacks, optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model, Sequence\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# only reserve 1 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.enable_eager_execution()\n",
    "# my_seed = 1234\n",
    "# tf.set_random_seed(my_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('training')\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_subject_writings(subject_file):\n",
    "    writings = []\n",
    "    with open(subject_file) as sf:\n",
    "        contents = sf.read()\n",
    "        root = ET.fromstring(contents)\n",
    "        try:\n",
    "            subject = root.findall('ID')[0].text.strip()\n",
    "        except Exception:\n",
    "            print('Cannot extract ID', contents[:500], '\\n-------\\n')        \n",
    "        for w in root.iter('WRITING'):\n",
    "            subject_writings = {'subject': subject}\n",
    "            for title in w.findall('TITLE'):\n",
    "                subject_writings['title'] = title.text\n",
    "            for text in w.findall('TEXT'):\n",
    "                subject_writings['text'] = text.text\n",
    "            for date in w.findall('DATE'):\n",
    "                subject_writings['date'] = date.text\n",
    "            writings.append(subject_writings)\n",
    "    return writings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/anasab/' \n",
    "# root_dir = '/home/ana/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eRisk 2020 T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir_T1 = root_dir + '/eRisk/data/eRisk2020_T1_train/eRISK2020_T1_training_data/eRISK2020_training_data/data/'\n",
    "labels_file_T1 = root_dir + '/eRisk/data//eRisk2020_T1_train/eRISK2020_T1_training_data/eRISK2020_training_data/golden_truth.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_2020(datadir_T1, labels_file_T1):\n",
    "    writings = []\n",
    "    for subject_file in os.listdir(datadir_T1):\n",
    "        print(subject_file)\n",
    "        writings.extend(read_subject_writings(os.path.join(datadir_T1, subject_file)))\n",
    "    writings_df = pd.DataFrame(writings)\n",
    "\n",
    "    labels_T1 = pd.read_csv(labels_file_T1, delimiter=' ', names=['subject', 'label'])\n",
    "    labels_T1 = labels_T1.set_index('subject')\n",
    "\n",
    "    writings_df['label'] = writings_df['subject'].apply(\n",
    "    lambda s: labels_T1.loc[s, 'label'])\n",
    "    \n",
    "    return writings_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eRisk 2019 T1 (Anorexia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadirs_T1_2019 = {\n",
    "    'train': ['2018 test/', '2018 train/positive_examples/', '2018 train/negative_examples/'],\n",
    "    'test': ['data/']\n",
    "}\n",
    "datadir_root_T1_2019 = {\n",
    "    'train': root_dir + '/eRisk/data/past/eRisk2019_T1/training data - t1/',\n",
    "    'test': root_dir + '/eRisk/data/past/eRisk2019_T1/test data - T1/'\n",
    "}\n",
    "    \n",
    "labels_files_T1_2019 = {\n",
    "    'train': ['2018 train/risk_golden_truth.txt', '2018 test/risk-golden-truth-test.txt'],\n",
    "    'test': ['T1_erisk_golden_truth.txt']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_2019(datadir_root_T1_2019,\n",
    "                   datadirs_T1_2019,\n",
    "                   labels_files_T1_2019,\n",
    "                   test_suffix='0000'):\n",
    "    writings = {'train': [], 'test': []}\n",
    "    writings_df = pd.DataFrame()\n",
    "    labels_df = pd.DataFrame()\n",
    "\n",
    "    for subset in ('train', 'test'):\n",
    "        for subdir in [os.path.join(datadir_root_T1_2019[subset], subp) for subp in datadirs_T1_2019[subset]]:\n",
    "            if subset=='train':\n",
    "                chunkdirs = [os.path.join(datadir_root_T1_2019[subset], subdir, chunkdir) \n",
    "                             for chunkdir in os.listdir(subdir)]\n",
    "            else:\n",
    "                chunkdirs = [os.path.join(datadir_root_T1_2019[subset], subdir)]\n",
    "                \n",
    "            for chunkdir in chunkdirs:\n",
    "                if not os.path.isdir(chunkdir):\n",
    "                    continue\n",
    "                for subject_file in os.listdir(chunkdir):\n",
    "                    writings[subset].extend(read_subject_writings(os.path.join(chunkdir, subject_file)))\n",
    "        writings_df_part = pd.DataFrame(writings[subset])\n",
    "        # add a suffix for users in the test -- the numbers are duplicated with the ones in train\n",
    "        if subset=='test':\n",
    "            writings_df_part['subject'] = writings_df_part['subject'].apply(lambda s: s+test_suffix)\n",
    "            print(subset, writings_df_part.subject)\n",
    "        writings_df_part['subset'] = subset\n",
    "        writings_df = pd.concat([writings_df, writings_df_part])\n",
    "        writings_df.reindex()\n",
    "\n",
    "        for label_file in labels_files_T1_2019[subset]:\n",
    "            labels = pd.read_csv(os.path.join(datadir_root_T1_2019[subset], label_file), \n",
    "                                 delimiter='\\s+', names=['subject', 'label'])\n",
    "            # add a suffix for users in the test -- the numbers are duplicated with the ones in train\n",
    "            if subset=='test':\n",
    "                labels['subject'] = labels['subject'].apply(lambda s: s+test_suffix)\n",
    "            labels_df = pd.concat([labels_df, labels])\n",
    "    labels_df = labels_df.drop_duplicates()\n",
    "    labels_df = labels_df.set_index('subject')\n",
    "\n",
    "    writings_df = writings_df.drop_duplicates()\n",
    "    \n",
    "    writings_df = writings_df.join(labels_df, on='subject')\n",
    "    \n",
    "    return writings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df = read_texts_2020(datadir_T1, labels_file_T1)\n",
    "# writings_df = read_texts_2019(datadir_root_T1_2019,\n",
    "#                    datadirs_T1_2019,\n",
    "#                    labels_files_T1_2019)\n",
    "writings_df = pickle.load(open('data/writings_df_anorexia_liwc', 'rb'))\n",
    "\n",
    "# CLPsych\n",
    "import json\n",
    "# writings_df = pd.DataFrame.from_dict(json.load(open('data/writings_df_clpsych_liwc_affect.json')))#read_texts_clpsych(datadir_root_clpsych, datadirs_clpsych, labels_files_clpsych)\n",
    "#     writings_df_test = pd.DataFrame.from_dict(json.load(open('writings_df_%s_test.json' % dataset_type)))#read_texts_clpsych(datadir_root_clpsych, datadirs_clpsych, labels_files_clpsych)\n",
    "#     writings_df_test = read_texts_clpsych(datadir_root_clpsych, datadirs_clpsych, labels_files_clpsych)\n",
    "# label_by = ['depression', 'ptsd']\n",
    "# writings_df = writings_df.drop(writings_df[writings_df['condition']=='depression'].index)\n",
    "# writings_df['label'] = writings_df['condition'].apply(lambda c: 1 if c in label_by else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2efa5b6710>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "writings_df.label.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>subject</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>subset</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>title_len</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>text_len</th>\n",
       "      <th>...</th>\n",
       "      <th>quant</th>\n",
       "      <th>sad</th>\n",
       "      <th>motion</th>\n",
       "      <th>verb</th>\n",
       "      <th>ingest</th>\n",
       "      <th>adverb</th>\n",
       "      <th>negemo</th>\n",
       "      <th>achieve</th>\n",
       "      <th>death</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-12-01 07:47:48</td>\n",
       "      <td>subject5452</td>\n",
       "      <td></td>\n",
       "      <td>mew_irl</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>[mew_irl]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-11-23 03:42:13</td>\n",
       "      <td>subject5452</td>\n",
       "      <td></td>\n",
       "      <td>Trippy Kong.</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>[trippy, kong]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-11-22 22:12:02</td>\n",
       "      <td>subject5452</td>\n",
       "      <td></td>\n",
       "      <td>Dk rap but the acid is slowly kicking in.</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>[dk, rap, but, the, acid, is, slowly, kicking,...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-11-09 21:35:42</td>\n",
       "      <td>subject5452</td>\n",
       "      <td>America just did a heel turn.</td>\n",
       "      <td></td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[america, just, did, a, heel, turn]</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-11-05 08:05:15</td>\n",
       "      <td>subject5452</td>\n",
       "      <td></td>\n",
       "      <td>Pokemon theme but everytime they say Pokemon ...</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>[pokemon, theme, but, everytime, they, say, po...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    date      subject                             text  \\\n",
       "0   2016-12-01 07:47:48   subject5452                                    \n",
       "1   2016-11-23 03:42:13   subject5452                                    \n",
       "2   2016-11-22 22:12:02   subject5452                                    \n",
       "3   2016-11-09 21:35:42   subject5452   America just did a heel turn.    \n",
       "4   2016-11-05 08:05:15   subject5452                                    \n",
       "\n",
       "                                               title subset  label  \\\n",
       "0                                           mew_irl   train      0   \n",
       "1                                      Trippy Kong.   train      0   \n",
       "2         Dk rap but the acid is slowly kicking in.   train      0   \n",
       "3                                                     train      0   \n",
       "4   Pokemon theme but everytime they say Pokemon ...  train      0   \n",
       "\n",
       "                                     tokenized_title  title_len  \\\n",
       "0                                          [mew_irl]        1.0   \n",
       "1                                     [trippy, kong]        2.0   \n",
       "2  [dk, rap, but, the, acid, is, slowly, kicking,...        9.0   \n",
       "3                                                 []        NaN   \n",
       "4  [pokemon, theme, but, everytime, they, say, po...       18.0   \n",
       "\n",
       "                        tokenized_text  text_len  ... quant  sad  motion  \\\n",
       "0                                   []       NaN  ...   0.0  0.0     0.0   \n",
       "1                                   []       NaN  ...   0.0  0.0     0.0   \n",
       "2                                   []       NaN  ...   0.0  0.0     0.0   \n",
       "3  [america, just, did, a, heel, turn]       6.0  ...   0.0  0.0     0.0   \n",
       "4                                   []       NaN  ...   0.0  0.0     0.0   \n",
       "\n",
       "       verb  ingest    adverb  negemo  achieve  death   article  \n",
       "0  0.000000     0.0  0.000000     0.0      0.0    0.0  0.000000  \n",
       "1  0.000000     0.0  0.000000     0.0      0.0    0.0  0.000000  \n",
       "2  0.111111     0.0  0.000000     0.0      0.0    0.0  0.111111  \n",
       "3  0.333333     0.0  0.166667     0.0      0.0    0.0  0.166667  \n",
       "4  0.166667     0.0  0.000000     0.0      0.0    0.0  0.111111  \n",
       "\n",
       "[5 rows x 75 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def tokenize(t):\n",
    "    return tokenizer.tokenize(t.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fields(writings_df):\n",
    "    writings_df['tokenized_title'] = writings_df['title'].apply(lambda t: tokenize(t) \n",
    "                                                                if type(t)==str and t else None)\n",
    "    writings_df['title_len'] = writings_df['tokenized_title'].apply(lambda t: len(t) \n",
    "                                                                    if type(t)==list and t else None)\n",
    "    writings_df['tokenized_text'] = writings_df['text'].apply(lambda t: tokenize(t) \n",
    "                                                              if type(t)==str and t else None)\n",
    "    writings_df['text_len'] = writings_df['tokenized_text'].apply(lambda t: len(t) \n",
    "                                                                  if type(t)==list and t else None)\n",
    "    return writings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    639057.000000\n",
       "mean         36.808782\n",
       "std          85.912342\n",
       "min           1.000000\n",
       "25%           7.000000\n",
       "50%          16.000000\n",
       "75%          37.000000\n",
       "max       14667.000000\n",
       "Name: text_len, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.text_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df.title_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title_len</th>\n",
       "      <th>text_len</th>\n",
       "      <th>present</th>\n",
       "      <th>discrep</th>\n",
       "      <th>cogmech</th>\n",
       "      <th>social</th>\n",
       "      <th>hear</th>\n",
       "      <th>family</th>\n",
       "      <th>preps</th>\n",
       "      <th>...</th>\n",
       "      <th>quant</th>\n",
       "      <th>sad</th>\n",
       "      <th>motion</th>\n",
       "      <th>verb</th>\n",
       "      <th>ingest</th>\n",
       "      <th>adverb</th>\n",
       "      <th>negemo</th>\n",
       "      <th>achieve</th>\n",
       "      <th>death</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1287.000000</td>\n",
       "      <td>1273.000000</td>\n",
       "      <td>1284.000000</td>\n",
       "      <td>1287.000000</td>\n",
       "      <td>1287.000000</td>\n",
       "      <td>1287.000000</td>\n",
       "      <td>1287.000000</td>\n",
       "      <td>1287.000000</td>\n",
       "      <td>1287.000000</td>\n",
       "      <td>1287.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1287.000000</td>\n",
       "      <td>1287.000000</td>\n",
       "      <td>1287.000000</td>\n",
       "      <td>1287.000000</td>\n",
       "      <td>1287.000000</td>\n",
       "      <td>1287.000000</td>\n",
       "      <td>1287.000000</td>\n",
       "      <td>1287.000000</td>\n",
       "      <td>1287.000000</td>\n",
       "      <td>1287.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.104118</td>\n",
       "      <td>10.089650</td>\n",
       "      <td>35.774332</td>\n",
       "      <td>0.168552</td>\n",
       "      <td>0.013915</td>\n",
       "      <td>0.132078</td>\n",
       "      <td>0.078925</td>\n",
       "      <td>0.005835</td>\n",
       "      <td>0.002811</td>\n",
       "      <td>0.091671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024190</td>\n",
       "      <td>0.003369</td>\n",
       "      <td>0.014693</td>\n",
       "      <td>0.208642</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>0.044145</td>\n",
       "      <td>0.023274</td>\n",
       "      <td>0.016521</td>\n",
       "      <td>0.002709</td>\n",
       "      <td>0.052764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.305532</td>\n",
       "      <td>3.664013</td>\n",
       "      <td>28.949297</td>\n",
       "      <td>0.046646</td>\n",
       "      <td>0.005484</td>\n",
       "      <td>0.028579</td>\n",
       "      <td>0.021278</td>\n",
       "      <td>0.004633</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007687</td>\n",
       "      <td>0.003084</td>\n",
       "      <td>0.006561</td>\n",
       "      <td>0.052488</td>\n",
       "      <td>0.006461</td>\n",
       "      <td>0.013276</td>\n",
       "      <td>0.010192</td>\n",
       "      <td>0.006604</td>\n",
       "      <td>0.003016</td>\n",
       "      <td>0.012605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006993</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.648649</td>\n",
       "      <td>18.695668</td>\n",
       "      <td>0.142407</td>\n",
       "      <td>0.010860</td>\n",
       "      <td>0.115937</td>\n",
       "      <td>0.065616</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>0.000972</td>\n",
       "      <td>0.080150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019705</td>\n",
       "      <td>0.002013</td>\n",
       "      <td>0.011178</td>\n",
       "      <td>0.180081</td>\n",
       "      <td>0.001655</td>\n",
       "      <td>0.036236</td>\n",
       "      <td>0.016592</td>\n",
       "      <td>0.013046</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.046027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.600000</td>\n",
       "      <td>28.785345</td>\n",
       "      <td>0.170200</td>\n",
       "      <td>0.013795</td>\n",
       "      <td>0.133358</td>\n",
       "      <td>0.077946</td>\n",
       "      <td>0.005233</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>0.092862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024078</td>\n",
       "      <td>0.003017</td>\n",
       "      <td>0.014181</td>\n",
       "      <td>0.211690</td>\n",
       "      <td>0.003414</td>\n",
       "      <td>0.044215</td>\n",
       "      <td>0.022517</td>\n",
       "      <td>0.015829</td>\n",
       "      <td>0.002124</td>\n",
       "      <td>0.052959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.968504</td>\n",
       "      <td>44.302545</td>\n",
       "      <td>0.194961</td>\n",
       "      <td>0.016753</td>\n",
       "      <td>0.149744</td>\n",
       "      <td>0.091704</td>\n",
       "      <td>0.006941</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>0.103655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028338</td>\n",
       "      <td>0.004093</td>\n",
       "      <td>0.016905</td>\n",
       "      <td>0.241227</td>\n",
       "      <td>0.005926</td>\n",
       "      <td>0.051493</td>\n",
       "      <td>0.028494</td>\n",
       "      <td>0.019092</td>\n",
       "      <td>0.003551</td>\n",
       "      <td>0.060480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>32.669000</td>\n",
       "      <td>431.611111</td>\n",
       "      <td>0.426819</td>\n",
       "      <td>0.065058</td>\n",
       "      <td>0.252986</td>\n",
       "      <td>0.190654</td>\n",
       "      <td>0.068221</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.166136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076106</td>\n",
       "      <td>0.068162</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>0.446222</td>\n",
       "      <td>0.080553</td>\n",
       "      <td>0.115700</td>\n",
       "      <td>0.087655</td>\n",
       "      <td>0.082778</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.114361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             label    title_len     text_len      present      discrep  \\\n",
       "count  1287.000000  1273.000000  1284.000000  1287.000000  1287.000000   \n",
       "mean      0.104118    10.089650    35.774332     0.168552     0.013915   \n",
       "std       0.305532     3.664013    28.949297     0.046646     0.005484   \n",
       "min       0.000000     1.000000     1.000000     0.006993     0.000000   \n",
       "25%       0.000000     7.648649    18.695668     0.142407     0.010860   \n",
       "50%       0.000000     9.600000    28.785345     0.170200     0.013795   \n",
       "75%       0.000000    11.968504    44.302545     0.194961     0.016753   \n",
       "max       1.000000    32.669000   431.611111     0.426819     0.065058   \n",
       "\n",
       "           cogmech       social         hear       family        preps  ...  \\\n",
       "count  1287.000000  1287.000000  1287.000000  1287.000000  1287.000000  ...   \n",
       "mean      0.132078     0.078925     0.005835     0.002811     0.091671  ...   \n",
       "std       0.028579     0.021278     0.004633     0.003186     0.018555  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.115937     0.065616     0.003661     0.000972     0.080150  ...   \n",
       "50%       0.133358     0.077946     0.005233     0.002004     0.092862  ...   \n",
       "75%       0.149744     0.091704     0.006941     0.003532     0.103655  ...   \n",
       "max       0.252986     0.190654     0.068221     0.035714     0.166136  ...   \n",
       "\n",
       "             quant          sad       motion         verb       ingest  \\\n",
       "count  1287.000000  1287.000000  1287.000000  1287.000000  1287.000000   \n",
       "mean      0.024190     0.003369     0.014693     0.208642     0.004995   \n",
       "std       0.007687     0.003084     0.006561     0.052488     0.006461   \n",
       "min       0.000000     0.000000     0.000000     0.018859     0.000000   \n",
       "25%       0.019705     0.002013     0.011178     0.180081     0.001655   \n",
       "50%       0.024078     0.003017     0.014181     0.211690     0.003414   \n",
       "75%       0.028338     0.004093     0.016905     0.241227     0.005926   \n",
       "max       0.076106     0.068162     0.077778     0.446222     0.080553   \n",
       "\n",
       "            adverb       negemo      achieve        death      article  \n",
       "count  1287.000000  1287.000000  1287.000000  1287.000000  1287.000000  \n",
       "mean      0.044145     0.023274     0.016521     0.002709     0.052764  \n",
       "std       0.013276     0.010192     0.006604     0.003016     0.012605  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.036236     0.016592     0.013046     0.000901     0.046027  \n",
       "50%       0.044215     0.022517     0.015829     0.002124     0.052959  \n",
       "75%       0.051493     0.028494     0.019092     0.003551     0.060480  \n",
       "max       0.115700     0.087655     0.082778     0.035714     0.114361  \n",
       "\n",
       "[8 rows x 67 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.groupby('subject').mean().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_len</th>\n",
       "      <th>text_len</th>\n",
       "      <th>present</th>\n",
       "      <th>discrep</th>\n",
       "      <th>cogmech</th>\n",
       "      <th>social</th>\n",
       "      <th>hear</th>\n",
       "      <th>family</th>\n",
       "      <th>preps</th>\n",
       "      <th>see</th>\n",
       "      <th>...</th>\n",
       "      <th>quant</th>\n",
       "      <th>sad</th>\n",
       "      <th>motion</th>\n",
       "      <th>verb</th>\n",
       "      <th>ingest</th>\n",
       "      <th>adverb</th>\n",
       "      <th>negemo</th>\n",
       "      <th>achieve</th>\n",
       "      <th>death</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1148</td>\n",
       "      <td>1150</td>\n",
       "      <td>1153</td>\n",
       "      <td>1153</td>\n",
       "      <td>1153</td>\n",
       "      <td>1153</td>\n",
       "      <td>1153</td>\n",
       "      <td>1153</td>\n",
       "      <td>1153</td>\n",
       "      <td>1153</td>\n",
       "      <td>...</td>\n",
       "      <td>1153</td>\n",
       "      <td>1153</td>\n",
       "      <td>1153</td>\n",
       "      <td>1153</td>\n",
       "      <td>1153</td>\n",
       "      <td>1153</td>\n",
       "      <td>1153</td>\n",
       "      <td>1153</td>\n",
       "      <td>1153</td>\n",
       "      <td>1153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "      <td>...</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       title_len  text_len  present  discrep  cogmech  social  hear  family  \\\n",
       "label                                                                         \n",
       "0           1148      1150     1153     1153     1153    1153  1153    1153   \n",
       "1            125       134      134      134      134     134   134     134   \n",
       "\n",
       "       preps   see  ...  quant   sad  motion  verb  ingest  adverb  negemo  \\\n",
       "label               ...                                                      \n",
       "0       1153  1153  ...   1153  1153    1153  1153    1153    1153    1153   \n",
       "1        134   134  ...    134   134     134   134     134     134     134   \n",
       "\n",
       "       achieve  death  article  \n",
       "label                           \n",
       "0         1153   1153     1153  \n",
       "1          134    134      134  \n",
       "\n",
       "[2 rows x 66 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.groupby('subject').mean().groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of comments per user 640.060606060606\n"
     ]
    }
   ],
   "source": [
    "# print(\"Average number of posts per user\", writings_df.groupby('subject').count().title.mean())\n",
    "print(\"Average number of comments per user\", writings_df.groupby('subject').count().text.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df.groupby('subject').count().title.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1287.000000\n",
       "mean      640.060606\n",
       "std       610.696588\n",
       "min         9.000000\n",
       "25%        98.500000\n",
       "50%       396.000000\n",
       "75%      1113.500000\n",
       "max      2000.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.groupby('subject').count().text.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import ClassificationModel\n",
    "# from simpletransformers.experimental.classification import ClassificationModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>subject</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>subset</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_title</th>\n",
       "      <th>title_len</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>text_len</th>\n",
       "      <th>...</th>\n",
       "      <th>quant</th>\n",
       "      <th>sad</th>\n",
       "      <th>motion</th>\n",
       "      <th>verb</th>\n",
       "      <th>ingest</th>\n",
       "      <th>adverb</th>\n",
       "      <th>negemo</th>\n",
       "      <th>achieve</th>\n",
       "      <th>death</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-12-01 07:47:48</td>\n",
       "      <td>subject5452</td>\n",
       "      <td></td>\n",
       "      <td>mew_irl</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>[mew_irl]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-11-23 03:42:13</td>\n",
       "      <td>subject5452</td>\n",
       "      <td></td>\n",
       "      <td>Trippy Kong.</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>[trippy, kong]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-11-22 22:12:02</td>\n",
       "      <td>subject5452</td>\n",
       "      <td></td>\n",
       "      <td>Dk rap but the acid is slowly kicking in.</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>[dk, rap, but, the, acid, is, slowly, kicking,...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-11-09 21:35:42</td>\n",
       "      <td>subject5452</td>\n",
       "      <td>America just did a heel turn.</td>\n",
       "      <td></td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[america, just, did, a, heel, turn]</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-11-05 08:05:15</td>\n",
       "      <td>subject5452</td>\n",
       "      <td></td>\n",
       "      <td>Pokemon theme but everytime they say Pokemon ...</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>[pokemon, theme, but, everytime, they, say, po...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    date      subject                             text  \\\n",
       "0   2016-12-01 07:47:48   subject5452                                    \n",
       "1   2016-11-23 03:42:13   subject5452                                    \n",
       "2   2016-11-22 22:12:02   subject5452                                    \n",
       "3   2016-11-09 21:35:42   subject5452   America just did a heel turn.    \n",
       "4   2016-11-05 08:05:15   subject5452                                    \n",
       "\n",
       "                                               title subset  label  \\\n",
       "0                                           mew_irl   train      0   \n",
       "1                                      Trippy Kong.   train      0   \n",
       "2         Dk rap but the acid is slowly kicking in.   train      0   \n",
       "3                                                     train      0   \n",
       "4   Pokemon theme but everytime they say Pokemon ...  train      0   \n",
       "\n",
       "                                     tokenized_title  title_len  \\\n",
       "0                                          [mew_irl]        1.0   \n",
       "1                                     [trippy, kong]        2.0   \n",
       "2  [dk, rap, but, the, acid, is, slowly, kicking,...        9.0   \n",
       "3                                                 []        NaN   \n",
       "4  [pokemon, theme, but, everytime, they, say, po...       18.0   \n",
       "\n",
       "                        tokenized_text  text_len  ... quant  sad  motion  \\\n",
       "0                                   []       NaN  ...   0.0  0.0     0.0   \n",
       "1                                   []       NaN  ...   0.0  0.0     0.0   \n",
       "2                                   []       NaN  ...   0.0  0.0     0.0   \n",
       "3  [america, just, did, a, heel, turn]       6.0  ...   0.0  0.0     0.0   \n",
       "4                                   []       NaN  ...   0.0  0.0     0.0   \n",
       "\n",
       "       verb  ingest    adverb  negemo  achieve  death   article  \n",
       "0  0.000000     0.0  0.000000     0.0      0.0    0.0  0.000000  \n",
       "1  0.000000     0.0  0.000000     0.0      0.0    0.0  0.000000  \n",
       "2  0.111111     0.0  0.000000     0.0      0.0    0.0  0.111111  \n",
       "3  0.333333     0.0  0.166667     0.0      0.0    0.0  0.166667  \n",
       "4  0.166667     0.0  0.000000     0.0      0.0    0.0  0.111111  \n",
       "\n",
       "[5 rows x 75 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by users\n",
    "writings_df = writings_df.fillna(value={'text': ' ', \n",
    "                                        'title':' '\n",
    "                                       })\n",
    "writings_per_user_df = writings_df.groupby('subject').aggregate({'text': lambda t: \" \".join(t), \n",
    "                                        'title': lambda t: \" \".join(t),\n",
    "                                        'text_len': 'sum',\n",
    "                                        'title_len': 'sum',\n",
    "                                          'label': 'min'})\n",
    "#                                          'subset': 'min'})\n",
    "# writings_per_user_df = writings_per_user_df.fillna(\"\")\n",
    "writings_per_user_df['text'] = writings_per_user_df['text'] + \" \" +  writings_per_user_df['title']\n",
    "writings_per_user_df['text_len'] = writings_per_user_df['text_len'] + writings_per_user_df['title_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_text(text):\n",
    "    return \" \".join(text.split()[::-1])\n",
    "def truncate_text_beginning(text, seq_len=seq_len, epsilon=20):\n",
    "    truncated_tokenized = text.split()[-seq_len-epsilon:]\n",
    "    return \" \".join(truncated_tokenized)\n",
    "writings_per_user_df['text'] = writings_per_user_df['text'].apply(truncate_text_beginning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>text_len</th>\n",
       "      <th>title_len</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subject10010000</th>\n",
       "      <td>religious significance? Does anyone know if th...</td>\n",
       "      <td>Sherlock Holmes isn't...</td>\n",
       "      <td>49687.0</td>\n",
       "      <td>6841.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject10370000</th>\n",
       "      <td>is a bummer. I thought holiday pay was somethi...</td>\n",
       "      <td>House Of Wolves, Digital, 1280X1920px        ...</td>\n",
       "      <td>6784.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject10440000</th>\n",
       "      <td>a Free Xbox Digital Download for FOX One Hot H...</td>\n",
       "      <td>The TMZ Narrator Guy Delivering A Eulogy     ...</td>\n",
       "      <td>38041.0</td>\n",
       "      <td>3069.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject10530000</th>\n",
       "      <td>make q small diorama. Thanks for your critics ...</td>\n",
       "      <td>I just added drif...</td>\n",
       "      <td>2779.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject1070000</th>\n",
       "      <td>his YouTube channel https://www.youtube.com/ch...</td>\n",
       "      <td>Ex-White House ethics chief: Trump just a...</td>\n",
       "      <td>2323.0</td>\n",
       "      <td>430.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              text  \\\n",
       "subject                                                              \n",
       "subject10010000  religious significance? Does anyone know if th...   \n",
       "subject10370000  is a bummer. I thought holiday pay was somethi...   \n",
       "subject10440000  a Free Xbox Digital Download for FOX One Hot H...   \n",
       "subject10530000  make q small diorama. Thanks for your critics ...   \n",
       "subject1070000   his YouTube channel https://www.youtube.com/ch...   \n",
       "\n",
       "                                                             title  text_len  \\\n",
       "subject                                                                        \n",
       "subject10010000                           Sherlock Holmes isn't...   49687.0   \n",
       "subject10370000   House Of Wolves, Digital, 1280X1920px        ...    6784.0   \n",
       "subject10440000   The TMZ Narrator Guy Delivering A Eulogy     ...   38041.0   \n",
       "subject10530000                               I just added drif...    2779.0   \n",
       "subject1070000        Ex-White House ethics chief: Trump just a...    2323.0   \n",
       "\n",
       "                 title_len  label  \n",
       "subject                            \n",
       "subject10010000     6841.0      0  \n",
       "subject10370000      300.0      0  \n",
       "subject10440000     3069.0      0  \n",
       "subject10530000      198.0      0  \n",
       "subject1070000       430.0      0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_per_user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      1287.000000\n",
       "mean      20379.048174\n",
       "std       27988.322069\n",
       "min          10.000000\n",
       "25%        2450.000000\n",
       "50%        9763.000000\n",
       "75%       29343.000000\n",
       "max      363626.000000\n",
       "Name: text_len, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writings_per_user_df.text_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_per_user_df['text'] = writings_per_user_df['text'].apply(\n",
    "            lambda t: t.encode('utf-16','surrogatepass').decode('utf-16', errors='ignore'))\n",
    "# writings_per_user_df['text'] = writings_per_user_df['text'].apply(\n",
    "#             lambda t: t.decode('utf-8', errors='strict'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function str.encode(encoding='utf-8', errors='strict')>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472 training users, 0 validation users, 815 test users.\n"
     ]
    }
   ],
   "source": [
    "def get_subjects_split(writings_df, train_prop=0.8, test_slice=2, nr_slices=5, valid_prop=0):\n",
    "    if 'subset' in writings_df.columns:\n",
    "        training_subjects = list(set(writings_df[writings_df['subset']=='train'].subject))\n",
    "        test_subjects = list(set(writings_df[writings_df['subset']=='test'].subject))\n",
    "    else:\n",
    "        all_subjects = sorted(list(set(writings_df.subject)))\n",
    "        training_subjects_size = int(len(all_subjects) * train_prop)\n",
    "        test_subjects_size = len(all_subjects) - training_subjects_size\n",
    "        # Cross-validation, with fixed slice as input\n",
    "        test_prop = 1-train_prop\n",
    "        test_slice = min(test_slice, nr_slices)\n",
    "        logger.debug(\"start index: %f, from %f\\n\" % (\n",
    "            len(all_subjects)*(1/nr_slices)*test_slice, test_prop*test_slice))\n",
    "        start_slice = int(len(all_subjects)*(1/nr_slices)*test_slice)\n",
    "        test_subjects = all_subjects[start_slice: start_slice+test_subjects_size]\n",
    "        training_subjects = [s for s in all_subjects if s not in test_subjects]\n",
    "    training_subjects = sorted(training_subjects) # ensuring reproducibility\n",
    "    valid_subjects_size = int(len(training_subjects) * valid_prop)\n",
    "    valid_subjects = training_subjects[:valid_subjects_size]\n",
    "    training_subjects = training_subjects[valid_subjects_size:]\n",
    "    logger.debug(\"%d training users, %d validation users, %d test users.\" % (\n",
    "        len(training_subjects), \n",
    "          len(valid_subjects),\n",
    "          len(test_subjects)))\n",
    "    subjects_split = {'train': training_subjects, \n",
    "                      'valid': valid_subjects, \n",
    "                      'test': test_subjects}\n",
    "    return subjects_split\n",
    "\n",
    "subjects_split = get_subjects_split(writings_df, nr_slices=5, test_slice=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2ef21a91d0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "writings_per_user_df.label.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = writings_per_user_df[writings_per_user_df.index.isin(subjects_split['train'])][['text', 'label']]\n",
    "test_df = writings_per_user_df[writings_per_user_df.index.isin(subjects_split['test'])][['text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = writings_df[writings_df.subject.isin(subjects_split['train'])][['text', 'label']]\n",
    "# test_df = writings_df[writings_df.subject.isin(subjects_split['test'])][['text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(472, 815)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text\n",
       "label      \n",
       "0       411\n",
       "1        61"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[340,\n",
       " 373,\n",
       " 423,\n",
       " 441,\n",
       " 658,\n",
       " 710,\n",
       " 719,\n",
       " 756,\n",
       " 777,\n",
       " 778,\n",
       " 885,\n",
       " 1041,\n",
       " 1075,\n",
       " 1092,\n",
       " 1135,\n",
       " 1155,\n",
       " 1238,\n",
       " 1358,\n",
       " 1391,\n",
       " 1552,\n",
       " 1623,\n",
       " 1692,\n",
       " 1730,\n",
       " 1734,\n",
       " 1776,\n",
       " 1797,\n",
       " 1845,\n",
       " 1859,\n",
       " 1899,\n",
       " 1908,\n",
       " 1931,\n",
       " 1935,\n",
       " 2041,\n",
       " 2070,\n",
       " 2134,\n",
       " 2166,\n",
       " 2191,\n",
       " 2242,\n",
       " 2308,\n",
       " 2355,\n",
       " 2423,\n",
       " 2432,\n",
       " 2434,\n",
       " 2452,\n",
       " 2483,\n",
       " 2504,\n",
       " 2544,\n",
       " 2579,\n",
       " 2584,\n",
       " 2586,\n",
       " 2610,\n",
       " 2634,\n",
       " 2639,\n",
       " 2655,\n",
       " 2662,\n",
       " 2669,\n",
       " 2671,\n",
       " 2676,\n",
       " 2692,\n",
       " 2699,\n",
       " 2707,\n",
       " 2712,\n",
       " 2713,\n",
       " 2713,\n",
       " 2715,\n",
       " 2718,\n",
       " 2721,\n",
       " 2724,\n",
       " 2728,\n",
       " 2729,\n",
       " 2732,\n",
       " 2732,\n",
       " 2737,\n",
       " 2737,\n",
       " 2737,\n",
       " 2740,\n",
       " 2744,\n",
       " 2747,\n",
       " 2749,\n",
       " 2758,\n",
       " 2758,\n",
       " 2760,\n",
       " 2761,\n",
       " 2762,\n",
       " 2764,\n",
       " 2765,\n",
       " 2765,\n",
       " 2771,\n",
       " 2771,\n",
       " 2772,\n",
       " 2773,\n",
       " 2775,\n",
       " 2777,\n",
       " 2778,\n",
       " 2779,\n",
       " 2780,\n",
       " 2784,\n",
       " 2789,\n",
       " 2791,\n",
       " 2792,\n",
       " 2794,\n",
       " 2796,\n",
       " 2797,\n",
       " 2801,\n",
       " 2801,\n",
       " 2801,\n",
       " 2802,\n",
       " 2804,\n",
       " 2804,\n",
       " 2806,\n",
       " 2806,\n",
       " 2806,\n",
       " 2812,\n",
       " 2813,\n",
       " 2813,\n",
       " 2814,\n",
       " 2814,\n",
       " 2817,\n",
       " 2820,\n",
       " 2821,\n",
       " 2821,\n",
       " 2821,\n",
       " 2821,\n",
       " 2824,\n",
       " 2828,\n",
       " 2829,\n",
       " 2830,\n",
       " 2832,\n",
       " 2833,\n",
       " 2834,\n",
       " 2835,\n",
       " 2836,\n",
       " 2836,\n",
       " 2837,\n",
       " 2840,\n",
       " 2841,\n",
       " 2842,\n",
       " 2846,\n",
       " 2846,\n",
       " 2847,\n",
       " 2850,\n",
       " 2850,\n",
       " 2851,\n",
       " 2852,\n",
       " 2853,\n",
       " 2854,\n",
       " 2854,\n",
       " 2856,\n",
       " 2858,\n",
       " 2861,\n",
       " 2861,\n",
       " 2861,\n",
       " 2861,\n",
       " 2861,\n",
       " 2861,\n",
       " 2861,\n",
       " 2862,\n",
       " 2867,\n",
       " 2868,\n",
       " 2869,\n",
       " 2869,\n",
       " 2870,\n",
       " 2870,\n",
       " 2870,\n",
       " 2870,\n",
       " 2872,\n",
       " 2872,\n",
       " 2873,\n",
       " 2873,\n",
       " 2873,\n",
       " 2874,\n",
       " 2875,\n",
       " 2876,\n",
       " 2876,\n",
       " 2877,\n",
       " 2878,\n",
       " 2878,\n",
       " 2878,\n",
       " 2878,\n",
       " 2879,\n",
       " 2880,\n",
       " 2881,\n",
       " 2881,\n",
       " 2882,\n",
       " 2882,\n",
       " 2883,\n",
       " 2884,\n",
       " 2884,\n",
       " 2884,\n",
       " 2885,\n",
       " 2890,\n",
       " 2893,\n",
       " 2893,\n",
       " 2894,\n",
       " 2894,\n",
       " 2894,\n",
       " 2896,\n",
       " 2896,\n",
       " 2897,\n",
       " 2897,\n",
       " 2897,\n",
       " 2897,\n",
       " 2898,\n",
       " 2898,\n",
       " 2898,\n",
       " 2898,\n",
       " 2898,\n",
       " 2898,\n",
       " 2899,\n",
       " 2899,\n",
       " 2900,\n",
       " 2901,\n",
       " 2901,\n",
       " 2904,\n",
       " 2904,\n",
       " 2905,\n",
       " 2906,\n",
       " 2906,\n",
       " 2907,\n",
       " 2909,\n",
       " 2910,\n",
       " 2910,\n",
       " 2910,\n",
       " 2910,\n",
       " 2911,\n",
       " 2912,\n",
       " 2913,\n",
       " 2913,\n",
       " 2914,\n",
       " 2916,\n",
       " 2916,\n",
       " 2917,\n",
       " 2918,\n",
       " 2919,\n",
       " 2919,\n",
       " 2920,\n",
       " 2920,\n",
       " 2920,\n",
       " 2921,\n",
       " 2921,\n",
       " 2922,\n",
       " 2923,\n",
       " 2923,\n",
       " 2924,\n",
       " 2924,\n",
       " 2925,\n",
       " 2925,\n",
       " 2925,\n",
       " 2925,\n",
       " 2926,\n",
       " 2926,\n",
       " 2927,\n",
       " 2927,\n",
       " 2927,\n",
       " 2927,\n",
       " 2928,\n",
       " 2928,\n",
       " 2929,\n",
       " 2930,\n",
       " 2932,\n",
       " 2932,\n",
       " 2933,\n",
       " 2934,\n",
       " 2934,\n",
       " 2935,\n",
       " 2936,\n",
       " 2938,\n",
       " 2939,\n",
       " 2939,\n",
       " 2939,\n",
       " 2941,\n",
       " 2941,\n",
       " 2942,\n",
       " 2942,\n",
       " 2942,\n",
       " 2943,\n",
       " 2943,\n",
       " 2943,\n",
       " 2943,\n",
       " 2944,\n",
       " 2945,\n",
       " 2947,\n",
       " 2947,\n",
       " 2947,\n",
       " 2948,\n",
       " 2951,\n",
       " 2952,\n",
       " 2952,\n",
       " 2953,\n",
       " 2954,\n",
       " 2955,\n",
       " 2955,\n",
       " 2956,\n",
       " 2958,\n",
       " 2958,\n",
       " 2958,\n",
       " 2959,\n",
       " 2959,\n",
       " 2962,\n",
       " 2962,\n",
       " 2963,\n",
       " 2965,\n",
       " 2965,\n",
       " 2965,\n",
       " 2966,\n",
       " 2967,\n",
       " 2967,\n",
       " 2967,\n",
       " 2967,\n",
       " 2969,\n",
       " 2969,\n",
       " 2970,\n",
       " 2971,\n",
       " 2973,\n",
       " 2973,\n",
       " 2973,\n",
       " 2973,\n",
       " 2974,\n",
       " 2975,\n",
       " 2976,\n",
       " 2976,\n",
       " 2977,\n",
       " 2977,\n",
       " 2977,\n",
       " 2978,\n",
       " 2978,\n",
       " 2981,\n",
       " 2983,\n",
       " 2983,\n",
       " 2983,\n",
       " 2983,\n",
       " 2984,\n",
       " 2985,\n",
       " 2985,\n",
       " 2986,\n",
       " 2986,\n",
       " 2986,\n",
       " 2986,\n",
       " 2986,\n",
       " 2987,\n",
       " 2988,\n",
       " 2988,\n",
       " 2989,\n",
       " 2990,\n",
       " 2990,\n",
       " 2991,\n",
       " 2991,\n",
       " 2992,\n",
       " 2995,\n",
       " 2995,\n",
       " 2996,\n",
       " 2997,\n",
       " 2998,\n",
       " 2998,\n",
       " 2998,\n",
       " 2998,\n",
       " 2999,\n",
       " 3000,\n",
       " 3000,\n",
       " 3000,\n",
       " 3001,\n",
       " 3002,\n",
       " 3003,\n",
       " 3004,\n",
       " 3005,\n",
       " 3005,\n",
       " 3005,\n",
       " 3006,\n",
       " 3008,\n",
       " 3009,\n",
       " 3009,\n",
       " 3009,\n",
       " 3010,\n",
       " 3010,\n",
       " 3010,\n",
       " 3011,\n",
       " 3012,\n",
       " 3013,\n",
       " 3015,\n",
       " 3016,\n",
       " 3018,\n",
       " 3018,\n",
       " 3018,\n",
       " 3018,\n",
       " 3020,\n",
       " 3020,\n",
       " 3021,\n",
       " 3021,\n",
       " 3022,\n",
       " 3023,\n",
       " 3023,\n",
       " 3024,\n",
       " 3024,\n",
       " 3024,\n",
       " 3024,\n",
       " 3025,\n",
       " 3025,\n",
       " 3025,\n",
       " 3027,\n",
       " 3028,\n",
       " 3029,\n",
       " 3030,\n",
       " 3030,\n",
       " 3030,\n",
       " 3031,\n",
       " 3031,\n",
       " 3032,\n",
       " 3032,\n",
       " 3033,\n",
       " 3034,\n",
       " 3035,\n",
       " 3037,\n",
       " 3038,\n",
       " 3039,\n",
       " 3039,\n",
       " 3040,\n",
       " 3041,\n",
       " 3042,\n",
       " 3042,\n",
       " 3043,\n",
       " 3046,\n",
       " 3046,\n",
       " 3047,\n",
       " 3047,\n",
       " 3049,\n",
       " 3050,\n",
       " 3051,\n",
       " 3051,\n",
       " 3052,\n",
       " 3052,\n",
       " 3052,\n",
       " 3053,\n",
       " 3055,\n",
       " 3055,\n",
       " 3055,\n",
       " 3055,\n",
       " 3056,\n",
       " 3058,\n",
       " 3059,\n",
       " 3059,\n",
       " 3060,\n",
       " 3061,\n",
       " 3061,\n",
       " 3061,\n",
       " 3062,\n",
       " 3063,\n",
       " 3064,\n",
       " 3064,\n",
       " 3064,\n",
       " 3064,\n",
       " 3066,\n",
       " 3066,\n",
       " 3067,\n",
       " 3067,\n",
       " 3069,\n",
       " 3069,\n",
       " 3070,\n",
       " 3070,\n",
       " 3070,\n",
       " 3070,\n",
       " 3071,\n",
       " 3071,\n",
       " 3072,\n",
       " 3073,\n",
       " 3074,\n",
       " 3075,\n",
       " 3076,\n",
       " 3077,\n",
       " 3078,\n",
       " 3078,\n",
       " 3078,\n",
       " 3078,\n",
       " 3079,\n",
       " 3079,\n",
       " 3080,\n",
       " 3080,\n",
       " 3080,\n",
       " 3081,\n",
       " 3082,\n",
       " 3082,\n",
       " 3083,\n",
       " 3083,\n",
       " 3085,\n",
       " 3085,\n",
       " 3086,\n",
       " 3087,\n",
       " 3087,\n",
       " 3088,\n",
       " 3088,\n",
       " 3089,\n",
       " 3091,\n",
       " 3091,\n",
       " 3092,\n",
       " 3093,\n",
       " 3094,\n",
       " 3094,\n",
       " 3095,\n",
       " 3096,\n",
       " 3097,\n",
       " 3098,\n",
       " 3098,\n",
       " 3099,\n",
       " 3099,\n",
       " 3099,\n",
       " 3099,\n",
       " 3100,\n",
       " 3100,\n",
       " 3103,\n",
       " 3103,\n",
       " 3103,\n",
       " 3105,\n",
       " 3105,\n",
       " 3106,\n",
       " 3106,\n",
       " 3107,\n",
       " 3107,\n",
       " 3107,\n",
       " 3109,\n",
       " 3109,\n",
       " 3110,\n",
       " 3113,\n",
       " 3114,\n",
       " 3115,\n",
       " 3115,\n",
       " 3115,\n",
       " 3118,\n",
       " 3118,\n",
       " 3118,\n",
       " 3119,\n",
       " 3119,\n",
       " 3121,\n",
       " 3124,\n",
       " 3124,\n",
       " 3124,\n",
       " 3124,\n",
       " 3125,\n",
       " 3125,\n",
       " 3125,\n",
       " 3126,\n",
       " 3128,\n",
       " 3130,\n",
       " 3130,\n",
       " 3130,\n",
       " 3132,\n",
       " 3133,\n",
       " 3133,\n",
       " 3133,\n",
       " 3135,\n",
       " 3135,\n",
       " 3135,\n",
       " 3138,\n",
       " 3139,\n",
       " 3141,\n",
       " 3141,\n",
       " 3141,\n",
       " 3142,\n",
       " 3143,\n",
       " 3143,\n",
       " 3145,\n",
       " 3145,\n",
       " 3146,\n",
       " 3146,\n",
       " 3147,\n",
       " 3147,\n",
       " 3148,\n",
       " 3149,\n",
       " 3149,\n",
       " 3150,\n",
       " 3150,\n",
       " 3151,\n",
       " 3151,\n",
       " 3153,\n",
       " 3153,\n",
       " 3154,\n",
       " 3155,\n",
       " 3155,\n",
       " 3156,\n",
       " 3157,\n",
       " 3159,\n",
       " 3161,\n",
       " 3163,\n",
       " 3164,\n",
       " 3164,\n",
       " 3164,\n",
       " 3167,\n",
       " 3167,\n",
       " 3169,\n",
       " 3169,\n",
       " 3169,\n",
       " 3170,\n",
       " 3173,\n",
       " 3173,\n",
       " 3174,\n",
       " 3174,\n",
       " 3175,\n",
       " 3175,\n",
       " 3176,\n",
       " 3177,\n",
       " 3182,\n",
       " 3182,\n",
       " 3183,\n",
       " 3184,\n",
       " 3185,\n",
       " 3185,\n",
       " 3187,\n",
       " 3188,\n",
       " 3189,\n",
       " 3190,\n",
       " 3190,\n",
       " 3194,\n",
       " 3196,\n",
       " 3198,\n",
       " 3199,\n",
       " 3200,\n",
       " 3200,\n",
       " 3200,\n",
       " 3202,\n",
       " 3202,\n",
       " 3202,\n",
       " 3203,\n",
       " 3208,\n",
       " 3208,\n",
       " 3210,\n",
       " 3211,\n",
       " 3214,\n",
       " 3215,\n",
       " 3215,\n",
       " 3218,\n",
       " 3218,\n",
       " 3219,\n",
       " 3219,\n",
       " 3220,\n",
       " 3220,\n",
       " 3220,\n",
       " 3225,\n",
       " 3226,\n",
       " 3227,\n",
       " 3227,\n",
       " 3228,\n",
       " 3228,\n",
       " 3228,\n",
       " 3229,\n",
       " 3230,\n",
       " 3232,\n",
       " 3233,\n",
       " 3233,\n",
       " 3236,\n",
       " 3237,\n",
       " 3237,\n",
       " 3239,\n",
       " 3241,\n",
       " 3243,\n",
       " 3245,\n",
       " 3246,\n",
       " 3247,\n",
       " 3249,\n",
       " 3249,\n",
       " 3250,\n",
       " 3252,\n",
       " 3253,\n",
       " 3254,\n",
       " 3255,\n",
       " 3257,\n",
       " 3257,\n",
       " 3261,\n",
       " 3262,\n",
       " 3262,\n",
       " 3263,\n",
       " 3264,\n",
       " 3266,\n",
       " 3267,\n",
       " 3267,\n",
       " 3268,\n",
       " 3269,\n",
       " 3270,\n",
       " 3272,\n",
       " 3273,\n",
       " 3273,\n",
       " 3274,\n",
       " 3276,\n",
       " 3276,\n",
       " 3277,\n",
       " 3277,\n",
       " 3278,\n",
       " 3279,\n",
       " 3280,\n",
       " 3280,\n",
       " 3280,\n",
       " 3282,\n",
       " 3282,\n",
       " 3286,\n",
       " 3287,\n",
       " 3287,\n",
       " 3289,\n",
       " 3290,\n",
       " 3293,\n",
       " 3294,\n",
       " 3294,\n",
       " 3294,\n",
       " 3296,\n",
       " 3298,\n",
       " 3299,\n",
       " 3299,\n",
       " 3300,\n",
       " 3302,\n",
       " 3303,\n",
       " 3303,\n",
       " 3307,\n",
       " 3308,\n",
       " 3312,\n",
       " 3314,\n",
       " 3314,\n",
       " 3314,\n",
       " 3317,\n",
       " 3317,\n",
       " 3318,\n",
       " 3319,\n",
       " 3319,\n",
       " 3322,\n",
       " 3326,\n",
       " 3328,\n",
       " 3330,\n",
       " 3331,\n",
       " 3332,\n",
       " 3334,\n",
       " 3335,\n",
       " 3335,\n",
       " 3337,\n",
       " 3340,\n",
       " 3341,\n",
       " 3343,\n",
       " 3343,\n",
       " 3344,\n",
       " 3346,\n",
       " 3346,\n",
       " 3348,\n",
       " 3349,\n",
       " 3349,\n",
       " 3353,\n",
       " 3354,\n",
       " 3357,\n",
       " 3358,\n",
       " 3360,\n",
       " 3360,\n",
       " 3361,\n",
       " 3361,\n",
       " 3368,\n",
       " 3369,\n",
       " 3371,\n",
       " 3377,\n",
       " 3377,\n",
       " 3378,\n",
       " 3380,\n",
       " 3384,\n",
       " 3391,\n",
       " 3391,\n",
       " 3392,\n",
       " 3399,\n",
       " 3404,\n",
       " 3406,\n",
       " 3411,\n",
       " 3417,\n",
       " 3417,\n",
       " 3418,\n",
       " 3419,\n",
       " 3423,\n",
       " 3423,\n",
       " 3424,\n",
       " 3426,\n",
       " 3428,\n",
       " 3429,\n",
       " 3433,\n",
       " 3434,\n",
       " 3435,\n",
       " 3435,\n",
       " 3436,\n",
       " 3437,\n",
       " 3437,\n",
       " 3437,\n",
       " 3442,\n",
       " 3444,\n",
       " 3445,\n",
       " 3445,\n",
       " 3446,\n",
       " 3447,\n",
       " 3449,\n",
       " 3449,\n",
       " 3455,\n",
       " 3457,\n",
       " 3459,\n",
       " 3468,\n",
       " 3470,\n",
       " 3476,\n",
       " 3490,\n",
       " 3493,\n",
       " 3497,\n",
       " 3499,\n",
       " 3509,\n",
       " 3525,\n",
       " 3535,\n",
       " 3553,\n",
       " 3568,\n",
       " 3572,\n",
       " 3587,\n",
       " 3592,\n",
       " 3634,\n",
       " 3649,\n",
       " 3712,\n",
       " 3791,\n",
       " 4074,\n",
       " 4331,\n",
       " 4538,\n",
       " 4703,\n",
       " 4923,\n",
       " 7064]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([len(t) for t in test_df.text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "#    'model_type':  'roberta',\n",
    "#    'model_name': 'roberta-base',\n",
    "   'output_dir': 'outputs/',\n",
    "   'cache_dir': 'cache/',\n",
    "    'fp16': True,\n",
    "   'fp16_opt_level': 'O1',\n",
    "   'max_seq_length': 512, #seq_len, #256, #128,\n",
    "    'sliding_window': False,\n",
    "#     'stride': 1,\n",
    "   'train_batch_size':4,\n",
    "   'eval_batch_size':4,\n",
    "   'gradient_accumulation_steps': 1,\n",
    "   'num_train_epochs': 2,\n",
    "#    'weight_decay': 0,\n",
    "#    'weight_decay': 4e-5,\n",
    "#    'learning_rate': 4e-5,\n",
    "   'learning_rate': 4e-5,\n",
    "   'adam_epsilon': 1e-8,\n",
    "   'warmup_ratio': 0.06,\n",
    "   'warmup_steps': 0,\n",
    "   'max_grad_norm': 1.0,\n",
    "    'logging_steps': 50,\n",
    "   'evaluate_during_training': True,\n",
    "   'save_steps': 2000,\n",
    "   'eval_all_checkpoints': False,\n",
    "    'evaluate_during_training': True,\n",
    "    'evaluate_during_training_verbose': True,\n",
    "    'evaluate_during_training_steps': 3,\n",
    "   'use_tensorboard': True,\n",
    "    'tensorboard_dir': 'tensorboard/roberta31',\n",
    "    'overwrite_output_dir': True,\n",
    "   'reprocess_input_data': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.backends.cuda.is_built()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TransformerModel\n",
    "model = ClassificationModel('roberta', 'roberta-base', args=args, weight=[1, 6])\n",
    "# model = ClassificationModel('albert', 'albert-base-v1', args=args, weight=[1, 12])\n",
    "# model = ClassificationModel('bert', 'bert-base-uncased', args=args, weight=[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apex import amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:267: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409bc44ce00d43c698834b99b133018a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=2.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49f062b23ea4e4b9f00fdc22f506cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=118.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 0.614638"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:91: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.641670"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.570053"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.469419"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.311344"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.079857"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.827368Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Running loss: 0.014709"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 3.255131Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Running loss: 3.640997"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 3.337398Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Running loss: 0.006085"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.008016"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.003291"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.701864"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.698069"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.470175"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.291477"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.128018"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.020131"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.005549"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.008868"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.043667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.104736"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.194596"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.945680"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.716884"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.587215"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.845726"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.841649"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.468747"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.875199"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.335207"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.001423"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.321613"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.083483"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.164865"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.150222"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.013206"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 3.382429Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Running loss: 3.427577"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.506361"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.038819"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.014564"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.012878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f38df04b114b53818689709dea75d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=118.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 3.032153\r",
      "Running loss: 2.011999"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.881616"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.773832"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anasab/tf2conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.094514"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.317054"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.274825"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.007655"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.005590"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.476521"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.003325"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.022691"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.002458"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.002194"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.005317"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.009480"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.329606"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.007793"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.001484"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.436206"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.042292"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.001425"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.066931"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 4.046536"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.025841"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.016586"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.011432"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 4.133573"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.571868"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.001987"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.007147"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.002488"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.003852"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.030310"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.042105"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.003706"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.003823"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.576352"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.004760"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.004786"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.004949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 14min 40s, sys: 1min 42s, total: 16min 22s\n",
      "Wall time: 29min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Train the model\n",
    "model.train_model(train_df=train_df, eval_df=test_df , acc=accuracy_score,\n",
    "                                                          prec=precision_score,\n",
    "                                                           f1=f1_score,\n",
    "                                                         auc=roc_auc_score)#, auto_weights=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anasab/tf2conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:690: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d8315d31f44d6e9703ea65f9c0e991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=204.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "result, model_outputs, wrong_predictions = model.eval_model(test_df,\n",
    "                                                           acc=accuracy_score,\n",
    "                                                           prec=precision_score,\n",
    "                                                            f1=f1_score, auc=roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mcc': 0.6745128849051766,\n",
       " 'tp': 51,\n",
       " 'tn': 721,\n",
       " 'fp': 21,\n",
       " 'fn': 22,\n",
       " 'acc': 0.947239263803681,\n",
       " 'prec': 0.7083333333333334,\n",
       " 'f1': 0.703448275862069,\n",
       " 'auc': 0.8351641250969243,\n",
       " 'eval_loss': 0.4038912161349264}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(result):\n",
    "    accuracy = (result['tp'] + result['tn']) / (result['tp'] + result['tn'] + result['fp'] + result['fn'])    \n",
    "    precision = result['tp'] / (result['tp'] + result['fp'])    \n",
    "    recall = result['tp'] / (result['tp'] + result['fn'])    \n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 0.000001)\n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.947239263803681,\n",
       " 'precision': 0.7083333333333334,\n",
       " 'recall': 0.6986301369863014,\n",
       " 'f1': 0.7034477758862056}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     43.000000\n",
       "mean     520.209302\n",
       "std       56.868921\n",
       "min      197.000000\n",
       "25%      532.000000\n",
       "50%      532.000000\n",
       "75%      532.000000\n",
       "max      532.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([len(wrong_predictions[i].text_a.split()) for i in range(len(wrong_predictions))]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1287.000000\n",
       "mean      514.753691\n",
       "std        70.998439\n",
       "min        10.000000\n",
       "25%       532.000000\n",
       "50%       532.000000\n",
       "75%       532.000000\n",
       "max       532.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([len(t.text.split()) for t in writings_per_user_df.itertuples()]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(stop)? (<ipython-input-52-74e119941311>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-52-74e119941311>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print stop\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(stop)?\n"
     ]
    }
   ],
   "source": [
    "print stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features and encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_features = {\n",
    "    \"max_features\": 40000,\n",
    "    # cut texts after this number of words\n",
    "    # (among top max_features most common words)\n",
    "    \"maxlen\": 500,\n",
    "    \"embedding_dim\": 100,\n",
    "    \"user_level\": True,\n",
    "    \"posts_per_user\": 200,\n",
    "    \"batch_size\": 32,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_NRC(nrc_path):\n",
    "    word_emotions = {}\n",
    "    emotion_words = {}\n",
    "    with open(nrc_path) as in_f:\n",
    "        for line in in_f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            word, emotion, label = line.split()\n",
    "            if word not in word_emotions:\n",
    "                word_emotions[word] = set()\n",
    "            if emotion not in emotion_words:\n",
    "                emotion_words[emotion] = set()\n",
    "            label = int(label)\n",
    "            if label:\n",
    "                word_emotions[word].add(emotion)\n",
    "                emotion_words[emotion].add(word)\n",
    "    return emotion_words\n",
    "\n",
    "nrc_lexicon_path = root_dir + '/resources/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'\n",
    "nrc_lexicon = load_NRC(nrc_lexicon_path)\n",
    "emotions = list(nrc_lexicon.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_emotions(tokens, emotion_lexicon, emotions, relative=True):\n",
    "    text_len = len(tokens)\n",
    "    encoded_emotions = [0 for e in emotions]\n",
    "    for i, emotion in enumerate(emotions):\n",
    "        try:\n",
    "            emotion_words = [t for t in tokens if t in emotion_lexicon[emotion]]\n",
    "            if relative:\n",
    "                encoded_emotions[i] = len(emotion_words) / len(tokens)\n",
    "            else:\n",
    "                encoded_emotions[i] = len(emotion_words)\n",
    "        except ValueError:\n",
    "            print(\"Emotion not found.\")\n",
    "    return encoded_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from liwc_readDict import readDict\n",
    "\n",
    "liwc = readDict(root_dir + '/resources/liwc.dic')\n",
    "\n",
    "categories = set([c for (w,c) in liwc])\n",
    "len(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Char n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(tokens):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Personal pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_person_pronouns = {\"i\", \"me\", \"my\", \"mine\", \"myself\"}\n",
    "def encode_pronouns(tokens, pronouns={\"i\", \"me\", \"my\", \"mine\", \"myself\"}, relative=True):\n",
    "    if not tokens:\n",
    "        return np.nan\n",
    "    text_len = len(tokens)\n",
    "    nr_pronouns = len([t for t in tokens if t in pronouns])\n",
    "    if relative:\n",
    "        return nr_pronouns/text_len\n",
    "    else:\n",
    "        return nr_pronouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words(\"english\")\n",
    "def encode_stopwords(tokens, stopwords=stopword_list):\n",
    "    encoded_stopwords = [0 for s in stopword_list]\n",
    "    if not tokens:\n",
    "        return encoded_stopwords\n",
    "    for i, stopword in enumerate(stopwords):\n",
    "        if stopword in tokens:\n",
    "            encoded_stopwords[i] += 1\n",
    "    return encoded_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def load_erisk_data(writings_df, voc_size, emotion_lexicon, seq_len, emotions =  \n",
    "                    ['anger', 'anticipation', 'disgust', 'fear', 'joy', \n",
    "                     'negative', 'positive', 'sadness', 'surprise', 'trust'],\n",
    "                    liwc_categories = categories,\n",
    "                    pronouns = [\"i\", \"me\", \"my\", \"mine\", \"myself\"],\n",
    "                    train_prop=0.7, valid_prop=0.3, test_slice=2,\n",
    "                    nr_slices=5,\n",
    "                    min_post_len=3, min_word_len=1, \n",
    "                    user_level=True, vocabulary=None,\n",
    "                   logger=logger):\n",
    "    logger.debug(\"Loading data...\\n\")\n",
    "    if not vocabulary:\n",
    "        vocabulary = {}\n",
    "        word_freqs = Counter()\n",
    "        for words in writings_df.tokenized_text:\n",
    "            word_freqs.update(words)\n",
    "        for words in writings_df.tokenized_title:\n",
    "            word_freqs.update(words)\n",
    "        i = 1\n",
    "        for w, f in word_freqs.most_common(voc_size-2): # keeping voc_size-1 for unk\n",
    "            if len(w) < min_word_len:\n",
    "                continue\n",
    "            vocabulary[w] = i\n",
    "            i += 1\n",
    "    tokens_data_train = []\n",
    "    categ_data_train = []\n",
    "    sparse_data_train = []\n",
    "    tokens_data_valid = []\n",
    "    categ_data_valid = []\n",
    "    sparse_data_valid = []\n",
    "    tokens_data_test = []\n",
    "    categ_data_test = []\n",
    "    sparse_data_test = []\n",
    "    labels_train = []\n",
    "    users_train = []\n",
    "    labels_valid = []\n",
    "    users_valid = []\n",
    "    users_test = []\n",
    "    labels_test = []\n",
    "    if 'subset' in writings_df.columns:\n",
    "        training_subjects = list(set(writings_df[writings_df['subset']=='train'].subject))\n",
    "        test_subjects = list(set(writings_df[writings_df['subset']=='test'].subject))\n",
    "    else:\n",
    "        all_subjects = sorted(list(set(writings_df.subject)))\n",
    "        training_subjects_size = int(len(all_subjects) * train_prop)\n",
    "        test_subjects_size = len(all_subjects) - training_subjects_size\n",
    "        # Cross-validation, with fixed slice as input\n",
    "        test_prop = 1-train_prop\n",
    "        test_slice = min(test_slice, nr_slices)\n",
    "        logger.debug(\"start index: %f, from %f\\n\" % (\n",
    "            len(all_subjects)*(1/nr_slices)*test_slice, test_prop*test_slice))\n",
    "        start_slice = int(len(all_subjects)*(1/nr_slices)*test_slice)\n",
    "        test_subjects = all_subjects[start_slice: start_slice+test_subjects_size]\n",
    "        training_subjects = [s for s in all_subjects if s not in test_subjects]\n",
    "    training_subjects = sorted(training_subjects) # ensuring reproducibility\n",
    "    valid_subjects_size = int(len(training_subjects) * valid_prop)\n",
    "    valid_subjects = training_subjects[:valid_subjects_size]\n",
    "    training_subjects = training_subjects[valid_subjects_size:]\n",
    "    categories = [c for c in liwc_categories if c in writings_df.columns]\n",
    "    logger.debug(\"%d training users, %d validation users, %d test users.\" % (\n",
    "        len(training_subjects), \n",
    "          len(valid_subjects),\n",
    "          len(test_subjects)))\n",
    "    subjects_split = {'train': training_subjects, \n",
    "                      'valid': valid_subjects, \n",
    "                      'test': test_subjects}\n",
    "\n",
    "    user_level_texts = {}\n",
    "    for row in writings_df.sort_values(by='date').itertuples():\n",
    "        words = []\n",
    "        if row.tokenized_title:\n",
    "            words.extend(row.tokenized_title)\n",
    "        if row.tokenized_text:\n",
    "            words.extend(row.tokenized_text)\n",
    "        if not words or len(words)<min_post_len:\n",
    "            print(row.subject)\n",
    "            continue\n",
    "        label = row.label\n",
    "        liwc_categs = [getattr(row, categ) for categ in categories]\n",
    "        if row.subject not in user_level_texts.keys():\n",
    "            user_level_texts[row.subject] = {}\n",
    "            user_level_texts[row.subject]['texts'] = [words]\n",
    "            user_level_texts[row.subject]['label'] = label\n",
    "            user_level_texts[row.subject]['liwc'] = [liwc_categs]\n",
    "        else:\n",
    "            user_level_texts[row.subject]['texts'].append(words)\n",
    "            user_level_texts[row.subject]['liwc'].append(liwc_categs)\n",
    "    return user_level_texts, subjects_split, vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_level_data, subjects_split, vocabulary = load_erisk_data(writings_df, \n",
    "                                                            seq_len=hyperparams_features['maxlen'],\n",
    "                                                            voc_size=hyperparams_features['max_features'],\n",
    "                                                           emotion_lexicon=nrc_lexicon,\n",
    "                                                           emotions=emotions,\n",
    "                                                           user_level=hyperparams_features['user_level'],\n",
    "                                                                                logger=logger\n",
    "#                                                            vocabulary=pickle.load(open('vocabulary20K_selfharm.pkl', 'rb'))\n",
    "                                                                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, user_level_data, subjects_split, set_type='train',\n",
    "                 batch_size=hyperparams_features['batch_size'], seq_len=hyperparams_features['maxlen'], \n",
    "                 voc_size=hyperparams_features['max_features'], emotion_lexicon=nrc_lexicon,\n",
    "                 emotions=emotions, pronouns=[\"i\", \"me\", \"my\", \"mine\", \"myself\"], \n",
    "                 max_posts_per_user=hyperparams_features['posts_per_user'],\n",
    "                 shuffle=True):\n",
    "        'Initialization'\n",
    "        self.seq_len = seq_len\n",
    "        self.subjects_split = subjects_split\n",
    "        self.set = set_type\n",
    "        self.emotion_lexicon = emotion_lexicon\n",
    "        self.batch_size = batch_size\n",
    "        self.data = user_level_data\n",
    "        self.emotions = emotions\n",
    "        self.pronouns = pronouns\n",
    "        self.shuffle = shuffle\n",
    "        self.voc_size = voc_size\n",
    "        self.max_posts_per_user = max_posts_per_user\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __encode_text(self, tokens):\n",
    "        # Using voc_size-1 value for OOV token\n",
    "        encoded_tokens = [vocabulary.get(w, self.voc_size-1) for w in tokens]\n",
    "        encoded_emotions = encode_emotions(tokens, self.emotion_lexicon, self.emotions)\n",
    "        encoded_pronouns = encode_pronouns(tokens, self.pronouns)\n",
    "        encoded_stopwords = encode_stopwords(tokens)\n",
    "        return (encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords)\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.subjects_split[self.set]) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        user_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Find users\n",
    "        users = [self.subjects_split[self.set][i] for i in user_indexes\n",
    "                    if self.subjects_split[self.set][i] in self.data.keys()] # TODO: maybe needs a warning that user is missing\n",
    "\n",
    "        post_indexes = {}\n",
    "        # Sample post ids\n",
    "        for subject in users:\n",
    "            posts_len = len(self.data[subject]['texts'])\n",
    "            posts_index_sample = sorted(np.random.choice(posts_len, \n",
    "                                                         min(self.max_posts_per_user, posts_len),\n",
    "                                                         replace=False))\n",
    "            post_indexes[subject] = posts_index_sample\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(users, post_indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.subjects_split[self.set]))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, users, post_indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        tokens_data = []\n",
    "        categ_data = []\n",
    "        sparse_data = []\n",
    "        subjects = []\n",
    "        labels = []\n",
    "        for subject in users:\n",
    "            texts = self.data[subject]['texts']\n",
    "            label = self.data[subject]['label']\n",
    "            \n",
    "            # Sample\n",
    "            texts = [texts[i] for i in post_indexes[subject]]\n",
    "            liwc_selection = [self.data[subject]['liwc'][i] for i in post_indexes[subject]]\n",
    "            all_words = [sum(texts, [])] # merge all texts in one list\n",
    "            liwc_aggreg = [np.array(liwc_selection).mean(axis=0).tolist()]\n",
    "\n",
    "            for i, words in enumerate(all_words):\n",
    "                encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords = self.__encode_text(words)\n",
    "                subject_id = int(subject.split('t')[1])\n",
    "                tokens_data.append(encoded_tokens)\n",
    "                categ_data.append(encoded_emotions + [encoded_pronouns] + liwc_aggreg[i])\n",
    "                sparse_data.append(encoded_stopwords)\n",
    "                labels.append(label)\n",
    "                subjects.append(subject_id)\n",
    "\n",
    "        \n",
    "        # using zeros for padding\n",
    "        tokens_data_padded = sequence.pad_sequences(tokens_data, maxlen=self.seq_len)\n",
    "\n",
    "        return ([np.array(tokens_data_padded), np.array(categ_data), np.array(sparse_data),\n",
    "                np.array(subjects)],\n",
    "                np.array(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGeneratorHierarchical(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, user_level_data, subjects_split, set_type='train',\n",
    "                 batch_size=hyperparams_features['batch_size'], seq_len=hyperparams_features['maxlen'], \n",
    "                 voc_size=hyperparams_features['max_features'], emotion_lexicon=nrc_lexicon,\n",
    "                 emotions=emotions, pronouns=[\"i\", \"me\", \"my\", \"mine\", \"myself\"], \n",
    "                 max_posts_per_user=hyperparams_features['posts_per_user'], stopwords=stopword_list,\n",
    "                 liwc_categories=categories,\n",
    "                 shuffle=True):\n",
    "        'Initialization'\n",
    "        self.seq_len = seq_len\n",
    "        self.subjects_split = subjects_split\n",
    "        self.set = set_type\n",
    "        self.emotion_lexicon = emotion_lexicon\n",
    "        self.batch_size = batch_size\n",
    "        self.data = user_level_data\n",
    "        self.emotions = emotions\n",
    "        self.pronouns = pronouns\n",
    "        self.shuffle = shuffle\n",
    "        self.voc_size = voc_size\n",
    "        self.max_posts_per_user = max_posts_per_user\n",
    "        self.categ_dim = len(emotions) + 1 + len(liwc_categories)\n",
    "        self.sparse_dim = len(stopwords)\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __encode_text(self, tokens):\n",
    "        # Using voc_size-1 value for OOV token\n",
    "        encoded_tokens = [vocabulary.get(w, self.voc_size-1) for w in tokens]\n",
    "        encoded_emotions = encode_emotions(tokens, self.emotion_lexicon, self.emotions)\n",
    "        encoded_pronouns = encode_pronouns(tokens, self.pronouns)\n",
    "        encoded_stopwords = encode_stopwords(tokens)\n",
    "        return (encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords)\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.subjects_split[self.set]) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        user_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "#         if len(user_indexes)<self.batch_size:\n",
    "#             return\n",
    "        # Find users\n",
    "        users = [self.subjects_split[self.set][i] for i in user_indexes\n",
    "                    if self.subjects_split[self.set][i] in self.data.keys()] # TODO: maybe needs a warning that user is missing\n",
    "\n",
    "        post_indexes = {}\n",
    "        # Sample post ids\n",
    "        for subject in users:\n",
    "            posts_len = len(self.data[subject]['texts'])\n",
    "            posts_index_sample = sorted(np.random.choice(posts_len, \n",
    "                                                         min(self.max_posts_per_user, posts_len),\n",
    "                                                         replace=False))\n",
    "            post_indexes[subject] = posts_index_sample\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(users, post_indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.subjects_split[self.set]))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, users, post_indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        user_tokens = []\n",
    "        user_categ_data = []\n",
    "        user_sparse_data = []\n",
    "        \n",
    "        labels = []\n",
    "        for subject in users:\n",
    "            tokens_data = []\n",
    "            categ_data = []\n",
    "            sparse_data = []\n",
    "            \n",
    "            texts = self.data[subject]['texts']\n",
    "            label = self.data[subject]['label']\n",
    "            \n",
    "            # Sample\n",
    "            texts = [texts[i] for i in post_indexes[subject]]\n",
    "            if len(texts) < self.max_posts_per_user:\n",
    "                # TODO: pad with zeros\n",
    "                pass\n",
    "                \n",
    "            liwc_selection = [self.data[subject]['liwc'][i] for i in post_indexes[subject]]\n",
    "            all_words = texts#= [sum(texts, [])] # merge all texts in one list\n",
    "            liwc_aggreg = liwc_selection#[np.array(liwc_selection).mean(axis=0).tolist()]\n",
    "\n",
    "            for i, words in enumerate(all_words):\n",
    "                encoded_tokens, encoded_emotions, encoded_pronouns, encoded_stopwords = self.__encode_text(words)\n",
    "                subject_id = int(subject.split('t')[1])\n",
    "                tokens_data.append(encoded_tokens)\n",
    "                # using zeros for padding\n",
    "                categ_data.append(encoded_emotions + [encoded_pronouns] + liwc_aggreg[i])\n",
    "                sparse_data.append(encoded_stopwords)\n",
    "            tokens_data_padded = np.array(sequence.pad_sequences(tokens_data, maxlen=self.seq_len))\n",
    "            user_tokens.append(tokens_data_padded)\n",
    "\n",
    "            user_categ_data.append(categ_data)\n",
    "            user_sparse_data.append(sparse_data)\n",
    "\n",
    "            labels.append(label)\n",
    "        # TODO: check this is correct. for when there are fewer posts than minimum\n",
    "        user_tokens = sequence.pad_sequences(user_tokens, value=np.zeros(self.seq_len))\n",
    "        user_tokens = np.rollaxis(np.dstack(user_tokens), -1)\n",
    "        \n",
    "        user_categ_data = sequence.pad_sequences(user_categ_data, value=np.zeros(self.categ_dim))\n",
    "        user_categ_data = np.rollaxis(np.dstack(user_categ_data), -1\n",
    "                                     )\n",
    "        user_sparse_data = sequence.pad_sequences(user_sparse_data, value=np.zeros(self.sparse_dim))\n",
    "        user_sparse_data = np.rollaxis(np.dstack(user_sparse_data), -1)\n",
    "        \n",
    "        return ((user_tokens, user_categ_data, user_sparse_data),\n",
    "                np.array(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# TODO: it is skipping the last batch\n",
    "x_data = {'train': [], 'valid': [], 'test': []}\n",
    "y_data = {'train': [], 'valid': [], 'test': []}\n",
    "for set_type in ['train', 'valid', 'test']:\n",
    "    total_positive = 0\n",
    "    for x, y in DataGeneratorHierarchical(user_level_data, subjects_split, \n",
    "                                          set_type=set_type):\n",
    "        total_positive += pd.Series(y).sum()\n",
    "        x_data[set_type].append(x)\n",
    "        y_data[set_type].append(y)\n",
    "    logger.info(\"%d %s positive examples\\n\" % (total_positive, set_type))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(subjects_split[s]) for s in ['train', 'valid', 'test']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weights = class_weight.compute_class_weight('balanced',\n",
    "#                                                  np.unique(y_data['train']),\n",
    "#                                                  y_data['train'])\n",
    "# class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path, embedding_dim, voc):\n",
    "    # random matrix with mean value = 0\n",
    "    embedding_matrix = np.random.random((len(voc)+2, embedding_dim)) - 0.5 # voc + unk + pad value(0)\n",
    "\n",
    "    f = open(path)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_i = voc.get(word)\n",
    "        if word_i is not None:\n",
    "            embedding_matrix[word_i] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print('Total %s word vectors.' % len(embedding_matrix))\n",
    "\n",
    " \n",
    "    return embedding_matrix\n",
    "\n",
    "pretrained_embeddings_path = root_dir + '/resources/glove.twitter.27B/glove.twitter.27B.%dd.txt' % hyperparams_features['embedding_dim']\n",
    "embedding_matrix = load_embeddings(pretrained_embeddings_path, hyperparams_features['embedding_dim'], vocabulary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'lstm_units': 10,\n",
    "    'lstm_units_user': 100,\n",
    "    'dense_bow_units': 10,\n",
    "    'dropout': 0.0,\n",
    "    'l2_dense': 0.0000011,\n",
    "    'l2_embeddings': 0.000001,\n",
    "    'dense_sentence_units': 0,#50,\n",
    "    'dense_user_units': 50,\n",
    "    'optimizer': None,#'adam',\n",
    "    'decay': 0.00001,\n",
    "    'lr': 0.001,\n",
    "    \"trainable_embeddings\": True,\n",
    "    \"reduce_lr_factor\": 0.0002,\n",
    "    \"reduce_lr_patience\": 50,\n",
    "    \"freeze_patience\": 500,\n",
    "    'threshold': 0.5,\n",
    "    'ignore_layer': [],\n",
    "    'norm_momentum': 0.1,\n",
    "\n",
    "}\n",
    "if not hyperparams['optimizer']:\n",
    "    hyperparams['optimizer'] = optimizers.Adam(lr=hyperparams['lr'], #beta_1=0.9, beta_2=0.999, epsilon=0.0001,\n",
    "                                   decay=hyperparams['decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics():\n",
    "    def __init__(self, threshold=0.5):\n",
    "        self.threshold=threshold\n",
    "        \n",
    "    def recall_m(self, y_true, y_pred):\n",
    "            y_labels = y_true\n",
    "            y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), self.threshold), K.floatx())        \n",
    "            possible_positives = K.sum(K.round(K.clip(y_labels, 0, 1)))\n",
    "            true_positives = K.sum(K.round(K.clip(y_labels * y_pred, 0, 1)))\n",
    "            recall = true_positives / (possible_positives + K.epsilon())\n",
    "            return recall\n",
    "\n",
    "    def precision_m(self, y_true, y_pred):\n",
    "            y_labels = y_true\n",
    "            y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), self.threshold), K.floatx())        \n",
    "            true_positives = K.sum(K.round(K.clip(y_labels * y_pred, 0, 1)))\n",
    "            predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "            precision = true_positives / (predicted_positives + K.epsilon())\n",
    "            return precision\n",
    "\n",
    "    def f1_m(self, y_true, y_pred):\n",
    "        precision = self.precision_m(y_true, y_pred)\n",
    "        recall = self.recall_m(y_true, y_pred)\n",
    "        return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def binary_crossentropy_custom(y_true, y_pred):\n",
    "    y_labels = y_true\n",
    "    return K.binary_crossentropy(y_labels, \n",
    "                                 y_pred)\n",
    "\n",
    "metrics_class = Metrics(threshold=hyperparams['threshold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopwords_list,\n",
    "                liwc_categories,\n",
    "               ignore_layer=[]):\n",
    "\n",
    "    tokens_features = Input(shape=(hyperparams_features['maxlen'],), name='word_seq')\n",
    "    embedding_layer = Embedding(hyperparams_features['max_features'], \n",
    "                                hyperparams_features['embedding_dim'], \n",
    "                                input_length=hyperparams_features['maxlen'],\n",
    "                                embeddings_regularizer=regularizers.l2(hyperparams['l2_embeddings']),\n",
    "                                weights=[embedding_matrix], \n",
    "                                trainable=hyperparams['trainable_embeddings'],\n",
    "                               name='embeddings_layer')(\n",
    "        tokens_features)\n",
    "#     if 'batchnorm' not in ignore_layer:\n",
    "#         embedding_layer_norm = BatchNormalization(axis=1, momentum=hyperparams['norm_momentum'],\n",
    "#                                                      name='embeddings_layer_norm')(embedding_layer)\n",
    "#     lstm_layers = Bidirectional(LSTM(hyperparams['lstm_units']))(embedding_layer)\n",
    "\n",
    "    if tf.test.is_gpu_available():\n",
    "        lstm_layers = CuDNNLSTM(hyperparams['lstm_units'], \n",
    "                                return_sequences='attention' not in ignore_layer, # only True if using attention\n",
    "                      name='LSTM_layer')(embedding_layer)\n",
    "    else:\n",
    "        lstm_layers = LSTM(hyperparams['lstm_units'], \n",
    "                           return_sequences='attention' not in ignore_layer,\n",
    "                      name='LSTM_layer')(embedding_layer)\n",
    "    \n",
    "    # Attention\n",
    "    if 'attention' not in ignore_layer:\n",
    "        attention = Dense(1, activation='tanh', name='attention')(lstm_layers)\n",
    "        attention = Flatten()(attention)\n",
    "        attention = Activation('softmax')(attention)\n",
    "        attention = RepeatVector(hyperparams['lstm_units'])(attention)\n",
    "        attention = Permute([2, 1])(attention)\n",
    "\n",
    "        sent_representation = Multiply()([lstm_layers, attention])\n",
    "        sent_representation = Lambda(lambda xin: K.sum(xin, axis=1), \n",
    "                                     output_shape=(hyperparams['lstm_units'],)\n",
    "                                    )(sent_representation)\n",
    "\n",
    "        \n",
    "    else:\n",
    "        sent_representation = lstm_layers\n",
    "        \n",
    "    \n",
    "    sent_representation = Dropout(hyperparams['dropout'], name='lstm_att_dropout')(sent_representation)\n",
    "    if hyperparams['dense_sentence_units']:\n",
    "        sent_representation = Dense(units=hyperparams['dense_sentence_units'],\n",
    "                                   name='dense_sent_representation')(sent_representation)\n",
    "    numerical_features = Input(shape=(len(emotions) + 1 + len(liwc_categories),), name='numeric_input') # emotions and pronouns\n",
    "    dense_layer = Dense(units=1,\n",
    "                        kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                        name='numerical_dense_layer',\n",
    "                       )(numerical_features)\n",
    "    sparse_features = Input(shape=(len(stopwords_list),), name='sparse_input') # stopwords\n",
    "\n",
    "    dense_layer_sparse = Dense(units=hyperparams['dense_bow_units'],\n",
    "                              name='sparse_feat_dense_layer',\n",
    "                                kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                              )(sparse_features)\n",
    "    \n",
    "    if 'batchnorm' not in ignore_layer:\n",
    "        numerical_features_norm = BatchNormalization(axis=-1, momentum=hyperparams['norm_momentum'],\n",
    "                                                     name='numerical_features_norm')(numerical_features)\n",
    "        sent_representation_norm = BatchNormalization(axis=-1, momentum=hyperparams['norm_momentum'],\n",
    "                                                      name='sent_repr_norm')(sent_representation)\n",
    "        dense_layer_sparse_norm = BatchNormalization(axis=-1, momentum=hyperparams['norm_momentum'],\n",
    "                                                     name='sparse_features_norm')(dense_layer_sparse)\n",
    "        \n",
    "    subjects = Input(shape=(1,), name='subjects')\n",
    "    \n",
    "\n",
    "    all_layers = {\n",
    "        'lstm_layers': sent_representation,\n",
    "        'numerical_dense_layer': numerical_features,\n",
    "        'sparse_feat_dense_layer': dense_layer_sparse\n",
    "    }\n",
    "    if 'batchnorm' not in ignore_layer:\n",
    "        all_layers = {\n",
    "            'lstm_layers': sent_representation_norm,\n",
    "            'numerical_dense_layer': numerical_features_norm,\n",
    "            'sparse_feat_dense_layer': dense_layer_sparse_norm\n",
    "        }\n",
    "    layers_to_merge = []\n",
    "    for n, l in all_layers.items():\n",
    "        if n in ignore_layer:\n",
    "            continue\n",
    "        layers_to_merge.append(l)\n",
    "        \n",
    "    if len(layers_to_merge) == 1:\n",
    "        merged_layers = layers_to_merge[0]\n",
    "    else:\n",
    "        merged_layers = concatenate(layers_to_merge)\n",
    "    output_layer = Dense(1, activation='sigmoid',\n",
    "                         name='output_layer',\n",
    "                        kernel_regularizer=regularizers.l2(hyperparams['l2_dense']))(merged_layers)\n",
    "\n",
    "    # Compile model\n",
    "    model = Model(inputs=[tokens_features, numerical_features, sparse_features, subjects], \n",
    "                  outputs=output_layer)\n",
    "\n",
    "    model.compile(hyperparams['optimizer'], binary_crossentropy_custom,\n",
    "                  metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hierarchical_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopwords_list,\n",
    "                liwc_categories,\n",
    "               ignore_layer=[]):\n",
    "\n",
    "    # Post/sentence representation - word sequence\n",
    "    tokens_features = Input(shape=(hyperparams_features['maxlen'],), name='word_seq')\n",
    "    embedding_layer = Embedding(hyperparams_features['max_features'], \n",
    "                                hyperparams_features['embedding_dim'], \n",
    "                                input_length=hyperparams_features['maxlen'],\n",
    "                                embeddings_regularizer=regularizers.l2(hyperparams['l2_embeddings']),\n",
    "                                weights=[embedding_matrix], \n",
    "                                trainable=hyperparams['trainable_embeddings'],\n",
    "                               name='embeddings_layer')(\n",
    "        tokens_features)\n",
    "\n",
    "    if tf.test.is_gpu_available():\n",
    "        lstm_layers = CuDNNLSTM(hyperparams['lstm_units'], \n",
    "                                return_sequences='attention' not in ignore_layer, # only True if using attention\n",
    "                      name='LSTM_layer')(embedding_layer)\n",
    "    else:\n",
    "        lstm_layers = LSTM(hyperparams['lstm_units'], \n",
    "                           return_sequences='attention' not in ignore_layer,\n",
    "                      name='LSTM_layer')(embedding_layer)\n",
    "    \n",
    "    # Attention\n",
    "    if 'attention' not in ignore_layer:\n",
    "        attention = Dense(1, activation='tanh', name='attention')(lstm_layers)\n",
    "        attention = Flatten()(attention)\n",
    "        attention = Activation('softmax')(attention)\n",
    "        attention = RepeatVector(hyperparams['lstm_units'])(attention)\n",
    "        attention = Permute([2, 1])(attention)\n",
    "\n",
    "        sent_representation = Multiply()([lstm_layers, attention])\n",
    "        sent_representation = Lambda(lambda xin: K.sum(xin, axis=1), \n",
    "                                     output_shape=(hyperparams['lstm_units'],)\n",
    "                                    )(sent_representation)       \n",
    "    else:\n",
    "        sent_representation = lstm_layers\n",
    "    \n",
    "    if 'batchnorm' not in ignore_layer:\n",
    "        sent_representation = BatchNormalization(axis=-1, momentum=hyperparams['norm_momentum'],\n",
    "                                                          name='sent_repr_norm')(sent_representation)\n",
    "    sent_representation = Dropout(hyperparams['dropout'], name='lstm_att_dropout')(sent_representation)\n",
    "    \n",
    "\n",
    "    # Hierarchy\n",
    "    sentEncoder = Model(inputs=tokens_features, outputs=sent_representation)\n",
    "    sentEncoder.summary()\n",
    "\n",
    "    posts_history_input = Input(shape=(hyperparams_features['posts_per_user'], \n",
    "                                 hyperparams_features['maxlen']\n",
    "                                      ), name='hierarchical_word_seq_input')\n",
    "\n",
    "    user_encoder = TimeDistributed(sentEncoder, name='user_encoder')(posts_history_input)\n",
    "\n",
    "    # Other features \n",
    "    numerical_features_history = Input(shape=(\n",
    "            hyperparams_features['posts_per_user'],\n",
    "            len(emotions) + 1 + len(liwc_categories)\n",
    "        ), name='numeric_input_hist') # emotions and pronouns\n",
    "    sparse_features_history = Input(shape=(\n",
    "            hyperparams_features['posts_per_user'],\n",
    "            len(stopwords_list)\n",
    "        ), name='sparse_input_hist') # stopwords\n",
    "    \n",
    "    \n",
    "    dense_layer_sparse = Dense(units=hyperparams['dense_bow_units'],\n",
    "                              name='sparse_feat_dense_layer',\n",
    "                                kernel_regularizer=regularizers.l2(hyperparams['l2_dense']),\n",
    "                              )\n",
    "    dense_layer_sparse_user = TimeDistributed(dense_layer_sparse)(sparse_features_history)\n",
    "\n",
    "    \n",
    "    # Concatenate features\n",
    "    if 'batchnorm' not in ignore_layer:\n",
    "        numerical_features_history_norm = BatchNormalization(axis=-1, momentum=hyperparams['norm_momentum'],\n",
    "                                                     name='numerical_features_norm')(numerical_features_history)\n",
    "        dense_layer_sparse_user = BatchNormalization(axis=-1, momentum=hyperparams['norm_momentum'],\n",
    "                                                     name='sparse_features_norm')(dense_layer_sparse_user)\n",
    "    all_layers = {\n",
    "        'lstm_layers': user_encoder,\n",
    "        'numerical_dense_layer': numerical_features_history if 'batchnorm' in ignore_layer else numerical_features_history_norm,\n",
    "        'sparse_feat_dense_layer': dense_layer_sparse_user\n",
    "    }\n",
    "    \n",
    "    layers_to_merge = [l for n,l in all_layers.items() if n not in ignore_layer]\n",
    "    if len(layers_to_merge) == 1:\n",
    "        merged_layers = layers_to_merge[0]\n",
    "    else:\n",
    "        merged_layers = concatenate(layers_to_merge)\n",
    "    \n",
    "    if tf.test.is_gpu_available():\n",
    "        lstm_user_layers = CuDNNLSTM(hyperparams['lstm_units_user'], \n",
    "                                return_sequences='attention_user' not in ignore_layer, # only True if using attention\n",
    "                      name='LSTM_layer_user')(merged_layers)\n",
    "    else:\n",
    "        lstm_user_layers = LSTM(hyperparams['lstm_units_user'], \n",
    "                           return_sequences='attention_user' not in ignore_layer,\n",
    "                      name='LSTM_layer_user')(merged_layers)\n",
    "    \n",
    "    # Attention\n",
    "    if 'attention' not in ignore_layer:\n",
    "        attention_user = Dense(1, activation='tanh', name='attention_user')(lstm_user_layers)\n",
    "        attention_user = Flatten()(attention_user)\n",
    "        attention_user = Activation('softmax')(attention_user)\n",
    "        attention_user = RepeatVector(hyperparams['lstm_units_user'])(attention_user)\n",
    "        attention_user = Permute([2, 1])(attention_user)\n",
    "\n",
    "        user_representation = Multiply()([lstm_user_layers, attention_user])\n",
    "        user_representation = Lambda(lambda xin: K.sum(xin, axis=1), \n",
    "                                     output_shape=(hyperparams['lstm_units_user'],)\n",
    "                                    )(user_representation)     \n",
    "    else:\n",
    "        user_representation = lstm_user_layers\n",
    "    \n",
    "    user_representation = Dropout(hyperparams['dropout'], name='lstm_att_dropout_user')(user_representation)\n",
    "    \n",
    "    \n",
    "    if hyperparams['dense_user_units']:\n",
    "        user_representation = Dense(units=hyperparams['dense_user_units'],\n",
    "                                   name='dense_user_representation')(user_representation)\n",
    "    \n",
    "    # TODO: concatenate before hierarchy? (include all features in th hierarchy)\n",
    "    \n",
    "    output_layer = Dense(1, activation='sigmoid',\n",
    "                         name='output_layer',\n",
    "                        kernel_regularizer=regularizers.l2(hyperparams['l2_dense']))(user_representation)\n",
    "\n",
    "    # Compile model\n",
    "    hierarchical_model = Model(inputs=[posts_history_input, numerical_features_history, sparse_features_history], \n",
    "                  outputs=output_layer)\n",
    "    hierarchical_model.summary()\n",
    "    \n",
    "    hierarchical_model.compile(hyperparams['optimizer'], binary_crossentropy_custom,\n",
    "                  metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m])\n",
    "    return hierarchical_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(hyperparams, hyperparams_features, embedding_matrix, emotions, stopword_list,\n",
    "                    liwc_categories=[c for c in categories if c in writings_df.columns]\n",
    ",\n",
    "                   ignore_layer=hyperparams['ignore_layer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, 'models/hierarchical_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(api_key=\"eoBdVyznAhfg3bK9pZ58ZSXfv\",\n",
    "                        project_name=\"mental\", workspace=\"ananana\", disabled=True)\n",
    "\n",
    "experiment.log_parameters(hyperparams_features)\n",
    "\n",
    "experiment.log_parameter('emotion_lexicon', nrc_lexicon_path)\n",
    "experiment.log_parameter('emotions', emotions)\n",
    "experiment.log_parameter('embeddings_path', pretrained_embeddings_path)\n",
    "if 'subset' in writings_df.columns:\n",
    "    experiment.add_tag('anorexia')\n",
    "\n",
    "experiment.log_parameters(hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightsHistory(callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.log_weights(0)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % 10 == 0:\n",
    "            self.log_weights(epoch)\n",
    "        \n",
    "    def log_weights(self, step):\n",
    "        for layer in model.layers:\n",
    "            try:\n",
    "                experiment.log_histogram_3d(layer.get_weights()[0], \n",
    "                                            name=layer.name, step=step)\n",
    "            except Exception as e:\n",
    "#                 logger.debug(\"Logging weights error: \" + str(e) + \"\\n\")\n",
    "                # Layer probably does not exist\n",
    "                pass\n",
    "\n",
    "\n",
    "class FreezeLayer(callbacks.Callback):\n",
    "    def __init__(self, logs={}, patience=5, layer={'user_encoder':'embeddings_layer'}, verbose=1, set_to=False):\n",
    "        super(FreezeLayer, self).__init__()\n",
    "        self.freeze_epoch = patience\n",
    "        self.freeze_layer = layer\n",
    "        self.verbose = verbose\n",
    "        self.set_to = set_to\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        if type(self.freeze_layer)==dict:\n",
    "            submodel = model.get_layer(list(self.freeze_layer.keys())[0])\n",
    "        else:\n",
    "            submodel = model\n",
    "        logging.debug(\"Trainable embeddings\", submodel.get_layer(self.freeze_layer).trainable)\n",
    "        if epoch == self.freeze_epoch:\n",
    "            try:\n",
    "                layer = submodel.get_layer(self.freeze_layer)\n",
    "                old_value = layer.trainable\n",
    "                layer.trainable = self.set_to\n",
    "                # TODO: does this reset the optimizer? should I also compile the top-level model?\n",
    "                model.compile(hyperparams['optimizer'], binary_crossentropy_custom,\n",
    "                  metrics=[metrics_class.f1_m, metrics_class.precision_m, metrics_class.recall_m])\n",
    "                if self.verbose:\n",
    "                    logging.debug(\"Setting %s layer from %s to trainable=%s...\\n\" % (layer.name, old_value,\n",
    "                                                                   submodel.get_layer(self.freeze_layer).trainable))\n",
    "            except Exception as e:\n",
    "                # layer probably does not exist\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_patience=50\n",
    "def train_model(model, \n",
    "                data_generator_train, data_generator_valid,\n",
    "                epochs, class_weight, start_epoch=0, workers=4,\n",
    "                callback_list = [],\n",
    "                model_path='/tmp/model',\n",
    "               verbose=1):\n",
    "    logging.info('Train...')\n",
    "    experiment.log_parameter('class_weight', class_weight.values())\n",
    "    experiment.log_parameter('callbacks', callbacks)\n",
    "\n",
    "    history = model.fit_generator(data_generator_train,\n",
    "                steps_per_epoch=100,\n",
    "              epochs=epochs, initial_epoch=start_epoch, \n",
    "              class_weight=class_weight,\n",
    "              validation_data=data_generator_valid,\n",
    "                        verbose=verbose,\n",
    "#               validation_split=0.3,\n",
    "                       workers=workers,\n",
    "            callbacks = [\n",
    "                callbacks.ModelCheckpoint(filepath='%s_best' % model_path, verbose=1, \n",
    "                                          save_best_only=True),\n",
    "                callbacks.EarlyStopping(patience=early_stopping_patience), *callback_list\n",
    "            ])\n",
    "    model.save(model_path)\n",
    "    experiment.log_parameter('model_path', model_path)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "freeze_layer = FreezeLayer(patience=hyperparams['freeze_patience'], set_to=not hyperparams['trainable_embeddings'])\n",
    "weights_history = WeightsHistory()\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=hyperparams['reduce_lr_factor'],\n",
    "                          patience=hyperparams['reduce_lr_patience'], min_lr=0.000001, verbose=1)\n",
    "data_generator_train = DataGeneratorHierarchical(user_level_data, subjects_split, set_type='train',\n",
    "                                                max_posts_per_user=hyperparams_features['posts_per_user'])\n",
    "data_generator_valid = DataGeneratorHierarchical(user_level_data, subjects_split, set_type='valid',\n",
    "                                                max_posts_per_user=hyperparams_features['posts_per_user'])\n",
    "# Note: FreezeLayer callback doesn't work with hierarchical architecture\n",
    "model, history = train_model(model, data_generator_train, data_generator_valid,\n",
    "                       epochs=100,\n",
    "                      class_weight={0:0.5, 1:5}, start_epoch=0,\n",
    "                      callback_list = [],#[weights_history, reduce_lr],\n",
    "                      model_path='models/ham_user_selfharm_seq1', workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: properly extract the test data without sampling\n",
    "model.evaluate(DataGenerator(user_level_data, subjects_split, set_type='test', max_posts_per_user=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies = {\n",
    "    'f1_m': metrics_class.f1_m,\n",
    "    'precision_m': metrics_class.precision_m,\n",
    "    'recall_m': metrics_class.recall_m,\n",
    "    'binary_crossentropy_custom': binary_crossentropy_custom\n",
    "}\n",
    "model = load_model(model_path, custom_objects=dependencies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([v for v in model.get_layer('attention').get_weights()[0].flatten()]).rolling(50).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([abs(v) for v in model.get_layer('output_layer').get_weights()[0].flatten()]).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    (e, 'nrc') for e in emotions] + ['pers_pronouns'] + [\n",
    "    (c, 'liwc') for c in list(categories) if c in writings_df.columns] + [\n",
    "(st, 'stopword') for st in stopword_list]\n",
    "weights = model.get_layer('output_layer').get_weights()[0].tolist()[-(len(features)):]\n",
    "\n",
    "print(len(weights), len(features))\n",
    "feature_importance = {}\n",
    "for (i, f) in enumerate(features):\n",
    "    feature_importance[f] = weights[i][0]\n",
    "\n",
    "sorted(feature_importance.items(), key=lambda t: abs(t[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_point(subject, voc, hyperparams_features=hyperparams_features, nrc_lexicon=nrc_lexicon,\n",
    "                      emotions=emotions):\n",
    "    eval_writings_df = writings_df[writings_df['subject']==subject]\n",
    "    correct_label = eval_writings_df.label.values[0]\n",
    "    (x_train, y_train), (x_valid, y_valid), (x_test, y_test), voc = load_erisk_data(eval_writings_df,\n",
    "                        seq_len=hyperparams_features['maxlen'],\n",
    "                        voc_size=hyperparams_features['max_features'],\n",
    "                        emotion_lexicon=nrc_lexicon,\n",
    "                        emotions=emotions, user_level=False,\n",
    "                        train_prop=0.0, vocabulary=voc)\n",
    "    return x_test, y_test, correct_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_per_user(writings_df, majority_prop=0.2, train_prop=0.7, majority_nr=0, validate=False, voc=None,\n",
    "                    random=False, nr_slices=5, test_slice=2):\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    thresh=0.5\n",
    "    majority_proportion=majority_prop\n",
    "    valid_prop = 0.3\n",
    "    \n",
    "    if 'subset' in writings_df.columns:\n",
    "        training_subjects = list(set(writings_df[writings_df['subset']=='train'].subject))\n",
    "        test_subjects = list(set(writings_df[writings_df['subset']=='test'].subject))\n",
    "    else:\n",
    "        all_subjects = sorted(list(set(writings_df.subject)))\n",
    "        training_subjects_size = int(len(all_subjects) * train_prop)\n",
    "        test_subjects_size = len(all_subjects) - training_subjects_size\n",
    "        # Cross-validation, with fixed slice as input\n",
    "        test_prop = 1-train_prop\n",
    "        test_slice = min(test_slice, nr_slices)\n",
    "        logger.debug(\"start index: %f, from %f\\n\" % (\n",
    "            len(all_subjects)*(1/nr_slices)*test_slice, test_prop*test_slice))\n",
    "        start_slice = int(len(all_subjects)*(1/nr_slices)*test_slice)\n",
    "        test_subjects = all_subjects[start_slice: start_slice+test_subjects_size]\n",
    "        training_subjects = [s for s in all_subjects if s not in test_subjects]\n",
    "    training_subjects = sorted(training_subjects) # ensuring reproducibility\n",
    "    valid_subjects_size = int(len(training_subjects) * valid_prop)\n",
    "    valid_subjects = training_subjects[:valid_subjects_size]\n",
    "    training_subjects = training_subjects[valid_subjects_size:]\n",
    "    \n",
    "    if validate:\n",
    "        subjects = valid_subjects\n",
    "    else:\n",
    "        subjects = test_subjects\n",
    "    for subject in subjects:\n",
    "        x_test_user, y_test_user, label = get_data_for_point(subject, voc=voc)\n",
    "        outputs = model.predict(x_test_user)\n",
    "        if random:\n",
    "            sigma = np.std(outputs)\n",
    "            mu = np.mean(outputs)\n",
    "            print(\"generating random outputs with sigma\", sigma, \"and mu\", mu)\n",
    "            outputs = sigma*np.random.randn(len(outputs))+mu\n",
    "        positive_pred = sum(outputs>=thresh)\n",
    "        negative_pred = sum(outputs<thresh)\n",
    "        majority_pred = 0\n",
    "        if majority_proportion and positive_pred >= majority_proportion*negative_pred:\n",
    "            majority_pred = 1\n",
    "        if majority_nr and positive_pred>=majority_nr:\n",
    "            majority_pred = 1\n",
    "        if label == 1:\n",
    "            if majority_pred == 1:\n",
    "                tp+=1\n",
    "            else:\n",
    "                fn+=1\n",
    "        else:\n",
    "            if majority_pred == 0:\n",
    "                tn+=1\n",
    "            else:\n",
    "                fp+=1\n",
    "        print(negative_pred, positive_pred, majority_pred)\n",
    "        all_predictions.append(majority_pred)\n",
    "        all_labels.append(label)\n",
    "    def prec_recall_f1(tp, fp, tn, fn):\n",
    "        recall = tp/(tp+fn+0.0000001)\n",
    "        precision = tp/(tp+fp+0.0000001)\n",
    "        f1 = 2*precision*recall/(precision+recall+0.0000001)\n",
    "        print(\"Recall\", recall, \"Precision\", precision, \"F1\", f1)\n",
    "    if majority_prop:\n",
    "        print(\"Vote proportion\", majority_prop)\n",
    "    if majority_nr:\n",
    "        print(\"Vote points\", majority_nr)\n",
    "    prec_recall_f1(tp, fp, tn, fn)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_per_user(writings_df=writings_df, voc=voc, majority_prop=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_per_slice = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nr_slices=5\n",
    "logger.setLevel(logging.INFO)\n",
    "for tslice in range(nr_slices): \n",
    "    (x_train, y_train), (x_valid, y_valid), (x_test, y_test), voc = load_erisk_data(writings_df, \n",
    "                                                                seq_len=hyperparams_features['maxlen'],\n",
    "                                                                voc_size=hyperparams_features['max_features'],\n",
    "                                                               emotion_lexicon=nrc_lexicon,\n",
    "                                                               emotions=emotions,\n",
    "                                                               user_level=hyperparams_features['user_level'],\n",
    "                                                                                    test_slice=tslice,\n",
    "                                                                                    nr_slices=nr_slices,\n",
    "    #                                                            vocabulary=pickle.load(open('vocabulary20K_selfharm.pkl', 'rb'))\n",
    "                                                                                   logger=logger)\n",
    "    model, history = train_model(model, x_train, y_train, x_valid, y_valid,\n",
    "           epochs=200, batch_size=hyperparams['batch_size'],\n",
    "                      class_weight={0:0.5, 1:5}, start_epoch=0,\n",
    "                      callback_list = [freeze_layer, weights_history, reduce_lr],\n",
    "                      workers=2, verbose=0)\n",
    "    results_per_slice[tslice] = model.evaluate(x_test, y_test)\n",
    "    logger.info(\"Results for slice %d: %s\\n\" % (tslice, results_per_slice[tslice]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average F1 score: \", np.array([results_per_slice[s][1] for s in results_per_slice.keys()]).mean(),\n",
    "     \"all F1 scores: \", {s: v[1] for (s,v) in results_per_slice.items()} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tokens(row):\n",
    "    tokens = []\n",
    "    if row.tokenized_text:\n",
    "        tokens += row.tokenized_text\n",
    "    if row.tokenized_title:\n",
    "        tokens += row.tokenized_title\n",
    "    return tokens\n",
    "writings_df['all_tokens'] = writings_df.apply (lambda row: merge_tokens(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: include the title\n",
    "def extract_emotions(tokens, emotion, relative=True):\n",
    "    if not tokens:\n",
    "        return None\n",
    "    emotion_words = [t for t in tokens \n",
    "                     if t in nrc_lexicon[emotion]]\n",
    "    if relative:\n",
    "        return len(emotion_words) / len(tokens)\n",
    "    else:\n",
    "        return len(emotion_words)\n",
    "    \n",
    "    return encoded_emotions\n",
    "\n",
    "from functools import partial\n",
    "for emotion in emotions:\n",
    "    writings_df[emotion] = writings_df['all_tokens'].apply(partial(extract_emotions, emotion=emotion, \n",
    "                                                                   relative=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['pronouns'] = writings_df['all_tokens'].apply(partial(encode_pronouns, relative=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len'] + emotions].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len'] + emotions].groupby('label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentAnalyzer, SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid.polarity_scores(\"We are here today happiness is all around\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['neg_vader'] = writings_df.text.apply(lambda t: sid.polarity_scores(t)['neg']\n",
    "                                                 if type(t)==str else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df['pos_vader'] = writings_df.text.apply(lambda t: sid.polarity_scores(t)['pos']\n",
    "                                                 if type(t)==str else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len', 'neg_vader', 'pos_vader'] + emotions].groupby('label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df[['text', 'label', 'pronouns', 'text_len', 'neg_vader', 'pos_vader'] + emotions].corr('spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from liwc_readDict import readDict\n",
    "\n",
    "liwc = readDict('/home/ana/resources/FakeOrFact/features/LIWC/LIWC/liwc.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [c for (w,c) in liwc]\n",
    "set(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_dict = {}\n",
    "for (w, c) in liwc:\n",
    "    if c not in liwc_dict:\n",
    "        liwc_dict[c] = []\n",
    "    liwc_dict[c].append(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_dict['pronoun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_liwc_categories(tokens, category_words, relative=True):\n",
    "    category_cnt = 0\n",
    "    if not tokens:\n",
    "        return None\n",
    "    text_len = len(tokens)\n",
    "    for t in tokens:\n",
    "        for word in category_words:\n",
    "            if t==word or (word[-1]=='*' and t.startswith(word[:-1])) \\\n",
    "            or (t==word.split(\"'\")[0]):\n",
    "                category_cnt += 1\n",
    "                break # one token cannot belong to more than one word in the category\n",
    "    if relative:\n",
    "        return category_cnt/text_len\n",
    "    else:\n",
    "        return category_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from functools import partial\n",
    "# for categ in ['negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']:#liwc_dict.keys():\n",
    "for categ in liwc_dict.keys():\n",
    "    if categ in writings_df.columns:\n",
    "        continue\n",
    "    print(\"Encoding for category %s...\" % categ)\n",
    "    writings_df[categ] = writings_df['all_tokens'].apply(partial(encode_liwc_categories, \n",
    "                                                                   category_words=liwc_dict[categ], \n",
    "                                                                   relative=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df.groupby('subject').mean()[['label', 'negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df[['label', 'negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']].groupby('label').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df.groupby('subject').mean()[['label'] + categories].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare your hyperparameters search:\n",
    "tune_epochs=150\n",
    "config = {\n",
    "      \"algorithm\": \"random\",\n",
    "      \"parameters\": {\n",
    "          \"lstm_units\": {\"type\": \"integer\", \"min\": 10, \"max\": 1000},\n",
    "          \"dense_bow_units\": {\"type\": \"integer\", \"min\": 1, \"max\": 50},\n",
    "          \"lr\": {\"type\": \"float\", \"min\": 0.00001, \"max\": 0.5, \"scalingType\": \"loguniform\"},\n",
    "          \"l2_dense\": {\"type\": \"float\", \"min\": 0.0000001, \"max\": 0.05, \"scalingType\": \"loguniform\"},\n",
    "          \"l2_embeddings\": {\"type\": \"float\", \"min\": 0.0000001, \"max\": 0.05, \"scalingType\": \"loguniform\"},\n",
    "          \"dropout\": {\"type\": \"float\", \"min\": 0, \"max\": 0.7, \"scalingType\": \"uniform\"},\n",
    "          \"norm_momentum\": {\"type\": \"float\", \"min\": 0.01, \"max\": 0.99, \"scalingType\": \"uniform\"},\n",
    "          \"optimizer\": {\"type\": \"categorical\", \"values\": [\"adam\", \"adagrad\", \"\"]},\n",
    "          \"batch_size\": {\"type\": \"integer\", \"min\": 10, \"max\": 512, \"scalingType\": \"loguniform\"},\n",
    "          \"positive_class_weight\": {\"type\": \"integer\", \"min\": 1, \"max\": 25},\n",
    "          \"trainable_embeddings\": {\"type\": \"discrete\", \"values\": [True, False]},\n",
    "          \"freeze_patience\": {\"type\": \"integer\", \"min\": 2, \"max\": tune_epochs+1},\n",
    "          \"lr_reduce_factor\": {\"type\": \"float\", \"min\": 0.0001, \"max\": 0.8},\n",
    "          \"lr_reduce_patience\": {\"type\": \"integer\", \"min\": 2, \"max\": tune_epochs+1},\n",
    "          \"decay\": {\"type\": \"float\", \"min\": 0.00000001, \"max\": 0.5, \"scalingType\": \"loguniform\"},\n",
    "          \"ignore_layers_values\": {\"type\": \"categorical\", \"values\": [\"attention\", \"batchnorm\", \"\"]}\n",
    "      },\n",
    "      \"spec\": {\n",
    "          \"metric\": \"loss\",\n",
    "          \"objective\": \"minimize\",\n",
    "      },\n",
    "  }\n",
    "optimizer = Optimizer(config, api_key=\"eoBdVyznAhfg3bK9pZ58ZSXfv\")\n",
    "\n",
    "for experiment in optimizer.get_experiments(project_name=\"mental\"):\n",
    "    experiment.add_tag(\"tune\")\n",
    "    \n",
    "    # Test the model\n",
    "    hyperparams_config = {\n",
    "        param: experiment.get_parameter(param) for param in config['parameters'].keys()}\n",
    "    if not hyperparams_config['optimizer']:\n",
    "        hyperparams_config['optimizer'] = optimizers.Adam(lr=hyperparams_config['lr'], \n",
    "                                   decay=hyperparams_config['decay'])\n",
    "    hyperparams_config[\"ignore_layers\"] = []\n",
    "    if hyperparams_config[\"ignore_layers_values\"]:\n",
    "        hyperparams_config[\"ignore_layers\"] = [hyperparams_config[\"ignore_layers_values\"]]\n",
    "    model = build_model(hyperparams=hyperparams_config,\n",
    "                        hyperparams_features=hyperparams_features, \n",
    "                        embedding_matrix=embedding_matrix, emotions=emotions,\n",
    "                       stopwords_list=stopword_list, liwc_categories=categories)\n",
    "    freeze_layer = FreezeLayer(patience=experiment.get_parameter('freeze_patience'),\n",
    "                              set_to=not experiment.get_parameter('trainable_embeddings'))\n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            factor=experiment.get_parameter('lr_reduce_factor'),\n",
    "                                            patience=experiment.get_parameter('lr_reduce_patience'), \n",
    "                                            min_lr=0.000001, verbose=1)\n",
    "    model, history = train_model(model, \n",
    "            x_train, y_train, x_test, y_test,\n",
    "            epochs=tune_epochs, batch_size=experiment.get_parameter('batch_size'),\n",
    "                      class_weight={0:1, 1:experiment.get_parameter('positive_class_weight')}, \n",
    "                          workers=2,\n",
    "                          callback_list = [freeze_layer, reduce_lr],\n",
    "                      model_path='models/experiment')\n",
    "    loss = history.history['loss'][-1]\n",
    "    \n",
    "    # Report the loss, if not auto-logged:\n",
    "    experiment.log_metric(\"loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2conda",
   "language": "python",
   "name": "tf2conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
