{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob, os\n",
    "import numpy as np\n",
    "from comet_ml import Experiment, Optimizer\n",
    "import pickle\n",
    "import logging\n",
    "import sys\n",
    "from sklearn.utils import class_weight\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TF_KERAS'] = '1'\n",
    "\n",
    "# only reserve 1 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# tf.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Lambda, BatchNormalization, TimeDistributed, \\\n",
    "     Bidirectional, Input, concatenate, Flatten, RepeatVector, Activation, Multiply, Permute#, CuDNNLSTM\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import callbacks, optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model, Sequence\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer, TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_seed = 1234\n",
    "# tf.set_random_seed(my_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('training')\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_subject_writings(subject_file):\n",
    "    writings = []\n",
    "    with open(subject_file) as sf:\n",
    "        contents = sf.read()\n",
    "        contents = clean_content(contents)\n",
    "#         try:\n",
    "#         print(contents[:500])\n",
    "\n",
    "        root = ET.fromstring(contents)\n",
    "#         except:\n",
    "#             print('Cannot extract text', contents[:500], '\\n-------\\n')\n",
    "            \n",
    "        try:\n",
    "            subject = root.findall('ID')[0].text.strip()\n",
    "        except Exception:\n",
    "            print('Cannot extract ID', contents[:500], '\\n-------\\n')        \n",
    "        for w in root.iter('WRITING'):\n",
    "            subject_writings = {'subject': subject}\n",
    "            for title in w.findall('TITLE'):\n",
    "                subject_writings['title'] = title.text\n",
    "            for text in w.findall('TEXT'):\n",
    "                subject_writings['text'] = text.text\n",
    "            for date in w.findall('DATE'):\n",
    "                subject_writings['date'] = date.text\n",
    "            writings.append(subject_writings)\n",
    "    return writings\n",
    "def clean_content(orig_content):\n",
    "    WINDOWS_LINE_ENDING = b'\\r\\n'\n",
    "    UNIX_LINE_ENDING = b'\\n'\n",
    "    byte_content = orig_content.encode(\"UTF-8\")\n",
    "    byte_content = byte_content.replace(WINDOWS_LINE_ENDING, UNIX_LINE_ENDING)\n",
    "    content = byte_content.decode(\"UTF-8\")\n",
    "#     content = re.sub(\"^<\", \"^\\<\", orig_content)    \n",
    "    content = re.sub(\"&\", \"&amp;\", content)\n",
    "    content = re.sub(\"^>\", \"&gt;\", content)    \n",
    "#     content = re.sub(\">$\", \"\\>$\", content)    \n",
    "    content = re.sub(\"<$\", \"&lt;\", content)\n",
    "    content = re.sub(\"<\\n\", \"&lt;\\n\", content)\n",
    "    content = re.sub(\"< \", \"&lt; \", content)\n",
    "    content = re.sub(\"<(?=\\d)\", \"&lt;\", content)\n",
    "    content = re.sub(\"<(?=[^\\w\\\\/])\", \"&lt;\", content)\n",
    "    content = re.sub(\"<(?=[^A-Z\\\\/])\", \"&lt;\", content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_content(\"> hello& there<\\n <~23l <TEXT/> lala\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dir = '/home/anasab/' \n",
    "root_dir = '/home/anasab/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir_T2 = root_dir + '/eRisk/data/eRisk2020_T2/eRisk2020_T2_TEST_DATA/'\n",
    "datadir_test_T2 = root_dir + 'eRisk/data/eRISK2021_T3_training_data/training BDI/2020 data'\n",
    "datadir_test2021 = root_dir + 'eRisk/data/eRisk2021_T3_Collection'\n",
    "# labels_file_T2 = root_dir + '/eRisk/data/eRisk2020_T2/eRisk2020_T2_TRAINING_DATA/Depression Questionnaires_anon.txt'\n",
    "labels_file_test_T2 = root_dir + 'eRisk//data/eRISK2021_T3_training_data/training BDI/2020 data/Depression Questionnaires_anon.txt'\n",
    "nr_questions = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts(datadir_T2,\n",
    "                labels_file_T2=None):\n",
    "    writings = []\n",
    "    writings_df = pd.DataFrame()\n",
    "    labels_df = pd.DataFrame()\n",
    "\n",
    "        \n",
    "    for subject_file in os.listdir(datadir_T2):\n",
    "        if not subject_file.endswith('xml'):\n",
    "            continue\n",
    "#         try:\n",
    "        print(subject_file)\n",
    "        writings.extend(read_subject_writings(os.path.join(datadir_T2, subject_file)))\n",
    "#         except:\n",
    "#             print(\"Couldn't parse\", subject_file)\n",
    "    writings_df = pd.DataFrame(writings)\n",
    "    if labels_file_T2:\n",
    "        labels_df = pd.read_csv(os.path.join(labels_file_T2), \n",
    "                                     delimiter='\\s+', names=['subject'] + ['label%i' % i for i in range(nr_questions)])\n",
    "\n",
    "        labels_df = labels_df.set_index('subject')\n",
    "\n",
    "        writings_df = writings_df.join(labels_df, on='subject')\n",
    "    \n",
    "    return writings_df, labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df_test, labels_df_test = read_texts(datadir_test_T2, labels_file_test_T2)\n",
    "writings_df_test2021, _ = read_texts(datadir_test2021)\n",
    "writings_df = pickle.load(open('data/writings_df_T2_liwc.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df = pd.concat([writings_df, writings_df_test])\n",
    "writings_df.groupby('subject').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df_test2021.groupby('subject').count()\n",
    "# writings_df_test2021\n",
    "# pickle.dump(writings_df_test2021, open('data/writings_df_t3_2021_test_liwc.pkl', 'wb+'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def tokenize(t):\n",
    "    return tokenizer.tokenize(t.lower())\n",
    "tt = TweetTokenizer()\n",
    "sw = stopwords.words(\"english\")\n",
    "def tokenize_tweets(t, tokenizer=tt, stop=True):\n",
    "    tokens = tokenizer.tokenize(t.lower())\n",
    "    tokens_clean = [token for token in tokens if \n",
    "                            re.match(\"^[a-z]*$\", token)]\n",
    "    if not stop:\n",
    "        tokens_clean = [token for token in tokens_clean \n",
    "                        if token not in sw]\n",
    "    return tokens_clean\n",
    "\n",
    "def tokenize_fields(writings_df):\n",
    "    writings_df['tokenized_title'] = writings_df['title'].apply(lambda t: tokenize_tweets(t) \n",
    "                                                                if type(t)==str and t else None)\n",
    "    writings_df['title_len'] = writings_df['tokenized_title'].apply(lambda t: len(t) \n",
    "                                                                    if type(t)==list and t else None)\n",
    "    writings_df['tokenized_text'] = writings_df['text'].apply(lambda t: tokenize_tweets(t) \n",
    "                                                              if type(t)==str and t else None)\n",
    "    writings_df['text_len'] = writings_df['tokenized_text'].apply(lambda t: len(t) \n",
    "                                                                  if type(t)==list and t else None)\n",
    "    return writings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df_test2021 = tokenize_fields(writings_df_test2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tokens(row):\n",
    "    tokens = []\n",
    "    if row.tokenized_text:\n",
    "        tokens += row.tokenized_text\n",
    "    if row.tokenized_title:\n",
    "        tokens += row.tokenized_title\n",
    "    return tokens\n",
    "writings_df_test2021['all_tokens'] = writings_df_test2021.apply (lambda row: merge_tokens(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df_test2021 = tokenize_fields(writings_df_test2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: include the title\n",
    "def extract_emotions(tokens, emotion, relative=True):\n",
    "    if not tokens:\n",
    "        return None\n",
    "    emotion_words = [t for t in tokens \n",
    "                     if t in nrc_lexicon[emotion]]\n",
    "    if relative:\n",
    "        return len(emotion_words) / len(tokens)\n",
    "    else:\n",
    "        return len(emotion_words)\n",
    "    \n",
    "    return encoded_emotions\n",
    "\n",
    "from functools import partial\n",
    "for emotion in emotions:\n",
    "    writings_df_test2021[emotion] = writings_df_test2021['all_tokens'].apply(partial(extract_emotions, emotion=emotion, \n",
    "                                                                   relative=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df_test2021['pronouns'] = writings_df_test2021['all_tokens'].apply(partial(encode_pronouns, relative=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from liwc_readDict import readDict\n",
    "\n",
    "liwc = readDict('/home/anasab/resources/liwc.dic')\n",
    "categories = [c for (w,c) in liwc]\n",
    "set(categories)\n",
    "liwc_dict = {}\n",
    "for (w, c) in liwc:\n",
    "    if c not in liwc_dict:\n",
    "        liwc_dict[c] = []\n",
    "    liwc_dict[c].append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_liwc_categories(tokens, category_words, relative=True):\n",
    "    category_cnt = 0\n",
    "    if not tokens:\n",
    "        return None\n",
    "    text_len = len(tokens)\n",
    "    for t in tokens:\n",
    "        for word in category_words:\n",
    "            if t==word or (word[-1]=='*' and t.startswith(word[:-1])) \\\n",
    "            or (t==word.split(\"'\")[0]):\n",
    "                category_cnt += 1\n",
    "                break # one token cannot belong to more than one word in the category\n",
    "    if relative:\n",
    "        return category_cnt/text_len\n",
    "    else:\n",
    "        return category_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from functools import partial\n",
    "# for categ in ['negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']:#liwc_dict.keys():\n",
    "for categ in liwc_dict.keys():\n",
    "    if categ in writings_df_test2021.columns:\n",
    "        continue\n",
    "    print(\"Encoding for category %s...\" % categ)\n",
    "    writings_df_test2021[categ] = writings_df_test2021['all_tokens'].apply(partial(encode_liwc_categories, \n",
    "                                                                   category_words=liwc_dict[categ], \n",
    "                                                                   relative=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df.text_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df.title_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df.groupby('subject').count().title.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df_test2021.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_NRC(nrc_path):\n",
    "    word_emotions = {}\n",
    "    emotion_words = {}\n",
    "    with open(nrc_path) as in_f:\n",
    "        for line in in_f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            word, emotion, label = line.split()\n",
    "            if word not in word_emotions:\n",
    "                word_emotions[word] = set()\n",
    "            if emotion not in emotion_words:\n",
    "                emotion_words[emotion] = set()\n",
    "            label = int(label)\n",
    "            if label:\n",
    "                word_emotions[word].add(emotion)\n",
    "                emotion_words[emotion].add(word)\n",
    "    return emotion_words\n",
    "\n",
    "nrc_lexicon_path = root_dir + '/resources/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'\n",
    "nrc_lexicon = load_NRC(nrc_lexicon_path)\n",
    "emotions = list(nrc_lexicon.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_emotions(tokens, emotion_lexicon, emotions, relative=True):\n",
    "    text_len = len(tokens)\n",
    "    encoded_emotions = [0 for e in emotions]\n",
    "    for i, emotion in enumerate(emotions):\n",
    "        try:\n",
    "            emotion_words = [t for t in tokens if t in emotion_lexicon[emotion]]\n",
    "            if relative:\n",
    "                encoded_emotions[i] = len(emotion_words) / len(tokens)\n",
    "            else:\n",
    "                encoded_emotions[i] = len(emotion_words)\n",
    "        except ValueError:\n",
    "            print(\"Emotion not found.\")\n",
    "    return encoded_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from liwc_readDict import readDict\n",
    "\n",
    "liwc = readDict(root_dir + '/resources/liwc.dic')\n",
    "\n",
    "categories = set([c for (w,c) in liwc])\n",
    "len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(labels):\n",
    "    '''Convert ia to i and ib to -i'''\n",
    "    encoded_labels = []\n",
    "    for i, l in enumerate(labels):\n",
    "        try:\n",
    "            encoded_labels.append(int(l))\n",
    "        except Exception as e:\n",
    "            logger.debug(\"Encoding label %s\\n\" % l)\n",
    "        \n",
    "            if str(l)[-1] == 'a':\n",
    "                encoded_labels.append(int(l[0]))\n",
    "            elif str(l)[-1] == 'b':\n",
    "                encoded_labels.append(-int(l[0]))\n",
    "            else:\n",
    "                logger.warning(\"Coult not encode label %s\\n\" % l)\n",
    "    return encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(21):\n",
    "    writings_df['label%d'%i] = writings_df['label%d'%i].apply(lambda l: encode_labels([l])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Personal pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_person_pronouns = {\"i\", \"me\", \"my\", \"mine\", \"myself\"}\n",
    "def encode_pronouns(tokens, pronouns={\"i\", \"me\", \"my\", \"mine\", \"myself\"}, relative=True):\n",
    "    if not tokens:\n",
    "        return np.nan\n",
    "    text_len = len(tokens)\n",
    "    nr_pronouns = len([t for t in tokens if t in pronouns])\n",
    "    if relative:\n",
    "        return nr_pronouns/text_len\n",
    "    else:\n",
    "        return nr_pronouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words(\"english\")\n",
    "def encode_stopwords(tokens, stopwords=stopword_list):\n",
    "    encoded_stopwords = [0 for s in stopwords]\n",
    "    if not tokens:\n",
    "        return encoded_stopwords\n",
    "    for i, stopword in enumerate(stopwords):\n",
    "        if stopword in tokens:\n",
    "            encoded_stopwords[i] += 1\n",
    "    return encoded_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by users\n",
    "def aggregate_df(writings_df):\n",
    "    writings_df = writings_df.fillna(value={'text': '', 'title':''})\n",
    "    column_functions = {'text': lambda t: \" \".join(t), \n",
    "                                            'title': lambda t: \" \".join(t),\n",
    "                                            'tokenized_text': 'sum',\n",
    "                                            'tokenized_title': 'sum',\n",
    "                                            'text_len': 'sum',\n",
    "                                            'title_len': 'sum'}\n",
    "    if 'label1' in writings_df.columns:\n",
    "        column_functions.update({'label%i'%i: 'min' for i in range(21)})\n",
    "    writings_per_user_df = writings_df.groupby('subject').aggregate(column_functions)\n",
    "    #                                          'subset': 'min'})\n",
    "    writings_per_user_df = writings_per_user_df.fillna(\"\")\n",
    "    writings_per_user_df['text'] = writings_per_user_df['text'] + \" \" +  writings_per_user_df['title']\n",
    "    writings_per_user_df['text_len'] = writings_per_user_df['text_len'] + writings_per_user_df['title_len']\n",
    "    return writings_per_user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by users\n",
    "writings_df_per_user = aggregate_df(writings_df)\n",
    "writings_df_test_per_user = aggregate_df(writings_df_test2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df_per_user.text_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_embedding(writings_df, subject, column):\n",
    "    return writings_df[writings_df['subject']==subject][column].apply(lambda l: np.array(l)).values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_embeddings_text = {s: get_avg_embedding(writings_df, s, 'use_embeddings_text') \n",
    "                       for s in set(writings_df.subject.values)}\n",
    "avg_embeddings_title = {s: get_avg_embedding(writings_df, s, 'use_embeddings_title') \n",
    "                       for s in set(writings_df.subject.values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_embeddings_text = pd.Series(avg_embeddings_text)\n",
    "series_embeddings_text.name = 'avg_embeddings_text'\n",
    "series_embeddings_title = pd.Series(avg_embeddings_title)\n",
    "series_embeddings_title.name = 'avg_embeddings_title'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_per_user_df = writings_per_user_df.join(series_embeddings_text, on='subject')\n",
    "writings_per_user_df = writings_per_user_df.join(series_embeddings_title, on='subject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_per_user_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_per_user_df.join?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df_per_user['subject'] = writings_df_per_user.index\n",
    "writings_df_test_per_user['subject'] = writings_df_test_per_user.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings_dict = pickle.load(open('data/user_embeddings_t2_2020_model101.pkl', 'rb'))\n",
    "all_embeddings_dict2 = pickle.load(open('data/user_embeddings_t2_test_2020_model101.pkl', 'rb'))\n",
    "all_embeddings_dict.update(all_embeddings_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings_dict_test = pickle.load(open('data/user_embeddings_t3_test2021_model101.pkl', 'rb'))\n",
    "len(all_embeddings_dict_test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df_per_user['user_embeddings_avg'] = writings_df_per_user.subject.apply(\n",
    "    lambda s: np.mean(all_embeddings_dict[s], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df_test_per_user['user_embeddings_avg'] = writings_df_test_per_user.subject.apply(\n",
    "    lambda s: np.mean(all_embeddings_dict_test[s], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df_test_per_user['user_embeddings_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# writings_embeddings['use_similarity'] = writings_embeddings['use_embeddings_text'].apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = [np.random.rand(75) for i in range(20)]\n",
    "# features = writings_per_user_df[list(categories) + emotions + [\"pronouns\"]]\n",
    "# features = writings_per_user_df['avg_embeddings_title'].values.tolist()\n",
    "features = writings_df_per_user['user_embeddings_avg'].values.tolist()\n",
    "\n",
    "def cross_validation(folds=2):\n",
    "    svmmodels= {}\n",
    "    total_score = 0\n",
    "    for l in range(21):\n",
    "#         print(\"Classifier for label\", l)\n",
    "        labels = writings_df_per_user['label%d' % l].values\n",
    "        svmmodels[l] = SVC(kernel='rbf', C=5)\n",
    "        cvscores = cross_val_score(svmmodels[l], features, labels, cv=folds)\n",
    "#         print(sum(cvscores)/folds, cvscores)\n",
    "        total_score += sum(cvscores)/folds\n",
    "    return total_score/21\n",
    "\n",
    "print(cross_validation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_for_label(features, l, train_examples=16):\n",
    "    labels = writings_per_user_df['label%d' % l].values\n",
    "    svmmodel=SVC()\n",
    "    svmmodel.fit(features[:train_examples], labels[:train_examples])\n",
    "    predictions = svmmodel.predict(features[train_examples:])\n",
    "    print(l, predictions, labels[train_examples:], labels[:train_examples])\n",
    "    return labels[train_examples:]==predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumresults = []\n",
    "for l in range(21):\n",
    "    results = results_for_label(features, l)\n",
    "    cumresults.append(results)\n",
    "\n",
    "nrusers = len(cumresults[0])\n",
    "nrques = 21\n",
    "correct_per_user = {u: 0 for u in range(nrusers)}\n",
    "for q, ques in enumerate(cumresults):\n",
    "    for u, answ in enumerate(cumresults[q]):\n",
    "        if answ:\n",
    "            correct_per_user[u] += 1\n",
    "\n",
    "for u in correct_per_user:\n",
    "    print(\"u\", u, correct_per_user[u]/nrques)\n",
    "print(\"AHR\", sum(correct_per_user.values())/nrusers/nrques)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(features).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "label_set = []\n",
    "for column in writings_per_user_df.columns:\n",
    "    if column.startswith('label'):\n",
    "        label_set.append(column)\n",
    "label_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_for_label(label_col, subjects, n=5):\n",
    "    neigh = KNeighborsClassifier(n_neighbors=n)\n",
    "#     X = [list(l) for l in writings_per_user_df[writings_per_user_df['subject'].isin(subjects)][\n",
    "#         'user_embeddings_avg'].values]\n",
    "#     y = writings_per_user_df[writings_per_user_df['subject'].isin(subjects)][label_col].values\n",
    "    X = []\n",
    "    y = []\n",
    "    for s in subjects:\n",
    "        label = writings_df_per_user[writings_df_per_user['subject']==s][label_col].values[0]\n",
    "        for e in all_embeddings_dict[s]:\n",
    "            y.append(label)\n",
    "            X.append(e)\n",
    "            \n",
    "    neigh.fit(X, y)\n",
    "    return neigh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=3\n",
    "all_subjects = list(all_embeddings_dict.keys())\n",
    "training_subjects = all_subjects\n",
    "test_subjects = all_embeddings_dict_test.keys()\n",
    "knns = {}\n",
    "for label in range(21):\n",
    "    knns['label%d'%label] = classifier_for_label('label%d'%label, subjects=training_subjects,n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "predictions_avg = {}\n",
    "predictions_maj = {}\n",
    "predictions_avglab = {}\n",
    "true_labels = {}\n",
    "for subject in test_subjects:\n",
    "    predictions_avg[subject] = []\n",
    "    predictions_maj[subject] = []\n",
    "    predictions_avglab[subject] = []\n",
    "    true_labels[subject] = []\n",
    "    for l in range(21):\n",
    "        label = \"label%d\" % l\n",
    "        print(\"\\n\" + subject)\n",
    "        if label in writings_df_test_per_user.columns:\n",
    "            true_label = writings_df_test_per_user[writings_df_test_per_user['subject']==subject][label].values[0]\n",
    "        else:\n",
    "            true_label = 0\n",
    "        print(\"True label:\", true_label)\n",
    "        average_pred = knns[label].predict([np.mean(all_embeddings_dict_test[subject], axis=0)])[0]\n",
    "        print(\"Average pred:\", average_pred)\n",
    "        all_preds = knns[label].predict(all_embeddings_dict_test[subject])\n",
    "        majority_label = Counter(all_preds).most_common(1)[0][0]\n",
    "        print(\"Majority label:\", majority_label)\n",
    "        print(\"All preds:\", all_preds)\n",
    "        average_lab_pred = np.mean(all_preds)\n",
    "        true_labels[subject].append(str(true_label))\n",
    "\n",
    "        \n",
    "        if int(average_pred) < 0:\n",
    "            average_pred = str(abs(average_pred)) + 'b'\n",
    "        elif l in [15, 17] and average_pred>0:\n",
    "            average_pred = str(average_pred) + 'a'\n",
    "            \n",
    "        if int(round(average_lab_pred)) < 0:\n",
    "            average_lab_pred = str(abs(round(average_lab_pred))) + 'b'\n",
    "        elif l in [15, 17] and average_lab_pred>0:\n",
    "            average_lab_pred = str(round(average_lab_pred)) + 'a'\n",
    "        else:\n",
    "            average_lab_pred = round(average_lab_pred)\n",
    "            \n",
    "        if int(majority_label) < 0:\n",
    "            majority_label = str(abs(majority_label)) + 'b'\n",
    "        elif l in [15, 17] and majority_label>0:\n",
    "            majority_label = str(majority_label) + 'a'\n",
    "            \n",
    "        predictions_avg[subject].append(str(average_pred))\n",
    "        predictions_maj[subject].append(str(majority_label))\n",
    "        predictions_avglab[subject].append(str(average_lab_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_avglab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(predictions_avg, open('t2_predictions_k5_alltexts_userembeddings101_avg_dev.pkl', 'wb+'))\n",
    "# pickle.dump(predictions_maj, open('t2_predictions_k5_alltexts_userembeddings101_maj_dev.pkl', 'wb+'))\n",
    "# pickle.dump(predictions_avglab, open('t2_predictions_k5_alltexts_userembeddings101_avglab_dev.pkl', 'wb+'))\n",
    "# pickle.dump(true_labels, open('t2_true_labels_dev.pkl', 'wb+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets    \n",
    "import numpy as np\n",
    "\n",
    "_DESCRIPTION_T3 = \"\"\"\\\n",
    "Metrics for measuring the performance of prediction models for eRisk 2021 Task 2 and 3.\n",
    "Include decision-based performance metrics: decision-based F1, lantency-weighted F1, ERDE score.\n",
    "\"\"\"\n",
    "\n",
    "_CITATION = \"\"\n",
    "\n",
    "_KWARGS_DESCRIPTION_T3 = \"\"\"\n",
    "Calculates how good are predictions of answers given to the depression assessment questionnaire,\n",
    "given some references, using different metrics.\n",
    "Each prediction and reference is expected to be of length 21, corresponding to the 21 questions.\n",
    "    predictions: list of predictions to score, one for each user. Each prediction\n",
    "        should be a list of 21 strings in {0, 1, 2, 3, 1a, 1b, 2a, 2b, 3a, 3b}.\n",
    "    references: list of references for each prediction, one for each user. Each\n",
    "        reference should be a list of 21 strings in {0, 1, 2, 3, 1a, 1b, 2a, 2b, 3a, 3b}.\n",
    "\n",
    "Returns:\n",
    "    AHR: average hit rate - ratio of cases where the automatic questionnaire\n",
    "        has exactly the same answer as the real questionnaire, averaged over users.\n",
    "    ACR: average closeness rate - the difference between the real and the automated answer,\n",
    "        (taking into account answers are on an ordinal scale), relative to the total number\n",
    "        of possible answers for the question, averaged over users.\n",
    "    ADODL: average difference between overall depression levels - difference in overall\n",
    "        depression levels, computed as the sum of the answers given for all questions,\n",
    "        (depression levels are integers between 0 and 63).\n",
    "        averaged over users.\n",
    "    DCHR: fraction of cases where the automated questionnaire led to a depression category\n",
    "        that is equivalent to the depression category obtained from the real questionnaire\n",
    "        (among 4 possible categories: minimal/mild/moderate/severe depression). \n",
    "    \n",
    "    \n",
    "Examples:\n",
    "    >>> t3_metric = EriskScoresT3()\n",
    "    >>> results = t3_metric.compute(\n",
    "        predictions = [['0', '1', '2', '3', '1a', '1b', '2a', '2b', '3', '2', \n",
    "                    '0', '1', '2', '3', '1a', '1b', '2a', '2b', '3', '2', '3b'],\n",
    "                  ['0', '1', '2', '3', '1a', '1b', '2a', '2b', '3', '2', \n",
    "                    '1', '2', '3', '3a', '1', '1', '2', '2a', '3b', '2', '3b']],\n",
    "        references = [['0', '0', '0', '3', '1', '1', '2a', '2b', '3', '2', \n",
    "                    '1', '1', '1', '3', '1a', '1b', '2', '2', '3', '2', '3b'],\n",
    "                 ['0', '1', '2', '3', '1', '1', '2', '2', '3', '2', \n",
    "                    '1', '2', '3', '3', '1', '1', '2', '2', '3', '2', '3']]\n",
    "        )\n",
    "    >>> print(results)\n",
    "    >>> {'AHR': 0.6190476190476191,\n",
    "         'ACR': 0.9603174603174602,\n",
    "         'ADODL': 0.9761904761904762,\n",
    "         'DCHR': 1.0}\n",
    "   \n",
    "\"\"\"\n",
    "\n",
    "def _depression_category(score):\n",
    "    if score >= 0 and score <=9:\n",
    "        return 'minimal'\n",
    "    if score >= 10 and score <= 18:\n",
    "        return 'mild'\n",
    "    if score >= 19 and score <= 29:\n",
    "        return 'moderate'\n",
    "    if score >= 30 and score <= 63:\n",
    "        return 'severe'\n",
    "    raise ValueError(\"Invalid score for depression questionnaire: %d\" % score)\n",
    "\n",
    "def _score(reference):\n",
    "    total_score = 0\n",
    "    for answer in reference:\n",
    "        # Consider only first letter and convert to int\n",
    "        answer_int = int(answer[0])\n",
    "        total_score += answer_int\n",
    "    return total_score\n",
    "\n",
    "\n",
    "\n",
    "def _dl(l1, l2, max_dl=63):\n",
    "    ad = abs(l1 - l2)\n",
    "    return (max_dl - ad)/max_dl\n",
    "    \n",
    "\n",
    "class EriskScoresT3(datasets.Metric):\n",
    "    def _info(self):\n",
    "        return datasets.MetricInfo(\n",
    "            description=_DESCRIPTION_T3,\n",
    "            citation=_CITATION,\n",
    "            inputs_description=_KWARGS_DESCRIPTION_T3,\n",
    "            features=datasets.Features({\n",
    "                'predictions': datasets.Sequence(datasets.Value('string')),\n",
    "                'references':datasets.Sequence(datasets.Value('string'))\n",
    "            }),\n",
    "            codebase_urls=[],\n",
    "            reference_urls=[],\n",
    "        )\n",
    "    \n",
    "    def _ahr(self, predictions, references):\n",
    "        hr_per_user = []\n",
    "        nr_users = len(predictions)\n",
    "        for u in range(nr_users):\n",
    "            prediction = predictions[u]\n",
    "            reference = references[u]\n",
    "            hr = sum(i == j for i, j in zip(prediction, reference)) / len(prediction)\n",
    "            hr_per_user.append(hr)\n",
    "        return sum(hr_per_user)/nr_users\n",
    "    \n",
    "    def _cr(self, prediction, reference, nr_answers=4):\n",
    "        closeness_scores = []\n",
    "        nr_questions = len(prediction)\n",
    "        for q in range(nr_questions):\n",
    "            # Consider only first letter and convert to int\n",
    "            predictionq_int = int(prediction[q][0])\n",
    "            referenceq_int = int(reference[q][0])\n",
    "            mad = nr_answers - 1\n",
    "            ad = abs(predictionq_int - referenceq_int)\n",
    "            closeness = (mad - ad) / mad\n",
    "            closeness_scores.append(closeness)\n",
    "        return sum(closeness_scores)/nr_questions\n",
    "            \n",
    "    def _acr(self, predictions, references):\n",
    "        nr_users = len(predictions)\n",
    "        cr_per_user = [self._cr(predictions[u], references[u]) \n",
    "                       for u in range(nr_users)]\n",
    "        return sum(cr_per_user)/nr_users\n",
    "    \n",
    "\n",
    "    def _adodl(self, predictions, references):\n",
    "        nr_users = len(predictions)\n",
    "        scores_predictions = [_score(p) for p in predictions]\n",
    "        scores_references = [_score(r) for r in references]\n",
    "        level_differences = [_dl(scores_predictions[u],scores_references[u])\n",
    "                             for u in range(nr_users)]\n",
    "        return sum(level_differences)/nr_users\n",
    "\n",
    "        \n",
    "    def _dchr(self, predictions, references):\n",
    "        nr_users = len(predictions)\n",
    "        dc_predictions = [_depression_category(_score(predictions[u])) for u in range(nr_users)]\n",
    "        dc_references = [_depression_category(_score(references[u])) for u in range(nr_users)]\n",
    "        hr = sum(i == j for i, j in zip(dc_predictions, dc_references)) / nr_users\n",
    "        return hr\n",
    "    \n",
    "    def _compute(self, predictions, references):\n",
    "        return {\n",
    "            'AHR': self._ahr(predictions, references),\n",
    "            'ACR': self._acr(predictions, references),\n",
    "            'ADODL': self._adodl(predictions, references),\n",
    "            'DCHR': self._dchr(predictions, references),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# predictions_avg = pickle.load(open('t2_predictions_avg_dev.pkl', 'rb'))\n",
    "# predictions_maj = pickle.load(open('t2_predictions_maj_dev.pkl', 'rb'))\n",
    "# true_labels = pickle.load(open('t2_true_labels_dev.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = EriskScoresT3()\n",
    "metrics.compute(\n",
    "predictions = predictions_avglab.values(),\n",
    "references = true_labels.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = EriskScoresT3()\n",
    "metrics.compute(\n",
    "predictions = predictions_maj.values(),\n",
    "references = true_labels.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = EriskScoresT3()\n",
    "metrics.compute(\n",
    "predictions = predictions_avg.values(),\n",
    "references = true_labels.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u, preds in predictions_avg.items():\n",
    "    print(u, \" \".join(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfconda",
   "language": "python",
   "name": "tfconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
