{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob, os\n",
    "import numpy as np\n",
    "from comet_ml import Experiment, Optimizer\n",
    "import pickle\n",
    "import logging\n",
    "import sys\n",
    "from sklearn.utils import class_weight\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['TF_KERAS'] = '1'\n",
    "\n",
    "# only reserve 1 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# tf.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Lambda, BatchNormalization, TimeDistributed, \\\n",
    "     Bidirectional, Input, concatenate, Flatten, RepeatVector, Activation, Multiply, Permute#, CuDNNLSTM\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import callbacks, optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model, Sequence\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer, TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_seed = 1234\n",
    "# tf.set_random_seed(my_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('training')\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_subject_writings(subject_file):\n",
    "    writings = []\n",
    "    with open(subject_file) as sf:\n",
    "        contents = sf.read()\n",
    "        contents = clean_content(contents)\n",
    "#         try:\n",
    "#         print(contents[:500])\n",
    "\n",
    "        root = ET.fromstring(contents)\n",
    "#         except:\n",
    "#             print('Cannot extract text', contents[:500], '\\n-------\\n')\n",
    "            \n",
    "        try:\n",
    "            subject = root.findall('ID')[0].text.strip()\n",
    "        except Exception:\n",
    "            print('Cannot extract ID', contents[:500], '\\n-------\\n')        \n",
    "        for w in root.iter('WRITING'):\n",
    "            subject_writings = {'subject': subject}\n",
    "            for title in w.findall('TITLE'):\n",
    "                subject_writings['title'] = title.text\n",
    "            for text in w.findall('TEXT'):\n",
    "                subject_writings['text'] = text.text\n",
    "            for date in w.findall('DATE'):\n",
    "                subject_writings['date'] = date.text\n",
    "            writings.append(subject_writings)\n",
    "    return writings\n",
    "def clean_content(orig_content):\n",
    "    WINDOWS_LINE_ENDING = b'\\r\\n'\n",
    "    UNIX_LINE_ENDING = b'\\n'\n",
    "    byte_content = orig_content.encode(\"UTF-8\")\n",
    "    byte_content = byte_content.replace(WINDOWS_LINE_ENDING, UNIX_LINE_ENDING)\n",
    "    content = byte_content.decode(\"UTF-8\")\n",
    "#     content = re.sub(\"^<\", \"^\\<\", orig_content)    \n",
    "    content = re.sub(\"&\", \"&amp;\", content)\n",
    "    content = re.sub(\"^>\", \"&gt;\", content)    \n",
    "#     content = re.sub(\">$\", \"\\>$\", content)    \n",
    "    content = re.sub(\"<$\", \"&lt;\", content)\n",
    "    content = re.sub(\"<\\n\", \"&lt;\\n\", content)\n",
    "    content = re.sub(\"< \", \"&lt; \", content)\n",
    "    content = re.sub(\"<(?=\\d)\", \"&lt;\", content)\n",
    "    content = re.sub(\"<(?=[^\\w\\\\/])\", \"&lt;\", content)\n",
    "    content = re.sub(\"<(?=[^A-Z\\\\/])\", \"&lt;\", content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dir = '/home/anasab/' \n",
    "root_dir = '/home/anasab/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir_T2 = root_dir + '/eRisk/data/eRisk2020_T2/eRisk2020_T2_TEST_DATA/'\n",
    "datadir_test_T2 = root_dir + 'eRisk/data/eRISK2021_T3_training_data/training BDI/2020 data'\n",
    "datadir_test2021 = root_dir + 'eRisk/data/eRisk2021_T3_Collection/clean'\n",
    "# labels_file_T2 = root_dir + '/eRisk/data/eRisk2020_T2/eRisk2020_T2_TRAINING_DATA/Depression Questionnaires_anon.txt'\n",
    "labels_file_test_T2 = root_dir + 'eRisk//data/eRISK2021_T3_training_data/training BDI/2020 data/Depression Questionnaires_anon.txt'\n",
    "nr_questions = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts(datadir_T2,\n",
    "                labels_file_T2=None):\n",
    "    writings = []\n",
    "    writings_df = pd.DataFrame()\n",
    "    labels_df = pd.DataFrame()\n",
    "\n",
    "        \n",
    "    for subject_file in os.listdir(datadir_T2):\n",
    "        if not subject_file.endswith('xml'):\n",
    "            continue\n",
    "#         try:\n",
    "        print(subject_file)\n",
    "        writings.extend(read_subject_writings(os.path.join(datadir_T2, subject_file)))\n",
    "#         except:\n",
    "#             print(\"Couldn't parse\", subject_file)\n",
    "    writings_df = pd.DataFrame(writings)\n",
    "    if labels_file_T2:\n",
    "        labels_df = pd.read_csv(os.path.join(labels_file_T2), \n",
    "                                     delimiter='\\s+', names=['subject'] + ['label%i' % i for i in range(nr_questions)])\n",
    "\n",
    "        labels_df = labels_df.set_index('subject')\n",
    "\n",
    "        writings_df = writings_df.join(labels_df, on='subject')\n",
    "    \n",
    "    return writings_df, labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df_test, labels_df_test = read_texts(datadir_test_T2, labels_file_test_T2)\n",
    "writings_df_test2021, _ = read_texts(datadir_test2021)\n",
    "writings_df = pickle.load(open('data/writings_df_T2_liwc.pkl', 'rb'))\n",
    "# writings_df_test = pickle.load(open('data/writings_df_t2_test_liwc.pkl', 'rb'))\n",
    "writings_df_test = pickle.load(open('data/writings_df_T2_test_liwc_wdays.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df = pickle.load(open('data/writings_df_T2_wsymanto.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df_test.writing_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df = pd.concat([writings_df, writings_df_test])\n",
    "# writings_df_test = writings_df_test.join(labels_df_test, on='subject')\n",
    "writings_df_test.groupby('subject').count()\n",
    "writings_df_test.writing_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df_test2021.groupby('subject').count()\n",
    "# writings_df_test2021\n",
    "# pickle.dump(writings_df_test2021, open('data/writings_df_t3_2021_test_liwc.pkl', 'wb+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(writings_df_test.subject))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def tokenize(t):\n",
    "    return tokenizer.tokenize(t.lower())\n",
    "tt = TweetTokenizer()\n",
    "sw = stopwords.words(\"english\")\n",
    "def tokenize_tweets(t, tokenizer=tt, stop=True):\n",
    "    tokens = tokenizer.tokenize(t.lower())\n",
    "    tokens_clean = [token for token in tokens if \n",
    "                            re.match(\"^[a-z]*$\", token)]\n",
    "    if not stop:\n",
    "        tokens_clean = [token for token in tokens_clean \n",
    "                        if token not in sw]\n",
    "    return tokens_clean\n",
    "\n",
    "def tokenize_fields(writings_df):\n",
    "    writings_df['tokenized_title'] = writings_df['title'].apply(lambda t: tokenize_tweets(t) \n",
    "                                                                if type(t)==str and t else None)\n",
    "    writings_df['title_len'] = writings_df['tokenized_title'].apply(lambda t: len(t) \n",
    "                                                                    if type(t)==list and t else None)\n",
    "    writings_df['tokenized_text'] = writings_df['text'].apply(lambda t: tokenize_tweets(t) \n",
    "                                                              if type(t)==str and t else None)\n",
    "    writings_df['text_len'] = writings_df['tokenized_text'].apply(lambda t: len(t) \n",
    "                                                                  if type(t)==list and t else None)\n",
    "    return writings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df_test2021 = tokenize_fields(writings_df_test2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df = tokenize_fields(writings_df)\n",
    "writings_df_test = tokenize_fields(writings_df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tokens(row):\n",
    "    tokens = []\n",
    "    if row.tokenized_text:\n",
    "        tokens += row.tokenized_text\n",
    "    if row.tokenized_title:\n",
    "        tokens += row.tokenized_title\n",
    "    return tokens\n",
    "writings_df_test2021['all_tokens'] = writings_df_test2021.apply (lambda row: merge_tokens(row), axis=1)\n",
    "writings_df_test['all_tokens'] = writings_df_test.apply (lambda row: merge_tokens(row), axis=1)\n",
    "writings_df['all_tokens'] = writings_df.apply (lambda row: merge_tokens(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_NRC(nrc_path):\n",
    "    word_emotions = {}\n",
    "    emotion_words = {}\n",
    "    with open(nrc_path) as in_f:\n",
    "        for line in in_f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            word, emotion, label = line.split()\n",
    "            if word not in word_emotions:\n",
    "                word_emotions[word] = set()\n",
    "            if emotion not in emotion_words:\n",
    "                emotion_words[emotion] = set()\n",
    "            label = int(label)\n",
    "            if label:\n",
    "                word_emotions[word].add(emotion)\n",
    "                emotion_words[emotion].add(word)\n",
    "    return emotion_words\n",
    "\n",
    "nrc_lexicon_path = root_dir + '/resources/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'\n",
    "nrc_lexicon = load_NRC(nrc_lexicon_path)\n",
    "emotions = list(nrc_lexicon.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: include the title\n",
    "def extract_emotions(tokens, emotion, relative=True):\n",
    "    if not tokens:\n",
    "        return None\n",
    "    emotion_words = [t for t in tokens \n",
    "                     if t in nrc_lexicon[emotion]]\n",
    "    if relative:\n",
    "        return len(emotion_words) / len(tokens)\n",
    "    else:\n",
    "        return len(emotion_words)\n",
    "    \n",
    "    return encoded_emotions\n",
    "\n",
    "from functools import partial\n",
    "for emotion in emotions:\n",
    "    writings_df_test2021[emotion] = writings_df_test2021['all_tokens'].apply(partial(extract_emotions, emotion=emotion, \n",
    "                                                                   relative=True))\n",
    "for emotion in emotions:\n",
    "    writings_df_test[emotion] = writings_df_test['all_tokens'].apply(partial(extract_emotions, emotion=emotion, \n",
    "                                                                   relative=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_person_pronouns = {\"i\", \"me\", \"my\", \"mine\", \"myself\"}\n",
    "def encode_pronouns(tokens, pronouns={\"i\", \"me\", \"my\", \"mine\", \"myself\"}, relative=True):\n",
    "    if not tokens:\n",
    "        return 0\n",
    "    text_len = len(tokens)\n",
    "    nr_pronouns = len([t for t in tokens if t in pronouns])\n",
    "    if relative and text_len:\n",
    "        return nr_pronouns/text_len\n",
    "    else:\n",
    "        return nr_pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "writings_df_test2021['pronouns'] = writings_df_test2021['all_tokens'].apply(partial(encode_pronouns, relative=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from liwc_readDict import readDict\n",
    "\n",
    "liwc = readDict('/home/anasab/resources/liwc.dic')\n",
    "categories = [c for (w,c) in liwc]\n",
    "set(categories)\n",
    "liwc_dict = {}\n",
    "for (w, c) in liwc:\n",
    "    if c not in liwc_dict:\n",
    "        liwc_dict[c] = []\n",
    "    liwc_dict[c].append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_liwc_categories(tokens, category_words, relative=True):\n",
    "    category_cnt = 0\n",
    "    if not tokens:\n",
    "        return None\n",
    "    text_len = len(tokens)\n",
    "    for t in tokens:\n",
    "        for word in category_words:\n",
    "            if t==word or (word[-1]=='*' and t.startswith(word[:-1])) \\\n",
    "            or (t==word.split(\"'\")[0]):\n",
    "                category_cnt += 1\n",
    "                break # one token cannot belong to more than one word in the category\n",
    "    if relative:\n",
    "        return category_cnt/text_len\n",
    "    else:\n",
    "        return category_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from functools import partial\n",
    "# for categ in ['negemo', 'posemo', 'affect', 'sad', 'anx', 'pronoun']:#liwc_dict.keys():\n",
    "for categ in liwc_dict.keys():\n",
    "    if categ in writings_df_test2021.columns:\n",
    "        continue\n",
    "    print(\"Encoding for category %s...\" % categ)\n",
    "    writings_df_test2021[categ] = writings_df_test2021['all_tokens'].apply(partial(encode_liwc_categories, \n",
    "                                                                   category_words=liwc_dict[categ], \n",
    "                                                                   relative=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df.text_len.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df.title_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df.groupby('subject').count().title.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df_test2021.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(labels):\n",
    "    '''Convert ia to i and ib to -i'''\n",
    "    encoded_labels = []\n",
    "    for i, l in enumerate(labels):\n",
    "        try:\n",
    "            encoded_labels.append(int(l))\n",
    "        except Exception as e:\n",
    "            logger.debug(\"Encoding label %s\\n\" % l)\n",
    "        \n",
    "            if str(l)[-1] == 'a':\n",
    "                encoded_labels.append(int(l[0]))\n",
    "            elif str(l)[-1] == 'b':\n",
    "                encoded_labels.append(-int(l[0]))\n",
    "            else:\n",
    "                logger.warning(\"Coult not encode label %s\\n\" % l)\n",
    "    return encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(21):\n",
    "    writings_df['label%d'%i] = writings_df['label%d'%i].apply(lambda l: encode_labels([l])[0])\n",
    "for i in range(21):\n",
    "    writings_df_test['label%d'%i] = writings_df_test['label%d'%i].apply(lambda l: encode_labels([l])[0])\n",
    "for i in range(21):\n",
    "    writings_df_test2021['label%d'%i] = writings_df_test2021['label%d'%i].apply(lambda l: encode_labels([l])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words(\"english\")\n",
    "def encode_stopwords(tokens, stopwords=stopword_list):\n",
    "    encoded_stopwords = [0 for s in stopwords]\n",
    "    if not tokens:\n",
    "        return encoded_stopwords\n",
    "    for i, stopword in enumerate(stopwords):\n",
    "        if stopword in tokens:\n",
    "            encoded_stopwords[i] += 1\n",
    "    return encoded_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by users\n",
    "def aggregate_df(writings_df):\n",
    "    writings_df = writings_df.fillna(value={'text': '', 'title':''})\n",
    "    column_functions = {'text': lambda t: \" \".join(t), \n",
    "                                            'title': lambda t: \" \".join(t),\n",
    "                                            'tokenized_text': 'sum',\n",
    "                                            'tokenized_title': 'sum',\n",
    "                                            'text_len': 'sum',\n",
    "                                            'title_len': 'sum'}\n",
    "    if 'label1' in writings_df.columns:\n",
    "        column_functions.update({'label%i'%i: 'min' for i in range(21)})\n",
    "    for c in list(set(categories)) + emotions:\n",
    "        if c in writings_df.columns:\n",
    "            column_functions.update({c: 'mean'})\n",
    "    writings_per_user_df = writings_df.groupby('subject').aggregate(column_functions)\n",
    "    #                                          'subset': 'min'})\n",
    "    writings_per_user_df = writings_per_user_df.fillna(\"\")\n",
    "    writings_per_user_df['text'] = writings_per_user_df['text'] + \" \" +  writings_per_user_df['title']\n",
    "    writings_per_user_df['text_len'] = writings_per_user_df['text_len'] + writings_per_user_df['title_len']\n",
    "    return writings_per_user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by users\n",
    "writings_df_per_user = aggregate_df(writings_df)\n",
    "# writings_df_test_per_user = aggregate_df(writings_df_test2021)\n",
    "writings_df_test_per_user = aggregate_df(writings_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df_test_per_user.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df_per_user.text_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_embedding(writings_df, subject, column):\n",
    "    return writings_df[writings_df['subject']==subject][column].apply(lambda l: np.array(l)).values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_embeddings_text = {s: get_avg_embedding(writings_df, s, 'use_embeddings_text') \n",
    "                       for s in set(writings_df.subject.values)}\n",
    "avg_embeddings_title = {s: get_avg_embedding(writings_df, s, 'use_embeddings_title') \n",
    "                       for s in set(writings_df.subject.values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_embeddings_text = pd.Series(avg_embeddings_text)\n",
    "series_embeddings_text.name = 'avg_embeddings_text'\n",
    "series_embeddings_title = pd.Series(avg_embeddings_title)\n",
    "series_embeddings_title.name = 'avg_embeddings_title'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_per_user_df = writings_per_user_df.join(series_embeddings_text, on='subject')\n",
    "writings_per_user_df = writings_per_user_df.join(series_embeddings_title, on='subject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_per_user_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_per_user_df.join?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df_per_user['subject'] = writings_df_per_user.index\n",
    "writings_df_test_per_user['subject'] = writings_df_test_per_user.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings_dict = pickle.load(open('data/user_embeddings_t2_2020_model115.pkl', 'rb'))\n",
    "all_embeddings_dict2 = pickle.load(open('data/user_embeddings_t2_test_2020_model115.pkl', 'rb'))\n",
    "# all_embeddings_dict.update(all_embeddings_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings_dict_test = all_embeddings_dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_embeddings_dict_test = pickle.load(open('data/user_embeddings_t3_test2021_model115.pkl', 'rb'))\n",
    "# len(all_embeddings_dict_test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df_per_user['user_embeddings_avg'] = writings_df_per_user.subject.apply(\n",
    "    lambda s: np.mean(all_embeddings_dict[s], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(writings_df_per_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df_test_per_user['user_embeddings_avg'] = writings_df_test_per_user.subject.apply(\n",
    "    lambda s: np.mean(all_embeddings_dict_test[s], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df_test_per_user['user_embeddings_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# writings_embeddings['use_similarity'] = writings_embeddings['use_embeddings_text'].apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([c for c in categories] + emotions)\n",
    "# len(writings_df_test_per_user[feature_cols].values.tolist()[0])\n",
    "len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = [np.random.rand(75) for i in range(20)]\n",
    "# features = writings_per_user_df[list(categories) + emotions + [\"pronouns\"]]\n",
    "# features = writings_per_user_df['avg_embeddings_title'].values.tolist()\n",
    "\n",
    "# User embeddings as features\n",
    "# features = writings_df_per_user['user_embeddings_avg'].values.tolist()\n",
    "\n",
    "# LIWC/emotions as features\n",
    "feature_cols = [c for c in set(categories)] + emotions\n",
    "feature_cols = emotions\n",
    "features = writings_df_per_user[feature_cols]\n",
    "writings_df_per_user['allcats'] = writings_df_per_user[feature_cols].values.tolist()\n",
    "writings_df_test_per_user['allcats'] = writings_df_test_per_user[feature_cols].values.tolist()\n",
    "\n",
    "def cross_validation(folds=2):\n",
    "    svmmodels= {}\n",
    "    total_score = 0\n",
    "    for l in range(21):\n",
    "#         print(\"Classifier for label\", l)\n",
    "        labels = writings_df_per_user['label%d' % l].values\n",
    "        svmmodels[l] = SVC(kernel='rbf', C=5)\n",
    "        cvscores = cross_val_score(svmmodels[l], features, labels, cv=folds)\n",
    "#         print(sum(cvscores)/folds, cvscores)\n",
    "        total_score += sum(cvscores)/folds\n",
    "    return total_score/21\n",
    "\n",
    "print(cross_validation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['allcats']= features.values.tolist()\n",
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_for_label(features, l, train_examples=16):\n",
    "    labels = writings_df_per_user['label%d' % l].values\n",
    "    svmmodel=SVC()\n",
    "    svmmodel.fit(features[:train_examples], labels[:train_examples])\n",
    "    predictions = svmmodel.predict(features[train_examples:])\n",
    "    print(l, predictions, labels[train_examples:], labels[:train_examples])\n",
    "    return labels[train_examples:]==predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumresults = []\n",
    "for l in range(21):\n",
    "    results = results_for_label(features, l)\n",
    "    cumresults.append(results)\n",
    "\n",
    "nrusers = len(cumresults[0])\n",
    "nrques = 21\n",
    "correct_per_user = {u: 0 for u in range(nrusers)}\n",
    "for q, ques in enumerate(cumresults):\n",
    "    for u, answ in enumerate(cumresults[q]):\n",
    "        if answ:\n",
    "            correct_per_user[u] += 1\n",
    "\n",
    "for u in correct_per_user:\n",
    "    print(\"u\", u, correct_per_user[u]/nrques)\n",
    "print(\"AHR\", sum(correct_per_user.values())/nrusers/nrques)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(features).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "label_set = []\n",
    "for column in writings_df_per_user.columns:\n",
    "    if column.startswith('label'):\n",
    "        label_set.append(column)\n",
    "label_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_for_label(label_col, subjects, n=5, user_level=True, feature_col='user_embeddings_avg'):\n",
    "    neigh = KNeighborsClassifier(n_neighbors=n)\n",
    "    if user_level:\n",
    "        X = [list(l) for l in writings_df_per_user[writings_df_per_user['subject'].isin(subjects)][\n",
    "        feature_col].values]\n",
    "        print(len(X), len(X[0]))\n",
    "        y = writings_df_per_user[writings_df_per_user['subject'].isin(subjects)][label_col].values\n",
    "        print(len(y))\n",
    "    else:\n",
    "        X = []\n",
    "        y = []\n",
    "        for s in subjects:\n",
    "            label = writings_df_per_user[writings_df_per_user['subject']==s][label_col].values[0]\n",
    "            for e in all_embeddings_dict[s]:\n",
    "                y.append(label)\n",
    "                X.append(e)\n",
    "            \n",
    "    neigh.fit(X, y)\n",
    "    return neigh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=5\n",
    "all_subjects = list(all_embeddings_dict.keys())\n",
    "training_subjects = all_subjects\n",
    "test_subjects = all_embeddings_dict_test.keys()\n",
    "knns = {}\n",
    "for label in range(21):\n",
    "    knns['label%d'%label] = classifier_for_label('label%d'%label, subjects=training_subjects,n=n, user_level=True,\n",
    "                                                feature_col='allcats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(writings_df_test_per_user[writings_df_test_per_user['subject']=='subject1009'][feature_col].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df_test_per_user.subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Using embeddings\n",
    "\n",
    "from collections import Counter\n",
    "predictions_avg = {}\n",
    "predictions_maj = {}\n",
    "predictions_avglab = {}\n",
    "true_labels = {}\n",
    "\n",
    "def label_to_str(l, q=0):\n",
    "    if int(round(l)) < 0:\n",
    "        l = str(abs(round(l))) + 'b'\n",
    "    elif q in [15, 17] and l>0:\n",
    "        l = str(round(l)) + 'a'\n",
    "    else:\n",
    "        l = str(round(l))\n",
    "    return l\n",
    "\n",
    "for subject in test_subjects:\n",
    "    predictions_avg[subject] = []\n",
    "    predictions_maj[subject] = []\n",
    "    predictions_avglab[subject] = []\n",
    "    true_labels[subject] = []\n",
    "    for l in range(21):\n",
    "        label = \"label%d\" % l\n",
    "        print(\"\\n\" + subject)\n",
    "        if label in writings_df_test_per_user.columns:\n",
    "            true_label = writings_df_test_per_user[writings_df_test_per_user['subject']==subject][label].values[0]\n",
    "        else:\n",
    "            true_label = 0\n",
    "        print(\"True label:\", true_label)\n",
    "        average_pred = knns[label].predict([np.mean(all_embeddings_dict_test[subject], axis=0)])[0]\n",
    "        print(\"Average pred:\", average_pred)\n",
    "        all_preds = knns[label].predict(all_embeddings_dict_test[subject])\n",
    "        majority_label = Counter(all_preds).most_common(1)[0][0]\n",
    "        print(\"Majority label:\", majority_label)\n",
    "        print(\"All preds:\", all_preds)\n",
    "        average_lab_pred = np.mean(all_preds)\n",
    "        true_labels[subject].append(label_to_str(true_label, l))\n",
    "\n",
    "        average_pred = label_to_str(average_pred, l)\n",
    "        average_lab_pred = label_to_str(average_lab_pred, l)\n",
    "        majority_label = label_to_str(majority_label, l)\n",
    "            \n",
    "        predictions_avg[subject].append(average_pred)\n",
    "        predictions_maj[subject].append(majority_label)\n",
    "        predictions_avglab[subject].append(average_lab_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using lexicon features\n",
    "from collections import Counter\n",
    "predictions = {}\n",
    "feature_col = 'allcats'\n",
    "\n",
    "def predict(writings_df_test_per_user, test_subjects):\n",
    "\n",
    "    def label_to_str(l, q=0):\n",
    "        if int(round(l)) < 0:\n",
    "            l = str(abs(round(l))) + 'b'\n",
    "        elif q in [15, 17] and l>0:\n",
    "            l = str(round(l)) + 'a'\n",
    "        else:\n",
    "            l = str(round(l))\n",
    "        return l\n",
    "\n",
    "    for subject in test_subjects:\n",
    "        predictions[subject] = []\n",
    "        true_labels[subject] = []\n",
    "        for l in range(21):\n",
    "            label = \"label%d\" % l\n",
    "            print(\"\\n\" + subject)\n",
    "            if label in writings_df_test_per_user.columns:\n",
    "                true_label = writings_df_test_per_user[writings_df_test_per_user['subject']==subject][label].values[0]\n",
    "            else:\n",
    "                true_label = 0\n",
    "            print(\"True label:\", true_label)\n",
    "\n",
    "            test_features = writings_df_test_per_user[writings_df_test_per_user['subject']==subject][feature_col\n",
    "                                                                                                    ].values.tolist()[0]\n",
    "    #         print(np.array(test_features).reshape(1,-1))\n",
    "            all_preds = knns[label].predict(np.array(test_features).reshape(1,-1))\n",
    "            print(\"All preds:\", all_preds)\n",
    "    #         true_labels[subject].append(label_to_str(true_label, l))\n",
    "            true_labels[subject].append(str(true_label))\n",
    "\n",
    "            pred = label_to_str(all_preds[0], l)\n",
    "\n",
    "\n",
    "            predictions[subject].append(pred)\n",
    "    #         predictions[subject].append('0')\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(writings_df_test_per_user, test_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(predictions_avg, open('t2_predictions_k5_alltexts_userembeddings101_avg_dev.pkl', 'wb+'))\n",
    "# pickle.dump(predictions_maj, open('t2_predictions_k5_alltexts_userembeddings101_maj_dev.pkl', 'wb+'))\n",
    "# pickle.dump(predictions_avglab, open('t2_predictions_k5_alltexts_userembeddings101_avglab_dev.pkl', 'wb+'))\n",
    "# pickle.dump(true_labels, open('t2_true_labels_dev.pkl', 'wb+'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evolution_series(df, emotion, subject, rolling_window, date_field='writing_days'):\n",
    "\n",
    "#     df[df[date_field]>=writing_day_cutoff][\n",
    "#             ['text', 'label', 'pronouns', 'text_len', 'subject', 'date', 'date_day', 'writing_days', 'negemo', 'posemo'\n",
    "#             ] + emotions + list(categories)\n",
    "#     ].groupby(date_field).mean()[emotion].rolling(rolling_window).mean().plot(label=label)\n",
    "    \n",
    "#     return df[df['date_day']>=writing_day_cutoff][\n",
    "    return df[df['subject']==subject][\n",
    "            ['text', 'text_len', 'subject', 'date', 'date_day', 'writing_days', 'writing_days_reverse', \n",
    "            'depression_mention', 'diagnosis'] + emotions + [c for c in set(categories) if c in df.columns \n",
    "                                                             and c not in emotions]\n",
    "    ].groupby(date_field).mean()[emotion].rolling(rolling_window).mean()\n",
    "#                                 ].apply(lambda c: np.log(c) if c>0 else 0\n",
    "#      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "def corr_users(writings_df, subject1, subject2, emotion, window=1):\n",
    "    hist_len1 = len(writings_df[writings_df['subject']==subject1].groupby('writing_days'))\n",
    "    hist_len2 = len(writings_df[writings_df['subject']==subject2].groupby('writing_days'))\n",
    "    \n",
    "    if hist_len1 < hist_len2:\n",
    "        series1 = get_evolution_series(writings_df, emotion, subject1, window).fillna(0).values\n",
    "        series2 = get_evolution_series(writings_df, emotion, subject2, window).fillna(0).values\n",
    "    else:\n",
    "        series2 = get_evolution_series(writings_df, emotion, subject1, window).fillna(0).values\n",
    "        series1 = get_evolution_series(writings_df, emotion, subject2, window).fillna(0).values\n",
    "    CORR_VALS = np.array(series1)\n",
    "    def get_correlation(vals):\n",
    "        return pearsonr(vals, CORR_VALS)[0]\n",
    "    df = pd.DataFrame(dict(x=series2))\n",
    "    correlations = df.rolling(window=len(CORR_VALS)).apply(get_correlation)\n",
    "#     return pearsonr(series1, series2)    \n",
    "    return correlations.max()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_users(writings_df, subject1, subject2, emotions):\n",
    "    return [corr_users(writings_df, subject1, subject2, e) for e in emotions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_users(writings_df_all, 'subject3993', 'subject5791' , ['i', 'we', 'ipron'] + emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_users(writings_df_all, 'subject1426', 'subject1426', ['i', 'we', 'ipron'] + emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writings_df_all = pd.concat([writings_df, writings_df_test])\n",
    "similarity_matrix_vectors = {}\n",
    "for subject1 in set(writings_df_all.subject):\n",
    "    similarity_matrix_vectors[subject1] = {}\n",
    "    for subject2 in set(writings_df_all.subject):\n",
    "        if subject1 == subject2:\n",
    "            continue\n",
    "        print(subject1, subject2)\n",
    "        sim = sim_users(writings_df_all, subject1, subject2, list(set(categories)) + emotions)\n",
    "        print(sim)\n",
    "        similarity_matrix_vectors[subject1][subject2] = sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(set(categories)) + emotions\n",
    "pickle.dump(features, open('correlations_time_similarity_vectors_features.pkl', 'wb+'))\n",
    "# pickle.dump(similarity_matrix_vectors, open('correlations_time_similarity_matrix_vectors_all_T3trainvalid.pkl', 'wb+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writings_df_all_per_user = pd.concat([writings_df_per_user, writings_df_test_per_user])\n",
    "writings_df_all_per_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix_vectors2 = {}\n",
    "for s1 in similarity_matrix_vectors:\n",
    "    print(s1)\n",
    "    similarity_matrix_vectors2[s1] = {}\n",
    "    \n",
    "    for s2 in similarity_matrix_vectors[s1]:\n",
    "        diffs = []\n",
    "        \n",
    "        for i, c in enumerate(similarity_matrix_vectors[s1][s2]):\n",
    "            f = features[i]\n",
    "            fval1 = writings_df_all_per_user[writings_df_all_per_user['subject']==s1][f].values[0]\n",
    "            fval2 = writings_df_all_per_user[writings_df_all_per_user['subject']==s2][f].values[0]\n",
    "            d = abs(fval1-fval2)\n",
    "            diffs.append(d)\n",
    "        similarity_matrix_vectors2[s1][s2] = (similarity_matrix_vectors[s1][s2],diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[features[i] for i in [23,28,14]]\n",
    "features.index('ipron')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_str(l, q=0):\n",
    "    if np.isnan(l):\n",
    "        return '0'\n",
    "    if int(round(l)) < 0:\n",
    "        l = str(abs(round(l))) + 'b'\n",
    "    elif q in [15, 17] and l>0:\n",
    "        l = str(round(l)) + 'a'\n",
    "    else:\n",
    "        l = str(round(l))\n",
    "    return l\n",
    "\n",
    "\n",
    "results = {'DCHR': [], 'ACR': [], 'ADODL': [], 'AHR': []}\n",
    "k = 15\n",
    "sim_thresh=1\n",
    "features_index = range(len(set(categories))+len(emotions)) # we consider just these features\n",
    "prons_index = [23,43,0,9]\n",
    "oth=[28,14]\n",
    "emotions_index = list(range(64,74))\n",
    "features_index =  prons_index + oth + emotions_index \n",
    "# \n",
    "import random\n",
    "# test_subjects_sets = [random.sample(set(writings_df_all.subject), 45) for r in range(10)]\n",
    "test_subjects_sets = [set(writings_df_test.subject)]\n",
    "\n",
    "\n",
    "for test_subjects in test_subjects_sets:\n",
    "    predictions = {}\n",
    "    true_labels = {}\n",
    "    print(test_subjects)\n",
    "    for subject in test_subjects:\n",
    "        predictions[subject] = []\n",
    "        true_labels[subject] = []\n",
    "        for l in range(21):\n",
    "            label = \"label%d\" % l\n",
    "            print(\"\\n\" + subject)\n",
    "            if label in writings_df_all_per_user.columns:\n",
    "                true_label = writings_df_all_per_user[writings_df_all_per_user['subject']==subject][label].values[0]\n",
    "            else:\n",
    "                true_label = 0\n",
    "            print(\"True label:\", true_label)\n",
    "\n",
    "            # t[1] is the correlation, and t[2] is the difference in prevalence\n",
    "            best_subjects = sorted([(s,np.array(c[0])[features_index],np.array(c[1])[features_index]) for s,c in similarity_matrix_vectors2[subject].items()\n",
    "                                    if s not in test_subjects], \n",
    "#                                    key = lambda t: np.nanmean(t[1]-t[2]), reverse=True)\n",
    "                                   key = lambda t: np.nanmean(-t[2]), reverse=True)\n",
    "\n",
    "            closest_subjects = [t[0] for t in best_subjects[:k]]\n",
    "            closest_correlations = [np.nanmean(t[1] + (1-t[2])) \n",
    "#             closest_correlations = [np.nanmean(1-t[2]) \n",
    "#             closest_correlations = [np.nanmean(t[1]) \n",
    "                                    for t in best_subjects[:k]]\n",
    "            for i,c in enumerate(closest_correlations):\n",
    "                cutoff=i\n",
    "                if c<sim_thresh:\n",
    "                    break\n",
    "            if cutoff==0:\n",
    "                cutoff=1\n",
    "            print('closest', cutoff, closest_subjects[:cutoff], closest_correlations[:cutoff], \n",
    "                  writings_df_all_per_user[writings_df_all_per_user['subject'\n",
    "                            ].isin(closest_subjects[:cutoff])]['label%d'%l].values[:cutoff])\n",
    "            if not closest_subject:\n",
    "                all_preds = 0\n",
    "            else:\n",
    "                all_preds = np.average(abs(writings_df_all_per_user[writings_df_all_per_user['subject'].isin(\n",
    "                    closest_subjects[:cutoff])]['label%d'%l].values[:cutoff]),\n",
    "                                      weights=closest_correlations[:cutoff])\n",
    "            print(\"All preds:\", all_preds)\n",
    "            true_labels[subject].append(label_to_str(true_label, l))\n",
    "    #         true_labels[subject].append(str(true_label))\n",
    "\n",
    "            pred = label_to_str(all_preds, l)\n",
    "\n",
    "\n",
    "            predictions[subject].append(pred)\n",
    "    metrics = EriskScoresT3()\n",
    "    res = metrics.compute(\n",
    "    predictions = predictions.values(),\n",
    "    references = true_labels.values())\n",
    "    for m in res.keys():\n",
    "        results[m].append(res[m])\n",
    "\n",
    "print(results)\n",
    "print({k:np.average(results[k]) for k in results})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_kernel(s1, s2):\n",
    "    \"\"\"\n",
    "    We create a custom kernel:\n",
    "\n",
    "                 (2  0)\n",
    "    k(X, Y) = X  (    ) Y.T\n",
    "                 (0  1)\n",
    "    \"\"\"\n",
    "    if s1[0]==s2[0]:\n",
    "        return 0.5\n",
    "    d = np.array(similarity_matrix_vectors2[s1[0]][s2[0]][0])[features_index]\n",
    "    c = np.array(similarity_matrix_vectors2[s1[0]][s2[0]][1])[features_index]\n",
    "    return 1-d+c/2\n",
    "    \n",
    "from sklearn import svm\n",
    "l = 0\n",
    "# we create an instance of SVM and fit out data.\n",
    "clf = svm.SVC(kernel=my_kernel)\n",
    "train_subjects = [[s] for s in set(writings_df_all.subject) if s not in test_subjects]\n",
    "train_labels = [writings_df_all[writings_df_all['subject']==s[0]]['label%d'%l].values[0] for s in train_subjects]\n",
    "# clf.fit(train_subjects, train_labels)\n",
    "# my_kernel(['subject4779'],['subject2903'])\n",
    "# len(train_subjects)\n",
    "print(train_subjects[0], train_subjects[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prons_index = [23,43,0,9]\n",
    "\n",
    "features_index =  prons_index + oth + emotions_index \n",
    "[features[i] for i in features_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets    \n",
    "import numpy as np\n",
    "\n",
    "_DESCRIPTION_T3 = \"\"\"\\\n",
    "Metrics for measuring the performance of prediction models for eRisk 2021 Task 2 and 3.\n",
    "Include decision-based performance metrics: decision-based F1, lantency-weighted F1, ERDE score.\n",
    "\"\"\"\n",
    "\n",
    "_CITATION = \"\"\n",
    "\n",
    "_KWARGS_DESCRIPTION_T3 = \"\"\"\n",
    "Calculates how good are predictions of answers given to the depression assessment questionnaire,\n",
    "given some references, using different metrics.\n",
    "Each prediction and reference is expected to be of length 21, corresponding to the 21 questions.\n",
    "    predictions: list of predictions to score, one for each user. Each prediction\n",
    "        should be a list of 21 strings in {0, 1, 2, 3, 1a, 1b, 2a, 2b, 3a, 3b}.\n",
    "    references: list of references for each prediction, one for each user. Each\n",
    "        reference should be a list of 21 strings in {0, 1, 2, 3, 1a, 1b, 2a, 2b, 3a, 3b}.\n",
    "\n",
    "Returns:\n",
    "    AHR: average hit rate - ratio of cases where the automatic questionnaire\n",
    "        has exactly the same answer as the real questionnaire, averaged over users.\n",
    "    ACR: average closeness rate - the difference between the real and the automated answer,\n",
    "        (taking into account answers are on an ordinal scale), relative to the total number\n",
    "        of possible answers for the question, averaged over users.\n",
    "    ADODL: average difference between overall depression levels - difference in overall\n",
    "        depression levels, computed as the sum of the answers given for all questions,\n",
    "        (depression levels are integers between 0 and 63).\n",
    "        averaged over users.\n",
    "    DCHR: fraction of cases where the automated questionnaire led to a depression category\n",
    "        that is equivalent to the depression category obtained from the real questionnaire\n",
    "        (among 4 possible categories: minimal/mild/moderate/severe depression). \n",
    "    \n",
    "    \n",
    "Examples:\n",
    "    >>> t3_metric = EriskScoresT3()\n",
    "    >>> results = t3_metric.compute(\n",
    "        predictions = [['0', '1', '2', '3', '1a', '1b', '2a', '2b', '3', '2', \n",
    "                    '0', '1', '2', '3', '1a', '1b', '2a', '2b', '3', '2', '3b'],\n",
    "                  ['0', '1', '2', '3', '1a', '1b', '2a', '2b', '3', '2', \n",
    "                    '1', '2', '3', '3a', '1', '1', '2', '2a', '3b', '2', '3b']],\n",
    "        references = [['0', '0', '0', '3', '1', '1', '2a', '2b', '3', '2', \n",
    "                    '1', '1', '1', '3', '1a', '1b', '2', '2', '3', '2', '3b'],\n",
    "                 ['0', '1', '2', '3', '1', '1', '2', '2', '3', '2', \n",
    "                    '1', '2', '3', '3', '1', '1', '2', '2', '3', '2', '3']]\n",
    "        )\n",
    "    >>> print(results)\n",
    "    >>> {'AHR': 0.6190476190476191,\n",
    "         'ACR': 0.9603174603174602,\n",
    "         'ADODL': 0.9761904761904762,\n",
    "         'DCHR': 1.0}\n",
    "   \n",
    "\"\"\"\n",
    "\n",
    "def _depression_category(score):\n",
    "    if score >= 0 and score <=9:\n",
    "        return 'minimal'\n",
    "    if score >= 10 and score <= 18:\n",
    "        return 'mild'\n",
    "    if score >= 19 and score <= 29:\n",
    "        return 'moderate'\n",
    "    if score >= 30 and score <= 63:\n",
    "        return 'severe'\n",
    "    raise ValueError(\"Invalid score for depression questionnaire: %d\" % score)\n",
    "\n",
    "def _score(reference):\n",
    "    total_score = 0\n",
    "    for answer in reference:\n",
    "        # Consider only first letter and convert to int\n",
    "        answer_int = int(answer[0])\n",
    "        total_score += answer_int\n",
    "    return total_score\n",
    "\n",
    "\n",
    "\n",
    "def _dl(l1, l2, max_dl=63):\n",
    "    ad = abs(l1 - l2)\n",
    "    return (max_dl - ad)/max_dl\n",
    "    \n",
    "\n",
    "class EriskScoresT3(datasets.Metric):\n",
    "    def _info(self):\n",
    "        return datasets.MetricInfo(\n",
    "            description=_DESCRIPTION_T3,\n",
    "            citation=_CITATION,\n",
    "            inputs_description=_KWARGS_DESCRIPTION_T3,\n",
    "            features=datasets.Features({\n",
    "                'predictions': datasets.Sequence(datasets.Value('string')),\n",
    "                'references':datasets.Sequence(datasets.Value('string'))\n",
    "            }),\n",
    "            codebase_urls=[],\n",
    "            reference_urls=[],\n",
    "        )\n",
    "    \n",
    "    def _ahr(self, predictions, references):\n",
    "        hr_per_user = []\n",
    "        nr_users = len(predictions)\n",
    "        for u in range(nr_users):\n",
    "            prediction = predictions[u]\n",
    "            reference = references[u]\n",
    "            hr = sum(i == j for i, j in zip(prediction, reference)) / len(prediction)\n",
    "            hr_per_user.append(hr)\n",
    "        return sum(hr_per_user)/nr_users\n",
    "    \n",
    "    def _cr(self, prediction, reference, nr_answers=4):\n",
    "        closeness_scores = []\n",
    "        nr_questions = len(prediction)\n",
    "        for q in range(nr_questions):\n",
    "            # Consider only first letter and convert to int\n",
    "            predictionq_int = int(prediction[q][0])\n",
    "            referenceq_int = int(reference[q][0])\n",
    "            mad = nr_answers - 1\n",
    "            ad = abs(predictionq_int - referenceq_int)\n",
    "            closeness = (mad - ad) / mad\n",
    "            closeness_scores.append(closeness)\n",
    "        return sum(closeness_scores)/nr_questions\n",
    "            \n",
    "    def _acr(self, predictions, references):\n",
    "        nr_users = len(predictions)\n",
    "        cr_per_user = [self._cr(predictions[u], references[u]) \n",
    "                       for u in range(nr_users)]\n",
    "        return sum(cr_per_user)/nr_users\n",
    "    \n",
    "\n",
    "    def _adodl(self, predictions, references):\n",
    "        nr_users = len(predictions)\n",
    "        scores_predictions = [_score(p) for p in predictions]\n",
    "        scores_references = [_score(r) for r in references]\n",
    "        level_differences = [_dl(scores_predictions[u],scores_references[u])\n",
    "                             for u in range(nr_users)]\n",
    "        return sum(level_differences)/nr_users\n",
    "\n",
    "        \n",
    "    def _dchr(self, predictions, references):\n",
    "        nr_users = len(predictions)\n",
    "        dc_predictions = [_depression_category(_score(predictions[u])) for u in range(nr_users)]\n",
    "        dc_references = [_depression_category(_score(references[u])) for u in range(nr_users)]\n",
    "        hr = sum(i == j for i, j in zip(dc_predictions, dc_references)) / nr_users\n",
    "        return hr\n",
    "    \n",
    "    def _compute(self, predictions, references):\n",
    "        return {\n",
    "            'AHR': self._ahr(predictions, references),\n",
    "            'ACR': self._acr(predictions, references),\n",
    "            'ADODL': self._adodl(predictions, references),\n",
    "            'DCHR': self._dchr(predictions, references),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# predictions_avg = pickle.load(open('t2_predictions_avg_dev.pkl', 'rb'))\n",
    "# predictions_maj = pickle.load(open('t2_predictions_maj_dev.pkl', 'rb'))\n",
    "# true_labels = pickle.load(open('t2_true_labels_dev.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = EriskScoresT3()\n",
    "metrics.compute(\n",
    "predictions = predictions.values(),\n",
    "references = true_labels.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = EriskScoresT3()\n",
    "metrics.compute(\n",
    "predictions = predictions_avglab.values(),\n",
    "references = true_labels.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = EriskScoresT3()\n",
    "metrics.compute(\n",
    "predictions = predictions_maj.values(),\n",
    "references = true_labels.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = EriskScoresT3()\n",
    "metrics.compute(\n",
    "predictions = predictions_avg.values(),\n",
    "references = true_labels.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u, preds in predictions.items():\n",
    "    print(u, \" \".join(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(writings_df_test.subject.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tfconda)",
   "language": "python",
   "name": "tfconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
